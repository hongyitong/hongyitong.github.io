<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>论孤独</title>
      <link href="/2025/12/01/%E8%AE%BA%E5%AD%A4%E7%8B%AC/"/>
      <url>/2025/12/01/%E8%AE%BA%E5%AD%A4%E7%8B%AC/</url>
      
        <content type="html"><![CDATA[<p>一直以来，都坚信读书和跑步的本质是孤独的，自己也常以“享受孤独”而自居。</p><p>随着阅读的积累，心得的增多，思考的拓展，阅历的增长，这个世界的面纱一层层的剥落，露出了其可怕的真相和悲剧的色彩（《悲剧的诞生》），孤独感也愈发强烈。</p><p>尼采说过，除了神灵、野兽和哲学家，人都忍受不了孤独（《格局》）。显然，我不属于前两者，离哲学家又甚远，离普通人又太近，因此，按尼采的说法，也不清楚自己究竟能忍受（享受）多久。</p><p>说到哲学家，尼采说过，需要兼备天才和疯子的要素，而我显然都没有。不过，尼采在《悲剧的诞生》中也提到，“就算人生是幕悲剧，我们也要有声有色地演这幕悲剧，不要失掉了悲剧的壮丽和快慰。”</p><p><strong>以强大的生命力对抗人生悲剧，赋予人生以意义（《悲剧的诞生》）。</strong>这或许就是我们作为“第四种人”的人生路。</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 尼采 </tag>
            
            <tag> 孤独 </tag>
            
            <tag> 哲学家 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>关于写读书心得</title>
      <link href="/2025/11/30/%E5%85%B3%E4%BA%8E%E5%86%99%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2025/11/30/%E5%85%B3%E4%BA%8E%E5%86%99%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>如今，网络上各种读书心得已是汗牛充栋，有时候我也会犹豫、动摇，在这样的时代，坚持写读书心得还有意义吗？AI时代来临之后，这种举棋不定的想法尤为强烈，特别对我这种非文字工作者的理科生来说，写作本来就是一件苦差事。</p><p>然而，就像鲁迅说过，“一部红楼梦，经学家看见《易》，道学家看见淫，才子看见缠绵，革命家看见排满，流言家看见宫闱秘事。”正是这种关于阅读的个体性、独特性，给了我持笔不停的力量。</p><p>我，一个平凡得不能再平凡的人，没有各种“家”的深刻视野，却能在书里读到自己生活与影子。这份体验独一无二，是以斗胆记之，谓之以“心得”。这些心得定格了当时当刻的内心，捕捉了那时那地的自我。它有别于别的读者，更有别于AI的概率。</p><p>此外，还有另外一股力量在支撑着我，那就是，这片小小天地其实是自己内心的港湾。</p><p>在这纷纷扰扰的世界里，这片小天地，安静而平和，仿佛“苟全性命于乱世，不求闻达于诸侯”的小室。可以在这里轻声细语的诉说，可以在这里敞开心扉的记录，可以在这里无所畏惧的幻想，这里就是自己知音，它懂得何时是高山、何时是流水。</p><p>我常常在想，如果有一天，不必为五斗米而折腰，选择自己喜欢的事情，那我的选择会是什么？</p><p>我想，那就是读很多很多的书，写很多很多的读书心得，这就是我所爱，这也是我和这个世界最好的对话方式，也是我这个短暂的生命能留给这个世界最好的礼物，也是我作为一个生命过客的最大意义吧。</p><p>也希望真的能有这么一天。</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 读书心得 </tag>
            
            <tag> 写作 </tag>
            
            <tag> 读后感 </tag>
            
            <tag> 坚持 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《鲁迅杂文选》读书心得</title>
      <link href="/2025/11/29/%E3%80%8A%E9%B2%81%E8%BF%85%E6%9D%82%E6%96%87%E9%80%89%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2025/11/29/%E3%80%8A%E9%B2%81%E8%BF%85%E6%9D%82%E6%96%87%E9%80%89%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>为了平复《一句顶一万句》和《挪威的森林》所带来的情绪波动，需要一些深刻的文字来中和一下。中间读了《长安的荔枝》，不过似乎力道不够，于是，想到了鲁迅，想到了鲁迅的杂文。</p><p>当年互联网刚兴起，网上满是鲁迅如是说的语录，真真假假。就像后来流行的马云如是说、马斯克如是说，自媒体总是能找到可以蹭的热点。就像鲁迅说的，“专制使人们变成冷嘲。我们天下太平，连冷嘲也没有。”于是，后来连鲁迅式的冷嘲都没有了，只剩鸡汤。不过也好，就像鲁迅还说过一样，“做梦的人是幸福的；倘没有看出可走的路，最要紧的是不要去惊醒他。”在心灵鸡汤的滋养下，这何尝不是一样幸福，虽然显得有些另类。</p><span id="more"></span><p>鲁迅的杂文，就像一把匕首，解剖着这个人世间，深深的扎入这个世界的心脏。虽然因此让人们看到了这个血淋淋的鲜血，但鲁迅还是提醒我们，“真的猛士，敢于直面惨淡的人生，敢于正视淋漓的鲜血。”鲁迅还提醒我们，“苟活者在淡红的血色中，会依稀看见微茫的希望；真的猛士，将更奋然前行。”这是何等的乐观主义精神。不像小说 《追风筝的人》所说“倘若周围一团漆黑，那就只能静等眼睛习惯黑暗。”鲁迅他总是希望能在这片黑暗中保留着一丝光，哪怕是用无声的呐喊。</p><p>就像《红楼梦》里焦大酒后骂尽大观园，说出，“只有两只石狮子是干净的”，结果被塞了一嘴马粪。鲁迅说他向来不惮以最坏的恶意推测中国人，于是，结局可想而知。鲁迅所处时代倒是文明了，自然也不会塞马粪了，但千夫指是避免不了的，鲁迅能做的也自然而然的是横眉冷对了，那横眉也就是他手中那把匕首----杂文。</p><p>想当年，学生时代，闻鲁迅而色变。那些年的语文考试，鲁迅文章所带来的暴击，至今，仍心有戚戚焉。但如今看完鲁迅的杂文集，却深深惊叹于其对人性和社会洞察之深刻。</p><blockquote><p>楼下一个男人病得要死，那间壁的一家唱着留声机；对面是弄孩子。楼上有两人狂笑；还有打牌声。河中的船上有女人哭着她死去的母亲。<br /><strong>人类的悲欢并不相通，我只觉得他们吵闹。</strong></p></blockquote><p>这句话，即使读过多次，但每次重读都为这入门三分的描述所震惊，比起网上那些所谓的同理心、感同身受、换位思考的鸡汤，其对人性的理解更为本质。虽然，心灵鸡汤说的这些，更符合我们的道德标准和价值观，但人类的悲欢并不相通却是更为根本的存在。</p><blockquote><p>中国人的性情是总喜欢调和，折中的。譬如你说，这屋子太暗，须在这里开一个窗，大家一定不允许的。但如果你主张拆掉屋顶，他们就会来调和，愿意开窗了。<strong>没有更激烈的主张，他们总连平和的改革也不肯行。</strong></p></blockquote><p>这就像特朗普的极限施压的谈判技巧一样，在冲突中，人是喜欢妥协和合作的一种动物，和所谓的“取乎其上，得乎其中；取乎其中，得乎其下；取乎其下，则无所得矣”一样的逻辑，所以这也是实在人常吃亏的道理。</p><blockquote><p><strong>曾经阔气的要复古，正在阔气的要保持现状，未曾阔气的要革新。</strong><br />大抵如是。大抵！<br />他们之所谓复古，是回到他们所记得的若干年前，并非虞夏商周。</p></blockquote><p>人是利益驱动的动物，所谓的屁股决定脑袋，每个人说出口的话，都是基于自己的利益在呼喊，即使表面讲得冠冕堂皇。所以兼听则明，偏信则暗。</p><blockquote><p>中国现在的社会情状，止有实地的革命战争，<strong>一首诗吓不走孙传芳，一炮就把孙传芳轰走了。</strong></p></blockquote><p>就像普京说的，真理在炮弹的射程范围之内，决定结果的永远是实力而不是道理。</p><blockquote><p>《颂》诗早已拍马，《春秋》已经隐瞒，战国时谈士蜂起，不是以危言耸听，就是以美词动听，于是夸大，装腔，撒谎，层出不穷。<strong>现在的文人虽然改著了洋服，而骨髓里却还埋着老祖宗</strong>，所以必须取消或折扣，这才显出几分真实。</p></blockquote><p>人性的进步是大大落后于科学技术的进步的。几千年前中国人的劣根性，如今还是劣根性。所以有说，以史为鉴，历史，表面记载的是故事，背后却是人性。</p><p>回顾这些，想起了卡耐基《人性的弱点》，其针对人性讲得更通俗易懂。而鲁迅讲人性，却显得更冷峻而尖锐，也显得更为形象，也更令人警醒。</p><p>读完《鲁迅杂文选》，整个人又显得深刻过了头，不禁怀念起前段潜心钻研AI书籍的日子，单纯而理性。</p><h3 id="文章精彩语句摘录">文章精彩语句摘录</h3><ul><li>专制使人们变成冷嘲。我们天下太平，连冷嘲也没有。我想：暴君的专制使人们变成冷嘲，愚民的专制使人们变成死相。 （<em>出处：热风·随感录三十九</em>）</li><li>勇者愤怒，抽刃向更强者；怯者愤怒，却抽刃向更弱者。 （<em>出处：杂忆</em>）</li><li>我每看运动会时，常常这样想：优胜者固然可敬，但那虽然落后而仍跑至终点不止的竞技者，和见了这样竞技者而肃然起敬的看客，乃正是中国将来的脊梁。 （<em>出处：热风·随感录二十五</em>）</li><li>于已成之局那么委曲求全，于初兴之事就这么求全责备？ （<em>出处：华盖集·这个与那个</em>）</li><li>写出来的倘不是不朽之作，就不要写。 （ <em>出处：致李霁野</em>）</li><li>孩子初学步的第一步，在成人看来，的确是幼稚，危险，不成样子，或者简直是可笑的。但无论怎样的愚妇人，却总以恳切的希望的心，看他跨出这第一步去，决不会因为他的走法幼稚，怕要阻碍阔人的路线而“逼死”他；也决不至于将他禁在床上，使他躺着研究到能够飞跑时再下地。因为她知道：假如这么办，即使长到一百岁也还是不会走路的。 （ <em>出处：华盖集续编·记“发隐”</em>）</li><li><strong>人生最痛苦的是梦醒了无路可以走。做梦的人是幸福的；倘没有看出可走的路，最要紧的是不要去惊醒他。</strong> （<em>出处：娜拉走后怎样</em>）</li><li>普度一切人类和救活一个人，大小实在是相去太远了，然而倘叫我挑选，我就立刻到菩提树下去坐着，因为免得脱下唯一的棉袄来冻杀自己。 （<em>出处：集外集拾遗·马上日记</em>）</li><li><strong>悲剧将人生有价值的东西毁灭给人看，喜剧将那无价值的撕破给人看。</strong> （<em>出处：再论雷峰塔的倒掉</em>）</li><li>“犯而不校(jiao)“是恕道，“以眼还眼以牙还牙”是直道。中国最多的却是枉道：不打落水狗，反被狗咬了。但是，这其实是老实人自己讨苦吃。 （<em>出处：华盖集·记“发隐”</em>）</li><li>坐听着远远近近的爆竹声，知道灶君先生们都在陆续上天，向玉皇大帝讲他的东家的坏话去了，但是他大概终于没有讲，否则，中国人一定比现在要更倒霉。 （ <em>出处：南腔北调集·灶</em>）</li><li><strong>诚然，自以为看穿了的话，有时也的确反而不免于浅薄。</strong> （<em>出处：三闲集·无声的中国</em>）</li><li>无刺的蔷薇是没有的。然而没有蔷薇的刺却很多。 （<em>出处：热风·随感录四十八</em>）</li><li><strong>当为历年潜心研究与冷眼观察之结果</strong>，大足以诏示国人，且为知识阶级所注意也。 （ <em>出处：伪自由书·观斗</em>）</li><li>志摩先生曰：“鲁迅先生的作品，说来大不敬得很，我拜读过很少，就只《呐喊》集里两三篇小说，以及新近因为有人尊他是中国的尼采他的《热风》集里的几页。他平常零星的东西，我即使看也等于白看，没有看进去或是没有看懂。”（《晨副》） （<em>出处：集外集拾遗·鲁迅先生之作品</em>）</li><li><strong>被毁则报，被誉则默，正是人之常情。</strong> （<em>出处：华盖集·这个与那个</em>）</li><li>他们都知道，有些东西，为显得他伤害你的时候的公正，在不相关的地方就称赞你几句，似乎有赏有罚，使别人看去，很像无私......。 （<em>出处：三闲集·现今的新文学的概观</em>）</li><li>我也早觉得有写一点东西的必要了，这虽然于死者毫不相干，但在生者，却大抵只能如此而已。 （<em>出处：纪念刘和珍君</em>）</li><li><strong>长歌当哭，是必须在痛定之后的。</strong> （<em>出处：记念刘和珍君</em>）</li><li><strong>真的猛士，敢于直面惨淡的人生，敢于正视淋漓的鲜血。这是怎样的哀痛者和幸福者。</strong> （<em>出处：记念刘和珍君</em>）</li><li>但我对于这些传说，竟至于颇为怀疑。<strong>我向来不惮以最坏的恶意，来揣测中国人的，然而我还不料，也不信竟会凶残到这地步。</strong> （<em>出处：记念刘和珍君</em>）</li><li>但段政府就有令，说她们是“暴徒”！<br />但接着就有流言，说她们是受人利用的。<br />当惨象，已使我目不忍视了；流言，尤使我耳不忍闻。我还有什么话可说呢？<br />（<em>出处：记念刘和珍君</em>）</li><li>中国军人的屠戮妇婴的伟绩，八国联军的惩创学生的武功，不幸全被这几缕血痕抹杀了。<br />但是中外的杀人者却居然昂起头来，不知道个个脸上都有着血污...... （<em>出处：记念刘和珍君</em>）</li><li>时间永是流驶，街市依旧太平，有限的几个生命，在中国是不算什么的，至多，不过供无恶意的闲人以饭后的谈资，或者给有恶意的闲人作“流言”的种子。至于此外的深的意义，我总觉得很寥寥，因为这实在不过是徒手的请愿。人类的血战前行的历史，正如煤的形成，当时用大量的木材，结果却只是一小块，但请愿是不在其中的，更何况是徒手。<br />然而既然有了血痕了，当然不觉要扩大。至少，也当浸渍了亲族、师友、爱人的心，纵使时光流驶，洗成绯红，也会在微漠的悲哀中永存微笑的和蔼的旧影。<strong>陶潜说过，“亲戚或余悲，他人亦已歌，死去何所道，托体同山阿。”</strong>倘能如此，这也就够了。<br />我已经说过：我向来是不惮以最坏的恶意来推测中国人的。但这回却有几点出于我的意外。一是当局者竟会这样地凶残，一是流言家竟至如此之下劣，一是中国的女性临难竟能如是之从容。 （<em>出处：记念刘和珍君</em>）</li><li><strong>苟活者在淡红的血色中，会依稀看见微茫的希望；真的猛士，将更奋然前行。</strong> （<em>出处：记念刘和珍君</em>）</li><li>我想：文学文学，是最不中用的，没有力量的人讲的；有实力的人并不开口，就杀人。 （<em>出处：南腔北调集·文学与武力</em>）</li><li>有人说：“文学是穷苦的时候做的”，其实未必，穷苦的时候必定没有文学作品的；我在北京时，一穷，就到处借钱，不写一个字，到薪俸发放时，才坐下来做文章。忙的时候也必定没有文学作品，挑担的人必要把担子放下，才能做文章；拉车的人也必要把车子放下，才能做文章。 （<em>出处：南腔北调集·文学与武力</em>）</li><li>中国现在的社会情状，止有实地的革命战争，<strong>一首诗吓不走孙传芳，一炮就把孙传芳轰走了。</strong>自然也有人以为文学于革命是有伟力的，但我个人总觉得怀疑，文学总是一种余裕的产物，可以表示一民族的文化，倒是真的。 （<em>出处：南腔北调集·文学与武力</em>）</li><li>人大概是不满于自己目前所做的事的，我一向只会做几篇文章，自己也做得厌了，而捏枪的诸君，却又要听讲文学。我呢，自然倒愿意听听大炮的声音，仿佛觉得大炮的声音或者比文学的声音要好听得多似的。 （<em>出处：南腔北调集·文学与武力</em>）</li><li>听说英国的培那特萧（Bernard Shaw），有过这样意思的话：世间最不行的是读书者。因为他只能看别人的思想艺术，不用自己。这也就是勖本华尔（Schopenhauer）之所谓<strong>脑子里给别人跑马</strong>。较好的是思索者。因为能用自己的生活力了，但还不免是空想，所以更好的是观察者，他用自己的眼睛去读世间这一部活书。 （<em>出处：三闲集·读书</em>）</li><li><strong>世间大抵只知道指挥刀所以指挥战士，而不想到也可以指挥文人。</strong> （<em>出处：南腔北调集·文学与武力</em>）</li><li><strong>曾经阔气的要复古，正在阔气的要保持现状，未曾阔气的要革新。</strong><br />大抵如是。大抵！<br />他们之所谓复古，是回到他们所记得的若干年前，并非虞夏商周。<br />（<em>出处：且介亭杂文末编·关于新文字</em>）</li><li>女人的天性中有母性，有女儿性；无妻性。<br />妻性是逼成的，只是母性和女儿性的混合。<br />（<em>出处：华盖集·孤独者</em>）</li><li>楼下一个男人病得要死，那间壁的一家唱着留声机；对面是弄孩子。楼上有两人狂笑；还有打牌声。河中的船上有女人哭着她死去的母亲。<br /><strong>人类的悲欢并不相通，我只觉得他们吵闹。</strong> （<em>出处：而已集·小杂感</em>）</li><li>人感到寂寞时，会创作；一感到干净时，即无创作，他已经一无所爱。<br />创作总根于爱。<br />杨朱无书。<br /><strong>创作虽说抒写自己的心，但总愿意有人看。</strong><br />创作是有社会性的。<br /><strong>但有时只要有一个人看便满足：好友，爱人。</strong><br />（<em>出处：而已集·小杂感</em>）</li><li>七子之中，特别的是孔融，他专喜和曹操捣乱。......比方操破袁氏兄弟，曹丕把袁熙的妻甄氏拿来，归了自己，孔融就写信给曹操，说当初武王伐纣，将妲己给了周公了。操问他的出典，他说，以今例古，大概那时也是这样的。又比方曹操要禁酒，说酒可以亡国，非禁不可，孔融又反对他，说也有以女人亡国的，何以不禁婚姻？<br />其实曹操也是喝酒的。我们看他的“何以解忧？惟有杜康”的诗句，就可以知道。为什么他的行为会和议论矛盾呢？<strong>此无他，因曹操是个办事人，所以不得不这样做；孔融是旁观的人，所以容易说些自由话。</strong>曹操见他屡屡反对自己，后来借故把他杀了。他杀孔融的罪状大概是不孝。<br />（<em>出处：且介亭杂文·七论</em>）</li><li>至于嵇康，一看他的《绝交书》，就知道他的态度很骄傲的；有一次，他在家打铁——他的性情是很喜欢打铁的——钟会(当时权臣)来看他了，他只打铁，不理钟会。钟会没有意味，只得走了。其时嵇康就问他：<strong>“何所闻而来，何所见而去？”</strong>钟会答道：<strong>“闻所闻而来，见所见而去。”</strong>这也是嵇康杀身的一条祸根。 （<em>出处：且介亭杂文·七论</em>）</li><li>“采菊东篱下，悠然见南山”。这样的自然状态，实在不易模仿。他穷到衣服也破烂不堪，而还在东篱下采菊，偶然抬起头来，悠然的见了南山，这是何等自然。现在有钱的人住在租界里，雇花匠种数十盆菊花，便做诗，叫作“秋日赏菊效陶彭泽体”，自以为合于渊明的高致，我觉得不大像。 （<em>出处：而已集·小杂感</em>）</li><li>但是，在中国，那时是确无写处的，<strong>禁锢得比罐头还严密</strong>。 （<em>出处：南腔北调集·文学与武力</em>）</li><li>中国人的性情是总喜欢调和，折中的。譬如你说，这屋子太暗，须在这里开一个窗，大家一定不允许的。但如果你主张拆掉屋顶，他们就会来调和，愿意开窗了。<strong>没有更激烈的主张，他们总连平和的改革也不肯行。</strong><br />那时白话文之得以通行，就因为有废掉中国字而用罗马字母的议论的缘故。<br />（<em>出处：无声的中国</em>）</li><li>这一年多，我不很向青年诸君说什么话了，因为革命以来，言论的路很窄小，不是过激，便是反动，于大家都无益处。 （<em>出处：南腔北调集·文学与武力</em>）</li><li>那题目，原是想在车上拟定的，但因为道路坏，汽车颠起来有尺多高，无从想起。我于是偶然感到，<strong>外来的东西，单取一件，是不行的，有汽车也须有好道路，一切事总免不掉环境的影响。</strong>文学——在中国的所谓新文学，所谓革命文学，也是如此。 （<em>出处：南腔北调集·文学与武力</em>）</li><li>希望革命的文人，革命一到，反而沉默下去的例子，在中国便曾有过的。即如清末的南社，便是鼓吹革命的文学团体，他们叹汉族的被压制，愤满人的凶横，渴望着“光复旧物”。但民国成立以后，倒寂然无声了。我想，这是因为他们的理想，是在革命以后，“重见汉官威仪”，峨冠博带。而事实并不这样，所以反而索然无味，不想执笔了。 （<em>出处：且介亭杂文末编·关于新文字</em>）</li><li>孔墨都不满于现状，要加以改革，但那第一步，是在说动人主，而那用以压服人主的家伙，则都是“天”。<br />孔子之徒为儒，墨子之徒为侠。“儒者，柔也”，当然不会危险的。惟侠老实，所以墨者的末流，至于以“死”为终极的目的。到后来，真老实的逐渐死完，止留下取巧的侠，汉的大侠，就已和公侯权贵相馈赠，以备危急时来作护符之用了。<br />司马迁说：“儒以文乱法，而侠以武犯禁”，“乱”之和“犯”，决不是“叛”，不过闹点小乱子而已，而况有权贵如“五侯”者在。<br />“侠”字渐消，强盗起了，但也是侠之流，他们的旗帜是“替天行道”。他们所反对的是奸臣，不是天子，他们所打劫的是平民，不是将相。李逵劫法场时，抡起板斧来排头砍去，而所砍的是看客。一部《水浒》，说得很分明：因为不反对天子，所以大军一到，便受招安，替国家打别的强盗——不“替天行道”的强盗去了。终于是奴才。 （<em>出处：且介亭杂文·七论</em>）</li><li>满洲入关，中国渐被压服了，连有“侠气”的人，也不敢再起盗心，不敢指斥奸臣，不敢直接为天子效力，于是跟一个好官员或钦差大臣，给他保镳，替他捕盗，一部《施公案》，也说得很分明，还有《彭公案》，《七侠五义》之流，至今没有穷尽。他们出身清白，连先前也并无坏处，虽在钦差之下，究居平民之上，对一方面固然必须听命，对别方面还是大可逞雄，安全之度增多了，奴性也跟着加足。<br />然而为盗要被官兵所打，捕盗也要被强盗所打，要十分安全的侠客，是觉得都不妥当的，于是有流氓。和尚喝酒他来打，男女通奸他来捉，私娼私贩他来凌辱，为的是维持风化；乡下人不懂租界章程他来欺侮，为的是看不起无知；剪发女人他来嘲骂，社会改革者他来憎恶，为的是宝爱秩序。但后面是传统的靠山，对手又都非浩荡的强敌，他就在其间横行过去。现在的小说，还没有写出这一种典型的书，惟《九尾龟》中的章秋谷，以为他给妓女吃苦，是因为她要敲人们竹杠，所以给以惩罚之类的叙述，约略近之。 （<em>出处：且介亭杂文·七论</em>）</li><li>这宗话，影子是有一点的。譬如罢，教育经费用光了，却还要开几个学堂，装装门面；全国的人们十之九不识字，然而总得请几位博士，使他对西洋人去讲中国的精神文明；至今还是随便拷问，随便杀头，一面却总支撑维持着几个洋式的“模范监狱”，给外国人看看。还有，离前敌很远的将军，他偏要大打电报，说要“为国前驱”。连体操班也不愿意上的学生少爷，他偏要穿上军装，说是“灭此朝食”。<br />不过，这些究竟还有一点影子；究竟还有几个学堂，几个博士，几个模范监狱，几个通电，几套军装。所以说是“说谎”是不对的。这就是我之所谓“做戏”。 （<em>出处：且介亭杂文·中国文坛上的鬼</em>）</li><li>枕戈待旦<br /></li><li>《颂》诗早已拍马，《春秋》已经隐瞒，战国时谈士蜂起，不是以危言耸听，就是以美词动听，于是夸大，装腔，撒谎，层出不穷。<strong>现在的文人虽然改著了洋服，而骨髓里却还埋着老祖宗</strong>，所以必须取消或折扣，这才显出几分真实。 （<em>出处：且介亭杂文末编·关于新文字</em>）</li><li>看《红楼梦》，觉得贾府上是言论颇不自由的地方。焦大以奴才的身分，仗着酒醉，从主子骂起，直到别的一切奴才，<strong>说只有两个石狮子干净</strong>。结果怎样呢？结果是主子深恶，奴才痛嫉，给他塞了一嘴马粪。<br />其实是，焦大的骂，并非要打倒贾府，倒是要贾府好，不过说主奴如此，贾府就要弄不下去罢了。然而得到的报酬是马粪。所以这焦大，实在是贾府的屈原，假使他能做文章，我想，恐怕也会有一篇《离骚》之类。 （<em>出处：三闲集·流氓的变迁</em>）</li><li>我早已想写一点文字，来记念几个青年的作家。这并非为了别的，只因为两年以来，悲愤总时时来袭击我的心，至今没有停止，我很想借此算是竦身一摇，将悲哀摆脱，给自己轻松一下，照直说，就是我倒要将他们忘却了。 （<em>出处：为了忘却的纪念</em>）</li><li><strong>前年的今日，我避在客栈里，他们却是走向刑场了；去年的今日，我在炮声中逃在英租界，他们则早已埋在不知那里的地下了；今年的今日，我才坐在旧寓里，人们都睡觉了，连我的女人和孩子。我又沉重的感到我失掉了很好的朋友，中国失掉了很好的青年，我在悲愤中沉静下去了，不料积习又从沉静中抬起头来，写下了以上那些字。</strong> （<em>出处：为了忘却的纪念</em>）</li><li>不是年青的为年老的写记念，而在这三十年中，却使我目睹许多青年的血，层层淤积起来，将我埋得不能呼吸，我只能用这样的笔墨，写几句文章，算是从泥土中挖一个小孔，自己延口残喘，这是怎样的世界呢。夜正长，路也正长，我不如忘却，不说的好罢。但我知道，即使不是我，将来总会有记起他们，再说他们的时候的。...... （<em>出处：为了忘却的纪念</em>）</li><li>忘记是谁说的了，总之是，要极省俭的画出一个人的特点，最好是画他的眼睛。我以为这话是极对的，倘若画了全副的头发，即使细得逼真，也毫无意思。<em>（出处：我怎么做起小说来）</em></li><li>中国的创作界固然幼稚，批评界更幼稚。<em>（出处：我怎么做起小说来）</em><br />PS：这句话，纯粹是想到中国足球。</li><li>他单责个人，正是最稳妥的办法，倘使兼责社会，可就得站出去战斗了。责人的“深于世故”而避开了“世”不谈，这是更“深于世故”的玩艺，倘若自己不觉得，那就更深更深了，离三昧境盖不远矣。<em>（出处：世故三昧）</em></li><li>据我所见，北人的优点是厚重，南人的优点是机灵。但厚重之弊也愚，机灵之弊也狡，所以某先生曾经指出缺点道：<strong>北方人是“饱食终日，无所用心”；南方人是“群居终日，言不及义”。</strong>就有闲阶级而言，我以为大体是的确的。<em>（出处：北人与南人）</em></li><li>高尔基很惊服巴尔札克小说里写对话的巧妙，以为并不描写人物的模样，却能使读者看了对话，便好像目睹了说话的那些人。<em>（出处：看书琐记）</em></li><li>现在只还有“书法拉丁化”的一条路。这和大众语文是分不开的。也还是从读书人首先试验起，先绍介过字母，拼法，然后写文章。开手是，像日本文那样，只留一点名词之类的汉字，而助词，感叹词，后来连形容词，动词也都用拉丁拼音写，那么，不但顺眼，对于了解也容易得远了。至于改作横行，那是当然的事。 这就是现在马上来实验，我以为也并不难。<br />不错，汉字是古代传下来的宝贝，但我们的祖先，比汉字还要古，所以我们更是古代传下来的宝贝。为汉字而牺牲我们，还是为我们而牺牲汉字呢？这是只要还没有丧心病狂的人，都能够马上回答的。<br /><em>（出处：汉字和拉丁化）</em><br />PS：鲁迅判断错误的地方，还好没有把汉字拉丁化。</li><li>不错，半农确是浅。但他的浅，却如一条清溪，澄澈见底，纵有多少沉渣和腐草，也不掩其大体的清。倘使装的是烂泥，一时就看不出它的深浅来了；如果是烂泥的深渊呢，那就更不如浅一点的好。<em>（出处：忆刘半农君）</em></li><li><strong>我们从古以来，就有埋头苦干的人，有拼命硬干的人，有为民请命的人，有舍身求法的人，……虽是等于为帝王将相作家谱的所谓“正史”，也往往掩不住他们的光耀，这就是中国的脊梁。</strong> <em>（出处：中国人失掉自信力了吗）</em></li><li>她们的死，不过像在无边的人海里添了几粒盐，虽然使扯淡的嘴巴们觉得有些味道，但不久也还是淡，淡，淡。<em>（出处：论“人言可畏”）</em></li><li>小市民总爱听人们的丑闻，尤其是有些熟识的人的丑闻。上海的街头巷尾的老虔婆，一知道近邻的阿二嫂家有野男人出入，津津乐道，但如果对她讲甘肃的谁在偷汉，新疆的谁在再嫁，她就不要听了。阮玲玉正在现身银幕，是一个大家认识的人，因此她更是给报章凑热闹的好材料，至少也可以增加一点销场。读者看了 这些，有的想：“我虽然没有阮玲玉那么漂亮，却比她正经”，有的想：“我虽然不及阮玲玉的有本领，却比她出身高”，连自杀了之后，也还可以给人想：“我虽然没有阮玲玉的技艺，却比她有勇气，因为我没有自杀”。化几个铜元就发现了自己的优胜，那当然是很上算的。 <em>（出处：论“人言可畏”）</em></li></ul><p>PS：以上文字的文章出处，依赖AI添加，未做核对。</p><p><img src="/img3/鲁迅杂文选.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 文学 </tag>
            
            <tag> 杂文 </tag>
            
            <tag> 鲁迅 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>文章精彩语句摘录</title>
      <link href="/2025/11/25/%E6%96%87%E7%AB%A0%E7%B2%BE%E5%BD%A9%E8%AF%AD%E5%8F%A5%E6%91%98%E5%BD%95/"/>
      <url>/2025/11/25/%E6%96%87%E7%AB%A0%E7%B2%BE%E5%BD%A9%E8%AF%AD%E5%8F%A5%E6%91%98%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<p>刚开始写读书心得时，还会很认真地把书中的精彩语句逐条摘录出来，整理成文字附在文章末尾。后来嫌麻烦，就改成直接拍照，以图片形式放在文末。这样做确实更省事，还能把书页边栏里随手写下的批注原汁原味地保留下来。</p><p>不过，这种方式也有明显的缺点。 其一，阅读体验不佳。文字摘录可以做到一目十行，也方便后期搜索，而图片则不够友好； 其二，页面加载速度慢，尤其是多图时，打开文章会变得非常卡顿。</p><p>接下来计划逐步把这些图片重新整理为文字。现在的 AI 识别已经很成熟，可以快速完成图片转文字的工作，整体工作量也在可控范围之内。</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 摘录 </tag>
            
            <tag> 金句 </tag>
            
            <tag> 拍照 </tag>
            
            <tag> 修改 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>关于喜欢</title>
      <link href="/2025/11/22/%E5%85%B3%E4%BA%8E%E5%96%9C%E6%AC%A2/"/>
      <url>/2025/11/22/%E5%85%B3%E4%BA%8E%E5%96%9C%E6%AC%A2/</url>
      
        <content type="html"><![CDATA[<p>年轻时，喜欢一个人就像麦当劳的广告词那样：我就喜欢；那时，喜欢一个人需要理由吗？不需要。需要吗？不需要。那时的喜欢就像小孩看到阳光就要奔跑一样，直接而单纯。</p><p>成年后，喜欢一个人就像林忆莲的那首歌“我就是喜欢你现在的样子”里面唱的，属于你的一切都是美丽。成年的喜欢是心动之上的欣赏和包容，欣赏他/她身上闪耀的光芒，接纳他/她身上的不足。懂得即使不完美，却仍愿意把温柔的目光停在对方身上。</p><p>喜欢的旅程，就是从“我喜欢这个感觉”，走向“我喜欢完整的你”。</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 喜欢 </tag>
            
            <tag> 欣赏 </tag>
            
            <tag> 包容 </tag>
            
            <tag> 心动 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《长安的荔枝》读书心得</title>
      <link href="/2025/11/21/%E3%80%8A%E9%95%BF%E5%AE%89%E7%9A%84%E8%8D%94%E6%9E%9D%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2025/11/21/%E3%80%8A%E9%95%BF%E5%AE%89%E7%9A%84%E8%8D%94%E6%9E%9D%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>前段时间，电视剧《长安的荔枝》因演员事件闹得沸沸扬扬，故也借机抽空把原著小说看了一遍。这是一部中篇小说，两天也便读完了。</p><p>本书虽然披着古装的外壳，但本质上却是一部讲述职场厚黑学、传统官场斗争与权谋心术的故事。和同类题材相比，小说在情节推进中加入了许多现代项目管理的思维方式，例如看板管理、瓶颈意识、风险管理等，算是有些新意。</p><span id="more"></span><p>总体而言，这只是一本偏轻松的快消类读物，用来消遣时间尚可，但思想深度有限，与经典之作《雍正王朝》相比自然差距不小。</p><p>网络上也能看到不少博主谈论马伯庸作品的读书心得，读完回头看，对本书也不必抱太高的期待，文本的思想深度有限，“网文爽感”的痕迹更为明显。而作者本人也提到，这本书的写作仅用了 11 天，与“送荔枝”的故事时长一致，从这点也能看出其构思与打磨时间的有限。我始终相信“慢工出细活”，经典之作往往都需要时间去打造。</p><p>不过，放在当下快餐文化盛行的环境下，从商业角度看，也不必对作者求全责备。</p><p>说回小说本身。</p><blockquote><p>杜牧《过华清宫绝句三首》<br />长安回望绣成堆，山顶千门次第开。<br />一骑红尘妃子笑，无人知是荔枝来。</p></blockquote><p>这首诗脍炙人口，本书正是借由此诗展开，以“必须在贵妃诞辰之前，从岭南运来新鲜荔枝”为主线。作者以基层办事员的视角审视历史事件，通过李善德的送荔枝之旅，表面写的是奔波劳碌，背后揭露的是封建制度的黑暗、官场的潜规则，以及层层人性的阴影。故事中虽有阿僮的淳朴善良，但终究只是深夜中的一抹微光。借古喻今，时间向前走，人性中的许多阴暗面与社会中的种种痼疾，却似乎并未随时代改变，并且美其名曰这是文化使然。</p><p>经历多年职场历练后再读此书，其中的许多情节确实能产生共鸣，每一段都仿佛能在现实中找到对照，恍惚间甚至有种“自己被写进了故事”的错觉。</p><p>按惯例，也摘录一下书中的名句：</p><ul><li>流程是弱者才需要遵循的规矩；</li><li>就算失败，我也想知道，自己倒在距离终点多远的地方；</li><li>有些冲动无法苟且，有些心思也无法隐藏；</li><li>一将功成万骨枯，其实一事功成，也是万人皆秃；</li><li>你会发现，上头一道命令，底下的人得忙活半天，要处理无数琐碎细节；</li></ul><p><img src="/img3/长安的荔枝/长安的荔枝1.pic.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 文学 </tag>
            
            <tag> 小说 </tag>
            
            <tag> 马伯庸 </tag>
            
            <tag> 职场 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>关于跑团和读书会</title>
      <link href="/2025/11/19/%E5%85%B3%E4%BA%8E%E8%B7%91%E5%9B%A2%E5%92%8C%E8%AF%BB%E4%B9%A6%E4%BC%9A/"/>
      <url>/2025/11/19/%E5%85%B3%E4%BA%8E%E8%B7%91%E5%9B%A2%E5%92%8C%E8%AF%BB%E4%B9%A6%E4%BC%9A/</url>
      
        <content type="html"><![CDATA[<p>刚开始跑步、读书的时候，对参加所谓的跑团或读书会很感兴趣。但后来渐渐淡了，因为慢慢明白：跑步和读书本就是属于孤独的个人体验。跑步时的思绪涌动，是与自己的身体、与天地万物对话的过程；而读书则是与心灵、与灵魂交谈。俗话说，一千个读者就有一千个哈姆雷特，读书的过程，是将个人的生活体验与书本发生化学反应的过程；离开了自己的经历，别人所谓的“感同身受”往往是不存在的。所以，许多读书分享会反而更像是一种社交，而这与读书所强调的个体性的、心灵与书本交融的孤独本质，未免有些背离。跑步亦然。</p><p>正因如此，这十多年来，始终在这样的孤独中坚持着跑步，也坚持着读书。</p><p>那句话说得好：孤独是一个人的狂欢，狂欢是一群人的孤独。对我而言，跑步和读书，就是属于在自己小宇宙里的狂欢。</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 跑团 </tag>
            
            <tag> 读书会 </tag>
            
            <tag> 跑步 </tag>
            
            <tag> 读书 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《挪威的森林》读书心得</title>
      <link href="/2025/11/15/%E3%80%8A%E6%8C%AA%E5%A8%81%E7%9A%84%E6%A3%AE%E6%9E%97%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2025/11/15/%E3%80%8A%E6%8C%AA%E5%A8%81%E7%9A%84%E6%A3%AE%E6%9E%97%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>读完《挪威的森林》，不由自主想起日本那部经典电影《情书》。相比前者那种近乎放纵的克制，我反而更喜欢《情书》里那份克制后的错失，也许这正是文化上的不同吧。但令我意外的是，日本的性观念竟然如此西化——如果村上所写能代表某种倾向的话。</p><p>第一次接触村上春树大概是在 2013 年。那时我开始夜跑，而村上又是业余马拉松爱好者的代言人，于是读了《当我谈跑步时我谈些什么》。时间久远，书中内容大多已模糊，而他挑战 100 公里超级马拉松的情节却历历在目。</p><span id="more"></span><p>从 2016 年才开始写读书心得，因此也错过了许多精彩与回忆。说起读书，那段青葱岁月总会浮现眼前。彼时，大学生真的是“天之骄子”，上了大学意味着真正的解放——60 分万岁，时间充裕，诱惑不多。再加上刚入学时情感上的一些困扰，我便用疯狂读书的方式麻醉自己。读的多是文学作品，当然也少不了金庸。大学前被视为“洪水猛兽”的小说，那时终于可以肆意阅读。</p><p>那时校园里还有许多小书店，店里最显眼的总是西方文学译著：《安娜·卡列尼娜》《基督山伯爵》《飘》《战争与和平》《简·爱》……我几乎保持一周一本的节奏博览群书（唯独专业书除外——看数学书反而是在工作后才培养的习惯，颇为惭愧）。在那个改革初期的年代，“外国的月亮更圆”，西方名著一本本翻译而来，我也一本本读过去。再加上和某人一起看的那些经典西方电影录像（去图书馆借录像带的时代），整个人仿佛都“西化”了。对《卡萨布兰卡》《罗马假日》之类的爱情片，当年的我并没有太深感触，不过女生大概都会喜欢吧。</p><p>说到电影，又想起大学时代为数不多在菁菁堂看的《廊桥遗梦》。那时完全无法理解，一个老头和一位主妇的爱情故事，竟能获如此高评价。而如今却懂了——成年人的爱，来自相互理解、相互欣赏、相互克制，比年轻时的炽热更显珍贵。正如那句话所说：“人不能同时拥有青春和对青春的感悟。”而如今，我终于有了后者。</p><p>言归正传。</p><p>村上春树的大名早已如雷贯耳，《挪威的森林》更是耳熟能详。但因为是言情小说，我这个重度理科男一直提不起兴趣。最近广州秋天气温忽冷忽热，阴晴不定，自己的情绪也随之起伏，莫名的多愁善感起来。前几天刷到书中的一些摘录视频，便突然生出了读一读的念头。</p><p>阅读过程中，我一直想到《第一次亲密接触》，想到痞子蔡，想到轻舞飞扬，想到那个早期 QQ 网恋的纯真年代。那时流行的是“你永远不知道对面是不是一条狗”；就像现在流行的“你不知道那个漂亮主播美颜背后是不是抠脚大叔，你也不知道安慰你的是不是 AI”。那个年代还会有轻舞飞扬，那时的网恋奔现也不会轻易“见光死”。回想那段青涩而克制的感情，虽然没有轻舞飞扬的悲情，虽然也充满遗憾，但如今再忆，连空气都是甜的。</p><p>《挪威的森林》扉页写着“献给许许多多的祭日”，从一开始便笼罩着淡淡哀愁。幸好绿子的出现，让这片哀愁中偶尔闪现轻松与笑意。</p><p>渡边与直子的悲情，渡边与绿子的牵绊，还有玲子、木月、永泽、初美、敢死队……一群鲜活的青春面孔，却伴随着一首首悲凉的挽歌。</p><p>直子的疗养院像是“正常社会里聚集了一群不正常的人”，而现实世界则像“不正常社会里住着所谓正常人”。在这混沌世界里，正常与不正常的界限似乎本就模糊。书中人物看似都不太“正常”，可或许每个人本质上都是不正常的，只是表现方式不同。那些放纵、多情的年轻人固然挣扎，但克制、理性就一定是正常吗？自古红颜多薄命，书里一个个祭日让人唏嘘。理性的“不正常”若能带来些许乐观，那就是，理性能成为保护壳，让人在这世界上艰难地自我维持。</p><p>本书因大量情爱描写而饱受争议，但放在整体情节中看，倒也并不突兀。不过，结尾处玲子与渡边的那一段，确实让人感到突然，或许还是观念差异吧。</p><p>过去的阅读偏好是计算机和数学专业书，其次是社科与管理，典型的 IT 男。但最近读了《一句顶一万句》《挪威的森林》，竟觉得自己敏感、悲观了不少。</p><p>以上便是《挪威的森林》的读后感，也算自己的青春回忆，写下来权作纪念。</p><p>最后，以某人最喜欢的苏东坡《观潮》作结：<br />庐山烟雨浙江潮，未至千般恨不消。<br />到得还来别无事，庐山烟雨浙江潮。</p><p><br></p><h3 id="文章精彩语句摘录">文章精彩语句摘录</h3><ul><li><p>一部描写无尽失落和再生的</p></li><li><p>青春物语</p></li><li><p>《挪》即是死者的安魂曲，又是青春的墓志铭。</p></li><li><p><strong>死并非生的对立面，而作为生的一部分永存</strong>；通过直子的死，明白任何这里都不可能治愈失去所爱之人造成的悲伤，唯一能做到的，就是从悲哀中挣脱出来。事实上渡边也最后穿越了那篇无边的泥沼和阴暗的森林，开始同现实世界接轨，摸索新的人生--借用村上的话，“所谓成长恰恰是这么回事”。</p></li><li><p><strong>文章这种不完整的容器所能容纳的，只能是不完整的记忆和不完整的概念。并且发觉，关于直子的记忆越是模糊，我才越更能深入的理解她。</strong><br />PS：记忆越是模糊，越能看到本质。</p></li><li><p>我在切身感受那一团薄雾样的东西的朝朝暮暮送走了十八岁的春天，<strong>同时努力使自己避免陷入深刻。我隐约感觉到，深刻未必是接近真实的同义词。但无论我怎样认为，死都是深刻的事实。</strong>在这令人窒息般的背反性当中，我重复着这种永无休止的圆周式思考。如今想来，那真是奇特的日日夜夜，在活得好端端的青春时代，居然凡事都以死为轴心旋转不休。</p></li><li><p>我开始思索，或许她想向我倾诉什么，却又无法准确的诉诸语言。不，是她无法在诉诸语言之前在心里把握它，唯其如此才无法诉诸语言。她不时摸一下发卡，或用手绢擦一下嘴角，或不知所以然地凝视我的眼睛。如果可能的话，有时我真想将她一把紧紧地搂在怀里，却又总是怅惘作罢。我生怕万一因此伤害直子。这样，我们继续在东京街头行走不止，直子在空漠中继续寻找语言。</p></li><li><p><strong>他也背负着他的十字架匍匐在人生途中</strong>。</p></li><li><p>（渡边和绿子的搞笑对话）<br />“图书室。“我说。<br />“别去那种地方，和我一同吃午饭去如何？”<br />“刚吃过。”<br />“那有什么，再吃一次就是”</p><p><br></p><p>"嗳，那边那个拄松木拐杖的老头儿，我们一进来就鬼鬼祟祟地往我腿上看，就那个穿蓝衣戴眼镜的老头儿。"绿子不无陶醉地说。<br />“当然要看，穿那样的裙子谁都得看。”<br />“不过也蛮好嘛，反正大伙都无聊之极，偶尔欣赏一下年轻姑娘的腿调剂调剂也好。兴奋起来促进康复也未可知。”<br />“但愿别适得其反。”我说。</p><p><br></p><p>喝完咖啡，我和绿子折回病房。她父亲还在酣睡，凑上耳朵听听，尚在微微喘息。随着午后时间的推移，窗外阳光的色调变得柔和而沉静，一派秋日气息。小鸟成群结伙的飞来，落在电线上，又一忽儿飞去。我和绿子两人并坐在屋角处，压低声音说个不止。她看了我的手相，预言我能活到一百零五岁，结婚三次，最后死于交通事故。我说这一生还算不赖。</p><p><br></p><p>"月经一来，我就戴两三天的红帽子。这回能知道了吧？"绿子笑道，“我一戴上红帽子，你在路上遇见也别打招呼，赶紧逃命。”</p></li><li><p>“那不是努力，只是劳动。”永泽断然说到，“我所说的努力与这截然不同。所谓努力，指的是主动而有目的的活动。”<br />“举例说，就是在职业确定之后其他人无不只顾庆幸的时间里开始学习西班牙语--是这样的吧？“<br />”正是这样......“</p></li><li><p>"我同渡边的相近之处，就在于不希望别人理解自己。"永泽说，“这点与其他人不同，那些家伙无不蝇营狗苟地设法让周围的人理解自己。但我不那样，渡边也不那样，而觉得不被别人理解也无关紧要。自己是自己，别人归别人。”<br />PS：<br />"蝇营"：说的是像苍蝇那样四处飞动，营营往来，追逐污秽之物。这个意象其实更早可以追溯到《诗经·小雅》中的《青蝇》篇，用"营营青蝇"来讽刺进谗言的小人。<br />"狗苟"：则是形容像狗那样苟且偷安，不顾廉耻地活着。<br />韩愈将这两个意象结合在一起，创造了"蝇营狗苟"这个成语，生动地刻画了那些为了名利而不择手段、四处钻营、卑鄙无耻的人的嘴脸。</p></li><li><p>围绕着“为什么手的中指比食指长，而脚趾则相反”的问题讲授一番。</p></li><li><p><strong>倘若周围一团漆黑，那就只能静等眼睛习惯黑暗。</strong></p></li><li><p>更何况我仍在爱着直子，尽管爱的方式在某一过程中被扭曲得难以思议，但我对直子的爱却是毋庸置疑的，我在自己的心田中为直子保留了相当大一片、未曾被别人染指的园地。</p></li><li><p>而直子的死还使我明白：无论熟知怎样的哲理，也无以消除所爱之人的死带来的悲哀。无论怎样的哲理，怎样的真诚，怎样的坚韧，怎样的柔情，也无法排遣这种悲哀。我们唯一能做到的，就是从这片悲哀中挣脱出来，并从中领悟某种哲理。而领悟后的任何哲理，在继之而来的意外悲哀面前，又是那样软弱无力。</p></li></ul><p><img src="/img3/挪威的森林/挪威的森林1.pic.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 文学 </tag>
            
            <tag> 小说 </tag>
            
            <tag> 日本 </tag>
            
            <tag> 村上春树 </tag>
            
            <tag> 言情 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《人间失格》读书心得</title>
      <link href="/2025/11/09/%E3%80%8A%E4%BA%BA%E9%97%B4%E5%A4%B1%E6%A0%BC%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2025/11/09/%E3%80%8A%E4%BA%BA%E9%97%B4%E5%A4%B1%E6%A0%BC%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>《人间失格》是日本著名小说家太宰治最具影响力的中篇（短篇？）小说之一，发表于1948年。这是一部明显带有自传色彩的作品，而在同年，太宰治便选择了自杀，令人读来更添几分唏嘘。</p><p>这本小说其实在去年就读完了，篇幅不长，是在方所书店断断续续看完的。没有在读后立即写下心得，是因为书中那种极致的颓废与深沉的绝望感，与现实世界所强调的正能量相去甚远，读完后的心境一时难以整理。</p><span id="more"></span><p>鲁迅在《呐喊·自序》中曾说：「但既然是呐喊，则当然须听将令的了，所以我往往不恤用了曲笔，在《药》的瑜儿的坟上平空添上一个花环，在《明天》里也不叙单四嫂子竟没有做到看见儿子的梦，因为那时的主将是不主张消极的。至于自己，却也并不愿将自以为苦的寂寞，再来传染给也如我那年青时候似的正做着好梦的青年。」</p><p>相比之下，鲁迅至少在那一圈红白相间的花里，还给人生留了一点念想与希望；而太宰治的作品，却像是一条路一路走到底，不给人任何喘息的余地。</p><p>今天忽然想把这本书记录下来，是因为在阅读村上春树的《挪威的森林》时，其中提到太宰治的大名，因而触发了记忆。当然，书中所传递的价值观，并不代表本人的立场。 :)</p><p><img src="/img3/人间失格.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 文学 </tag>
            
            <tag> 小说 </tag>
            
            <tag> 太宰治 </tag>
            
            <tag> 日本 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《一句顶一万句》读书心得</title>
      <link href="/2025/11/06/%E3%80%8A%E4%B8%80%E5%8F%A5%E9%A1%B6%E4%B8%80%E4%B8%87%E5%8F%A5%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2025/11/06/%E3%80%8A%E4%B8%80%E5%8F%A5%E9%A1%B6%E4%B8%80%E4%B8%87%E5%8F%A5%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>刘震云2011年茅盾文学奖的长篇小说《一句顶一万句》，终于抽出时间拜读完毕。</p><p>小说分上下两部分，上半部“出延津记”，写的是杨百顺身上发生一连串变故，最终与养女巧玲离乡背井、踏上寻人之路；下半部“回延津记”，讲的是巧玲的儿子牛爱国，在经历人生种种的是是非非之后，因为母亲收到的一封信，走回延津......</p><p>上下两部分，就像个轮回，变的是人物，不变的是故事背后的柴米油盐，不变的是漂泊的人生和孤独的灵魂。虽然本书没有《百年孤独》的魔幻，但那种宿命感的底色并无二致，读罢心头始终萦绕着一股悲凉。 不过在终篇，牛爱国的那句话：“不，得找。”又仿佛留住了人生最后的一丝希望，不让人生显得太过苍白。</p><span id="more"></span><p>小说描写的是乡村底层人民生活中鸡毛蒜皮的小事，琐碎而又平凡，没有快意恩仇的情节，连文字都是那么朴素无华；书中人物众多，但个性鲜明、形象生动。作者以普通故事串联起一群普通人，又借这些普通故事，把人性中那点劣根性（非理性的那一面）刻画得入木三分。终篇看来，书中中国女性整体比男性表现得更勇敢一些，这也符合大众对中国女性的普遍印象。总的来说，这是一本文学成就很高的作品，但绝不是那种通俗爽书。</p><p>书中不断出现对“生命中遗失的最重要的那句话”的寻找，这成了整部小说起承转合的关键 。牛爱国的情人章楚红要告诉他的那句话（大概也是终篇时牛爱国坚持得找的理由吧），吴摩西（杨百顺）临终前想对巧玲说的那句话，曾志远想亲口告诉牛爱国的那句话......最终都错过了。人们寻找的，看似是一句话，其实是生命中那个能说出“一句顶一万句”的人 。</p><p>本书常被拿来与路遥1991年获茅盾文学奖的大作《平凡的世界》比较，两书在豆瓣上的评分同为9.0，不分伯仲。《平凡的世界》是大学刚毕业时读的，时隔多年细节早已模糊，只剩整体印象：那是一本典型的“努力就能成功”的主流叙事，励志得近乎太过不平凡。相比之下，《一句顶一万句》更像我们普通人的人生，无奈而真实，也许是上了年纪，看问题、看人生的态度已然不同。</p><p>这本书还有一个插曲，本想去广图借阅，可能是借阅者太多，书架上根本找不到，只能预约才能借阅。于是改去购书中心阅读，倒也坚持读完这本大块头。只是购书中心那本可直接翻阅的样书（非塑封），也破旧不堪（详见附图），足见其受欢迎程度。</p><p>以下是本书的一些精彩语句和思想摘录：</p><ul><li><p>反复使用的句式</p><p>A发生，不是因为显而易见的B，也不是相关的C，而是新的D，从而引出了与D相关的故事。</p><blockquote><p>例如： 牛爱国他妈叫曹青娥。牛爱国他妈本不该姓曹，应该姓姜；本也不该姓姜，应该姓吴；本也不该姓吴，应该姓杨。曹青娥五岁那年，被人从河南卖到山西。</p></blockquote><p>又例如：</p><blockquote><p>但等孩子买下之后，老曹才知道，老婆要这个孩子，既不是为了孩子，也不是为了老曹两口，也不是为了造七级浮屠，而是为了跟二叔置气。</p></blockquote></li><li><p>每个人都是孤独和漂泊的灵魂<br />书中有一段写意大利神父老詹在延津传教，由此带出人生三大哲学问题：你是谁，从哪里来，到哪里去；为小说的孤独和漂泊主题注入一层背景色。<br />尾篇提到神父老詹的教堂如今成了“金盘洗脚屋”，讽刺意味扑面而来。或许这正是中国人魂无所依、注定漂泊的原因吧。</p></li><li><p>过日子是过以后，不是过从前<br />大道理人人都懂，但是大道理之所以称为大道理，是因为难以做到。过去是生命的一部分，割舍过去，就像把自己的一部分切割开，这岂是我们这些凡夫俗子能轻易做到。</p></li><li><p>世界上的人，只有说的着和说不着<br />小说中说：“有些人说得着，有些人说不得着；有些人现在说不得着，将来或许能说得着；有些人现在说得着，将来未必能说得着；有些人这一辈子也说不得着。找一个能说得着的人过一辈子是福分，不管是爱人、朋友还是亲人。”</p></li></ul><p><img src="/img3/一句顶一万句.png" /></p><p><img src="/img3/一句顶一万句1.png" /></p><p><img src="/img3/一句顶一万句2.png" /></p><p><img src="/img3/一句顶一万句3.jpg" /></p><p><img src="/img3/一句顶一万句4.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 文学 </tag>
            
            <tag> 小说 </tag>
            
            <tag> 刘震云 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AI学习书籍介绍</title>
      <link href="/2025/11/01/AI%E5%AD%A6%E4%B9%A0%E4%B9%A6%E7%B1%8D%E4%BB%8B%E7%BB%8D/"/>
      <url>/2025/11/01/AI%E5%AD%A6%E4%B9%A0%E4%B9%A6%E7%B1%8D%E4%BB%8B%E7%BB%8D/</url>
      
        <content type="html"><![CDATA[<blockquote><p>"你无法在摘要上进行模式匹配，阅读他人对知识的压缩会创造知识的幻觉；二手洞见不是洞见，模式识别需要高质量的数据。"</p></blockquote><p>这句话说得极好，学习一门新知识，最终还是要回到系统的、成体系的经典书籍上。</p><p>关于AI学习，读过很多相关书籍，结合自己的体会，按照较为合适的阅读顺序推荐以下几本：</p><p><strong>1.《Deep Learning with Python》（《Python 深度学习》）</strong></p><p>——“知其然”的最佳入门书</p><p>这本书深入浅出地介绍了深度学习的核心思想，是非常优秀的宏观入门读物。行文清晰、结构紧凑，让初学者能够迅速建立对深度学习整体框架的理解。国内已有第二版的正式译本。</p><span id="more"></span><p><img src="/img3/Python深度学习.jpg" width="70%"></p><p><strong>2.《Hands-On Large Language Models》</strong></p><p>——图文并茂的大语言模型基础指南</p><p>这本 2024 年出版的新书目前没有正式中文版（阅读的中文版是自行翻译的）。全书以极丰富的图示讲解大模型的关键概念，人是视觉动物，一张图胜过千言万语，这使得读者能更容易理解其底层的思想和原理。</p><p>这本书在技术深度上把握较好，在能够讲清楚技术原理的基础上，也不过于技术化。不过还是建议在读过《Deep Learning with Python》之后再看这本书，因为大语言模型的技术根基仍然是深度学习，有一定神经网络基础后阅读本书的体验会更好。</p><p><img src="/img3/Hands-On Large Language Models.png" width="70%"></p><p><strong>3.《Practical Statistics for Data Scientists》</strong></p><p>——数据科学视角的统计学基础</p><p>这本书从数据科学的实际需求出发，以统计学为主轴，知识点介于基础数学和计算机应用之间，系统讲解了数据分析中常见的统计概念与易混点，并配合 Python 与 R 给出了大量示例。国内已有第一版的正式中文版（阅读的是自行翻译的第二版）。</p><p>相比传统统计学教材，它更关注“能否用得上”，因而弱化了大量推导，更适合作为数据科学家的统计基础读物。</p><p><img src="/img3/Practical Statistics For Data Scientists.png" width="70%"></p><p><strong>4.《Mathematics for Machine Learning》（《机器学习的数学基础》）</strong></p><p>——“知其所以然”的关键一课</p><p>这本书系统梳理了机器学习相关的核心数学知识，结构严谨、联系清晰，整体性和系统性非常好，是少数真正能让读者在数学层面理解机器学习底层原理的教材。网上有部分中文译稿，为方便学习，自己将 Part II 译成中文后学习。</p><p>如果你想了解支撑深度学习的线性代数、概率论、微积分、优化理论，这本书几乎是最佳选择。</p><p>附：如果对机器学习数学基础感兴趣，可参考文章《打好数据科学和机器学习的基础——6本书带你学数学》<br />https://www.dataapplab.com/6-best-books-to-learn-mathematics-for-data-science-machine-learning/</p><p><img src="/img3/机器学习数学基础.png" width="70%"></p><p></br></p><blockquote><p><strong>以上几本书主要从原理层面展开，涵盖了深度学习框架、数学基础与统计基础。以下这本，则更偏向 AI 的“应用层”——智能体（Agent）系统的设计与实践。</strong></p></blockquote><p></br></p><p><strong>5.《Agentic Design Patterns: A Hands-On Guide to Building Intelligent Systems》</strong></p><p>——智能体系统的全面实践指南</p><p>这本书对现代 AI 系统中的 Agent 架构进行了系统而深入的讲解，涵盖概念、模式与工程实现，是近年来少见的高质量智能体设计指南。</p><p>令人印象深刻的是，作者在正式出版前，就将英文原稿以 Google Docs 形式公开，而 GitHub 上的中文翻译也在英文版付印前便已完成。这正是 AI 带来的知识民主化：借助 AI 翻译工具，让中文读者几乎同步获得最新前沿资料(当然这是在开源协议允许的前提下完成）。</p><p><img src="/img3/agentic-design-patterns 封面.png" width="70%"></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 大语言模型 </tag>
            
            <tag> LLM </tag>
            
            <tag> 数学 </tag>
            
            <tag> 算法 </tag>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《机器学习的数学基础》</title>
      <link href="/2025/10/30/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%B8%80/"/>
      <url>/2025/10/30/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%B8%80/</url>
      
        <content type="html"><![CDATA[<p>英文版的原书是《MATHEMATICS FOR MACHINE LEARNING》。</p><blockquote><p>数学是这个世界上最精确的语言！</p></blockquote><p>在学习世界里，人们常说马斯克的“第一性原理”重要，又常用“概念清晰”来评价一个好学生。归根到底，这些都指向同一件事：<strong>抓住根本概念的重要性</strong>。这也正是《Mathematics for Machine Learning》最突出的特点。</p><p>当然，由于本书涵盖面广——线性代数、解析几何、微积分、概率等一网打尽——读者仍需要一定的数学基础。有些章节在推导上略显紧凑，必要时配合 ChatGPT 等工具理解概念，会大大提升效率。</p><p>这本书最大的价值在于：它并非简单罗列公式，而是<strong>从数学的角度、用数学的语言，把机器学习所需的数学概念系统梳理了一遍</strong>，并把各个知识点之间的关联讲得十分透彻。阅读过程中，你能明显感受到作者试图搭建的是一张“概念图谱”，而不是一本“知识清单”。</p><p><span id="more"></span></p><p>看完这本书之后，不禁想起前段时间读的那套数学书《鸢尾花套书》。整整七本，每一本都是大块头。这套书最大的特色是图文并茂，乍一翻开仿佛是在看一本艺术画册：色彩丰富、图示大量，视觉体验极强。然而，也正因为为了避免内容过于数学化，它往往只是简单罗列知识点，对知识之间的关联着墨不多。再加上为了强调可视化效果，许多概念被过度简化，读者得到的往往只是表面印象，反而把一些本质性的内容遗漏了，甚至可能形成片面或错误的理解。</p><p>我也因此受过不小的误导。直到读完这本书，才恍然大悟——原来某些概念真正的含义竟是如此！</p><p>学习本身就是“理论—实践—再理论”的往复过程。概念先被理解，然后在练习、应用或阅读中接受检验；理解出现缝隙，再回头重新咀嚼概念，于是便有了升华。概念的扩展，就像重新定义“人”。在国内时，也许我们以为“人”就是黄种人的样子；出国后见到不同族群，再回头看“人”的定义，就会发现自己对这一概念的理解被重新拉宽、拉深。</p><p>这本书也是类似的体验。它用数学语言为概念划清边界、直指本质。例如向量、内积等，看似熟悉的词语，在本书中常常被重新定义：我们过去将“内积”等同于“点积”，但本书会告诉你，真正的内积概念要广泛得多，其应用也远超日常所见。</p><p>这份笔记是我结合英文原版和部分中文资料整理而成的，希望能为后续学习打下一层更扎实的概念地基。</p><p><a href="/img3/机器学习的数学基础Part1/mml-book.pdf">原书英文版下载</a><br />本书分两部分：</p><ul><li>Part I：Mathematical Foundations</li><li>PartII：Central Machine Learning Problems</li></ul><p>Part I 中文版已有网络资源，收录在译者这个<a href="https://binaryai.blog.csdn.net/article/details/115050415">专栏</a></p><p>不过，该译作者针对原书作了小小的修改，例如在例6.1中，就把$换成人民币¥；在矩阵那个章节增加了分块矩阵的运算！最后一章节的“信息论”原书并没有（或者是否我看的原书和译者不一致）。</p><p>以下是本书的一些摘录。</p><h2 id="导言">1 导言</h2><blockquote><p>核心的概念是内积！内积的定义以及从内积引申出的正定矩阵、范数、距离等概念。</p></blockquote><p><strong>注意函数和曲面的区别。详见《函数与曲面.md》</strong></p><p>理解这些原理可以帮助创建新的机器学习解决方案，理解和调试现有方法，了解我们正在使用的方法的固有假设和局限性。</p><h3 id="为直觉寻找词语">1.1 为直觉寻找词语</h3><p>作为另一个关于词语是多么微妙的例子，<strong>有（至少）三种不同的方式来思考向量</strong>：</p><ul><li>向量作为数字数组（计算机科学观点）</li><li>向量作为具有方向和大小的箭头（物理学观）</li><li>向量作为一个服从加法和缩放的对象（数学观点）。</li></ul><p>不同学科的研究范围：</p><ul><li>向量和矩阵的研究称为 <em>线性代数（linear algebra）</em></li><li>相似度和距离的构造是 <em>解析几何（analytic geometry）</em> 的核心</li><li>不确定性的量化是 <em>概率论（probability theory）</em> 的范畴</li><li>梯度的研究： <em>向量微积分（vector calculus）</em> 为了训练机器学习模型，我们通常会找到最大化某些性能指标的参数。 许多优化技术需要梯度的概念，它告诉我们搜索解决方案的方向。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 数学 </tag>
            
            <tag> 算法 </tag>
            
            <tag> 统计 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《机器学习的数学基础》</title>
      <link href="/2025/10/29/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%BA%8C/"/>
      <url>/2025/10/29/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%BA%8C/</url>
      
        <content type="html"><![CDATA[<h2 id="线性代数">2 线性代数</h2><p>Linear Algebra</p><p>在形式化一些直观概念时，常见的方法是构造一组对象(符号)和一些操作这些对象的规则。 这就是所谓的代数(algebra)。线性代数是研究向量以及使用某些确定的规则来操作向量的 一门学科。 我们许多人从学校里知道的向量被称为“几何向量”，通常用上方带一个小箭头的字母表示。</p><blockquote><p>向量是特殊的对象，将它们<strong><u>相加并乘以</u></strong>标量产生的是另一个相同类型的对象。从抽象的数学来看，任何满足这两个性质的物体都可以被认为是向量。</p></blockquote><span id="more"></span><h3 id="线性方程求解">2.1 线性方程求解</h3><p>出于数值精度的原因，通常不建议计算逆或伪逆。因此，在下文中，我们将简要讨论求解线性方程组的其他方法。</p><p><strong><u>高斯消元法</u></strong>在计算行列式、检查向量集是否线性独立、计算矩阵的逆，计算矩阵的秩，和确定向量空间的基时起着重要作用。高斯消元法是一种直观而有建设性的方法来解决一个含成千上万变量的线性方程组。然而，对于具有数百万个变量的方程组，这是不切实际的，因为所需的运算量是按联立方程组的数量的立方增长的。</p><p>在实践中，许多线性方程组都是通过定<strong><u>常迭代方法</u></strong>(stationary iterative methods)间接求解的，如Richardson方法、Jacobi方法、Gauß-Seidel方法和逐次超松弛方法，或Krylov子空间方法，如共轭梯度、广义最小残差或双共轭梯度。</p><h3 id="群">2.2 群</h3><blockquote><p>群的概念，它包含一组元素和一个定义在这些元素上的操作，该操作可以保持集合的某些结构完整。</p></blockquote><ul><li>封闭性(Closure)</li><li>结合律(Associativity)</li><li>单位元(Neutral elemen)</li><li>逆元(Inverse element)</li></ul><p>群在计算机科学中扮演着重要的角色。除了为集合上的运算提供一个基本框架外，它们还被大量应用于密码学、编码理论和图形学。</p><p>如果可交换运算顺序，是阿贝尔群( Abelian group）(<strong>个人注：很多时候交换律都不是必须的。</strong>)</p><blockquote><p>向量空间的定义太赞了，通过群的定义引出！</p></blockquote><h3 id="线性变换的定义">2.3 线性变换的定义</h3><p>保持<strong>向量空间结构不变</strong>需满足:</p><p><span class="math display">\[\forall x, y \in V\ \forall \lambda, \psi \in \mathbb{R} : \Phi(\lambda \mathbf{x} + \psi \mathbf{y}) = \lambda \Phi(\mathbf{x}) + \psi \Phi(\mathbf{y}).\]</span></p><p>我们可以把线性映射用矩阵(2.7.1节)表示。回想一下前面的内容：我们将向量集合用矩阵的列表示。<strong>在处理矩阵时，我们必须判断矩阵所表示的内容：是线性映射还是向量集合。</strong></p><p><strong>空间结构不变的定义</strong>：</p><blockquote><p>从几何角度理解线性变换保持空间结构不变，可以帮助直观掌握线性变换的本质。</p><p>1、线性变换几何视角的理解</p><p><strong>线性变换</strong> <span class="math inline">\(\Phi: V \to W\)</span> 是一种映射，满足：</p><ul><li>加法保持：<span class="math inline">\(\Phi(\mathbf{x} + \mathbf{y}) = \Phi(\mathbf{x}) + \Phi(\mathbf{y})\)</span></li><li>数乘保持：<span class="math inline">\(\Phi(\lambda \mathbf{x}) = \lambda \Phi(\mathbf{x})\)</span></li></ul><p>这意味着：</p><p>1）<strong>直线映射为直线</strong> 任何向量空间中的直线（即所有形如 <span class="math inline">\(\mathbf{v}_0 + t\mathbf{v}\)</span> 的点集，其中 <span class="math inline">\(t \in \mathbb{R}\)</span>）经过线性变换后，仍然是直线（或退化成一点）。换句话说，线性变换不会把直线扭曲成曲线。（详见”向量空间中直线的定义.md“）</p><p>2）<strong>原点保持不变</strong> 由于 <span class="math inline">\(\Phi(\mathbf{0}) = \mathbf{0}\)</span>，线性变换一定把原点映射为原点。这保持了空间的“参考点”。</p><p>3）<strong>比例保持</strong> 对于任意标量 <span class="math inline">\(\lambda\)</span>，线性变换保证拉伸或压缩比例保持。例如，<span class="math inline">\(\mathbf{x}\)</span> 拉伸两倍后变为 <span class="math inline">\(2\mathbf{x}\)</span>，变换后也是变换 <span class="math inline">\(\Phi(\mathbf{x})\)</span> 的两倍：<span class="math inline">\(\Phi(2\mathbf{x}) = 2\Phi(\mathbf{x})\)</span>。</p><p>4）<strong>平行保持</strong> 如果两条线在原空间平行，那么它们的像也平行（或者重合）。这是因为线性变换保持向量加法关系。</p><p>5）<strong>向量加法的几何含义保持</strong> 变换前后，向量的组合关系（比如三角形的顶点、平行四边形的形状）在变换后的空间里依然成立（尽管大小和方向可能改变）。</p><p>2、换句话说</p><p>线性变换是“刚性”变换的一个推广，它可以拉伸、压缩、旋转、反射空间，但不会产生弯曲或折叠，也不会“撕裂”空间结构。<strong>它保持了“直线”、“比例”、“原点”这些最基础的几何性质。</strong></p><p>3、举个例子</p><ul><li><strong>旋转</strong> 是线性变换，保持长度和角度（保持空间结构不变的同时还保持距离）。</li><li><strong>缩放</strong>（拉伸/压缩）也是线性变换，改变大小但不改变“直线性”和“比例”。</li><li><strong>投影</strong> 是线性变换，把三维空间映射到二维平面，保持直线的映射，但改变维度和形状。</li></ul></blockquote><h3 id="变换矩阵">2.4 变换矩阵</h3><p><strong>注意变换矩阵(transformation matrix)的定义。</strong>使用变换矩阵将相对于<span class="math inline">\(V\)</span>的有序基的坐标映射到相对于<span class="math inline">\(W\)</span>中有序基的坐标。</p><p>后文提到，为了简化变换矩阵，通过基变换处理。</p><p>在矩阵和有限维向量空间之间的线性映射之间建立一个显式联系（<strong>重点是理解线性变换和矩阵之间的本质联系！</strong>）。</p><h3 id="相似矩阵">2.5 相似矩阵</h3><p>如果存在可逆矩阵 <span class="math inline">\(\boldsymbol{P}\)</span>，使得 <span class="math display">\[\boldsymbol{D} = \boldsymbol{P}^{-1} \boldsymbol{A} \boldsymbol{P}\]</span> 则称矩阵 <span class="math inline">\(\boldsymbol{A}\)</span> 与 <span class="math inline">\(\boldsymbol{D}\)</span> 是相似(Similarity)的（定义 2.22）。</p><h3 id="恒等映射">2.6 恒等映射</h3><p><span class="math inline">\(\mathrm{id}_V : V \to V,\ x \mapsto x\)</span> 为 <span class="math inline">\(V\)</span> 中的恒等映射或单位自同构（<strong>详见文件：‘恒等映射的存在意义.md“</strong>）。</p><h3 id="基变换">2.7 基变换</h3><p>重要的作用：同一个点，在不同的基的不同坐标表示！变换矩阵相当于坐标的映射（不同基）。</p><p>在第四章中，我们将利用基变换的概念来寻找一个基，使得自同态的变换矩阵有一个特别简单的（对角）形式。在第十章降维中，我们将利用基变换研究一个数据压缩问题，即找到一个基并在这个基上投影数据从而压缩数据，同时最小化压缩损失。</p><h3 id="变量变换和基变换区别">2.8 变量变换和基变换区别</h3><blockquote><p><strong>要分清楚基变换和变量变换的区别！不要混淆；两者得出的变换矩阵是不同的。</strong></p><p><img src="/img3/机器学习的数学基础Part1/变换.png" alt="变换" style="zoom:50%;" /></p><p>1、变量变换</p><p>如果给定两个向量 <span class="math inline">\(\boldsymbol{b}_{1} = [1,0]^{\top}, \ \boldsymbol{b}_{2} = [0,1]^{\top}\)</span> 作为单位正方形（图5.5中的蓝色区域）的边，则该正方形的面积为： <span class="math display">\[\left|\det\!\begin{bmatrix}1 &amp; 0 \\0 &amp; 1\end{bmatrix}\right| = 1\]</span></p><p>如果我们取一个平行四边形（图5.5中的橙色区域），它的边为 <span class="math inline">\(\boldsymbol{c}_{1} = [-2,1]^{\top}, \ \boldsymbol{c}_{2} = [1,1]^{\top}\)</span>， 则它的面积是行列式（见第4.1节）的绝对值： <span class="math display">\[\left|\det\!\begin{bmatrix}-2 &amp; 1 \\1 &amp; 1\end{bmatrix}\right|= |-3| = 3\]</span></p><p>即它的面积正好是单位方形的三倍。我们可以通过一个将单位方形转换成另一个方形的映射来找到这个缩放因子。用线性代数的术语说，就是有效地执行从 <span class="math inline">\((\boldsymbol{b}_{1}, \boldsymbol{b}_{2})\)</span> 到 <span class="math inline">\((\boldsymbol{c}_{1}, \boldsymbol{c}_{2})\)</span> 的变量变换。在我们的例子中，这个映射是线性的，且它的行列式的绝对值正好给出了我们要寻找的缩放因子。</p><p>因为<span class="math inline">\(\boldsymbol{b}_{1},\boldsymbol{b}_{2}\)</span>是基，根据变换矩阵的定义，故<strong>变换矩阵</strong>是： <span class="math display">\[A = \begin{bmatrix}-2 &amp; 1 \\1 &amp; 1\end{bmatrix}\]</span></p><p>2、基变换</p><p>为了开始使用线性代数的方法，我们首先确定 <span class="math inline">\(\{\boldsymbol{b}_{1}, \boldsymbol{b}_{2}\}\)</span> 和 <span class="math inline">\(\{\boldsymbol{c}_{1}, \boldsymbol{c}_{2}\}\)</span> 都是 <span class="math inline">\(\mathbb{R}^2\)</span> 的基。我们要有效地执行的是从 <span class="math inline">\(\{\boldsymbol{b}_{1}, \boldsymbol{b}_{2}\}\)</span> 到 <span class="math inline">\(\{\boldsymbol{c}_{1}, \boldsymbol{c}_{2}\}\)</span> 的基变换，就得寻找实现基变换的变换矩阵。 利用第2.7.2节的结果，我们确定了所需的<strong>基变换矩阵</strong>为： <span class="math display">\[\boldsymbol{J} = \begin{bmatrix}-2 &amp; 1 \\1 &amp; 1\end{bmatrix} \tag{5.62}\]</span></p><p>它使得 <span class="math inline">\(\boldsymbol{J}\boldsymbol{b}_{1} = \boldsymbol{c}_{1}, \quad \boldsymbol{J}\boldsymbol{b}_{2} = \boldsymbol{c}_{2}\)</span>。 矩阵 <span class="math inline">\(\boldsymbol{J}\)</span> 的行列式的绝对值为： <span class="math display">\[\left|\det(\boldsymbol{J})\right| = 3,\]</span> 这正是我们在寻找的缩放因子。也就是说， <span class="math inline">\((\boldsymbol{c}_{1}, \boldsymbol{c}_{2})\)</span> 所张成的平行四边形的面积是 <span class="math inline">\((\boldsymbol{b}_{1}, \boldsymbol{b}_{2})\)</span> 所张成面积的三倍。</p><p>把 <span class="math inline">\(\{\boldsymbol{b}_{1}, \boldsymbol{b}_{2}\}\)</span> 坐标变换成 <span class="math inline">\(\{\boldsymbol{c}_{1}, \boldsymbol{c}_{2}\}\)</span> 的坐标，是左乘<span class="math inline">\(\boldsymbol{J}^{-1}\)</span>；反向是左乘<span class="math inline">\(\boldsymbol{J}\)</span>。</p></blockquote><h3 id="像与核">2.8 像与核</h3><p>“直观地说，核是被 <span class="math inline">\(\Phi\)</span> 映射到单位元 <span class="math inline">\(\mathbf{0}_W \in W\)</span> 上的一组向量 <span class="math inline">\(\mathbf{v} \in V\)</span>。”</p><p>具体见原书图2.12</p><p><img src="/img3/机器学习的数学基础Part1/F2.12.png" alt="F2.12" style="zoom:50%;" /></p><h3 id="仿射空间">2.9 仿射空间</h3><p>在下面，我们将研究从原点偏移的空间，即不再是向量子空间的空间。此外，我们将简要讨论这些仿射空间之间类似线性映射的一些性质。</p><p><strong>备注</strong>： 在机器学习领域的文献中，线性和仿射之间的区别有时是不明确的，因此我们可以将线性空间/映射作为仿射空间/映射的参考。</p><h3 id="仿射映射">2.10 仿射映射</h3><p>仿射映射保持几何结构不变。它们还保留了尺寸比例和平行度。</p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 数学 </tag>
            
            <tag> 算法 </tag>
            
            <tag> 统计 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《机器学习的数学基础》</title>
      <link href="/2025/10/28/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%B8%89/"/>
      <url>/2025/10/28/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%B8%89/</url>
      
        <content type="html"><![CDATA[<h2 id="解析几何">3、解析几何</h2><p>Analytic Geometry</p><p><strong>内积及其相应的范数和度量可以得到相似性和距离的直观概念</strong></p><blockquote><p>范数表示向量的长度，范数可以由内积引出（但不必要，例如曼哈顿范数就不是）<br />内积可以是不同的定义（所以不同的内积定义的长度是不一样的），<strong>内积由唯一的对称正定矩阵确定</strong>（正定矩阵的定义符合要求）。<br />内积来计算向量的长度和向量之间的夹角.</p></blockquote><span id="more"></span><h3 id="范数">3.1 范数</h3><p>向量空间 <span class="math inline">\(V\)</span> 的范数是一个指定每个向量 <span class="math inline">\(x\)</span> 的长度的函数:</p><p><span class="math display">\[\|\cdot\|: V \to \mathbb{R}\]</span></p><p><span class="math display">\[x \mapsto \|x\|\]</span></p><p>并且对于任何 <span class="math inline">\(\lambda \in \mathbb{R}\)</span> 以及 <span class="math inline">\(x, y \in V\)</span>，以下成立：</p><ul><li><strong>绝对一次齐次性</strong> (Absolutely homogeneous)：<span class="math inline">\(\|\lambda x\| = |\lambda| \|x\|\)</span></li><li><strong>三角不等式</strong> (Triangle inequality)：<span class="math inline">\(\|x + y\| \leq \|x\| + \|y\|\)</span></li><li><strong>正定性</strong> (Positive definite)：<span class="math inline">\(\|x\| \geq 0\)</span> 且 <span class="math inline">\(\|x\| = 0 \iff x = 0\)</span></li></ul><p>注意，范数可以是多种，例如<span class="math inline">\(\mathcal l_1\)</span>范数（<em>曼哈顿范数</em>）、<span class="math inline">\(\mathcal l_2\)</span>范数（<em>欧氏距离</em>）。</p><h3 id="内积">3.2 内积</h3><p>内积可以引入一些直观的几何概念，例如<strong>向量的长度和两个向量之间的角度或距离</strong>。内积的一个主要目的是确定向量之间是否正交。</p><p><strong>3.2.1 点积</strong></p><p>我们可能已经熟悉了一种特殊类型的内积，<span class="math inline">\(\mathbb{R}^n\)</span>中的标量积/点积（scalar product/dot product）：</p><p><span class="math display">\[\mathbf{x}^\top \mathbf{y} = \sum_{i=1}^n x_i y_i \tag{3.5}\]</span></p><p>在这本书中，我们将<strong>把这种特殊的内积称为点积</strong>。但是，<strong>内积是具有特定性质的更一般的概念</strong>，我们现在将介绍这些概念。</p><p><strong>定义 3.3：内积与内积空间</strong></p><p>设 <span class="math inline">\(V\)</span> 为向量空间，<span class="math inline">\(\langle \cdot,\cdot \rangle : V \times V \to \mathbb{R}\)</span> 为一个双线性映射，即它将 <span class="math inline">\(V\)</span> 中两个向量映射为一个实数，则有：</p><ul><li>若 <span class="math inline">\(\langle \cdot,\cdot \rangle\)</span> 是<strong>正定且对称</strong>的，则称其为 <span class="math inline">\(V\)</span> 上的内积，常记作 <span class="math inline">\(\langle x, y \rangle\)</span>；</li><li>则 <span class="math inline">\((V, \langle \cdot,\cdot \rangle)\)</span> 被称为一个内积空间（或带内积的实向量空间）；</li><li><strong>若该内积为点积，则称 <span class="math inline">\((V, \langle \cdot,\cdot \rangle)\)</span> 为一个欧氏向量空间</strong>。</li></ul><p><strong>注1</strong>：本书将上述所有空间统称为“内积空间”。<br /><strong>注2</strong>：点积只是内积的一种特殊的定义，而非唯一。<br /><strong>注3</strong>，正定性是指<span class="math inline">\(\forall \boldsymbol{x} \in V \setminus \{\mathbf{0}\}:\quad \boldsymbol{x}^{\top} \boldsymbol{A} \boldsymbol{x} &gt; 0.\)</span>,而不是：<span class="math inline">\(\boldsymbol{x}^T\boldsymbol{A} \boldsymbol{y} &gt; 0.\)</span></p><h3 id="正定矩阵">3.3 正定矩阵</h3><blockquote><p>正定矩阵一定是对称的。《正定矩阵和半正定矩阵的区别.md》<br /><span class="math inline">\(A\)</span> 正定 <span class="math inline">\(\iff\)</span> 所有特征值 <span class="math inline">\(\lambda_i &gt; 0\)</span>。（故行列式也大于0）</p></blockquote><p><strong>对称正定矩阵</strong></p><p><strong><u><em>对称正定矩阵在机器学习中起着重要的作用，它们是通过内积定义的</em></u></strong>。在 4.3 节矩阵分解中将涉及到对称正定矩阵。对称半正定矩阵的思想也是机器学习中核技巧的关键（12.4 节）。</p><p>考虑一个 <span class="math inline">\(n\)</span> 维向量空间 <span class="math inline">\(V\)</span> 和内积 <span class="math display">\[\langle \cdot, \cdot \rangle : V \times V \to \mathbb{R}\]</span> （见定义 3.3），以及 <span class="math inline">\(V\)</span> 的有序基 <span class="math display">\[B = (\boldsymbol{b}_{1}, \ldots, \boldsymbol{b}_{n}).\]</span> 对于合适的 <span class="math inline">\(\psi_i, \lambda_j \in \mathbb{R}\)</span>，任何向量 <span class="math inline">\(\boldsymbol{x}, \boldsymbol{y} \in V\)</span> 都可以写成基向量的线性组合： <span class="math display">\[\boldsymbol{x} = \sum_{i=1}^{n} \psi_i \boldsymbol{b}_i \in V, \qquad \boldsymbol{y} = \sum_{j=1}^{n} \lambda_j \boldsymbol{b}_j \in V.\]</span></p><p>由于内积的双线性，对于所有 <span class="math inline">\(\boldsymbol{x}, \boldsymbol{y} \in V\)</span>，有： <span class="math display">\[\langle \boldsymbol{x}, \boldsymbol{y} \rangle= \left\langle \sum_{i=1}^{n} \psi_i \boldsymbol{b}_i,\ \sum_{j=1}^{n} \lambda_j \boldsymbol{b}_j \right\rangle= \sum_{i=1}^{n}\sum_{j=1}^{n} \psi_i \langle \boldsymbol{b}_i, \boldsymbol{b}_j \rangle \lambda_j= \hat{\boldsymbol{x}}^{\top} \boldsymbol{A} \hat{\boldsymbol{y}}.\]</span></p><p>以 <span class="math inline">\(n = 2\)</span> 为例，考虑内积： <span class="math display">\[\begin{aligned}\left\langle \sum_{i=1}^{2} \psi_i \boldsymbol{b}_i, \sum_{j=1}^{2} \lambda_j \boldsymbol{b}_j \right\rangle  &amp; = \left\langle \psi_1 \boldsymbol{b}_1 + \psi_2 \boldsymbol{b}_2,\ \lambda_1 \boldsymbol{b}_1 + \lambda_2 \boldsymbol{b}_2 \right\rangle \\&amp; = \psi_1 \left\langle \boldsymbol{b}_1, \lambda_1 \boldsymbol{b}_1 + \lambda_2 \boldsymbol{b}_2 \right\rangle  + \psi_2 \left\langle \boldsymbol{b}_2, \lambda_1 \boldsymbol{b}_1 + \lambda_2 \boldsymbol{b}_2 \right\rangle.  \end{aligned}\]</span></p><p>展开后得到： <span class="math display">\[\begin{aligned}&amp; \psi_1 [ \lambda_1 \langle \boldsymbol{b}_1, \boldsymbol{b}_1 \rangle + \lambda_2 \langle \boldsymbol{b}_1, \boldsymbol{b}_2 \rangle ]+ \psi_2 [ \lambda_1 \langle \boldsymbol{b}_2, \boldsymbol{b}_1 \rangle + \lambda_2 \langle \boldsymbol{b}_2, \boldsymbol{b}_2 \rangle ] \\&amp;= [\psi_1, \psi_2]\begin{bmatrix}\langle \boldsymbol{b}_1, \boldsymbol{b}_1 \rangle &amp; \langle \boldsymbol{b}_1, \boldsymbol{b}_2 \rangle \\\langle \boldsymbol{b}_2, \boldsymbol{b}_1 \rangle &amp; \langle \boldsymbol{b}_2, \boldsymbol{b}_2 \rangle\end{bmatrix}\begin{bmatrix}\lambda_1 \\ \lambda_2\end{bmatrix} \\&amp;= [\psi_1, \psi_2]\, \boldsymbol{A} \begin{bmatrix} \lambda_1 \\ \lambda_2 \end{bmatrix},\end{aligned}\]</span> 其中 <span class="math inline">\(A_{ij} := \langle \boldsymbol{b}_i, \boldsymbol{b}_j \rangle\)</span>，<span class="math inline">\(\hat{\boldsymbol{x}}, \hat{\boldsymbol{y}}\)</span> 为 <span class="math inline">\(\boldsymbol{x}, \boldsymbol{y}\)</span> 相对于基 <span class="math inline">\(B\)</span> 的坐标。这意味着内积 <span class="math inline">\(\langle \cdot, \cdot \rangle\)</span> 由 <span class="math inline">\(\boldsymbol{A}\)</span> 唯一确定。由于内积是对称的，矩阵 <span class="math inline">\(\boldsymbol{A}\)</span> 也是对称的。此外，内积的正定性意味着： <span class="math display">\[\forall \boldsymbol{x} \in V \setminus \{\mathbf{0}\}:\quad \boldsymbol{x}^{\top} \boldsymbol{A} \boldsymbol{x} &gt; 0.\]</span></p><p><strong>定义 3.4 对称正定矩阵</strong></p><p>一个对称矩阵 <span class="math inline">\(\boldsymbol{A} \in \mathbb{R}^{n \times n}\)</span> 称为 对称正定矩阵（symmetric, positive definite），如果它满足： <span class="math display">\[\forall \boldsymbol{x} \in V \setminus \{\mathbf{0}\} : \quad \boldsymbol{x}^{\top} \boldsymbol{A} \boldsymbol{x} &gt; 0.\]</span></p><p>如果只满足 <span class="math display">\[\forall \boldsymbol{x} \in V \setminus \{\mathbf{0}\} : \quad \boldsymbol{x}^{\top} \boldsymbol{A} \boldsymbol{x} \ge 0,\]</span> 则称 <span class="math inline">\(\boldsymbol{A}\)</span> 为对称半正定矩阵（symmetric, positive semidefinite）。</p><p><strong>例 3.4 对称正定矩阵</strong></p><p>考虑矩阵 <span class="math display">\[\boldsymbol{A}_1 = \begin{bmatrix} 9 &amp; 6 \\ 6 &amp; 5 \end{bmatrix}, \quad\boldsymbol{A}_2 = \begin{bmatrix} 9 &amp; 6 \\ 6 &amp; 3 \end{bmatrix}.\]</span></p><p>对于任意 <span class="math inline">\(\boldsymbol{x} = [x_1, x_2]^\top \neq \mathbf{0}\)</span>，有 <span class="math display">\[\begin{aligned} \boldsymbol{x}^{\top} \boldsymbol{A}_1 \boldsymbol{x} &amp; = [x_1, x_2] \begin{bmatrix} 9 &amp; 6 \\ 6 &amp; 5 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}  \\&amp; = 9 x_1^2 + 12 x_1 x_2 + 5 x_2^2  \\&amp;= (3 x_1 + 2 x_2)^2 + x_2^2 &gt; 0,\end{aligned}\]</span> 因此 <span class="math inline">\(\boldsymbol{A}_1\)</span> 是对称且正定的。而</p><p><span class="math display">\[\boldsymbol{x}^{\top} \boldsymbol{A}_2 \boldsymbol{x} = 9 x_1^2 + 12 x_1 x_2 + 3 x_2^2 = (3 x_1 + 2 x_2)^2 - x_2^2,\]</span> 当 <span class="math inline">\(\boldsymbol{x} = [2, -3]^\top\)</span> 时，<span class="math inline">\(\boldsymbol{x}^{\top} \boldsymbol{A}_2 \boldsymbol{x} &lt; 0\)</span>，所以 <span class="math inline">\(\boldsymbol{A}_2\)</span> 仅对称，但不是正定矩阵。</p><p><strong>如果 <span class="math inline">\(\boldsymbol{A} \in \mathbb{R}^{n \times n}\)</span> 是对称正定的，则可以定义内积：</strong> <span class="math display">\[\langle \boldsymbol{x}, \boldsymbol{y} \rangle = \hat{\boldsymbol{x}}^\top \boldsymbol{A} \hat{\boldsymbol{y}},\]</span> 其中 <span class="math inline">\(\hat{\boldsymbol{x}}, \hat{\boldsymbol{y}}\)</span> 为 <span class="math inline">\(\boldsymbol{x}, \boldsymbol{y}\)</span> 相对于有序基 <span class="math inline">\(B\)</span> 的坐标。</p><p><strong>定理 3.5 对称正定矩阵与内积</strong></p><p>设 <span class="math inline">\(V\)</span> 是一个实值、有限维向量空间，<span class="math inline">\(B\)</span> 是 <span class="math inline">\(V\)</span> 的一个有序基： <span class="math display">\[B = (\boldsymbol{b}_1, \ldots, \boldsymbol{b}_n),\]</span> 并且 <span class="math inline">\(\langle \cdot , \cdot \rangle : V \times V \to \mathbb{R}\)</span> 是 <span class="math inline">\(V\)</span> 上的一个内积。</p><p><strong>定理：</strong>内积 <span class="math inline">\(\langle \cdot , \cdot \rangle\)</span> 当且仅当存在一个对称正定矩阵 <span class="math inline">\(\boldsymbol{A} \in \mathbb{R}^{n \times n}\)</span>，使得对于任意 <span class="math inline">\(\boldsymbol{x}, \boldsymbol{y} \in V\)</span>，有： <span class="math display">\[\langle \boldsymbol{x}, \boldsymbol{y} \rangle = \hat{\boldsymbol{x}}^\top \boldsymbol{A} \hat{\boldsymbol{y}},\]</span> 其中 <span class="math inline">\(\hat{\boldsymbol{x}}, \hat{\boldsymbol{y}}\)</span> 为 <span class="math inline">\(\boldsymbol{x}, \boldsymbol{y}\)</span> 相对于基 <span class="math inline">\(B\)</span> 的坐标向量。</p><p><strong>如果 <span class="math inline">\(\boldsymbol{A}\)</span> 是对称正定矩阵，则它具有以下性质：</strong></p><ul><li><p>零空间（核）只包含零向量： 对于所有 <span class="math inline">\(\boldsymbol{x} \neq \mathbf{0}\)</span>，有 <span class="math display">\[    \boldsymbol{x}^\top \boldsymbol{A} \boldsymbol{x} &gt; 0,\]</span> ​ 这意味着如果 <span class="math inline">\(\boldsymbol{x} \neq \mathbf{0}\)</span>，则 <span class="math inline">\(\boldsymbol{A} \boldsymbol{x} \neq \mathbf{0}\)</span>。</p></li><li><p><strong>对角元素为正</strong>： 矩阵 <span class="math inline">\(\boldsymbol{A}\)</span> 的对角元素 <span class="math inline">\(a_{ii}\)</span> 满足 <span class="math display">\[a_{ii} = \boldsymbol{e}_i^\top \boldsymbol{A} \boldsymbol{e}_i &gt; 0,\]</span> 其中 <span class="math inline">\(\boldsymbol{e}_i\)</span> 是 <span class="math inline">\(\mathbb{R}^n\)</span> 的标准基向量</p></li></ul><h3 id="长度和距离">3.4 长度和距离</h3><p>在第3.1节中，我们已经讨论了可以用来计算向量长度的范数。<strong><u><em>内积与范数密切相关，因为任何内积都自然地引出范数</em></u></strong>： <span class="math display">\[\|\boldsymbol{x}\| := \sqrt{\langle \boldsymbol{x}, \boldsymbol{x} \rangle}\]</span> 这使得我们可以用内积来计算向量的长度。<strong><em><u>然而，并不是每一个范数都是由内积引起的。曼哈顿范数就是一种没有对应内积的范数</u></em></strong>。在下面，<strong>我们将集中讨论由内积导出的范数，并介绍一些几何概念，如长度、距离和角度。</strong></p><p><strong>柯西-施瓦兹不等式</strong></p><p>对于一个内积向量空间 <span class="math inline">\((V, \langle \cdot, \cdot \rangle)\)</span>，其引出的范数 <span class="math inline">\(\|\cdot\|\)</span> 满足<strong>柯西-施瓦兹不等式（Cauchy-Schwarz Inequality）</strong>： <span class="math display">\[|\langle \boldsymbol{x}, \boldsymbol{y} \rangle| \leqslant \|\boldsymbol{x}\| \, \|\boldsymbol{y}\|\]</span></p><h3 id="距离和度量">3.5 距离和度量</h3><p>考虑一个内积空间 <span class="math inline">\((V, \langle \cdot, \cdot \rangle)\)</span>，对于任意 <span class="math inline">\(\boldsymbol{x}, \boldsymbol{y} \in V\)</span>，定义： <span class="math display">\[d(\boldsymbol{x}, \boldsymbol{y}) := \|\boldsymbol{x} - \boldsymbol{y}\| = \sqrt{\langle \boldsymbol{x} - \boldsymbol{y}, \boldsymbol{x} - \boldsymbol{y} \rangle}\]</span> 该式称为 <span class="math inline">\(\boldsymbol{x}\)</span> 和 <span class="math inline">\(\boldsymbol{y}\)</span> 之间的<strong>距离（distance）</strong>。如果我们采用点积（dot product）作为内积，则该距离被称为<strong>欧几里得距离（Euclidean distance）</strong>。定义映射： <span class="math display">\[d : V \times V \to \mathbb{R}, \quad (\boldsymbol{x}, \boldsymbol{y}) \mapsto d(\boldsymbol{x}, \boldsymbol{y})\]</span> 称为该空间上的一个<strong>度量（metric）</strong>。 与向量的长度类似，向量之间的距离不一定需要依赖于内积：一般的范数已经足够用于定义距离。<strong>如果一个范数由内积引出，那么该范数导出的距离也依赖于所选用的内积，不同内积可能导致不同的距离定义。</strong></p><p>乍一看，内积和度量的一系列属性看起来非常相似。然而，通过比较定义3.3和定义3.6，我们发现 <span class="math inline">\(\langle \boldsymbol{x}, \boldsymbol{y} \rangle\)</span> 和 <span class="math inline">\(d(\boldsymbol{x}, \boldsymbol{y})\)</span> 的表现是<strong>相反的</strong>：</p><p>当 <span class="math inline">\(\boldsymbol{x}\)</span> 和 <span class="math inline">\(\boldsymbol{y}\)</span> 非常相似时，其内积值较大，而度量值却很小（因为度量涉及两个向量的差，即 <span class="math inline">\(\boldsymbol{x} - \boldsymbol{y}\)</span>）。</p><h3 id="角度和正交">3.6 角度和正交</h3><p>直观地说，<strong>两个向量之间的夹角告诉我们它们的<em><u>方向</u></em>有多相似。</strong>(个人注：和长度无关！)</p><p>两个向量 <span class="math inline">\(\boldsymbol{x}\)</span> 和 <span class="math inline">\(\boldsymbol{y}\)</span> 是<strong>正交（orthogonal）</strong>的，当且仅当 <span class="math display">\[\langle \boldsymbol{x}, \boldsymbol{y} \rangle = 0,\]</span> 写作 <span class="math inline">\(\boldsymbol{x} \perp \boldsymbol{y}\)</span>。若 <span class="math inline">\(\|\boldsymbol{x}\| = 1 = \|\boldsymbol{y}\|\)</span>，即两个向量都是单位向量，则称 <span class="math inline">\(\boldsymbol{x}\)</span> 和 <span class="math inline">\(\boldsymbol{y}\)</span> 为<strong>标准正交（orthonormal）</strong>的。</p><p><strong>两个向量 <span class="math inline">\(\boldsymbol{x}\)</span> 和 <span class="math inline">\(\boldsymbol{y}\)</span> 之间的角度 <span class="math inline">\(\omega\)</span> 取决于所采用的内积。</strong>关于一种内积正交的向量不一定关于其他内积正交。</p><p><strong>夹角的定义：</strong>两个非零向量 <span class="math inline">\(\boldsymbol{x}\)</span> 和 <span class="math inline">\(\boldsymbol{y}\)</span> 之间的夹角 <span class="math inline">\(\omega\)</span> 满足： <span class="math display">\[\cos \omega = \frac{\langle \boldsymbol{x}, \boldsymbol{y} \rangle}{\|\boldsymbol{x}\| \, \|\boldsymbol{y}\|}\]</span></p><h3 id="正交矩阵">3.7 正交矩阵</h3><p>设<strong><u><em>方阵</em></u></strong> <span class="math inline">\(\boldsymbol{A} \in \mathbb{R}^{n \times n}\)</span>。当且仅当 <span class="math inline">\(\boldsymbol{A}\)</span> 的<strong>列向量构成一组<em><u>标准正交</u></em></strong>（orthonormal）的向量组时，<span class="math inline">\(\boldsymbol{A}\)</span> 被称为正交矩阵（orthogonal matrix）。</p><p>换句话说，<span class="math inline">\(\boldsymbol{A}\)</span> 是正交矩阵当且仅当满足： <span class="math display">\[\boldsymbol{A} \boldsymbol{A}^{\top} = \boldsymbol{I} = \boldsymbol{A}^{\top} \boldsymbol{A}\]</span> <strong>正交矩阵保持长度不变</strong>；正交矩阵变换是特殊的，因为当一个向量 <span class="math inline">\(\boldsymbol{x}\)</span> 被<strong>正交矩阵 <span class="math inline">\(\boldsymbol{A}\)</span> 变换后，其长度保持不变</strong>。以点积为内积时，可以如下推导： <span class="math display">\[\|\boldsymbol{A} \boldsymbol{x}\|^{2} = (\boldsymbol{A} \boldsymbol{x})^{\top} (\boldsymbol{A} \boldsymbol{x}) = \boldsymbol{x}^{\top} \boldsymbol{A}^{\top} \boldsymbol{A} \boldsymbol{x} = \boldsymbol{x}^{\top} \boldsymbol{I} \boldsymbol{x} = \boldsymbol{x}^{\top} \boldsymbol{x} = \|\boldsymbol{x}\|^{2}\]</span> 因此，<strong><em><u>正交变换</u></em>不会改变向量的长度或两向量之间的角度（即它<em><u>保持内积结构</u></em>）</strong>，这使得正交矩阵广泛应用于几何变换、图像旋转、特征值分解等领域。<strong>这表明正交矩阵定义的变换是旋转（也可能是翻转）</strong>.</p><p><strong>正交补</strong>：<strong>三维向量空间中的平面<span class="math inline">\(U\)</span>可以由它的法向量来描述</strong>。法向量张成某子空间 <span class="math inline">\(U\)</span> 的正交补 <span class="math inline">\(U^\perp\)</span>;在 <span class="math inline">\(n\)</span> 维向量空间和仿射空间中，通常可以用正交补来描述超平面。</p><h3 id="函数的内积">3.8 函数的内积</h3><p>设两个函数 <span class="math inline">\(\boldsymbol{u}: \mathbb{R} \rightarrow \mathbb{R}\)</span> 与 <span class="math inline">\(\boldsymbol{v}: \mathbb{R} \rightarrow \mathbb{R}\)</span>，则它们在区间 <span class="math inline">\([a, b]\)</span> 上的内积可以定义为： <span class="math display">\[\langle u, v \rangle := \int_{a}^{b} u(x)\, v(x)\, dx, \quad \text{其中 } a, b &lt; \infty\]</span> 与一般的内积一样，我们可以使用这个定义来引出函数的范数和正交性。特别地，若<span class="math inline">\(\langle u, v \rangle = 0,\)</span>则称函数 <span class="math inline">\(u\)</span> 和 <span class="math inline">\(v\)</span> 在该内积下是<strong>正交的</strong>。</p><p><strong>一个例子：</strong> <span class="math inline">\(\sin(x)\)</span> 与 <span class="math inline">\(\cos(x)\)</span> 的正交性</p><p>若取函数 <span class="math inline">\(u(x) = \sin(x)\)</span>，<span class="math inline">\(v(x) = \cos(x)\)</span>，则 <span class="math display">\[f(x) = u(x) v(x) = \sin(x) \cos(x)\]</span> 如图 3.8 所示，该函数是一个奇函数，即满足：<span class="math inline">\(f(-x) = -f(x)\)</span> 因此，在对称区间 <span class="math inline">\([a, b] = [-\pi, \pi]\)</span> 上，其积分为 0，即： <span class="math display">\[\int_{-\pi}^{\pi} \sin(x) \cos(x)\, dx = 0\]</span> 由此可知，<span class="math inline">\(\sin(x)\)</span> 与 <span class="math inline">\(\cos(x)\)</span> 是正交函数。</p><p>如果积分区间为 <span class="math inline">\([-\pi, \pi]\)</span>，则下列函数集： <span class="math display">\[\{1, \cos(x), \cos(2x), \cos(3x), \ldots\}\]</span> 构成一个正交函数集。也就是说，集合中任意两个不同的函数在该区间上的内积为 0。</p><p>该集合张成了一个函数的巨大子空间。这个子空间中的函数在 <span class="math inline">\([-\pi, \pi)\)</span> 上是偶函数且具有周期性。<strong>将任意函数投影到这个子空间，是傅里叶级数（Fourier series）展开的核心思想。</strong>(<strong>备注：傅里叶级数中的正交函数集</strong>)</p><h3 id="正交投影">3.9 正交投影</h3><p>投影是一类重要的线性变换（还有旋转和反射）。</p><p><strong>投影</strong>（Projection）</p><p>设 <span class="math inline">\(V\)</span> 为一个向量空间，<span class="math inline">\(U\subseteq V\)</span>为 <span class="math inline">\(V\)</span> 的一个子空间。如果一个线性映射 <span class="math inline">\(\pi: V \rightarrow U\)</span> 满足： <span class="math display">\[\pi^2 = \pi \circ \pi = \pi,\]</span> 则称 <span class="math inline">\(\pi\)</span> 是一个从 <span class="math inline">\(V\)</span> 到 <span class="math inline">\(U\)</span> 的<strong>投影（projection）</strong>。</p><p>由于<strong>线性映射可以由矩阵表示</strong>，因此上述定义也适用于一类特殊的矩阵，这类矩阵被称为<strong>投影矩阵（projection matrix）</strong>，记作 <span class="math inline">\(\boldsymbol{P}_\pi\)</span>，它满足： <span class="math display">\[\boldsymbol{P}_\pi^2 = \boldsymbol{P}_\pi\]</span> 也就是说，<strong>投影矩阵是满足幂等性质（idempotent）的线性变换矩阵</strong>。<strong>投影 <span class="math inline">\(\pi_{U}(\boldsymbol{x}) \in \mathbb{R}^{n}\)</span> 仍然是 <span class="math inline">\(n\)</span> 维向量而不是标量。</strong>我们可以用张成子空间 <span class="math inline">\(U\)</span> 的基向量 <span class="math inline">\(\boldsymbol{b}\)</span> 来表示投影，这样我们就只需要一个坐标 <span class="math inline">\(\lambda\)</span> 来表示投影(针对一维子空间（线）上的投影)，而不再需要 <span class="math inline">\(n\)</span> 个坐标。在第四章矩阵分解中，我们将展示 <span class="math inline">\(\pi_{U}(\boldsymbol{x})\)</span> 是 <span class="math inline">\(\boldsymbol{P}_{\pi}\)</span> 的特征向量，对应的特征值为 <span class="math inline">\(1\)</span>。</p><p>通过投影，我们可以近似求解无解的线性方程组 <span class="math inline">\(\boldsymbol{A} \boldsymbol{x} = \boldsymbol{b}\)</span>。线性方程组无解，意味着 <span class="math inline">\(\boldsymbol{b}\)</span> 不在 <span class="math inline">\(\boldsymbol{A}\)</span> 的张成空间中，也就是说，向量 <span class="math inline">\(\boldsymbol{b}\)</span> 不在 <span class="math inline">\(\boldsymbol{A}\)</span> 的列所张成的子空间内。<strong>具体的原理见资料《线性方程解的本质.md》文件。</strong>如果线性方程不能精确求解，那么我们可以尝试找到一个近似解（approximate solution）。其思想是在 <span class="math inline">\(\boldsymbol{A}\)</span> 的列所张成的子空间中找到最接近 <span class="math inline">\(\boldsymbol{b}\)</span> 的向量，即计算 <span class="math inline">\(\boldsymbol{b}\)</span> 在 <span class="math inline">\(\boldsymbol{A}\)</span> 的列所张成的子空间上的正交投影。这类问题在实践中经常出现，这个解叫做超定系统的最小二乘解（least-squares solution）（假设点积为内积）。</p><h3 id="旋转">3.10 旋转</h3><p>旋转(rotation)是一种线性映射（更具体地说，是欧氏向量空间的自同构），它将平面绕原点旋转<span class="math inline">\(θ\)</span>角，即原点是一个不动点。</p><p><span class="math inline">\(\mathbb{R}^{2}\)</span> 中，旋转使物体绕平面内的一个原点旋转。如果旋转角度是正的，我们就称为逆时针旋转。</p><p>在 <span class="math inline">\(\mathbb{R}^{3}\)</span> 中的旋转与 <span class="math inline">\(\mathbb{R}^{2}\)</span> 不同的是，在 <span class="math inline">\(\mathbb{R}^{3}\)</span> 中，我们可以<strong>围绕其中一维的轴旋转</strong>任何二维平面。确定通用旋转矩阵的最简单方法是确定标准基 <span class="math inline">\(e_1, e_2, e_3\)</span> 旋转得到的像，并确保这些像 <span class="math inline">\(\boldsymbol{R e}_1, \boldsymbol{R e}_2, \boldsymbol{R e}_3\)</span> 彼此正交。然后，我们可以通过组合标准基的像得到一个通用的旋转矩阵 <span class="math inline">\(\boldsymbol{R}\)</span></p><p><span class="math inline">\(\mathbb{R}^{n}\)</span> 中的旋转：</p><p>从二维和三维推广到 <span class="math inline">\(n\)</span> 维的欧氏向量空间的旋转可以直观地描述为：固定其 <span class="math inline">\(n-2\)</span> 维，旋转 <span class="math inline">\(\mathbb{R}^{n}\)</span> 空间中的二维平面。就像在三维情况下，我们可以旋转任意平面（<span class="math inline">\(\mathbb{R}^{n}\)</span> 的二维子空间）。</p><p><strong>旋转的性质</strong>:</p><p><strong>旋转表现出许多有用的性质，这些性质可以通过将它们视为正交矩阵来说明</strong>（定义 3.8）：</p><ul><li>旋转保持距离，即</li></ul><p><span class="math display">\[    \|\boldsymbol{x} - \boldsymbol{y}\| = \left\| \boldsymbol{R}_{\theta}(\boldsymbol{x}) - \boldsymbol{R}_{\theta}(\boldsymbol{y}) \right\|\]</span></p><p>​ 换句话说，任意两点经过旋转变换后，它们之间的距离保持不变。</p><ul><li><p>旋转保持角度，即 <span class="math inline">\(\boldsymbol{R}_{\theta} \boldsymbol{(x)}\)</span> 和 <span class="math inline">\(\boldsymbol{R}_{\theta} \boldsymbol{(y)}\)</span> 之间的夹角与原始向量 <span class="math inline">\(\boldsymbol{x}\)</span> 和 <span class="math inline">\(\boldsymbol{y}\)</span> 之间的夹角相同。</p></li><li><p>三维（或更高维）旋转通常是不可交换的。因此，应用旋转的顺序是重要的，即使它们是围绕同一点旋转。</p><p>在二维空间中，旋转是可交换的。也就是说，对于所有 <span class="math inline">\(\phi, \theta \in [0, 2\pi)\)</span>，都有： <span class="math display">\[\boldsymbol{R}(\phi)\boldsymbol{R}(\theta) = \boldsymbol{R}(\theta)\boldsymbol{R}(\phi)\]</span> 当且仅当它们围绕同一个点（例如原点）旋转时，二维旋转矩阵形成一个关于乘法的阿贝尔群。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 数学 </tag>
            
            <tag> 算法 </tag>
            
            <tag> 统计 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《机器学习的数学基础》</title>
      <link href="/2025/10/27/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%9B%9B/"/>
      <url>/2025/10/27/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%9B%9B/</url>
      
        <content type="html"><![CDATA[<h2 id="矩阵分解">4、矩阵分解</h2><p>Matrix Decomposition</p><p>在本章中，我们将介绍矩阵的三个方面：如何概括矩阵，如何分解矩阵，以及<strong>如何利用矩阵分解进行矩阵近似</strong>，<strong>矩阵分解的一个类比是因数分解</strong>(factoring of numbers)。</p><span id="more"></span><h3 id="行列式与迹">4.1 行列式与迹</h3><ul><li><p><strong>行列式只适用于方阵</strong>。</p></li><li><p>可以看作是将矩阵映射到一个实数的函数。</p></li><li><p>当且仅当行列式不等于0时，矩阵可逆。</p></li><li><p><strong>上三角（下三角）矩阵的行列式是其对角元素的积。</strong></p></li><li><p>行列式度量体积；列向量代表各条边。</p></li><li><p>一个方阵 <span class="math inline">\(\boldsymbol{A} \in \mathbb{R}^{n \times n}\)</span> 的行列式 <span class="math inline">\(\operatorname{det}(\boldsymbol{A}) \neq 0\)</span>，当且仅当 <span class="math inline">\(\operatorname{rk}(\boldsymbol{A}) = n.\)</span>换句话说，<span class="math inline">\(\boldsymbol{A}\)</span> 可逆当且仅当它是满秩的</p></li></ul><blockquote><p>为什么：行列式只适用于方阵。<br />行列式衡量方阵线性变换的体积(面积、体积、n维体积)对应的变化，而非方阵既不构成同维线性变换，也不满足代数定义，并且可逆只针对方阵有意义。</p></blockquote><p>对于 <span class="math inline">\(n \times n\)</span> 矩阵的行列式，需要一个通用的算法来解决 <span class="math inline">\(n &gt; 3\)</span> 的情况，我们将在下面探讨这个问题。定理 4.2 将计算 <span class="math inline">\(n \times n\)</span> 矩阵的行列式的问题简化为计算 <span class="math inline">\((n - 1) \times (n - 1)\)</span> 矩阵的行列式。通过递归地应用<strong>拉普拉斯展开（Laplace Expansion）</strong>，我们最终可以通过计算 <span class="math inline">\(2 \times 2\)</span> 矩阵的行列式来计算 <span class="math inline">\(n \times n\)</span> 矩阵的行列式。</p><p><strong>行列式具有以下特性</strong>：</p><ul><li>矩阵相乘的行列式是相应行列式的乘积，即</li></ul><p><span class="math display">\[\operatorname{det}(\boldsymbol{AB}) = \operatorname{det}(\boldsymbol{A}) \operatorname{det}(\boldsymbol{B})\]</span></p><ul><li>矩阵转置不改变行列式的值，即</li></ul><p><span class="math display">\[\operatorname{det}(\boldsymbol{A}) = \operatorname{det}(\boldsymbol{A}^{\top})\]</span></p><ul><li>如果 <span class="math inline">\(\boldsymbol{A}\)</span> 是正则的（可逆的），</li></ul><p><span class="math display">\[\operatorname{det}(\boldsymbol{A}^{-1}) = \frac{1}{\operatorname{det}(\boldsymbol{A})}\]</span></p><ul><li><p><strong>相似矩阵（定义 2.22）具有相同的行列式。因此，对于线性映射 <span class="math inline">\(\Phi: V \rightarrow V\)</span>，<span class="math inline">\(\Phi\)</span> 的所有变换矩阵 <span class="math inline">\(\boldsymbol{A}_\Phi\)</span> 都具有相同的行列式。因此，线性映射基的选择不影响行列式的值。</strong>（注意前提是<span class="math inline">\(V \rightarrow V\)</span>）</p></li><li><p>将一列/行的倍数加到另一列/行不会更改 <span class="math inline">\(\operatorname{det}(\boldsymbol{A})\)</span>。</p></li><li><p>用标量 <span class="math inline">\(\lambda \in \mathbb{R}\)</span> 乘矩阵 <span class="math inline">\(\boldsymbol{A}\)</span> 的行/列，将 <span class="math inline">\(\lambda\)</span> 倍缩放 <span class="math inline">\(\operatorname{det}(\boldsymbol{A})\)</span>。特别地，</p></li></ul><p><span class="math display">\[\operatorname{det}(\lambda \boldsymbol{A}) = \lambda^n \operatorname{det}(\boldsymbol{A})\]</span></p><ul><li>交换两行/列将更改 <span class="math inline">\(\operatorname{det}(\boldsymbol{A})\)</span> 的符号。</li></ul><p><strong>迹</strong></p><p>方阵 <span class="math inline">\(\boldsymbol{A} \in \mathbb{R}^{n \times n}\)</span> 的<strong>迹</strong>（trace）定义为：</p><p><span class="math display">\[\operatorname{tr}(\boldsymbol{A}) := \sum_{i=1}^{n} a_{ii}\]</span></p><p>即，迹是 <span class="math inline">\(\boldsymbol{A}\)</span> 的对角元素之和。</p><p>迹满足以下属性：</p><ol type="1"><li>对于 <span class="math inline">\(\boldsymbol{A}, \boldsymbol{B} \in \mathbb{R}^{n \times n}\)</span>，有：</li></ol><p><span class="math display">\[tr⁡(A+B)== \operatorname{tr}(\boldsymbol{A}) + \operatorname{tr}(\boldsymbol{B})\]</span></p><ol start="2" type="1"><li>对于 <span class="math inline">\(\boldsymbol{A} \in \mathbb{R}^{n \times n}\)</span>，<span class="math inline">\(\alpha \in \mathbb{R}\)</span>，有：</li></ol><p><span class="math display">\[tr⁡(αA) = \alpha \operatorname{tr}({A})\]</span></p><ol start="3" type="1"><li>单位矩阵的迹为：</li></ol><p><span class="math display">\[tr⁡(In) = n\]</span></p><ol start="4" type="1"><li>对于 <span class="math inline">\(\boldsymbol{A} \in \mathbb{R}^{n \times k}\)</span>，<span class="math inline">\(\boldsymbol{B} \in \mathbb{R}^{k \times n}\)</span>，有：</li></ol><p><span class="math display">\[tr⁡(AB)= \operatorname{tr}({B}{A})\]</span></p><p>可以证明，<strong>只有一个函数同时满足这四个性质——即迹</strong>（Gohberg et al., 2012）。</p><blockquote><p><strong>线性映射的矩阵表示依赖于基，而线性映射<span class="math inline">\(\phi\)</span>的迹独立于基。 </strong> 具体原理见资料《迹的相似不变性.md》</p></blockquote><p><strong>特征多项式</strong>:</p><p>将行列式和迹作为描述方阵的函数来讨论。</p><p>对于 <span class="math inline">\(\lambda \in \mathbb{R}\)</span> 和一个方阵 <span class="math inline">\(\boldsymbol{A} \in \mathbb{R}^{n \times n}\)</span>，其特征多项式（Characteristic Polynomial）定义为：</p><p><span class="math display">\[p_A(\lambda) := \det(\boldsymbol{A} - \lambda \boldsymbol{I}) = c_0 + c_1 \lambda + c_2 \lambda^2 + \cdots + c_{n-1} \lambda^{n-1} + (-1)^n \lambda^n\]</span></p><p>其中 <span class="math inline">\(c_0, \ldots, c_{n-1} \in \mathbb{R}\)</span>。特别地，</p><p><span class="math display">\[c_0 = \det(\boldsymbol{A}), \quad c_{n-1} = (-1)^{n-1} \operatorname{tr}(\boldsymbol{A})\]</span></p><p>特征多项式将允许我们计算特征值和特征向量。</p><h3 id="特征值和特征向量">4.2 特征值和特征向量</h3><p>指向同一方向的两个向量称为共向的(codirected)。如果两个向量指向相同或相反的方向，则它们是共线的（collinear）。</p><p>令 <span class="math inline">\(A \in \mathbb{R}^{n \times n}\)</span> 为一个<u><strong>方阵</strong></u>。</p><p><span class="math display">\[\boldsymbol{A}\boldsymbol{x} = \lambda \boldsymbol{x}\]</span></p><p>我们称这个方程为特征方程（eigenvalue equation）。其中 <span class="math inline">\(\lambda \in \mathbb{R}\)</span> 为 <span class="math inline">\(\boldsymbol{A}\)</span> 的特征值（eigenvalue），<span class="math inline">\(\boldsymbol{x} \in \mathbb{R}^{n} \setminus \{\boldsymbol{0}\}\)</span> 为相应的特征向量（eigenvector）。</p><p>以下说法是等效的：</p><ol type="1"><li><span class="math inline">\(\lambda\)</span> 是 <span class="math inline">\(\boldsymbol{A} \in \mathbb{R}^{n \times n}\)</span> 的特征值。<br /></li><li>存在 <span class="math inline">\(\boldsymbol{x} \in \mathbb{R}^{n} \backslash \{\mathbf{0}\}\)</span>，使得 <span class="math inline">\(\boldsymbol{A}\boldsymbol{x} = \lambda \boldsymbol{x},\)</span> 或等价地 <span class="math inline">\((\boldsymbol{A} - \lambda \boldsymbol{I}_{n}) \boldsymbol{x} = \mathbf{0}\)</span> 有非平凡解，即 <span class="math inline">\(\boldsymbol{x} \neq \mathbf{0}\)</span>。<br /></li><li><span class="math inline">\(\operatorname{rk}\!\left(\boldsymbol{A} - \lambda \boldsymbol{I}_{n}\right) &lt; n\)</span><br /></li><li><span class="math inline">\(\det\!\left(\boldsymbol{A} - \lambda \boldsymbol{I}_{n}\right) = 0\)</span></li></ol><p><strong>特征空间和特征谱</strong>:</p><p>对于 <span class="math inline">\(A \in \mathbb{R}^{n \times n}\)</span>，<span class="math inline">\(\boldsymbol{A}\)</span> 对应于特征值 <span class="math inline">\(\lambda\)</span> 的特征向量集合张成 <span class="math inline">\(\mathbb{R}^{n}\)</span> 的子空间，这个子空间称为 <span class="math inline">\(\boldsymbol{A}\)</span> 关于 <span class="math inline">\(\lambda\)</span> 的特征空间（eigenspace），记为：<span class="math inline">\(E_{\lambda}\)</span> ; <strong><span class="math inline">\(\boldsymbol{A}\)</span> 的特征值构成的集合称为特征谱（eigenspectrum），或 <span class="math inline">\(\boldsymbol{A}\)</span> 的谱。</strong></p><blockquote><p><strong>特征值为 0 的特征向量是矩阵零空间中的非零向量；它揭示了矩阵在某些方向上把向量“压扁”为零；</strong></p></blockquote><p>单位矩阵 <span class="math inline">\(\boldsymbol{I} \in \mathbb{R}^{n \times n}\)</span> 的特征方程为：</p><p><span class="math display">\[P_{I}(\lambda) = \operatorname{det}(\boldsymbol{I} - \lambda \boldsymbol{I}) = (1 - \lambda)^n = 0\]</span></p><p>它只拥有 <span class="math inline">\(\lambda = 1\)</span> 这个特征值，并出现 $ n $ 次。而且，对于任意 <span class="math inline">\(\boldsymbol{x} \in \mathbb{R}^{n} \setminus \{\boldsymbol{0}\}\)</span>，有：</p><p><span class="math display">\[\boldsymbol{I} \boldsymbol{x} = \lambda \boldsymbol{x} = 1 \cdot \boldsymbol{x}\]</span></p><p>因此，单位矩阵的唯一特征空间 <span class="math inline">\(E_1\)</span> 张成 <span class="math inline">\(n\)</span> 维，<span class="math inline">\(\mathbb{R}^{n}\)</span> 的 <span class="math inline">\(n\)</span> 个标准基向量都是 <span class="math inline">\(\boldsymbol{I}\)</span> 的特征向量。</p><p><strong>关于特征值和特征向量的常用特性包括</strong>：</p><ul><li><p>矩阵 <span class="math inline">\(\boldsymbol{A}\)</span> 及其转置 <span class="math inline">\(\boldsymbol{A}^{\top}\)</span> 具有相同的特征值，但不一定具有相同的特征向量。</p></li><li><p>特征空间 <span class="math inline">\(E_{\lambda}\)</span> 是 <span class="math inline">\(\boldsymbol{A} - \lambda \boldsymbol{I}\)</span> 的零空间，因为：</p></li></ul><p><span class="math display">\[  \boldsymbol{A} \boldsymbol{x} = \lambda \boldsymbol{x}   \Longleftrightarrow   \boldsymbol{A} \boldsymbol{x} - \lambda \boldsymbol{x} = \boldsymbol{0}   \Longleftrightarrow   (\boldsymbol{A} - \lambda \boldsymbol{I}) \boldsymbol{x} = \boldsymbol{0}   \Longleftrightarrow   \boldsymbol{x} \in \ker(\boldsymbol{A} - \lambda \boldsymbol{I})\]</span></p><ul><li><p>相似矩阵（定义 2.22）具有相同的特征值。基变换得到的是相似矩阵。因此，<strong>线性映射 <span class="math inline">\(\Phi\)</span> 的特征值与它的变换矩阵的基的选择无关。这使得特征值、行列式和迹成为线性映射的重要不变量。</strong></p></li><li><p>对称正定矩阵总是有正的实特征值。</p></li></ul><blockquote><p><strong>剪切映射</strong>：</p><p><strong>剪切映射（Shearing Mapping）</strong> 是一种保持某些几何特征（如面积、体积）不变，但改变形状的线性变换。在二维或三维欧几里得空间中，它通过“滑动”坐标轴方向的某些分量，使图形在不旋转也不缩放的前提下发生<strong>倾斜变形</strong>。</p><p><strong>几何直观</strong>：</p><ul><li>剪切会把一个<strong>矩形变成平行四边形</strong>。</li><li>面积保持不变。</li><li>角度和形状会改变，但线之间的平行关系保持。</li><li>单位正方形在剪切后变成平行四边形，但底边长度不变。</li></ul><p>详见《剪切映射的变换矩阵.md》</p></blockquote><p>一个矩阵 <span class="math inline">\(\boldsymbol{A} \in \mathbb{R}^{n \times n}\)</span> 的 <span class="math inline">\(n\)</span> 个特征向量 <span class="math inline">\(\boldsymbol{x}_{1}, \ldots, \boldsymbol{x}_{n}\)</span> 对应 <span class="math inline">\(n\)</span> 个不同的特征值 <span class="math inline">\(\lambda_{1}, \ldots, \lambda_{n}\)</span>，则它们是线性独立的。这个定理指出，具有 <span class="math inline">\(n\)</span> 个不同特征值的矩阵的特征向量构成 <span class="math inline">\(\mathbb{R}^{n}\)</span> 的一组基。</p><p>给定一个矩阵 <span class="math inline">\(\boldsymbol{A} \in \mathbb{R}^{m \times n}\)</span>，我们总能通过<span class="math inline">\(\boldsymbol{S} := \boldsymbol{A}^{\top} \boldsymbol{A}\)</span> 得到一个<strong>对称的半正定矩阵</strong><span class="math inline">\(\boldsymbol{S} \in \mathbb{R}^{n \times n}\)</span>。</p><blockquote><p>很多地方只需要半正定矩阵即可。</p></blockquote><h3 id="谱定理">4.3 谱定理</h3><p>Spectral Theorem</p><p>如果 <span class="math inline">\(\boldsymbol{A} \in \mathbb{R}^{n \times n}\)</span> 是<strong>对称的</strong>，则存在 <span class="math inline">\(\boldsymbol{A}\)</span> 的特征向量组成向量空间 <span class="math inline">\(V\)</span> 的一个正交基，且每个特征值都是实数。</p><p>谱定理的直接含义是：对称矩阵 <span class="math inline">\(\boldsymbol{A}\)</span> 存在特征值分解（具有实特征值），并且我们可以由特征向量构造一个标准正交基，使得 <span class="math display">\[\boldsymbol{A} = \boldsymbol{P} \boldsymbol{D} \boldsymbol{P}^{\top},\]</span> 其中 <span class="math inline">\(\boldsymbol{D}\)</span> 是对角矩阵，<span class="math inline">\(\boldsymbol{P}\)</span> 的列是 <span class="math inline">\(\boldsymbol{A}\)</span> 的单位特征向量，满足 <span class="math inline">\(\boldsymbol{P}^{\top} \boldsymbol{P} = \boldsymbol{I}\)</span>。</p><h3 id="特征值特征向量行列式迹的联系">4.4 特征值、特征向量、行列式、迹的联系</h3><p><strong>定理</strong> 4.16：</p><p>设矩阵 <span class="math inline">\(\boldsymbol{A} \in \mathbb{R}^{n \times n}\)</span>，则其行列式等于其所有特征值的乘积，即 <span class="math display">\[\det(\boldsymbol{A}) = \prod_{i=1}^{n} \lambda_i\]</span> 其中 <span class="math inline">\(\lambda_i \in \mathbb{C}\)</span>（可重复）为矩阵 <span class="math inline">\(\boldsymbol{A}\)</span> 的特征值。</p><p><strong>定理</strong> 4.17：</p><p>设矩阵 <span class="math inline">\(\boldsymbol{A} \in \mathbb{R}^{n \times n}\)</span>，则其迹等于其所有特征值的和，即 <span class="math display">\[\operatorname{tr}(\boldsymbol{A}) = \sum_{i=1}^{n} \lambda_i\]</span> 其中 <span class="math inline">\(\lambda_i \in \mathbb{C}\)</span>（可重复）为矩阵 <span class="math inline">\(\boldsymbol{A}\)</span> 的特征值。</p><h3 id="矩阵分解-1">4.5 矩阵分解</h3><p>Matrix Decomposition</p><h4 id="cholesky分解">4.5.1 Cholesky分解</h4><p><strong>对称正定矩阵</strong> <span class="math inline">\(\boldsymbol{A}\)</span> 可以分解为： <span class="math display">\[\boldsymbol{A} = \boldsymbol{L} \boldsymbol{L}^{\top},\]</span> 其中 <span class="math inline">\(\boldsymbol{L}\)</span> 是具有正对角元素的下三角矩阵。 <span class="math inline">\(\boldsymbol{L}\)</span> 被称为 <span class="math inline">\(\boldsymbol{A}\)</span> 的Cholesky 因子（Cholesky factor），且是唯一的。</p><p><strong>对称正定矩阵的类平方根运算，即Cholesky分解。</strong></p><h4 id="特征值分解与对角化">4.5.2 特征值分解与对角化</h4><p>矩阵 <span class="math inline">\(\boldsymbol{A} \in \mathbb{R}^{n \times n}\)</span> 的对角化，实际是在另一个基中表示相同线性映射的一种方法，这个基由 <span class="math inline">\(\boldsymbol{A}\)</span> 的特征向量组成。</p><p>定理 4.20<strong>特征值分解</strong>，Eigendecomposition</p><p>设 <span class="math inline">\(\boldsymbol{A} \in \mathbb{R}^{n \times n}\)</span>，则当且仅当 <span class="math inline">\(\boldsymbol{A}\)</span> 有 <span class="math inline">\(n\)</span> 个线性无关的特征向量时，<span class="math inline">\(\boldsymbol{A}\)</span> 可以被对角化，即存在可逆矩阵 <span class="math inline">\(\boldsymbol{P}\)</span> 和对角矩阵 <span class="math inline">\(\boldsymbol{D}\)</span>，使得： <span class="math display">\[\boldsymbol{A} = \boldsymbol{P} \boldsymbol{D} \boldsymbol{P}^{-1}\]</span></p><p>其中：<span class="math inline">\(\boldsymbol{P} \in \mathbb{R}^{n \times n}\)</span> 的列是 <span class="math inline">\(\boldsymbol{A}\)</span> 的 <span class="math inline">\(n\)</span> 个线性无关的特征向量；<span class="math inline">\(\boldsymbol{D}\)</span> 是对角矩阵，其对角元素为 <span class="math inline">\(\boldsymbol{A}\)</span> 的特征值（可重复）；因此，只有当 <span class="math inline">\(\boldsymbol{A}\)</span> 的特征向量张成 <span class="math inline">\(\mathbb{R}^n\)</span>，即 <span class="math inline">\(\boldsymbol{A}\)</span> 是非亏损的，<span class="math inline">\(\boldsymbol{A}\)</span> 才可以对角化。</p><p>定理 4.21<strong><em><u>对称矩阵</u></em>的特征值分解</strong></p><p>设 <span class="math inline">\(\boldsymbol{S} \in \mathbb{R}^{n \times n}\)</span> 是对称矩阵（即 <span class="math inline">\(\boldsymbol{S}^{\top} = \boldsymbol{S}\)</span>），则 <span class="math inline">\(\boldsymbol{S}\)</span> 一定可以被对角化。</p><p>根据谱定理，对于任意对称矩阵 <span class="math inline">\(\boldsymbol{S}\)</span>，存在一个标准正交基，由 <span class="math inline">\(\boldsymbol{S}\)</span> 的特征向量构成。即存在正交矩阵 <span class="math inline">\(\boldsymbol{P}\)</span>，使得：</p><p><span class="math display">\[\boldsymbol{S} = \boldsymbol{P} \boldsymbol{D} \boldsymbol{P}^\top\]</span></p><p>其中：</p><ul><li><span class="math inline">\(\boldsymbol{P} \in \mathbb{R}^{n \times n}\)</span> 是正交矩阵，列向量为单位正交的特征向量；</li><li><span class="math inline">\(\boldsymbol{D}\)</span> 是对角矩阵，对角元素为 <span class="math inline">\(\boldsymbol{S}\)</span> 的特征值；</li><li>此分解方式也可表示为 <span class="math inline">\(\boldsymbol{D} = \boldsymbol{P}^\top \boldsymbol{S} \boldsymbol{P}\)</span>。</li></ul><p><img src="/img3/机器学习的数学基础Part1/图解特征值分解.png" alt="图解特征值分解" style="zoom:50%;" /></p><blockquote><p><strong>在相同的向量空间中应用相同的基变化，然后撤消</strong>。</p></blockquote><h4 id="奇异值分解">4.5.3 奇异值分解</h4><p>singular value decomposition, SVD</p><p>表示线性映射 <span class="math inline">\(\Phi: V \rightarrow W\)</span> 的矩阵 <span class="math inline">\(\boldsymbol{A}\)</span> 的奇异值分解量化了这两个向量空间的潜在几何变化。</p><p><strong>奇异值分解（SVD）公式</strong></p><p><span class="math display">\[\boldsymbol{A} = \boldsymbol{U} \boldsymbol{\Sigma} \boldsymbol{V}^\top\]</span></p><p>以及它在几何上的意义（旋转+缩放+旋转）.</p><p>其中 <span class="math inline">\(\boldsymbol{U} \in \mathbb{R}^{m \times m}\)</span> 为正交矩阵，其列向量为 <span class="math inline">\(\boldsymbol{u}_i, \ i=1, \ldots, m\)</span>；<span class="math inline">\(\boldsymbol{V} \in \mathbb{R}^{n \times n}\)</span> 也为正交矩阵，其列向量为<span class="math inline">\(\boldsymbol{v}_i, \ i=1, \ldots, n\)</span>。另外，<span class="math inline">\(\boldsymbol{\Sigma}\)</span> 为 <span class="math inline">\(m \times n\)</span> 矩阵，且<span class="math inline">\(\Sigma_{ii} = \sigma_i \geqslant 0, \quad \Sigma_{ij} = 0, \ i \neq j .\)</span> <span class="math inline">\(\boldsymbol{\Sigma}\)</span> 的对角元素<span class="math inline">\(\sigma_i, \quad i = 1, \ldots, r\)</span>称为奇异值（singular values），<span class="math inline">\(\boldsymbol{u}_i\)</span> 称为左奇异向量（left-singular vectors），<span class="math inline">\(\boldsymbol{v}_i\)</span> 称为右奇异向量（right-singular vectors）。按照惯例，奇异值是有序的：<span class="math inline">\(\sigma_1 \geqslant \sigma_2 \geqslant \cdots \geqslant \sigma_r \geqslant 0 .\)</span></p><p>奇异值矩阵 <span class="math inline">\(\boldsymbol{\Sigma}\)</span> 是唯一的。需要注意 <span class="math inline">\(\boldsymbol{\Sigma} \in \mathbb{R}^{m \times n}\)</span> 是矩形的，且与 <span class="math inline">\(\boldsymbol{A}\)</span> 尺寸相同。这意味着 <span class="math inline">\(\boldsymbol{\Sigma}\)</span> 有一个包含奇异值的对角子矩阵，其他元素为零。 备注：SVD 对任意 <span class="math inline">\(\boldsymbol{A} \in \mathbb{R}^{m \times n}\)</span> 都成立。</p><blockquote><p><strong>旋转矩阵</strong>就是一个行列式为 <span class="math inline">\(+1\)</span> 的正交矩阵，也就是： <span class="math inline">\(R^\top R = I, \quad \det(R) = 1\)</span></p></blockquote><blockquote><p>SVD表示域和陪域（见线性代数中的像与核）的基变换。这与在同一向量空间中操作的特征值分解形成对比：后者在相同的向量空间中应用相同的基变化，然后撤消。奇异值分解的特殊之处在于这两个不同的基同时被奇异值矩阵<span class="math inline">\(\boldsymbol{\Sigma}\)</span> 连接起来。</p></blockquote><blockquote><p>标准正交基(ONB)</p></blockquote><blockquote><p>左奇异向量和右奇异向量的直观理解，详见《SVD 中的左奇异向量和右奇异向量.md》</p></blockquote><h4 id="特征值分解-vs.-奇异值分解">4.5.4 特征值分解 vs. 奇异值分解</h4><p>考虑特征值分解 <span class="math display">\[\boldsymbol{A} = \boldsymbol{P} \boldsymbol{D} \boldsymbol{P}^{-1}\]</span> 和奇异值分解 <span class="math display">\[\boldsymbol{A} = \boldsymbol{U} \boldsymbol{\Sigma} \boldsymbol{V}^{-1},\]</span> 下面回顾一下它们的核心内容：</p><ul><li><p>对于任意矩阵 <span class="math inline">\(\mathbb{R}^{m \times n}\)</span>，奇异值分解总是存在的。而特征值分解仅对平方矩阵 <span class="math inline">\(\mathbb{R}^{n \times n}\)</span> 有效，并且仅当我们能找到 <span class="math inline">\(\mathbb{R}^n\)</span> 的特征向量的基时才存在。</p></li><li><p>特征值分解矩阵 <span class="math inline">\(\boldsymbol{P}\)</span> 中的向量不一定是正交的，即基的变化不是简单的旋转和缩放。而另一方面，奇异值分解中矩阵 <span class="math inline">\(\boldsymbol{U}\)</span> 和 <span class="math inline">\(\boldsymbol{V}\)</span> 中的向量是正交的，因此它们确实表示旋转。</p></li><li><p>特征值分解和奇异值分解都是三个线性映射的组合：</p><ul><li>定义域的<strong>基变换</strong>；</li><li>每个新基向量的缩放都是独立的，并从定义域映射到陪域；</li><li>陪域的<strong>基变换</strong>。 特征值分解和奇异值分解的一个关键区别是，<strong>在奇异值分解中，定义域和陪域可以是不同维数的向量空间。</strong></li></ul></li><li><p>在奇异值分解中，左奇异向量矩阵 <span class="math inline">\(\boldsymbol{U}\)</span> 和右奇异向量矩阵 <span class="math inline">\(\boldsymbol{V}\)</span> 通常不是互逆的（它们在不同的向量空间中执行基变换）。在特征值分解中，基变化矩阵 <span class="math inline">\(\boldsymbol{U}\)</span> 和 <span class="math inline">\(\boldsymbol{U}^{-1}\)</span> 是互逆的。</p></li><li><p>在奇异值分解中，对角矩阵 <span class="math inline">\(\boldsymbol{\Sigma}\)</span> 中的项都是实的、非负的，这对于特征值分解中的对角矩阵则不一定成立。</p></li><li><p>奇异值分解和特征值分解通过它们的投影关系密切相关：</p><ul><li><span class="math inline">\(\boldsymbol{A}\)</span> 的左奇异向量是 <span class="math inline">\(\boldsymbol{A} \boldsymbol{A}^\top\)</span> 的特征向量；</li><li><span class="math inline">\(\boldsymbol{A}\)</span> 的右奇异向量是 <span class="math inline">\(\boldsymbol{A}^\top \boldsymbol{A}\)</span> 的特征向量；</li><li><span class="math inline">\(\boldsymbol{A}\)</span> 的非零奇异值是 <span class="math inline">\(\boldsymbol{A} \boldsymbol{A}^\top\)</span> 和 <span class="math inline">\(\boldsymbol{A}^\top \boldsymbol{A}\)</span> 的非零特征值的平方根。</li></ul></li><li><p>对于对称矩阵 <span class="math inline">\(\boldsymbol{A} \in \mathbb{R}^{n \times n}\)</span>，特征值分解和奇异值分解是相同的，这符合谱定理（定理 4.15）。</p></li><li><p><img src="/img3/机器学习的数学基础Part1/图解SVD_1.png" alt="图解SVD_1" style="zoom:50%;" /></p></li></ul><p><img src="/img3/机器学习的数学基础Part1/图解SVD_2.png" alt="图解SVD_2" style="zoom:50%;" /></p><h4 id="矩阵逼近">4.5.5 矩阵逼近</h4><p>我们可以使用奇异值分解（SVD）将秩<span class="math inline">\(r\)</span> 矩阵<span class="math inline">\(\boldsymbol{A}\)</span>降为秩 <span class="math inline">\(k\)</span> 矩阵 <span class="math inline">\(\widehat{\boldsymbol{A}}\)</span> ，这是一种取主要成分并达到最优的（在谱范数意义上）方式。我们可以将秩<span class="math inline">\(k\)</span>矩阵对<span class="math inline">\(\boldsymbol{A}\)</span>的逼近解释为有损压缩的一种方法。因此，矩阵的低秩逼近出现在许多机器学习应用中，例如图像处理、噪声滤波和不适定问题的正则化。此外，它在降维和主成分分析中起着关键作用</p><p>秩为 <span class="math inline">\(r\)</span> 的矩阵 <span class="math inline">\(\boldsymbol{A} \in \mathbb{R}^{m \times n}\)</span> 能被写成秩 <span class="math inline">\(1\)</span> 矩阵 <span class="math inline">\(\boldsymbol{A}_i\)</span> 的和： <span class="math display">\[\boldsymbol{A} = \sum_{i=1}^{r} \sigma_{i} \boldsymbol{u}_{i} \boldsymbol{v}_{i}^{\top}= \sum_{i=1}^{r} \sigma_{i} \boldsymbol{A}_{i},\]</span> 其中，外积矩阵 <span class="math inline">\(\boldsymbol{A}_{i}\)</span> 的权重为第 <span class="math inline">\(i\)</span> 个奇异值 <span class="math inline">\(\sigma_{i}\)</span>。</p><h3 id="矩阵分类关系图">4.6 矩阵分类关系图</h3><p>Matrix Phylogeny</p><blockquote><p>可对角化其实就是可特征值分解。</p></blockquote><p><img src="/img3/机器学习的数学基础Part1/矩阵分类关系图.png" alt="矩阵分类关系图" style="zoom:50%;" /></p><p>在第2章和第3章中，我们介绍了线性代数和解析几何的基础知识。在这一章中，我们研究了矩阵和线性映射的基本特征。图4.13描述了不同类型矩阵之间关系的 Phylogeny（黑色箭头表示子集）以及我们可以对其执行的操作（蓝色）。</p><p>我们考虑所有实矩阵（real matrices）<span class="math inline">\(\boldsymbol{A} \in \mathbb{R}^{n \times m}\)</span>。对于非方阵（其中 <span class="math inline">\(n \neq m\)</span>），奇异值分解总是存在的，正如我们在本章中看到的。以方阵（square matrices）<span class="math inline">\(\boldsymbol{A} \in \mathbb{R}^{n \times n}\)</span> 为中心，行列式告诉我们方阵是否具有逆矩阵（inverse matrix），即它是否属于正则可逆矩阵类。如果 <span class="math inline">\(n \times n\)</span> 矩阵具有 <span class="math inline">\(n\)</span> 个线性无关的特征向量，则矩阵是非退化的（non-defective），并且存在特征值分解（定理4.12）。我们知道，重复的特征值可能导致矩阵退化，这种矩阵是不能对角化的。</p><blockquote><p>No basis of eigenvectors 特征向量不是基； Basis of eigenvectors 特征向量是基。 Singular 奇异的 Orthogonal 正交的</p></blockquote><p>非奇异矩阵和非退化矩阵是不同的。例如，旋转矩阵是可逆的（行列式是非零的），但不一定可对角化（特征值不能保证是实数）。</p><blockquote><p>以上这句话的例子详见《非奇异矩阵和非退化矩阵.md》</p></blockquote><p>我们进一步研究了非退化 <span class="math inline">\(n \times n\)</span> 方阵的分支。如果条件<span class="math inline">\(\boldsymbol{A}^{\top} \boldsymbol{A} = \boldsymbol{A} \boldsymbol{A}^{\top}\)</span>成立，则 <span class="math inline">\(\boldsymbol{A}\)</span> 是正规的（normal）。此外，如果更严格的条件<span class="math inline">\(\boldsymbol{A}^{\top} \boldsymbol{A} = \boldsymbol{A} \boldsymbol{A}^{\top} = \boldsymbol{I}\)</span>成立，则 <span class="math inline">\(\boldsymbol{A}\)</span> 称为正交（orthogonal，见定义3.8）。正交矩阵集是正则（可逆）矩阵的子集，满足<span class="math inline">\(\boldsymbol{A}^{\top} = \boldsymbol{A}^{-1}.\)</span></p><p>正规矩阵有一个常见的子集，即对称矩阵 <span class="math inline">\(\boldsymbol{S} \in \mathbb{R}^{n \times n}\)</span>，它满足<span class="math inline">\(\boldsymbol{S} = \boldsymbol{S}^{\top}.\)</span>对称矩阵只有实特征值。对称矩阵的子集由正定矩阵 <span class="math inline">\(\boldsymbol{P}\)</span> 组成，正定矩阵 <span class="math inline">\(\boldsymbol{P}\)</span> 对所有 <span class="math inline">\(\boldsymbol{x} \in \mathbb{R}^{n} \backslash \{\mathbf{0}\}\)</span> 满足条件<span class="math inline">\(\boldsymbol{x}^{\top} \boldsymbol{P} \boldsymbol{x} &gt; 0.\)</span>在这种情况下，存在唯一的 Cholesky 分解（Cholesky decomposition，定理4.18）。<strong>正定矩阵只有正特征值且总是可逆的（即，具有非零行列式）。</strong></p><p>对称矩阵的另一个子集由对角矩阵（diagonal matrices）<span class="math inline">\(\boldsymbol{D}\)</span> 组成。对角矩阵在乘法和加法下是闭合的，但不一定形成一个群（只有当所有的对角项都不为零时才是这种情况，这样矩阵才是可逆的）。一种特殊的对角矩阵是单位矩阵 <span class="math inline">\(\boldsymbol{I}\)</span>。</p><blockquote><p>Identity matrix 单位矩阵</p></blockquote><figure><img src="./常见矩阵分解总结表.png" alt="常见矩阵分解总结表" /><figcaption aria-hidden="true">常见矩阵分解总结表</figcaption></figure><p>上表来源鸢尾花那本书Book4_ch24 《矩阵力量》</p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 数学 </tag>
            
            <tag> 算法 </tag>
            
            <tag> 统计 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《机器学习的数学基础》</title>
      <link href="/2025/10/26/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%BA%94/"/>
      <url>/2025/10/26/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%BA%94/</url>
      
        <content type="html"><![CDATA[<h2 id="向量微积分">向量微积分</h2><p>Vector Calculus</p><p><strong><span class="math inline">\(f\)</span>导数的方向指向<span class="math inline">\(f\)</span>最陡峭的上升方向。</strong></p><blockquote><p>组合符号 <span class="math inline">\(C_n^m\)</span> 也可写成 <span class="math inline">\(\binom{n}{m}\)</span>。</p></blockquote><span id="more"></span><h3 id="泰勒级数">泰勒级数</h3><p>泰勒级数是函数<span class="math inline">\(f\)</span>的无穷项和的表示。这些项是用<span class="math inline">\(f\)</span>的导数来确定的。<strong>多项式逼近函数的泰勒级数</strong>。</p><p><strong>泰勒级数</strong></p><p>对于一个平滑的函数 $ f ^{},  f:   $ （$ f ^{} $ 表示 <span class="math inline">\(f\)</span> 连续且可微无穷多次）， <span class="math inline">\(f\)</span> 在 <span class="math inline">\(x_0\)</span> 的泰勒级数Taylor series）定义为：<br /><span class="math display">\[T_{\infty}(x) = \sum_{k=0}^{\infty} \frac{f^{(k)}\left(x_{0}\right)}{k !} \left(x - x_{0}\right)^{k}\]</span></p><p>当 $ x_0 = 0 $ 时，我们得到麦克劳林级数（Maclaurin series），它是泰勒级数的特殊实例。 如果 $ f(x) = T_{}(x) $，那么 <span class="math inline">\(f\)</span> 称为解析的（analytic）。</p><p><strong>备注</strong>： 一般来说，$ n $ 次泰勒多项式是非多项式函数的近似值。 它在 <span class="math inline">\(x_0\)</span> 附近与 <span class="math inline">\(f\)</span> 相似。 然而，$ n $ 次泰勒多项式用 $ k n $ 次多项式表示 <span class="math inline">\(f\)</span> 已经足够精确了， 因为导数 $ f^{(i)},  i &gt; k $ 可能为 <span class="math inline">\(0\)</span>。</p><blockquote><p>泰勒级数中的 <span class="math inline">\(x_0\)</span>（也叫展开点）<strong>原则上可以选任意实数或复数</strong>，不过能不能用它准确表示函数，还得看<strong>收敛性和函数的可导性</strong>条件。</p></blockquote><p><strong>备注</strong>：<br />泰勒级数是幂级数的特例，幂级数表达式为：<br /><span class="math display">\[f(x) = \sum_{k=0}^{\infty} a_{k} (x - c)^{k}\]</span> 其中 <span class="math inline">\(a_k\)</span>为系数，<span class="math inline">\(c\)</span>为常数。定义5.4中的式子是它的特殊形式。</p><h3 id="微分法则">微分法则</h3><p>下面，我们用 $ f' $ 表示 <span class="math inline">\(f\)</span> 的导数，简要地说明基本的微分规则：</p><ol type="1"><li>乘积法则：</li></ol><p><span class="math display">\[(f(x) g(x))&#39; = f&#39;(x) g(x) + f(x) g&#39;(x)\]</span></p><ol start="2" type="1"><li>除法法则：</li></ol><p><span class="math display">\[\left(\frac{f(x)}{g(x)}\right)&#39; = \frac{f&#39;(x) g(x) - f(x) g&#39;(x)}{\left(g(x)\right)^{2}}\]</span></p><ol start="3" type="1"><li>加法法则：</li></ol><p><span class="math display">\[(f(x) + g(x))&#39; = f&#39;(x) + g&#39;(x)\]</span></p><ol start="4" type="1"><li>链式法则：</li></ol><p><span class="math display">\[(g(f(x)))&#39; = (g \circ f)&#39;(x) = g&#39;(f(x)) \cdot f&#39;(x)\]</span></p><p>​ 这里，$ g f $ 表示函数复合：<br /><span class="math display">\[x \mapsto f(x) \mapsto g(f(x))\]</span></p><h3 id="偏微分与梯度">偏微分与梯度</h3><p><strong>导数对多变量函数的推广是梯度(gradient)。</strong></p><p>我们通过一次改变一个变量并保持其他变量不变来求函数<span class="math inline">\(f\)</span>相对于<span class="math inline">\(x\)</span>的偏导数。<strong>梯度就是这些偏导数</strong>(partial derivatives)构成的的集合。</p><p>将它们组成一个<strong>行向量</strong>： <span class="math display">\[  \nabla_{\boldsymbol{x}} f = \operatorname{grad} f   = \frac{\mathrm{d} f}{\mathrm{d} \boldsymbol{x}}  = \left[   \frac{\partial f(\boldsymbol{x})}{\partial x_{1}},\   \frac{\partial f(\boldsymbol{x})}{\partial x_{2}},\   \cdots,\   \frac{\partial f(\boldsymbol{x})}{\partial x_{n}}  \right]   \in \mathbb{R}^{1 \times n}   \qquad (5.40)\]</span></p><p>其中 $ n $ 是变量个数，<span class="math inline">\(1\)</span> 是 <span class="math inline">\(f\)</span> 的像/值域/陪域的维数。 这里，我们定义了列向量<span class="math inline">\(\boldsymbol{x} = \left[ x_{1}, \ldots, x_{n} \right]^{\top} \in \mathbb{R}^n.\)</span> 式 (5.40) 中的行向量称为 <span class="math inline">\(f\)</span> 的梯度(gradient) 或 雅可比矩阵(Jacobian)，是第 5.1 节中导数的推广。</p><p><strong>备注（梯度用行向量表示）：</strong></p><p>向量通常用列向量表示，将梯度向量定义为列向量在文献中并不少见。 我们将梯度向量定义为行向量的原因有两个： 首先，我们可以一致地将梯度推广到向量值函数<span class="math inline">\(f: \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}\)</span>（然后梯度变成矩阵）。 其次，我们可以很方便地应用多变量链式法则，而不必注意梯度的维数。</p><h4 id="链式法则">链式法则</h4><p>考虑两个变量 <span class="math inline">\(x_{1}\)</span>, <span class="math inline">\(x_{2}\)</span> 的函数<span class="math inline">\(f: \mathbb{R}^{2} \rightarrow \mathbb{R}.\)</span>此外，<span class="math inline">\(x_{1}(t)\)</span> 和 <span class="math inline">\(x_{2}(t)\)</span> 本身就是 <span class="math inline">\(t\)</span> 的函数。 为了计算 <span class="math inline">\(f\)</span> 相对于 <span class="math inline">\(t\)</span> 的导数，我们需要对多元函数使用链式法则： <span class="math display">\[\frac{\mathrm{d} f}{\mathrm{d} t}=\begin{bmatrix}\frac{\partial f}{\partial x_{1}} &amp; \frac{\partial f}{\partial x_{2}}\end{bmatrix}\begin{bmatrix}\frac{\partial x_{1}(t)}{\partial t} \\\frac{\partial x_{2}(t)}{\partial t}\end{bmatrix}=\frac{\partial f}{\partial x_{1}} \frac{\partial x_{1}}{\partial t}+\frac{\partial f}{\partial x_{2}} \frac{\partial x_{2}}{\partial t}.\]</span> <strong>其中 <span class="math inline">\(\mathrm{d}\)</span> 表示全导数，<span class="math inline">\(\partial\)</span> 表示偏导数。</strong></p><h4 id="多变量链式法则">多变量链式法则</h4><p>如果 <span class="math inline">\(f(x_{1}, x_{2})\)</span> 是 <span class="math inline">\(x_{1}\)</span> 和 <span class="math inline">\(x_{2}\)</span> 的函数，其中 <span class="math inline">\(x_{1}(s, t)\)</span> 和 <span class="math inline">\(x_{2}(s, t)\)</span> 是两个变量 <span class="math inline">\(s\)</span> 和 <span class="math inline">\(t\)</span> 的函数，则用链式法则可得偏导数： <span class="math display">\[\frac{\partial f}{\partial s}=\frac{\partial f}{\partial x_{1}} \frac{\partial x_{1}}{\partial s}+\frac{\partial f}{\partial x_{2}} \frac{\partial x_{2}}{\partial s},\]</span></p><p><span class="math display">\[\frac{\partial f}{\partial t}=\frac{\partial f}{\partial x_{1}} \frac{\partial x_{1}}{\partial t}+\frac{\partial f}{\partial x_{2}} \frac{\partial x_{2}}{\partial t}.\]</span></p><p>将梯度写成矩阵乘法的形式为： <span class="math display">\[\frac{\mathrm{d} f}{\mathrm{d}(s, t)}=\frac{\partial f}{\partial \boldsymbol{x}}\frac{\partial \boldsymbol{x}}{\partial (s, t)}=\underbrace{\begin{bmatrix}\frac{\partial f}{\partial x_{1}} &amp; \frac{\partial f}{\partial x_{2}}\end{bmatrix}}_{\frac{\partial f}{\partial \boldsymbol{x}}}\underbrace{\begin{bmatrix}\frac{\partial x_{1}}{\partial s} &amp; \frac{\partial x_{1}}{\partial t} \\\frac{\partial x_{2}}{\partial s} &amp; \frac{\partial x_{2}}{\partial t}\end{bmatrix}}_{\frac{\partial \boldsymbol{x}}{\partial (s, t)}}.\]</span></p><p><strong>这种将链式法则写成矩阵乘法的简洁方法，只有在将梯度定义为行向量时才直接成立。</strong>否则，需要对矩阵进行转置以匹配维数。当对象是向量或矩阵时，转置很简单；但当对象是张量时（将在后面讨论），转置就不再是小事了。</p><h3 id="向量值函数的梯度">向量值函数的梯度</h3><p><strong>雅可比矩阵</strong></p><p><strong>向量值函数</strong>的所有一阶偏导数的集合称为雅可比矩阵（Jacobian）。雅可比矩阵 <span class="math inline">\(\mathbf{J}\)</span> 是一个 <span class="math inline">\(m \times n\)</span> 矩阵，我们将其定义如下： <span class="math display">\[\mathbf{J}= \nabla_{\mathbf{x}} \mathbf{f}= \frac{\mathrm{d} \mathbf{f}(\mathbf{x})}{\mathrm{d} \mathbf{x}}= \left[\frac{\partial \mathbf{f}(\mathbf{x})}{\partial x_{1}} \quad\cdots \quad\frac{\partial \mathbf{f}(\mathbf{x})}{\partial x_{n}}\right].\]</span></p><p>具体写成矩阵形式为： <span class="math display">\[\begin{equation}\mathbf{J} =\begin{bmatrix}\dfrac{\partial f_{1}(\mathbf{x})}{\partial x_{1}} &amp; \cdots &amp; \dfrac{\partial f_{1}(\mathbf{x})}{\partial x_{n}} \\\vdots &amp; \ddots &amp; \vdots \\\dfrac{\partial f_{m}(\mathbf{x})}{\partial x_{1}} &amp; \cdots &amp; \dfrac{\partial f_{m}(\mathbf{x})}{\partial x_{n}}\end{bmatrix},\tag{5.58}\end{equation}\]</span> 其中 <span class="math display">\[\mathbf{x} =\begin{bmatrix}x_{1} \\ \vdots \\ x_{n}\end{bmatrix},\quadJ(i,j) = \frac{\partial f_{i}}{\partial x_{j}}.\]</span></p><p>雅可比矩阵表示我们想要的坐标变换。如果坐标变换是线性的（如我们的例子），那么它是精确的，（5.66）精确地恢复了（5.62）中的基变化矩阵。如果坐标变换是非线性的，雅可比矩阵则用一个线性变换局部地逼近这个非线性变换。雅可比行列式 <span class="math inline">\(|\operatorname{det}(\boldsymbol{J})|\)</span> 的绝对值是变换坐标时面积或体积的缩放因子。</p><p><img src="/img3/机器学习的数学基础Part1/变换.png" alt="变换" style="zoom:50%;" /></p><p><strong>方法</strong>一、</p><p>为了开始使用线性代数的方法，我们首先确定 <span class="math inline">\(\{\boldsymbol{b}_{1}, \boldsymbol{b}_{2}\}\)</span> 和 <span class="math inline">\(\{\boldsymbol{c}_{1}, \boldsymbol{c}_{2}\}\)</span> 都是 <span class="math inline">\(\mathbb{R}^2\)</span> 的基。我们要有效地执行的是从 <span class="math inline">\(\{\boldsymbol{b}_{1}, \boldsymbol{b}_{2}\}\)</span> 到 <span class="math inline">\(\{\boldsymbol{c}_{1}, \boldsymbol{c}_{2}\}\)</span> 的基变换，就得寻找实现基变换的变换矩阵。 利用第2.7.2节的结果，我们确定了所需的<strong>基变换矩阵</strong>为： <span class="math display">\[\boldsymbol{J} = \begin{bmatrix}-2 &amp; 1 \\1 &amp; 1\end{bmatrix} \tag{5.62}\]</span></p><p>它使得 <span class="math inline">\(\boldsymbol{J}\boldsymbol{b}_{1} = \boldsymbol{c}_{1}, \quad \boldsymbol{J}\boldsymbol{b}_{2} = \boldsymbol{c}_{2}\)</span>。 矩阵 <span class="math inline">\(\boldsymbol{J}\)</span> 的行列式的绝对值为： <span class="math display">\[\left|\det(\boldsymbol{J})\right| = 3,\]</span> 这正是我们在寻找的缩放因子。也就是说， <span class="math inline">\((\boldsymbol{c}_{1}, \boldsymbol{c}_{2})\)</span> 所张成的平行四边形的面积是 <span class="math inline">\((\boldsymbol{b}_{1}, \boldsymbol{b}_{2})\)</span> 所张成面积的三倍。</p><p>把 <span class="math inline">\(\{\boldsymbol{b}_{1}, \boldsymbol{b}_{2}\}\)</span> 坐标变换成 <span class="math inline">\(\{\boldsymbol{c}_{1}, \boldsymbol{c}_{2}\}\)</span> 的坐标，是左乘<span class="math inline">\(\boldsymbol{J}^{-1}\)</span>；反向是左乘<span class="math inline">\(\boldsymbol{J}\)</span>。</p><p><strong>方法二</strong>、</p><p>线性代数方法适用于线性变换；对于非线性变换（与第6.7节有关），我们有基于偏微分的更一般的方法。</p><p>在这种方法中，我们考虑执行变量变换的函数<span class="math inline">\(\boldsymbol{f}: \mathbb{R}^{2} \rightarrow \mathbb{R}^{2}.\)</span>在我们的例子中，<span class="math inline">\(\boldsymbol{f}\)</span> 将关于 <span class="math inline">\((\boldsymbol{b}_{1}, \boldsymbol{b}_{2})\)</span> 的任意向量 <span class="math inline">\(\boldsymbol{x} \in \mathbb{R}^{2}\)</span> 的坐标映射到关于 <span class="math inline">\((\boldsymbol{c}_{1}, \boldsymbol{c}_{2})\)</span> 的坐标 <span class="math inline">\(\boldsymbol{y} \in \mathbb{R}^{2}\)</span>。 我们想确定这个映射，这样就可以计算出一个面积（或体积）在 <span class="math inline">\(\boldsymbol{f}\)</span> 变换下是如何变化的。 为此，我们需要找出 <span class="math inline">\(\boldsymbol{f}(\boldsymbol{x})\)</span> 在 <span class="math inline">\(\boldsymbol{x}\)</span> 微小变化时的变化方式。 雅可比矩阵 <span class="math display">\[\frac{\mathrm{d} \boldsymbol{f}}{\mathrm{d} \boldsymbol{x}} \in \mathbb{R}^{2 \times 2}\]</span> 正是这个问题的答案。 由下式定义的映射： <span class="math display">\[y_{1} = -2x_{1} + x_{2}, \qquady_{2} = x_{1} + x_{2},\]</span> 我们得到了 <span class="math inline">\(\boldsymbol{x}\)</span> 和 <span class="math inline">\(\boldsymbol{y}\)</span> 之间的函数关系。 这允许我们计算偏导数： <span class="math display">\[\frac{\partial y_{1}}{\partial x_{1}}=-2, \quad \frac{\partial y_{1}}{\partial x_{2}}=1, \quad \frac{\partial y_{2}}{\partial x_{1}}=1, \quad \frac{\partial y_{2}}{\partial x_{2}}=1.\]</span></p><p>将它们组合成雅可比矩阵： <span class="math display">\[\boldsymbol{J} =\begin{bmatrix}\frac{\partial y_{1}}{\partial x_{1}} &amp; \frac{\partial y_{1}}{\partial x_{2}} \\\frac{\partial y_{2}}{\partial x_{1}} &amp; \frac{\partial y_{2}}{\partial x_{2}}\end{bmatrix}=\begin{bmatrix}-2 &amp; 1 \\1 &amp; 1\end{bmatrix} \tag{5.66}\]</span></p><p>雅可比矩阵表示我们想要的坐标变换。 如果坐标变换是线性的（如我们的例子），那么它是精确的，（5.66）正好恢复了（5.62）中的基变化矩阵。 <strong>如果坐标变换是非线性的，雅可比矩阵则用一个线性变换局部地逼近这个非线性变换</strong>。 雅可比行列式的绝对值 <span class="math display">\[\left|\det(\boldsymbol{J})\right| = 3\]</span> 就是变换坐标时面积或体积的缩放因子。在我们的例子中，结果正好是 3。</p><figure><img src="./偏导数的维度.png" alt="偏导数的维度" /><figcaption aria-hidden="true">偏导数的维度</figcaption></figure><p><strong>向量值函数的梯度</strong></p><p>给定： <span class="math display">\[\mathbf{f}(\mathbf{x}) = \mathbf{A} \mathbf{x}\]</span> 其中：</p><ul><li><span class="math inline">\(\mathbf{f}(\mathbf{x}) \in \mathbb{R}^M\)</span>（输出是 <span class="math inline">\(M\)</span> 维向量）</li><li><span class="math inline">\(\mathbf{A} \in \mathbb{R}^{M \times N}\)</span></li><li><span class="math inline">\(\mathbf{x} \in \mathbb{R}^N\)</span></li></ul><p>这是一个<strong>线性映射</strong>：<span class="math inline">\(\mathbf{x} \mapsto \mathbf{A} \mathbf{x}\)</span>。</p><p>向量值函数的梯度是 <span class="math inline">\(A\)</span>。</p><h4 id="反向传播与自动微分法">反向传播与自动微分法</h4><p>在许多机器学习应用中，我们通过执行梯度下降（第7.1节）来找到好的模型参数，这基于我们可以计算目标函数相对于模型参数的梯度。对于给定的目标函数，我们可以通过微积分和应用链式法则来获得关于模型参数的梯度；见第5.2.2节。在第5.3节中，我们已经研究了线性回归模型的平方损失函数的梯度。</p><p>反向传播是数值分析中称为自动微分(automatic differentiation)技术的一个特例。我们可以把自动微分看作是一套技术，它通过处理中间变量和应用链式法则，在数值上（而不是符号化地）计算函数的精确（达到机器精度）梯度。自动微分应用了一系列基本算术运算，例如加法和乘法，以及基本函数，例如：<span class="math inline">\(⁡ \sin , \cos , \exp , \log\)</span>将链式法则应用到这些运算中，可以自动计算相当复杂函数的梯度。自动微分适用于一般的计算机程序，有正向和反向两种模式。</p><p>直观地说，正向和反向模式在乘法的顺序上是不同的。由于矩阵乘法的结合性，我们有以下两种选择： <span class="math display">\[\begin{equation}\frac{\mathrm{d} y}{\mathrm{d} x} =\left(\frac{\mathrm{d} y}{\mathrm{d} b}\frac{\mathrm{d} b}{\mathrm{d} a}\right)\frac{\mathrm{d} a}{\mathrm{d} x}\qquad (5.120)\end{equation}\]</span></p><p><span class="math display">\[\begin{equation}\frac{\mathrm{d} y}{\mathrm{d} x} =\frac{\mathrm{d} y}{\mathrm{d} b}\left(\frac{\mathrm{d} b}{\mathrm{d} a}\frac{\mathrm{d} a}{\mathrm{d} x}\right)\qquad (5.121)\end{equation}\]</span></p><p>方程（5.120）是反向模式（reverse mode），因为梯度通过数据流向后传播。 方程（5.121）是正向模式（forward mode），其中梯度随数据从左到右流过整个图。</p><p>下面，我们将重点介绍反向模式的自动微分，即反向传播。<strong>在神经网络中，输入的维数通常比标签的维数高得多，所以反向模式比正向模式计算量要少得多。</strong></p><blockquote><p>具体的例子详见《损失函数的梯度计算例子.md》</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 数学 </tag>
            
            <tag> 算法 </tag>
            
            <tag> 统计 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《机器学习的数学基础》</title>
      <link href="/2025/10/25/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%85%AD/"/>
      <url>/2025/10/25/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%85%AD/</url>
      
        <content type="html"><![CDATA[<h2 id="概率与分布">概率与分布</h2><p>Probability and Distributions</p><p>概率论可以看作是布尔逻辑的推广。在机器学习的背景下，它经常以这种方式应用于自动推理系统的形式化设计。</p><p>在机器学习和统计学中，有两种主要的概率解释：贝叶斯主义和频率主义(Bishop, 2006;Efron and Hastie, 2016)。贝叶斯主义使用概率来指定用户对事件的不确定性程度。它有时被称为“主观概率”或“置信程度”。频率主义则考虑感兴趣的事件与所发生事件的总数的相对频率。一个事件的概率定义为当发生事件的总数趋于无限时，该事件的相对频率。</p><span id="more"></span><p>许多机器学习的概率模型描述使用惰性符号和术语，这会令人困惑的。这篇文章也不例外。有多个不同的概念都被称为“概率分布”，读者往往要从上下文中获知其中的含义。一个有助于理解概率分布的技巧是检查我们是在尝试建立一个分类的（离散的随机变量）模型还是一个连续的（连续的随机变量）模型。我们在机器学习中处理的问题的种类与我们要考虑分类模型还是连续模型密切相关。</p><blockquote><p>在抽第二枚硬币之前放回了第一枚抽到的硬币，这意味着两次抽硬币是相互独立的。</p></blockquote><h3 id="概率空间-omega-mathcala-p">概率空间 <span class="math inline">\((\Omega, \mathcal{A}, P)\)</span>，</h3><p>示例：连续两次抛硬币</p><ul><li><p><strong>样本空间</strong> <span class="math display">\[\Omega = \{hh, ht, th, tt\}\]</span> 这里每个元素（样本点）表示两次抛硬币的具体结果。例如 <span class="math inline">\(hh\)</span> = 第一次正面、第二次正面。</p></li><li><p><strong>事件空间</strong> 事件就是样本空间的子集。比如：</p><ul><li>事件 <span class="math inline">\(A=\{\text{hh}\}\)</span>：两次都是正面。</li><li>事件 <span class="math inline">\(B=\{\text{ht},\text{th}\}\)</span>：恰好出现一次正面。</li><li>事件 <span class="math inline">\(C=\{\text{hh},\text{ht},\text{th}\}\)</span>：至少出现一次正面。</li></ul><p>所有可能的事件集合（即事件空间 <span class="math inline">\(\mathcal{A}\)</span>）在离散情况下就是 <span class="math inline">\(\Omega\)</span> 的幂集： <span class="math display">\[\mathcal{A} = \{ \emptyset, \{hh\}, \{ht\}, \{th\}, \{tt\}, \{hh,ht\}, \dots, \Omega \}.\]</span></p></li><li><p><strong>概率分布</strong> 假设硬币均匀独立，样本空间中每个结果的概率都是 <span class="math inline">\(1/4\)</span>。那么：</p><ul><li><span class="math inline">\(P(A)=P(\{hh\})=1/4\)</span>。</li><li><span class="math inline">\(P(B)=P(\{ht,th\})=1/4+1/4=1/2\)</span>。</li><li><span class="math inline">\(P(C)=P(\{hh,ht,th\})=3/4\)</span>。</li></ul></li></ul><p><strong>总结</strong></p><ul><li><strong>样本空间 <span class="math inline">\(\Omega\)</span></strong>：所有可能的基本结果。</li><li><strong>事件空间 <span class="math inline">\(\mathcal{A}\)</span></strong>：由样本空间的子集组成，每个子集就是一个“事件”。</li><li><strong>概率 <span class="math inline">\(P\)</span></strong>：为每个事件赋予一个数，满足概率公理。</li></ul><p>给定一个概率空间 <span class="math inline">\((\Omega, \mathcal{A}, P)\)</span>，我们希望使用它来模拟一些现实世界的现象。在机器学习中，我们经常避免直接引用概率空间，而是引用感兴趣的量上的概率，我们用 <span class="math inline">\(\mathcal{T}\)</span> 表示。在这本书中，我们把 <span class="math inline">\(\mathcal{T}\)</span> 称为目标空间（target space），把 <span class="math inline">\(\mathcal{T}\)</span> 的元素称为状态。我们引入了一个函数 <span class="math inline">\(X : \Omega \to \mathcal{T}\)</span>它接受 <span class="math inline">\(\Omega\)</span> 元素（一个结果，样本点），并返回感兴趣对象 <span class="math inline">\(x\)</span> 在 <span class="math inline">\(\mathcal{T}\)</span> 中的特定量（值）。从 <span class="math inline">\(\Omega\)</span> 到 <span class="math inline">\(\mathcal{T}\)</span> 的这种关联/映射称为随机变量（random variable）。</p><p>例如，考虑投掷两枚硬币并计算正面数。随机变量 <span class="math inline">\(X\)</span> 映射到三种可能的结果：<span class="math inline">\(X(hh) = 2, \quad X(ht) = 1, \quad X(th) = 1, \quad X(tt) = 0\)</span>在这个特殊的情况下，<span class="math inline">\(\mathcal{T} = \{0,1,2\}\)</span>，我们感兴趣的是 <span class="math inline">\(\mathcal{T}\)</span> 中元素的概率。对于有限的样本空间 <span class="math inline">\(\Omega\)</span> 和有限的 <span class="math inline">\(\mathcal{T}\)</span>，随机变量对应的函数本质上是一个查找表。对于任意子集 <span class="math inline">\(S \subseteq \mathcal{T}\)</span>，我们将<span class="math inline">\(P_X(S) \in [0,1]\)</span>（概率）与随机变量 <span class="math inline">\(X\)</span> 对应的特定事件联系起来。例 6.1 提供了具体说明。</p><p><strong>将 <span class="math inline">\(X\)</span><u><em>输出的概率</em></u>和<span class="math inline">\(\Omega\)</span>中样本的概率这两个不同的概念等同起来!</strong></p><p>Example 6.1</p><p>This toy example is essentially a biased coin flip example. We assume that the reader is already familiar with computing probabilities of intersections and unions of sets of events. A gentler introduction to probability with many examples can be found in chapter 2 of Walpole et al. (2011).</p><p>Consider a statistical experiment where we model a funfair game consisting of drawing two coins from a bag (with replacement). There are coins from USA (denoted as $) and UK (denoted as £) in the bag, and since we draw two coins from the bag, there are four outcomes in total. The state space or sample space <span class="math inline">\(\Omega\)</span> of this experiment is then <span class="math display">\[(\$, \$), (\$, £), (£, \$), (£, £).\]</span> Let us assume that the composition of the bag of coins is such that a draw returns at random a $ with probability 0.3.</p><p>The event we are interested in is the total number of times the repeated draw returns $. Let us define a random variable <span class="math inline">\(X\)</span> that maps the sample space <span class="math inline">\(\Omega\)</span> to <span class="math inline">\(\mathcal{T}\)</span>, which denotes the number of times we draw $ out of the bag. We can see from the preceding sample space we can get zero $, one $, or two $s, and therefore <span class="math inline">\(\mathcal{T} = \{0, 1, 2\}\)</span>. The random variable <span class="math inline">\(X\)</span> (a function or lookup table) can be represented as a table like the following: <span class="math display">\[\begin{align}X((\$, \$)) &amp;= 2 \tag{6.1}\\X((\$, £)) &amp;= 1 \tag{6.2}\\X((£, \$)) &amp;= 1 \tag{6.3}\\X((£, £)) &amp;= 0. \tag{6.4}\end{align}\]</span> Since we return the first coin we draw before drawing the second, this implies that the two draws are independent of each other, which we will discuss in Section 6.4.5. Note that there are two experimental outcomes which map to the same event, where only one of the draws returns $. Therefore, the probability mass function (Section 6.2.1) of <span class="math inline">\(X\)</span> is given by <span class="math display">\[\begin{align}P(X=2) &amp;= P((\$, \$)) \nonumber \\&amp;= P(\$) \cdot P(\$) \nonumber \\&amp;= 0.3 \cdot 0.3 = 0.09 \tag{6.5}\\P(X=1) &amp;= P((\$, £) \cup (\,£, \$)) \nonumber \\&amp;= P((\$, £)) + P((\,£, \$)) \nonumber \\&amp;= 0.3 \cdot (1 - 0.3) + (1 - 0.3) \cdot 0.3 = 0.42 \tag{6.6}\\P(X=0) &amp;= P((\,£, £)) \nonumber \\&amp;= P(£) \cdot P(£) \nonumber \\&amp;= (1 - 0.3) \cdot (1 - 0.3) = 0.49. \tag{6.7}\end{align}\]</span></p><p>离散状态数值化特别有用，因为我们经常需要考虑随机变量的期望值</p><p>不幸的是，机器学习许多相关文献使用的符号和术语隐藏了样本空间 <span class="math inline">\(\Omega\)</span>、目标空间 <span class="math inline">\(\mathcal{T}\)</span> 和随机变量 <span class="math inline">\(X\)</span> 之间的区别。对于随机变量 <span class="math inline">\(X\)</span> 的一组可能结果的值 <span class="math inline">\(x\)</span>，即 <span class="math inline">\(x \in \mathcal{T}\)</span>，<span class="math inline">\(p(x)\)</span> 表示随机变量 <span class="math inline">\(X\)</span> 取结果 <span class="math inline">\(x\)</span> 的概率。对于离散随机变量，这表示为 <span class="math inline">\(p(X = x)\)</span>，这称为概率质量函数（probability mass function）。概率质量函数通常被称为“分布”。</p><p>对于连续变量，<span class="math inline">\(p(x)\)</span> 称为概率密度函数（probability density function，通常称为密度），而累积分布函数 <span class="math inline">\(P(x \le X)\)</span> 通常也被称为“分布”。在本章中，我们将使用符号 <span class="math inline">\(X\)</span> 来表示一元和多元随机变量，并分别用 <span class="math inline">\(x\)</span> 和 <span class="math inline">\(\boldsymbol{x}\)</span> 表示状态。</p><p>我们用“概率分布”表达离散的概率质量函数以及连续的概率密度函数，尽管这在技术上是不正确的。与大多数机器学习文献一样，我们也依赖上下文来区分“概率分布”这个短语的不同用法。</p><h3 id="加法法则乘法法则和贝叶斯定理">加法法则、乘法法则和贝叶斯定理</h3><p>1、<strong>求和法则</strong> <span class="math display">\[p(x) =\begin{cases}\sum_{y \in \mathcal{Y}} p(x,y), &amp; \text{如果 $y$ 是离散的}, \\[1em]\int_{\mathcal{Y}} p(x,y)\,dy, &amp; \text{如果 $y$ 是连续的}.\end{cases}\]</span></p><p>求和法则将<strong>联合分布与边缘分布</strong>联系了起来。</p><p><strong>备注：</strong> 概率建模的许多计算困难都是由于应用了求和法则。当有许多变量或具有许多状态的离散变量时，求和法则执行的是高维求和或积分。执行高维求和或积分通常是难以计算，因为没有已知的多项式时间(polynomial-time)算法来精确计算它们。</p><p>2、<strong>乘法法则</strong></p><p>乘法法则(<em>product rule</em>)，它将<strong>联合分布与条件分布</strong>联系起来： <span class="math display">\[\begin{equation}p(\boldsymbol{x}, \boldsymbol{y}) = p(\boldsymbol{y} \mid \boldsymbol{x}) \, p(\boldsymbol{x}) \qquad (6.22)\end{equation}\]</span> 乘积法则可以解释为：两个随机变量的联合分布能被因子分解（乘积形式）为其他两个分布。这两个因子分别是：</p><ul><li>第一个随机变量的边缘分布<span class="math inline">\(p(\boldsymbol{x})\)</span></li><li>给定第一个随机变量时，第二个随机变量的条件分布<span class="math inline">\(p(\boldsymbol{y} \mid \boldsymbol{x})\)</span></li></ul><p>联合分布中随机变量的顺序是任意的，这意味着： <span class="math display">\[p(\boldsymbol{x}, \boldsymbol{y}) = p(\boldsymbol{x} \mid \boldsymbol{y}) \, p(\boldsymbol{y})\]</span> <strong>准确地说，式（6.22）表示的是离散随机变量的概率质量函数。对于连续随机变量，乘积规则用概率密度函数（第 6.2.3 节）表示。</strong></p><p>3、<strong>贝叶斯定理</strong></p><p>在机器学习和贝叶斯统计中，如果我们观察到部分随机变量，我们通常会对未观察到的（潜在的）随机变量的推断感兴趣。假设我们有一些关于未观测随机变量 <span class="math inline">\(\boldsymbol{x}\)</span> 的先验（prior）知识 <span class="math inline">\(p(\boldsymbol{x})\)</span>，并且我们可以观察到 <span class="math inline">\(\boldsymbol{x}\)</span> 和第二个随机变量 <span class="math inline">\(\boldsymbol{y}\)</span> 之间的关系 <span class="math inline">\(p(\boldsymbol{y} \mid \boldsymbol{x})\)</span>。那么，如果我们观察到 <span class="math inline">\(\boldsymbol{y}\)</span>，就可以利用贝叶斯定理，在给定 <span class="math inline">\(\boldsymbol{y}\)</span> 的观测值前提下，得出 <span class="math inline">\(\boldsymbol{x}\)</span> 的一些结论。</p><p><strong>贝叶斯定理（也叫贝叶斯法则或贝叶斯定律）：</strong> <span class="math display">\[\begin{equation}\underbrace{p(\boldsymbol{x} \mid \boldsymbol{y})}_{\text{后验}}= \frac{\overbrace{p(\boldsymbol{y} \mid \boldsymbol{x})}^{\text{似然}} \; \overbrace{p(\boldsymbol{x})}^{\text{先验}}}{\underbrace{p(\boldsymbol{y})}_{\text{证据}}}\qquad (6.23)\end{equation}\]</span> 似然（likelihood，有时也被称为“测量模型”）<span class="math inline">\(p(\boldsymbol{y} \mid \boldsymbol{x})\)</span>描述了<span class="math inline">\(\boldsymbol{x}\)</span>和<span class="math inline">\(\boldsymbol{y}\)</span>是如何相关的。对于离散概率分布，它是在已知潜在变量<span class="math inline">\(\boldsymbol{x}\)</span>前提下，数据<span class="math inline">\(\boldsymbol{y}\)</span>的概率。注意，似然<span class="math inline">\(p(\boldsymbol{y} \mid \boldsymbol{x})\)</span>不是<span class="math inline">\(\boldsymbol{x}\)</span>的分布，而是<span class="math inline">\(\boldsymbol{y}\)</span>的分布(<strong>个人注：但是关于<span class="math inline">\(x\)</span>的函数，见下面的场景</strong>)。并且我们称<span class="math inline">\(p(\boldsymbol{y} \mid \boldsymbol{x})\)</span>为“<span class="math inline">\(\boldsymbol{x}\)</span>的似然（给定<span class="math inline">\(\boldsymbol{y}\)</span>）”或“给定<span class="math inline">\(\boldsymbol{x}\)</span>，<span class="math inline">\(\boldsymbol{y}\)</span>的概率”，但不是<span class="math inline">\(\boldsymbol{y}\)</span>的似然（MacKay, 2003）。</p><blockquote><p>1、<strong>场景：连续测量——身高推断</strong></p><p>假设我们想根据某人的身高 <span class="math inline">\(Y\)</span> 推断其年龄 <span class="math inline">\(X\)</span>（以年为单位）。</p><ul><li><p><strong>先验分布 <span class="math inline">\(p(x)\)</span></strong>： 假设我们认为年龄 <span class="math inline">\(X\)</span> 在 0–100 岁之间大致服从均匀分布： <span class="math display">\[p(x) = \frac{1}{100}, \quad 0 \le x \le 100\]</span></p></li><li><p><strong>似然函数 <span class="math inline">\(p(y \mid x)\)</span></strong>： 假设对于给定年龄 <span class="math inline">\(X=x\)</span>，身高 <span class="math inline">\(Y\)</span> 服从正态分布： <span class="math display">\[Y \mid X=x \sim \mathcal{N}(\mu=x\cdot0.5 + 50, \sigma^2=10^2)\]</span> 即 <span class="math inline">\(\mu = 0.5x + 50\)</span> 厘米，<span class="math inline">\(\sigma = 10\)</span> 厘米。</p></li><li><p><strong>观测值</strong>： 假设测得身高 <span class="math inline">\(Y = 80\)</span> 厘米，我们想推断年龄 <span class="math inline">\(X\)</span>。</p></li></ul><p>2、<strong>连续贝叶斯公式</strong></p><p>连续情况下的贝叶斯定理为： <span class="math display">\[p(x \mid y) = \frac{p(y \mid x)\, p(x)}{p(y)}, \quad \text{其中 } p(y) = \int_0^{100} p(y \mid x)\, p(x)\, dx\]</span></p><ul><li><strong>分子</strong>：似然 × 先验</li><li><strong>分母</strong>：归一化因子（确保后验密度积分为1）</li></ul><p>所以： <span class="math display">\[p(x \mid Y=80) = \frac{\frac{1}{\sqrt{2\pi}10} \exp\Big[-\frac{(80-(0.5x+50))^2}{2\cdot10^2}\Big] \cdot \frac{1}{100}}{\int_0^{100} \frac{1}{\sqrt{2\pi}10} \exp\Big[-\frac{(80-(0.5x+50))^2}{2\cdot10^2}\Big] \cdot \frac{1}{100} dx}\]</span></p><blockquote><p>$p(y x) =  $</p><p>表示根据<span class="math inline">\(y\)</span>的分布函数，在<span class="math inline">\(y=80\)</span>对应的关于<span class="math inline">\(x\)</span>的函数。也就是，针对不同的<span class="math inline">\(x\)</span>, <span class="math inline">\(y=80\)</span> 的概率。</p></blockquote><p>3、直观理解</p><ul><li><strong>先验 <span class="math inline">\(p(x)\)</span></strong>：在观测身高之前我们对年龄的猜测。</li><li><strong>似然 <span class="math inline">\(p(y\mid x)\)</span></strong>：身高为 80 厘米时，不同年龄产生这个观测的可能性。</li><li><strong>后验 <span class="math inline">\(p(x\mid y)\)</span></strong>：结合观测数据后的年龄分布。</li></ul></blockquote><p><span class="math display">\[p(y) := \int p(y \mid x) p(x) \, dx = \mathbb{E}_{X}[p(y \mid x)] \tag{6.27}\]</span></p><p>是边缘似然 / 证据（marginal likelihood / evidence）。</p><p>(6.27) 的右边使用我们在第 6.4.1 节中定义的期望操作符。 根据定义，边缘似然是 (6.23) 贝叶斯公式的分子对潜在变量 <span class="math inline">\(\boldsymbol{x}\)</span> 的积分。因此，边缘似然与 <span class="math inline">\(\boldsymbol{x}\)</span> 无关，它保证了后验 $ p( ) $ 被标准化。 边缘似然也可以解释为期望似然，关于先验 <span class="math inline">\(p(\boldsymbol{x})\)</span> 的期望。 除了用于后验标准化之外，边缘似然在贝叶斯模型选择中也起着重要作用，我们将在第 8.6 节中讨论。 由于 (8.44) 中的积分，证据（evidence）通常很难计算。</p><p><strong>贝叶斯定理 (6.23) 允许我们反转由似然给出的 <span class="math inline">\(\boldsymbol{x}\)</span> 和 <span class="math inline">\(\boldsymbol{y}\)</span> 之间的关系。 因此，贝叶斯定理有时也被称为概率逆（probabilistic inverse）。</strong></p><p><strong>备注：</strong> 在贝叶斯统计中，后验分布是我们感兴趣的量，因为它包含了所有来自先验和数据的可用信息。<strong>我们把重点放在后验的一些统计量上，例如后验的最大值</strong>，这将在8.3节中讨论。<strong>然而，只关注后验的统计量会导致信息的丢失</strong>。如果我们在更大的背景下思考，那么后验还可以在决策系统中使用。<strong>拥有完整的后验非常有用</strong>，它可以得到对干扰具有鲁棒性的决策。例如，在基于模型的强化学习中，Deisenroth等人(2015)表明，使用似然转移函数的完全后验分布可以非常快速(数据/样本高效)学习，而关注最大的后验则会导致一致性失败。因此，拥有完整的后验对于下游任务非常有用。在第9章中，我们将在线性回归的背景下继续这个讨论。</p><h3 id="期望值">期望值</h3><p>期望值的概念是机器学习的中心，概率本身的一些基本概念可以从期望值派生 (Whittle, 2000)。</p><p><strong>定义 6.3 期望值</strong></p><p>关于单变量连续随机变量 <span class="math inline">\(X \sim p(X)\)</span> 的函数<br /><span class="math inline">\(g : \mathbb{R} \to \mathbb{R}\)</span> 的期望值（expected value）为： <span class="math display">\[\mathbb{E}_{X}[g(x)] = \int_{\mathcal{X}} g(x) \, p(x) \, \mathrm{d}x\]</span> 相应地，关于离散随机变量 <span class="math inline">\(X \sim p(X)\)</span> 的函数 <span class="math inline">\(g\)</span> 的期望值为： <span class="math display">\[\mathbb{E}_{X}[g(x)] = \sum_{x \in \mathcal{X}} g(x) \, p(x)\]</span> 其中 <span class="math inline">\(\mathcal{X}\)</span> 是随机变量 <span class="math inline">\(X\)</span> 的可能结果的集合（目标空间）。</p><p>在本节中，我们考虑离散随机变量的数值结果。通过<strong>观察函数 <span class="math inline">\(g\)</span></strong> 以实数作为输入可以看出这一点。</p><p><strong>定义 6.4 均值:</strong></p><p>状态 <span class="math inline">\(\boldsymbol{x} \in \mathbb{R}^D\)</span> 的随机变量 <span class="math inline">\(X\)</span> 的均值（mean）为平均值（average），定义为<br /><span class="math display">\[\mathbb{E}_{X}[\boldsymbol{x}] = \begin{bmatrix}\mathbb{E}_{X_1}[x_1] \\\vdots \\\mathbb{E}_{X_D}[x_D]\end{bmatrix}\in \mathbb{R}^{D}\]</span></p><p>对于 <span class="math inline">\(d = 1, \ldots, D\)</span>：<br /><span class="math display">\[\mathbb{E}_{X_d}[x_d] :=\begin{cases}\int_{\mathcal{X}} x_d \, p(x_d) \, dx_d, &amp; \text{如果 $X$ 为连续型随机变量}, \\[1em]\sum\limits_{x_i \in \mathcal{X}} x_i \, p(x_d = x_i), &amp; \text{如果 $X$ 为离散型随机变量}.\end{cases}\qquad (6.32)\]</span></p><p>其中下标 <span class="math inline">\(d\)</span> 表示 <span class="math inline">\(\boldsymbol{x}\)</span> 对应的维度。上式是对随机变量 <span class="math inline">\(X\)</span> 的目标空间状态 <span class="math inline">\(\mathcal{X}\)</span> 的积分以及求和。</p><p>在一个维度中，还有另外两个直观的“平均”概念，即 中位数 (median) 和 众数 (mode)。<br />如果我们对这些值进行排序，中位数就是“最中间”的值，即 <span class="math inline">\(50\%\)</span> 的值大于中位数，<span class="math inline">\(50\%\)</span> 的值小于中位数。这一思想可以推广到连续值，考虑累计分布函数（定义 6.2）为 <span class="math inline">\(0.5\)</span> 的值。 对于不对称或有长尾的分布，中位数提供了一个典型值的估计，该值比平均值更接近人类的直觉。 此外，中位数对异常值的鲁棒性比平均值强。中位数向更高维度的推广是非平凡的，因为目前没有方法可以在不止一个维度中“排序” (Hallin et al., 2010; Kong and Mizera, 2012)。<br />众数 (mode)是最常出现的值。对于离散随机变量，众数定义为出现频率最高的 <span class="math inline">\(x\)</span> 的值。 对于连续随机变量，众数定义为密度 <span class="math inline">\(p(\boldsymbol{x})\)</span> 上的一个峰值。 一个特定的密度 <span class="math inline">\(p(\boldsymbol{x})\)</span> 可能有不止一个众数，而且在高维分布中可能有大量的众数。 因此，找到一个分布的所有众数在计算上是具有挑战性的。</p><p>定义 6.3 定义了符号 <span class="math inline">\(\mathbb{E}_X\)</span> 的意义，作为一个算子，它表示我们应该取关于概率密度的积分（对于连续分布）或关于所有状态的和（对于离散分布）。<strong>均值的定义（定义 6.4），是期望值的一种特殊情况，通过取 <span class="math inline">\(g\)</span> 为恒等函数得到。</strong></p><p>在一个维度中，还有另外两个直观的“平均”概念，即中位数（median）和众数（mode）。如果我们对这些值进行排序，中位数就是“最中间”的值，即 <span class="math inline">\(50\%\)</span> 的值大于中位数，<span class="math inline">\(50\%\)</span> 的值小于中位数。这一思想可以推广到连续值，考虑累计分布函数（定义 6.2）为 <span class="math inline">\(0.5\)</span> 的值。对于不对称或有长尾的分布，中位数提供了一个典型值的估计值，该值比平均值更接近人类的直觉。此外，中位数对异常值的鲁棒性比平均值强。中位数向更高维度的推广是非平凡的，因为目前没有方法可以在不止一个维度中“排序”Hallin2010,Kong2012。</p><p>众数（mode）是最常出现的值。对于离散随机变量，众数定义为出现频率最高的 <span class="math inline">\(x\)</span> 的值。对于连续随机变量，众数定义为密度 <span class="math inline">\(p(x)\)</span> 上的一个峰值。一个特定的密度 <span class="math inline">\(p(x)\)</span> 可能有不止一个众数，而且在高维分布中可能有大量的众数。因此，找到一个分布的所有众数在计算上是具有挑战性的。</p><blockquote><p>众数是类似概率分布函数的局部最值吗？</p></blockquote><h3 id="协方差">协方差</h3><p>对于两个随机变量，我们可以描述它们之间的对应关系。协方差直观地表示随机变量之间的相关性。</p><blockquote><p>方差的概念是从协方差引出的，而不是相反！</p></blockquote><p><strong>协方差(一元)</strong></p><p>两个单变量随机变量 <span class="math inline">\(X, Y \in \mathbb{R}\)</span> 之间的协方差（covariance）由其偏离各自均值的期望积给出，即<br /><span class="math display">\[\operatorname{Cov}_{X, Y}[x, y] := \mathbb{E}_{X, Y}\left[ \left(x - \mathbb{E}_{X}[x]\right)\left(y - \mathbb{E}_{Y}[y]\right) \right]\]</span> <strong>术语：多元随机变量的协方差 <span class="math inline">\(\operatorname{Cov}[x, y]\)</span> 有时被称为交叉协方差（cross-covariance），其中协方差指的是 <span class="math inline">\(\operatorname{Cov}[x, x]\)</span>。</strong></p><p><strong>备注：</strong><br />当与期望或协方差相关的随机变量的参数明确时，下标通常被去掉（例如，<span class="math inline">\(\mathbb{E}_X[x]\)</span> 经常被写成 <span class="math inline">\(\mathbb{E}[x]\)</span>）。</p><p>利用期望的线性性，定义 6.5 中的表达式可以改写为乘积的期望值减去期望值的乘积，即<br /><span class="math display">\[\operatorname{Cov}[x, y] = \mathbb{E}[xy] - \mathbb{E}[x]\,\mathbb{E}[y]\]</span> <strong>一个变量与自身的协方差 <span class="math inline">\(\operatorname{Cov}[x, x]\)</span> 称为方差（variance），用 <span class="math inline">\(\mathbb{V}_X[x]\)</span> 表示。方差的平方根称为标准差（standard deviation），通常用 <span class="math inline">\(\sigma(x)\)</span> 表示。协方差的概念可以推广到多元随机变量。</strong></p><p><strong>定义 6.6 协方差（多元）</strong></p><p>如果我们考虑两个多元随机变量 <span class="math inline">\(\mathbf{X}\)</span> 和 <span class="math inline">\(\mathbf{Y}\)</span>，分别对应状态 <span class="math inline">\(\mathbf{x} \in \mathbb{R}^D\)</span> 和 <span class="math inline">\(\mathbf{y} \in \mathbb{R}^E\)</span>，则 <span class="math inline">\(\mathbf{X}\)</span> 和 <span class="math inline">\(\mathbf{Y}\)</span> 之间的协方差定义为： <span class="math display">\[{Cov}[\mathbf{x}, \mathbf{y}] = \mathbb{E}\!\left[\mathbf{x} \mathbf{y}^{\top}\right] - \mathbb{E}[\mathbf{x}]\,\mathbb{E}[\mathbf{y}]^{\top} = \operatorname{Cov}[\mathbf{y}, \mathbf{x}]^{\top} \in \mathbb{R}^{D \times E}.\]</span> <strong>定义 6.6 可以应用于两个相同的多元随机变量，从而产生一个有用的概念，直观地捕捉随机变量的“扩散程度”。对于一个多元随机变量，方差描述了该随机变量的单个维度之间的关系。</strong></p><p><strong>方差与协方差矩阵</strong> 状态为 <span class="math inline">\(\boldsymbol{x} \in \mathbb{R}^{D}\)</span> 且均值向量为 <span class="math inline">\(\boldsymbol{\mu} \in \mathbb{R}^{D}\)</span> 的随机变量 <span class="math inline">\(X\)</span> 的方差 (variance) 定义为<br /><span class="math display">\[\begin{align}V_X[\boldsymbol{x}] &amp;= \mathrm{Cov}_X[\boldsymbol{x}, \boldsymbol{x}] \\&amp;= \mathbb{E}_X \big[ (\boldsymbol{x} - \boldsymbol{\mu})(\boldsymbol{x} - \boldsymbol{\mu})^\top \big] \\&amp;= \mathbb{E}_X[\boldsymbol{x} \boldsymbol{x}^\top] - \mathbb{E}_X[\boldsymbol{x}] \ \mathbb{E}_X[\boldsymbol{x}]^\top \\&amp;=\begin{bmatrix}\mathrm{Cov}[x_1, x_1] &amp; \mathrm{Cov}[x_1, x_2] &amp; \cdots &amp; \mathrm{Cov}[x_1, x_D] \\\mathrm{Cov}[x_2, x_1] &amp; \mathrm{Cov}[x_2, x_2] &amp; \cdots &amp; \mathrm{Cov}[x_2, x_D] \\\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\\mathrm{Cov}[x_D, x_1] &amp; \mathrm{Cov}[x_D, x_2] &amp; \cdots &amp; \mathrm{Cov}[x_D, x_D]\end{bmatrix}.\end{align}\]</span> <strong>上式中的 <span class="math inline">\(D \times D\)</span> 矩阵称为多元随机变量 <span class="math inline">\(X\)</span> 的 协方差矩阵 (covariance matrix)。 协方差矩阵是对称的，且半正定，它描述了数据的扩散程度。</strong>协方差矩阵的对角线元素包含了边缘分布 <span class="math display">\[\begin{equation}p(x_i) = \int p(x_1, \ldots, x_D) \ \mathrm{d}x_{\backslash i}\end{equation}\]</span> 的方差，其中符号 <span class="math inline">\(\backslash i\)</span>表示“除了 <span class="math inline">\(i\)</span> 之外的所有变量”。 非对角线元素为交叉协方差项 <span class="math inline">\(\mathrm{Cov}[x_i, x_j]\)</span>，其中 <span class="math inline">\(i, j = 1, \ldots, D, \ i \neq j\)</span>。</p><h3 id="相关"><strong>相关</strong></h3><p>当我们想比较不同随机变量对之间的协方差时，每个随机变量的方差都会影响协方差的值。协方差的标准化版本称为<strong>相关</strong>（correlation）</p><p>两个随机变量 <span class="math inline">\(X, Y\)</span> 的相关（correlation）为： <span class="math display">\[\operatorname{corr}[x, y] = \frac{\operatorname{Cov}[x, y]}{\sqrt{\mathbb{V}[x] \, \mathbb{V}[y]}} \in [-1, 1]\]</span> 相关矩阵是被标准化的随机变量<span class="math inline">\(x/{\sigma(x)}\)</span>协方差矩阵。换句话说，每个随机变量在相关矩阵中都除以其标准差（方差的平方根）。</p><p><strong>经验均值和协方差</strong></p><blockquote><p><strong>为什么 <span class="math inline">\(N-1\)</span> 是“无偏(unbiased)”？</strong></p><ul><li>当我们从总体中抽样时，用样本均值 <span class="math inline">\(\bar{x}\)</span> 和 <span class="math inline">\(\bar{y}\)</span> 代替真实均值，会引入<strong>低估总体方差和协方差</strong>的偏差。</li><li>分母用 <span class="math inline">\(N-1\)</span> 而不是 <span class="math inline">\(N\)</span> 可以修正这个偏差，使得估计值的期望等于真实值。</li></ul></blockquote><h3 id="方差的三个表达式">方差的三个表达式</h3><p>1、方差的标准定义</p><p>与协方差的定义（定义6.5）相对应，是随机变量 <span class="math inline">\(X\)</span> 与其期望值 <span class="math inline">\(\mu\)</span> 的平方偏差的期望，即<br /><span class="math display">\[\begin{equation}\mathbb{V}_{X}[x] := \mathbb{E}_{X} \left[ (x - \mu)^{2} \right]\tag{6.43}\end{equation}\]</span> 式（6.43）中的期望和平均值<span class="math inline">\(\mu = \mathbb{E}_{X}(x)\)</span>使用（6.32）计算，取决于 <span class="math inline">\(X\)</span> 是离散的还是连续的随机变量。式（6.43）中表示的方差可以说是一个新的随机变量<span class="math inline">\(Z := (X - \mu)^{2}\)</span>的均值。 当根据经验估计（6.43）中的方差时，我们需要使用一个两阶段的算法：首先利用数据使用（6.41）计算平均值 <span class="math inline">\(\mu\)</span>，然后使用这个估计值 <span class="math inline">\(\hat{\mu}\)</span> 计算方差。</p><p>2、事实证明，我们可以通过整理表达式来避免两个阶段。方差的标准定义： <span class="math display">\[\begin{equation}\mathbb{V}_{X}[x] := \mathbb{E}_{X}\left[(x-\mu)^2\right]\end{equation}\]</span> （6.43）可以转换为所谓的“方差的原始分数公式”（raw-score formula for variance）： <span class="math display">\[\begin{equation}\mathbb{V}_{X}[x] = \mathbb{E}_{X}\left[x^2\right] - \left(\mathbb{E}_{X}[x]\right)^2 \qquad (6.44)\end{equation}\]</span> (6.44)中的表达式可以这样记住：“平方的均值减去均值的平方”。这种方法只需对数据进行一次遍历计算，因为我们可以同时计算每个观测值 <span class="math inline">\(x_i\)</span> 的平均值 <span class="math inline">\(\bar{x} = \frac{1}{n}\sum_i x_i\)</span> 和平方平均值 <span class="math inline">\(\frac{1}{n}\sum_i x_i^2\)</span>。</p><p>不幸的是，如果以这种方式计算，它在数值上可能不稳定，尤其是当 <span class="math inline">\(x_i\)</span> 的数值很大时，减法可能导致有效数字损失。</p><p>3、理解方差的第三种方式是，它可以表示为所有观测值对的差的总和。<br />考虑随机变量 <span class="math inline">\(X\)</span> 的一个样本 <span class="math inline">\(x_1, \ldots, x_N\)</span>，我们计算 <span class="math inline">\(x_i\)</span> 和 <span class="math inline">\(x_j\)</span> 对之间的平方差。通过展开平方差，可以证明 <span class="math inline">\(N^2\)</span> 个观测值对的差的总和正好与观测值的经验方差相关： <span class="math display">\[\begin{equation}\frac{1}{N^2} \sum_{i,j=1}^{N} (x_i - x_j)^2 = 2 \left[ \frac{1}{N} \sum_{i=1}^{N} x_i^2 - \left( \frac{1}{N} \sum_{i=1}^{N} x_i \right)^2 \right] \qquad (6.45)\end{equation}\]</span> 由此可见，(6.45) 是原始分数公式 (6.44) 的两倍。这意味着，我们可以用观测值两两之间的距离总和（<span class="math inline">\(N^2\)</span> 个）来表示偏离均值的偏离值总和（<span class="math inline">\(N\)</span> 个）。 <strong>从几何上来看，这意味着在一个点集中，点两两之间的距离总和与每个点到点集中心的距离总和是等价的。</strong></p><h3 id="随机变量的和与变换">随机变量的和与变换</h3><p>均值和协方差在随机变量的仿射变换中表现出一些有用的特性。<br />假设随机变量 <span class="math inline">\(\boldsymbol{X}\)</span> 的均值向量为 <span class="math inline">\(\boldsymbol{\mu}\)</span>，协方差矩阵为 <span class="math inline">\(\boldsymbol{\Sigma}\)</span>，且 <span class="math inline">\(\boldsymbol{X}\)</span> 的（确定性）仿射变换为： <span class="math display">\[\boldsymbol{y} = \boldsymbol{A} \boldsymbol{x} + \boldsymbol{b}\]</span> 由于 <span class="math inline">\(\boldsymbol{y}\)</span> 本身是一个随机变量，其均值向量和协方差矩阵分别为： <span class="math display">\[\begin{equation}\mathbb{E}_Y[\boldsymbol{y}] = \mathbb{E}_X[\boldsymbol{A}\boldsymbol{x} + \boldsymbol{b}]= \boldsymbol{A}\,\mathbb{E}_X[\boldsymbol{x}] + \boldsymbol{b} = \boldsymbol{A}\boldsymbol{\mu} + \boldsymbol{b} \qquad (6.50)\end{equation}\]</span></p><p><span class="math display">\[\begin{equation}\mathbb{V}_Y[\boldsymbol{y}] = \mathbb{V}_X[\boldsymbol{A}\boldsymbol{x} + \boldsymbol{b}]= \mathbb{V}_X[\boldsymbol{A}\boldsymbol{x}]= \boldsymbol{A}\,\mathbb{V}_X[\boldsymbol{x}]\,\boldsymbol{A}^\top= \boldsymbol{A}\boldsymbol{\Sigma}\boldsymbol{A}^\top \qquad (6.51)\end{equation}\]</span></p><p>此外，<span class="math inline">\(\boldsymbol{X}\)</span> 与 <span class="math inline">\(\boldsymbol{Y}\)</span> 的协方差为： <span class="math display">\[\begin{align}\mathrm{Cov}[\boldsymbol{x}, \boldsymbol{y}]&amp;= \mathbb{E}\big[\boldsymbol{x} (\boldsymbol{A}\boldsymbol{x} + \boldsymbol{b})^\top\big]   - \mathbb{E}[\boldsymbol{x}]\,\mathbb{E}[\boldsymbol{A}\boldsymbol{x} + \boldsymbol{b}]^\top \\&amp;= \mathbb{E}[\boldsymbol{x}]\boldsymbol{b}^\top + \mathbb{E}[\boldsymbol{x}\boldsymbol{x}^\top]\boldsymbol{A}^\top   - \boldsymbol{\mu}\boldsymbol{b}^\top - \boldsymbol{\mu}\boldsymbol{\mu}^\top \boldsymbol{A}^\top \\&amp;= (\mathbb{E}[\boldsymbol{x}\boldsymbol{x}^\top] - \boldsymbol{\mu}\boldsymbol{\mu}^\top)\,\boldsymbol{A}^\top= \boldsymbol{\Sigma}\,\boldsymbol{A}^\top\end{align}\]</span> 其中，<span class="math inline">\(\boldsymbol{\Sigma} = \mathbb{E}[\boldsymbol{x}\boldsymbol{x}^\top] - \boldsymbol{\mu}\boldsymbol{\mu}^\top\)</span> 为 <span class="math inline">\(\boldsymbol{X}\)</span> 的方差矩阵。</p><h3 id="统计独立性">(统计)独立性</h3><p>独立推出不相关（协方差为0）</p><p>但这一点是充分不必然的，即，两个随机变量的协方差为零，但在统计上可能不独立。为了理解为什么，回想一下<strong>协方差是只能测量线性相关</strong>。而非线性相关的随机变量可能协方差也为零。</p><p>详见《协方差只反映线性相关.md》</p><h3 id="随机变量的内积">随机变量的内积</h3><p><strong>把协方差作为内积的一个例子，协方差符合内积的定义。例如点积符合内积的定义一样。</strong>两个随机变量的“正交”就意味着它们协方差为零。具体的内积的定义见前文。在用协方差定义内积空间后，可以得出： <span class="math display">\[\|X\| = \sqrt{\mathbb{V}[X]} = \sigma[X]\]</span> <strong>这就是标准差。在普通向量空间里，向量长短衡量了它在空间中“延伸”的程度。在<em><u>随机变量空间里</u></em>，“长度”衡量的是它在概率空间中“波动”的程度。如果 <span class="math inline">\(\sigma[X] = 0\)</span>，那么 <span class="math inline">\(\mathbb{V}[X] = 0\)</span>，意味着 <span class="math inline">\(X\)</span> <em><u>在概率意义上</u></em>是常数，不会随机变化。</strong></p><p>如果我们考虑两个随机变量 <span class="math inline">\(X\)</span>、<span class="math inline">\(Y\)</span> 之间的夹角 <span class="math inline">\(\theta\)</span>，我们得到： <span class="math display">\[\begin{equation}\cos\theta = \frac{\langle X, Y \rangle}{\|X\|\,\|Y\|}= \frac{\operatorname{Cov}[x, y]}{\sqrt{\mathbb{V}[x]\,\mathbb{V}[y]}}\end{equation}\]</span> 这是两个随机变量之间的相关性（定义 6.8）。这意味着，当我们从几何角度考虑两个随机变量时，可以把它们的相关性看作是两个随机变量之间夹角的余弦。</p><p>根据定义 3.7，我们知道：<span class="math inline">\(X \perp Y \quad \Longleftrightarrow \quad \langle X, Y\rangle = 0\)</span> 在我们的例子中，这意味着 <span class="math inline">\(X\)</span> 和 <span class="math inline">\(Y\)</span> 是正交的当且仅当：<span class="math inline">\(\operatorname{Cov}[X, Y] = 0\)</span>即它们是不相关的。图 6.6 说明了这种关系。</p><p><img src="/img3/机器学习的数学基础Part1/随机变量的几何.png" alt="随机变量的几何" style="zoom:50%;" /></p><p>随机变量的几何。如果随机变量<span class="math inline">\(X\)</span>和<span class="math inline">\(Y\)</span>不相关，则它们是对应向量空间中的正交向量，且勾股定理也适用。</p><h3 id="概率分布的距离定义">概率分布的距离定义</h3><p>使用之前内积定义的欧几里得距离来比较概率分布似乎是个不错的选择，但不幸的是，这不是获得分布之间距离的最佳方法。回想一下，概率质量(或密度)是正的，需要加起来等于1。这些限制意味着分布存在于一种叫做统计流形(statistical manifold)的东西上。对概率分布空间的研究被称为信息几何( information geometry)。计算分布之间的距离通常使用Kullback-Leibler散度(KL散度)来完成，它是距离的推广，它解释了统计流形的性质。正如欧氏距离是矩阵的一种特殊情况一样(第3.3节)，KL散度是另外两种广义散度的一种特殊情况，它们被称为Bregman散度和f ff-散度。关于它们区别的研究超出了这本书的范围，读者可以参考信息几何领域的创始人之一Amari(2016)的新书了解更多细节。</p><p>具体的例子见《概率分布的距离定义.md》</p><h3 id="高斯分布">高斯分布</h3><p>由于高斯分布完全由其均值和协方差来表示，我们通常可以通过对随机变量的均值和协方差进行变换来得到变换后的分布。<strong>高斯分布的边缘分布和条件分布是高斯分布</strong>。高斯随机变量的任何线性/仿射变换后依然服从高斯分布。</p><h3 id="共轭与指数族">共轭与指数族</h3><p>在应用概率运算法则时，存在一些“封闭性”，如贝叶斯定理。<strong>封闭是指对一类对象应用特定操作后返回相同类型的对象。</strong></p><h4 id="beta分布"><strong>Beta分布</strong></h4><p>我们可能想在有限区间上建立一个连续随机变量的模型。Beta分布是一个连续随机变量 <span class="math inline">\(\mu \in [0,1]\)</span> 上的分布，通常用来表示一些二值事件的概率（例如，控制伯努利分布的参数）。Beta分布 <span class="math inline">\(\mathrm{Beta}(\alpha, \beta)\)</span>（如图6.11所示）本身由两个参数控制 <span class="math inline">\(\alpha &gt; 0, \ \beta &gt; 0\)</span>，并被定义为 <span class="math display">\[\begin{equation}p(\mu \mid \alpha, \beta) = \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha) \, \Gamma(\beta)} \, \mu^{\alpha-1} (1-\mu)^{\beta-1}\tag{6.98}\end{equation}\]</span></p><p><span class="math display">\[\mathbb{E}[\mu] = \frac{\alpha}{\alpha+\beta}, \quad\mathbb{V}[\mu] = \frac{\alpha \beta}{(\alpha+\beta)^2 (\alpha+\beta+1)}\]</span></p><p>其中 <span class="math inline">\(\Gamma(\cdot)\)</span> 为伽马（Gamma）函数，定义为： <span class="math display">\[\Gamma(t) := \int_{0}^{\infty} x^{t-1} e^{-x} \, dx, \quad t &gt; 0,\qquad\Gamma(t+1) = t \, \Gamma(t)\]</span> 注意，（6.98）中的Gamma函数的分数标准化了Beta分布。</p><p>直观地看，<span class="math inline">\(\alpha\)</span> 将概率质量移向 <span class="math inline">\(1\)</span>，而 <span class="math inline">\(\beta\)</span> 将概率质量移向 <span class="math inline">\(0\)</span>。一些特殊的情况：</p><ul><li>对于 <span class="math inline">\(\alpha = 1 = \beta\)</span>，我们得到均匀分布 <span class="math inline">\(\mathcal{U}[0,1]\)</span>。</li><li>对于 <span class="math inline">\(\alpha, \beta &lt; 1\)</span>，我们得到双峰分布，峰值在 <span class="math inline">\(0\)</span> 和 <span class="math inline">\(1\)</span> 处。</li><li>对于 <span class="math inline">\(\alpha, \beta &gt; 1\)</span>，分布是单峰的。</li><li>对于 <span class="math inline">\(\alpha, \beta &gt; 1\)</span> 且 <span class="math inline">\(\alpha = \beta\)</span>，分布是单峰对称的，且集中在区间 <span class="math inline">\([0,1]\)</span>，即均值在 <span class="math inline">\(1/2\)</span>。</li></ul><p><img src="/img3/机器学习的数学基础Part1/beta分布.png" alt="beta分布" style="zoom:50%;" /></p><p>有一大堆有名字的分布，它们以不同的方式相互联系(Leemis和McQueston, 2008)。值得记住的是，<strong>每个被命名的分布都是为了特定的原因而创建的，但是可能还有其他的应用。了解创建特定分布背后的原因可以知道如何最好地使用它</strong>。</p><h4 id="共轭">共轭</h4><p><strong>共轭先验</strong></p><p>如果<strong>后验与先验具有相同的形式/类型</strong>，则<strong><em><u>先验</u></em></strong>是<u><strong><em>似然函数</em></strong></u>的共轭(conjugate)。共轭特别方便，因为我们可以通过更新先验分布的参数来用代数方法计算后验分布。</p><p><strong>备注：</strong> <strong>在考虑概率分布的几何时，共轭先验保留了似然的距离结构(Agarwal and Daum´e III, 2010)</strong></p><p>对于二项分布似然函数中的参数<span class="math inline">\(\mu\)</span>，Beta先验是共轭的。Beta分布是伯努利分布的共轭先验。</p><p><img src="/img3/机器学习的数学基础Part1/常见似然函数的共轭先验.png" alt="常见似然函数的共轭先验" style="zoom:50%;" /></p><p>表6.2列出了一些<strong>用于概率建模的标准似然的参数的共轭先验</strong>。分布如多项式分布、逆Gamma分布、逆Wishart分布和Dirichlet分布可以在任何统计文本中找到，例如在Bishop(2006)中就进行了描述。</p><p>Beta分布是二项分布和伯努利分布似然中<strong>关于参数<span class="math inline">\(\mu\)</span>的共轭先验</strong>。对于高斯似然函数，我们可以在均值上设置一个共轭高斯先验。高斯似然在表中出现两次的原因是我们需要区分一元和多元情况。在一元（标量）情况下，逆Gamma是<strong>方差</strong>的共轭先验。在多元情形下，我们使用逆Wishart分布作为<strong>协方差矩阵</strong>的共轭先验。Dirichlet分布是多项式似然函数的共轭先验。</p><h4 id="指数族">指数族</h4><p><strong>分布三个抽象级别</strong>：</p><p>在考虑分布（离散或连续的随机变量）时，我们可以有三个可能的抽象级别。</p><p>在第一级（这是最具体的级别），我们有一个具有固定参数的特定“命名”分布，例如一个均值为零，方差为单位矩阵的一元高斯分布 <span class="math inline">\(\mathcal{N}(0,1)\)</span>。</p><p>而在机器学习中，我们经常使用第二层抽象，即我们采用参数形式固定的分布（如一元高斯分布），并从数据中推断出它的参数。例如，我们假设一个未知均值 <span class="math inline">\(\mu\)</span> 和未知方差 <span class="math inline">\(\sigma^2\)</span> 的一元高斯分布 <span class="math inline">\(\mathcal{N}\left(\mu, \sigma^{2}\right)\)</span>，并使用最大似然拟合来确定最佳参数 <span class="math inline">\((\mu, \sigma^2)\)</span>。我们将在第9章讨论线性回归时看到一个例子。</p><p>第三个抽象的层次是考虑分布的族，在本书中，我们考虑指数族。一元高斯分布是指数族中的一个例子。许多广泛使用的统计模型，包括表 6.2 中所有的“命名”模型，都属于指数族。它们都可以统一成一个概念。</p><p><strong>我们研究指数族的主要动机是它们具有有限维的充分统计信息。此外，共轭分布很容易写出来，而且也来自一个指数族。</strong>从推理的角度来看，指数族的极大似然估计表现得很好，因为它的充分统计量的经验估计是充分统计量总体的最佳估计（回忆一下高斯分布的均值和协方差）。<strong>从优化的角度来看，指数族的的对数似然函数是凹的，允许我们应用有效的优化方法</strong>(第7章)。</p><h3 id="变量替换逆变换">变量替换/逆变换</h3><p>似乎有很多已知的分布，但实际上，我们可以命名的分布是非常有限的。因此，理解随机变量在变换后是如何分布的通常很有用。例如，假设 <span class="math inline">\(X\)</span> 是根据一元正态分布 <span class="math inline">\(\mathcal{N}(0,1)\)</span> 得到的一个随机变量。那么 <span class="math inline">\(X^2\)</span> 的分布是什么？ 另一个在机器学习中很常见的例子是，假设 <span class="math inline">\(X_1\)</span> 和 <span class="math inline">\(X_2\)</span> 是一元标准正态分布，那么 <span class="math inline">\(\frac{1}{2} (X_1 + X_2)\)</span>的分布是什么？计算 <span class="math inline">\(\frac{1}{2} (X_1 + X_2)\)</span> 的分布的一个选择是计算 <span class="math inline">\(X_1\)</span> 和 <span class="math inline">\(X_2\)</span> 的均值和方差，然后组合它们。正如我们在第 6.4.4 节中看到的，当我们考虑随机变量的仿射变换时，我们可以计算变换后得到的随机变量的均值和方差。 然而，我们也可能无法得到变换后分布的函数形式。此外，我们还可能关心随机变量的非线性变换，这时变换后的封闭形式的表达式是不容易得到的。</p><p>我们将介绍两种通过随机变量变换获取分布的方法：<strong>一种是使用累积分布函数定义的直接方法，另一种是使用微积分的链式法则(第5.2.2节)的变量替换(change-of-variable)方法</strong>。<strong>变量替换方法被广泛使用，因为它提供了一个用于计算由于转换而产生的分布的“秘诀”。</strong>我们将解释关于一元随机变量的变量替换技术，并将简要地给出多元随机变量的一般情况的结果。</p><h4 id="分布函数技术">分布函数技术</h4><p>均匀分布在统计中的重要性</p><ul><li><strong>基准分布</strong>：在随机数生成和蒙特卡罗模拟中，均匀分布是最常用的基础分布（大多数随机数生成器先生成 <span class="math inline">\(U(0,1)\)</span>，再通过变换得到其他分布）。</li><li><strong>概率积分变换</strong>：<strong>定理 6.15</strong> 就是基于这个思想：如果 <span class="math inline">\(X\)</span> 的 CDF 是严格单调的，那么 <span class="math inline">\(Y = F_X(X)\)</span> 服从 <span class="math inline">\(U(0,1)\)</span>。</li></ul><p>将随机变量<span class="math inline">\(X\)</span>的累积分布函数<span class="math inline">\(F_X(x)\)</span>作为变换函数<span class="math inline">\(U(x)\)</span>，可以得到一个有用的结果,这导出了下面的定理。</p><p><strong>定理 6.15</strong> 令 <span class="math inline">\(X\)</span> 为连续随机变量，且具有严格单调的累积分布函数 <span class="math inline">\(F_X(x)\)</span>。 那么定义为 <span class="math display">\[Y := F_X(X)\]</span> 的随机变量 <span class="math inline">\(Y\)</span> 具有均匀分布。</p><p><strong>证明：</strong></p><p>对任意 <span class="math inline">\(y\in[0,1]\)</span>，由于 <span class="math inline">\(F_X\)</span> 严格单调且连续，存在反函数 <span class="math inline">\(F_X^{-1}:(0,1)\to \mathbb{R}\)</span>。于是 <span class="math display">\[\begin{aligned}F_Y(y)&amp;:=\mathbb{P}(Y\le y)=\mathbb{P}\big(F_X(X)\le y\big) \\&amp;=\mathbb{P}\big(X\le F_X^{-1}(y)\big)=F_X\!\big(F_X^{-1}(y)\big)= y,\qquad y\in[0,1].\end{aligned}\]</span> 对区间外的 <span class="math inline">\(y\)</span>，有 <span class="math inline">\(F_Y(y)=0\)</span>（当 <span class="math inline">\(y&lt;0\)</span>）和 <span class="math inline">\(F_Y(y)=1\)</span>（当 <span class="math inline">\(y&gt;1\)</span>）。 因此 <span class="math inline">\(F_Y(y)=y\)</span>（<span class="math inline">\(y\in[0,1]\)</span>），即 <span class="math inline">\(Y\sim \mathrm{Unif}(0,1)\)</span>。</p><p>定理6.15被称为<strong>概率积分变换</strong>(probability integral transform)，它用于推导<strong><em><u>从分布中采样</u></em></strong>的算法，<strong>这个算法通过均匀随机变量的采样结果进行变换（Bishop，2006）。该算法的工作原理是首先从均匀分布生成样本，然后通过逆累计密度函数（假设这是可以得到的）对其进行变换，以从所需分布获得样本。</strong>概率积分变换也用于假设检验样本是否来自特定分布（Lehmann和Romano，2005）。累积分布函数的输出是均匀分布的这一观点也构成了copulas的基础（Nelsen，2006）。</p><p><strong>直观理解</strong>：</p><ul><li>均匀分布随机数就像一个“百分比刻度”</li><li>逆 CDF 就是把这个刻度映射到目标分布的位置</li><li>这样就能用均匀分布随机数生成任何我们想要的分布样本</li></ul><h4 id="变量替换">变量替换</h4><p>区间可逆的函数要么严格递增要么严格递减。</p><blockquote><p>事件的等价变换不会改变概率</p></blockquote><p>微积分的基本定理中，有一个非常经典的结论：<strong>积分上限函数求导</strong>，<strong>它把导数与积分直接联系了起来</strong>。</p><p>详见《积分上限函数求导.md》。</p><p><strong>备注：</strong><br />“变量替换”这个名字来源于当我们面对一个困难的积分时改变积分变量的想法。 对于一元函数，我们使用换元积分法：<br /><span class="math display">\[\begin{equation}\int f(g(x)) g^{\prime}(x)\, \mathrm{d}x = \int f(u)\, \mathrm{d}u, \quad \text{其中} \quad u = g(x) \qquad (6.133)\end{equation}\]</span> 该法则的推导基于微积分的链式法则 (5.32)，以及应用两次微积分基本定理。 微积分基本定理证明了积分和微分在某种程度上是互“逆”的。 通过（松散地）考虑方程<span class="math inline">\(u = g(x)\)</span>的微小变化（微分），即把<span class="math inline">\(\Delta u = g^{\prime}(x)\, \Delta x\)</span>看作 <span class="math inline">\(u = g(x)\)</span> 的微分，可以直观地理解这个规则。 将 <span class="math inline">\(u = g(x)\)</span> 代入，积分 (6.133) 右边的参数变成了 <span class="math inline">\(f(g(x))\)</span>。 通过假设 <span class="math inline">\(\mathrm{d}u \approx \Delta u = g^{\prime}(x)\, \Delta x,\mathrm{d}x \approx \Delta x,\)</span>我们最终得到了 (6.133)。</p><p><strong>定理</strong> 6.16:</p><p>令 <span class="math inline">\(f(\boldsymbol{x})\)</span> 是多变量连续随机变量 <span class="math inline">\(\boldsymbol{X}\)</span> 的概率密度函数。如果向量值函数 <span class="math inline">\(\boldsymbol{y} = U(\boldsymbol{x})\)</span>在定义域内对于所有 <span class="math inline">\(\boldsymbol{x}\)</span> 可微且可逆，那么对应的随机变量 <span class="math inline">\(\boldsymbol{Y} = U(\boldsymbol{X})\)</span>的概率密度函数由下式给出： <span class="math display">\[f(\boldsymbol{y}) = f_{\boldsymbol{x}}\bigl(U^{-1}(\boldsymbol{y})\bigr) \cdot \left|\frac{d}{d \boldsymbol{y}} U^{-1}(\boldsymbol{y})\right|,\]</span> 其中 <span class="math inline">\(\left|\frac{d}{d \boldsymbol{y}} U^{-1}(\boldsymbol{y})\right|\)</span> 表示雅可比矩阵的行列式的绝对值。</p><p>这个定理的关键是多元随机变量的变量替换遵循单变量变量替换的过程。首先需要求出逆变换 <span class="math inline">\(U^{-1}\)</span>，并将其代入 <span class="math inline">\(\boldsymbol{x}\)</span> 的密度函数 <span class="math inline">\(f_{\boldsymbol{x}}(\boldsymbol{x})\)</span> 中，然后计算雅可比矩阵的行列式，并与密度函数相乘得到结果。</p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 数学 </tag>
            
            <tag> 算法 </tag>
            
            <tag> 统计 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《机器学习的数学基础》</title>
      <link href="/2025/10/24/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%B8%83/"/>
      <url>/2025/10/24/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%B8%83/</url>
      
        <content type="html"><![CDATA[<h2 id="连续优化">连续优化</h2><p>Continuous Optimization</p><p>由于机器学习算法是在计算机上实现的，其中许多<strong>数学方程式都表示为数值优化方法</strong>。本章描述了训练机器学习模型的基本数值方法。训练机器学习模型通常归结为找到一组好的参数。“好”的概念是由目标函数或概率模型来决定的，我们将在本书的第二部分看到这些例子。给定一个目标函数，使用优化算法来寻找最佳值。$ ^{D}$ 中考虑数据和模型，所以我们面临的优化问题是连续优化问题，而不是离散变量的组合优化问题。</p><p>一般情况下，机器学习中的大多数目标函数都是要被最小化的，即最优值就是最小值。直观上，梯度为目标函数每个点的上坡方向，而我们的目的是下坡（与梯度方向相反），希望找到最深的点。</p><span id="more"></span><p>我们将学习一类称为凸函数的函数，它们对优化算法的起点没有依赖性。对于凸函数，局部最小值就是全局最小值。事实证明，许多机器学习目标函数的设计都是凸的.</p><blockquote><p>一般研究最小值，所以对应研究的也是凸函数。</p></blockquote><h3 id="使用梯度下降法优化">使用梯度下降法优化</h3><p>梯度下降法是一种一阶优化算法。为了用梯度下降法寻找函数的局部最小值，我们取与函数在当前点的梯度的一定比例的负值调整<span class="math inline">\(x\)</span>。回想第5.1节，梯度指向最陡的上升方向。另一个直观的表现是考虑函数的等高线。梯度指向与我们希望优化的函数的等高线正交的方向。</p><blockquote><p><strong>梯度一定是指向上升的方向</strong>：</p><p>严格来说，<strong>梯度确实指向函数在某一点上“最陡上升”的方向</strong>。梯度的符号和大小告诉我们<strong>函数值局部上升最快的方向</strong>和<strong>速率</strong>。对于一元函数，梯度的正负直接指明函数上升的方向。</p><p><strong>梯度的几何意义</strong></p><ul><li>梯度方向是<strong>函数值增加最快的方向</strong>。</li><li>梯度的大小 <span class="math inline">\(\|\nabla f(\boldsymbol{x})\|\)</span> 表示<strong>该方向上函数上升的速率</strong>。</li><li>如果沿着梯度的负方向移动 <span class="math inline">\(-\nabla f(\boldsymbol{x})\)</span>，则是下降最快的方向（常用于梯度下降算法）。</li></ul><p><strong>梯度是函数上升最快的方向</strong>。但它只描述局部方向，想找最大值还需要结合函数整体信息。</p><p>例如 <span class="math inline">\(y=x^2\)</span>,导数是<span class="math inline">\(2x\)</span>，针对负轴，导数&lt;0;所以沿<span class="math inline">\(x\)</span>轴负方向是函数值上升最快的方向，反之也能推导出。</p></blockquote><p>自适应梯度方法能根据函数的局部性质，在每次迭代时重新调整步长。有两种简单的方法（Toussaint，2012）：</p><ul><li>当采用梯度下降后函数值增大了，则表明步长过大了。这时我们可以撤消并减小步长。</li><li>当函数值减小时，我们可以尝试增大步长。</li></ul><p>尽管“撤消”步骤似乎是浪费资源，但使用这种启发式方法可以保证单调收敛。</p><h5 id="动量梯度下降">动量梯度下降：</h5><p>如图7.3所示，如果优化的曲面的存在缩放效果不佳的区域，那么梯度下降的收敛速度可能非常慢。改进收敛性的一个方法是给梯度下降添加记忆项。动量梯度下降法（Rumelhart et al.，1986）引入了附加项来记忆上一次迭代中发生了什么。这种记忆可以抑制振荡，平滑梯度更新。继续用球做类比，动量项模拟了重球从斜坡滚下不会轻易改变方向的现象。其思想是使用记忆进行梯度更新，以实现滑动平均( moving average)。<strong>基于动量的方法记忆了每次迭代的更新量<span class="math inline">\(\Delta \boldsymbol{x}_{i}\)</span>,下一次更新则用当前和之前梯度的线性组合确定：</strong></p><h5 id="随机梯度下降法">随机梯度下降法：</h5><p><strong>计算梯度非常耗时。然而，通常可以找到梯度的“<em>cheap</em>”近似值。只要近似梯度指向与真实梯度大致相同的方向，那么它就是有用的。</strong></p><p>随机梯度下降法（Stochastic gradient descent，通常简称为SGD）是梯度下降法的一种随机近似方法，用于最小化目标函数，且目标函数应是可微函数之和。这里的“随机”一词指的是这样一个事实，即我们承认我们并不精确地知道梯度，而是只知道它的噪声近似值。通过约束近似梯度的概率分布，我们仍然可以从理论上保证SGD收敛。</p><p>如前所述，标准梯度下降法是一种“批量”优化方法，即选取合适的步长参数 <span class="math inline">\(gamma_i\)</span>，并使用所有训练集，然后通过以下公式更新向量参数： <span class="math display">\[\boldsymbol{\theta}_{i+1} = \boldsymbol{\theta}_{i} - \gamma_{i} \left( \nabla L(\boldsymbol{\theta}_{i}) \right)^{\top} = \boldsymbol{\theta}_{i} - \gamma_{i} \sum_{n=1}^{N} \left( \nabla L_{n}(\boldsymbol{\theta}_{i}) \right)^{\top} \qquad (7.15)\]</span> 求梯度和可能需要对所有<span class="math inline">\(L_n\)</span>的梯度进行计算。当训练集是巨大的，或者（并且）损失函数不是简单的公式时，计算梯度和需要消耗巨大的计算资源。</p><p>考虑 (7.15) 中的<span class="math inline">\(\sum_{n=1}^{N}\left(\nabla L_{n}\left(\boldsymbol{\theta}_{i}\right)\right),\)</span>我们可以取较小的 <span class="math inline">\(L_n\)</span> 集合来求它们的梯度和以减少计算量。与批量梯度下降法使用 <span class="math inline">\(n=1,\ldots,N\)</span> 的所有 <span class="math inline">\(L_n\)</span> 不同，小批量<strong>梯度下降随机选取 <span class="math inline">\(L_n\)</span> 的子集</strong>。在极端情况下，我们还可以随机选择一个 <span class="math inline">\(L_n\)</span> 来估计梯度。</p><p>关于为什么选取数据子集是明智的，关键的见解是要认识到，为了使梯度下降收敛，我们只要求梯度是真实梯度的无偏估计。事实上，(7.15) 中的<span class="math inline">\(\sum_{n=1}^{N}\left(\nabla L_{n}\left(\boldsymbol{\theta}_{i}\right)\right)\)</span>是梯度期望值（第 6.4.1 节）的经验估计。因此，对期望值的任何其他无偏经验估计，例如使用数据的任何子样本，将足以使梯度下降收敛。</p><p><strong>备注</strong>： <strong>当学习率以适当的速率下降时，在相对不严格的假设下，随机梯度下降必然收敛到局部最小值（Bottou, 1998）。</strong></p><p>为什么要考虑使用近似梯度？一个主要原因是硬件的限制，例如中央处理器（CPU）/图形处理器（GPU）内存大小或计算时间的限制。我们可以像在估计经验平均数时考虑样本的大小一样（第 6.4.1 节），来考虑用来估计梯度的子集的大小。 较大的小批量将提供准确的梯度估计，减少参数更新中的方差。此外，较大的小批量可以使代价和梯度向量化，从而能使用高度优化的矩阵运算。方差的减少导致更稳定的收敛性，但每次梯度计算的计算量都很大。</p><p>相比之下，较小的小批量可以很快估计。如果我们保持较小的小批量，梯度估计中的噪声将允许我们脱离局部最优解。<strong>在机器学习中，优化方法是利用训练数据，通过最小化目标函数来进行训练的，但总体目标是提高泛化性能（第 8 章）</strong>。<strong>由于机器学习的目标不一定需要精确估计目标函数的最小值，所以使用小批量方法的近似梯度被广泛使用。</strong></p><h3 id="约束优化与拉格朗日乘子">约束优化与拉格朗日乘子</h3><p><strong>拉格朗日对偶( Lagrangian duality)</strong>的概念。一般来说，对偶优化的思想是将一组变量<span class="math inline">\(\boldsymbol{x}\)</span>（称为原始变量）的优化问题转化为另一组变量<span class="math inline">\(\boldsymbol{\lambda}\)</span>（称为对偶变量）的优化问题。我们将会介绍两种不同的对偶方法：在本节中，我们讨论的是拉格朗日对偶；在第7.3.3节中，我们将讨论Legendre-Fenchel对偶。</p><blockquote><p><strong>对偶问题计算上的好处</strong></p><ul><li>有时原问题不好解，但对偶问题更容易（例如约束简单、凸结构更明显）。</li><li>在大规模问题（如支持向量机 SVM）中，通常是通过对偶问题求解的。</li></ul></blockquote><p>不等式约束对应的拉格朗日乘子约束为非负，而等式约束对应的拉格朗日乘子则不加约束。</p><p>通过引入与每个不等式约束分别对应的拉格朗日乘子 <span class="math inline">\(\lambda_i \geq 0\)</span> （Boyd 和 Vandenberghe，2004，第 4 章），得到：</p><p><span class="math display">\[\begin{equation}\mathfrak{L}(\boldsymbol{x}, \boldsymbol{\lambda}) = f(\boldsymbol{x}) + \sum_{i=1}^{m} \lambda_i g_i(\boldsymbol{x}) = f(\boldsymbol{x}) + \boldsymbol{\lambda}^{\top} \boldsymbol{g}(\boldsymbol{x}) \qquad (7.20)\end{equation}\]</span></p><p>其中我们将所有约束 <span class="math inline">\(g_i(\boldsymbol{x})\)</span> 写成一个向量 <span class="math inline">\(\boldsymbol{g}(\boldsymbol{x})\)</span>； 把所有拉格朗日乘子也写成向量 <span class="math inline">\(\boldsymbol{\lambda} \in \mathbb{R}^{m}\)</span>。</p><p>我们现在介绍拉格朗日对偶 (Lagrangian duality)}的概念。 一般来说，对偶优化的思想是将一组变量 <span class="math inline">\(\boldsymbol{x}\)</span>（称为原始变量）的优化问题转化为另一组变量 <span class="math inline">\(\boldsymbol{\lambda}\)</span>（称为对偶变量）的优化问题。 <strong>定义7.1</strong> 问题 <span class="math display">\[\begin{equation}\min_{\boldsymbol{x}} f(\boldsymbol{x})\quad \text{subject to} \quad g_i(\boldsymbol{x}) \leqslant 0, \quad i=1,\ldots,m\end{equation}\]</span> 被称为 原始问题 (primal problem)，对应原始变量 <span class="math inline">\(\boldsymbol{x}\)</span>。</p><p>与它对应的拉格朗日对偶问题 (Lagrangian dual problem)由下式给出： <span class="math display">\[\begin{equation}\max_{\boldsymbol{\lambda} \in \mathbb{R}^m} \; \mathfrak{D}(\boldsymbol{\lambda})\quad \text{subject to} \quad \boldsymbol{\lambda} \geqslant \mathbf{0},\end{equation}\]</span> 其中 <span class="math inline">\(\boldsymbol{\lambda}\)</span> 为对偶变量，且 <span class="math display">\[\begin{equation}\mathfrak{D}(\boldsymbol{\lambda}) = \min_{\boldsymbol{x} \in \mathbb{R}^d} \; \mathfrak{L}(\boldsymbol{x}, \boldsymbol{\lambda}).\end{equation}\]</span></p><h3 id="凸优化">凸优化</h3><p>让我们我们重点讨论一类特别有用的优化问题上，因为在这类问题上我们可以保证得到全局最优解。目标函数<span class="math inline">\(f(·)\)</span>是一个凸函数，且约束<span class="math inline">\(g(·)\)</span>和<span class="math inline">\(h(·)\)</span>是凸集的问题称为凸优化问题( convex optimization problem)。在这种情况下，<strong>称它具有强对偶性(strong duality)：对偶问题的最优解与原问题的最优解相同。</strong>在机器学习文献中，凸函数和凸集通常是不严格区别的，但是人们通常可以从上下文中推断出隐含的含义。</p><blockquote><p>凸函数的非负加权和是凸的.</p></blockquote><h4 id="线性规划">线性规划</h4><p>考虑所有函数都是线性的特殊情况，即： <span class="math display">\[\begin{equation}\min_{\boldsymbol{x} \in \mathbb{R}^{d}} \boldsymbol{c}^{\top} \boldsymbol{x} \qquad (7.39)\end{equation}\]</span></p><p><span class="math display">\[\text{subject to } \quad \boldsymbol{A}\boldsymbol{x} \leqslant \boldsymbol{b}\]</span> 其中 <span class="math inline">\(\boldsymbol{A} \in \mathbb{R}^{m \times d}, \ \boldsymbol{b} \in \mathbb{R}^{m}\)</span>。</p><p>取 <span class="math inline">\(\mathfrak{L}(\boldsymbol{x}, \boldsymbol{\lambda})\)</span> 对 <span class="math inline">\(\boldsymbol{x}\)</span> 的导数并将其设为零(因为是<span class="math inline">\(x\)</span>的一次函数，求导消去了<span class="math inline">\(x\)</span>)，得到对偶优化问题，是关于<span class="math inline">\(\lambda\)</span>的线性规划问题，但有<span class="math inline">\(m\)</span>个变量。我们可以选择求解原始规划问题（7.39）或其对偶规划问题（7.43），这取决于<span class="math inline">\(m\)</span>和<span class="math inline">\(d\)</span>哪个更大。</p><h4 id="二次规划">二次规划</h4><p>考虑凸二次目标函数的情况，其中约束是仿射的，即： <span class="math display">\[\min_{\boldsymbol{x} \in \mathbb{R}^{d}} \frac{1}{2} \boldsymbol{x}^{\top} \boldsymbol{Q} \boldsymbol{x}+ \boldsymbol{c}^{\top} \boldsymbol{x}  \]</span></p><p><span class="math display">\[\text{subject to} \quad \boldsymbol{A}\boldsymbol{x} \leqslant \boldsymbol{b}\]</span> 其中 <span class="math inline">\(\boldsymbol{A} \in \mathbb{R}^{m \times d},\ \boldsymbol{b} \in \mathbb{R}^{m},\ \boldsymbol{c} \in \mathbb{R}^{d}\)</span>。</p><p>对称方阵 <span class="math inline">\(\boldsymbol{Q} \in \mathbb{R}^{d \times d}\)</span> 是正定的，因此目标函数是凸的。这就是所谓的二次规划。注意它有 <span class="math inline">\(d\)</span> 个变量和 <span class="math inline">\(m\)</span> 个线性约束。</p><blockquote><p>一般定义<strong>二次规划（QP）</strong>时，只要求 <span class="math inline">\(Q\)</span> <strong>半正定（positive semidefinite, PSD）</strong>，这样目标函数就是凸的；</p></blockquote><p>同线性规划，取 <span class="math inline">\(\mathfrak{L}(\boldsymbol{x}, \boldsymbol{\lambda})\)</span> 对 <span class="math inline">\(\boldsymbol{x}\)</span> 的导数，并将其设为零(因为是<span class="math inline">\(x\)</span>的二次函数，求导得到关于<span class="math inline">\(x\)</span>的一次方程，可以用<span class="math inline">\(\lambda\)</span>替换<span class="math inline">\(x\)</span>代入对耦优化问题)，得到对偶优化问题，是关于<span class="math inline">\(\lambda\)</span>的二次规划问题。</p><h4 id="legendre-fenchel变换和凸共轭">Legendre-Fenchel变换和凸共轭</h4><p>这一节没深入看！</p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 数学 </tag>
            
            <tag> 算法 </tag>
            
            <tag> 统计 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第7章 无监督学习</title>
      <link href="/2025/09/25/%E7%AC%AC7%E7%AB%A0%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
      <url>/2025/09/25/%E7%AC%AC7%E7%AB%A0%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<p><a href="/img3/面向数据科学家的实用统计学/Practical-Statistics-for-Data-Scientists.pdf">《Practical Statistics for Data Scientists》书籍英文版</a><br /><a href="/img3/面向数据科学家的实用统计学/面向数据科学家的实用统计学.pdf">《面向数据科学家的实用统计学》中文版书籍</a></p><h2 id="第7章-无监督学习"><strong>第7章 无监督学习</strong></h2><p>Unsupervised Learning</p><p><strong>无监督学习</strong>这个术语指的是在<strong>没有标注数据</strong>（即已知感兴趣结果的数据）的情况下，从数据中提取意义的统计方法。在第四章到第六章中，目标是构建一个模型（一套规则），用一组预测变量来预测一个响应变量。这就是<strong>监督学习</strong>。与此相反，<strong>无监督学习</strong>也构建数据的模型，但它<strong>不区分响应变量和预测变量</strong>。</p><p>无监督学习可以用于实现不同的目标。在某些情况下，当没有带标签的响应变量时，它可用于创建预测规则。例如，<strong>聚类方法</strong>可以用于识别有意义的数据组。我们可以使用用户在网站上的点击和人口统计数据，将不同类型的用户分组。然后，网站可以根据这些不同类型进行<strong>个性化</strong>。</p><p>在另一些情况下，目标可能是将数据的<strong>维度降至</strong>一个更易于管理的变量集。然后，这个缩减后的集合可以作为输入用于预测模型，比如回归或分类。例如，我们可能有成千上万个传感器来监测一个工业过程。通过将数据简化为一个更小的特征集，我们也许能够构建一个比包含数千个传感器数据流更强大、更可解释的<strong>过程故障预测模型</strong>。</p><p>最后，无监督学习可以被视为<strong>探索性数据分析</strong>（参见第一章）的延伸，适用于您面对大量变量和记录的情况。其目的是深入了解一组数据以及不同变量之间的相互关系。无监督技术让您能够筛选和分析这些变量，并发现其中的关系。 <span id="more"></span></p><blockquote><p>通用注解：</p><p><strong>无监督学习与预测</strong>（Unsupervised Learning and Prediction）</p><p>无监督学习在<strong>预测</strong>中可以扮演重要角色，无论是对于回归还是分类问题。在某些情况下，我们想在没有任何标注数据的情况下预测一个类别。例如，我们可能想根据一组卫星传感器数据来预测某个区域的植被类型。由于我们没有响应变量来训练模型，聚类为我们提供了一种识别<strong>常见模式</strong>并对区域进行<strong>分类</strong>的方法。</p><p>聚类是解决<strong>“冷启动问题”</strong>（cold-start problem）的一个特别重要的工具。在这种类型的问题中，例如发起一项新的营销活动或识别潜在的新型欺诈或垃圾邮件，我们最初可能没有任何响应变量来训练模型。随着时间的推移，当数据被收集起来后，我们可以更多地了解该系统并构建一个传统的预测模型。但聚类通过识别<strong>人口细分</strong>来帮助我们更快地启动学习过程。</p><p>无监督学习作为回归和分类技术的<strong>构建模块</strong>也很重要。对于<strong>大数据</strong>，如果一个小的子群体在整体群体中没有得到很好的代表，那么训练出的模型在该子群体上的表现可能不佳。通过<strong>聚类</strong>，可以识别并标记这些子群体。然后，可以为不同的子群体分别拟合单独的模型。或者，子群体可以用其自身的<strong>特征</strong>来表示，迫使整体模型明确地将子群体身份作为预测变量来考虑。</p></blockquote><h3 id="主成分分析"><strong>主成分分析</strong></h3><p>Principal Components Analysis</p><p>通常，变量会<strong>共同变化（协变）</strong>，其中一个变量的某些变异实际上被另一个变量的变异所<strong>重复</strong>（例如，餐馆账单和给的小费）。<strong>主成分分析（PCA）</strong>是一种用于发现<strong>数值变量如何协变</strong>的技术。</p><p><strong>主成分分析的关键术语</strong></p><ul><li><p><strong>主成分（Principal component）</strong> 预测变量的<strong>线性组合</strong>。</p></li><li><p><strong>载荷（Loadings）</strong> 将预测变量转换为成分的<strong>权重</strong>。 同义词：<strong>权重（Weights）</strong></p></li><li><p><strong>碎石图（Screeplot）</strong> <strong>成分方差图</strong>，显示了成分的相对重要性，可以是<strong>解释方差</strong>或<strong>解释方差的比例</strong>。</p></li></ul><p>PCA 的思想是将多个<strong>数值预测变量</strong>组合成一个<strong>更小的变量集</strong>，这些变量是原始变量集的<strong>加权线性组合</strong>。这个更小的变量集（即<strong>主成分</strong>）“解释”了<strong>大部分原始变量集的变异性</strong>，从而<strong>降低了数据的维度</strong>。用于形成主成分的权重揭示了原始变量对新主成分的相对贡献。</p><p>PCA 最初由<strong>卡尔·皮尔逊</strong>提出。在一篇可能是关于<strong>无监督学习</strong>的开创性论文中，皮尔逊认识到在许多问题中，预测变量存在变异性，因此他开发了 PCA 作为一种对这种变异性进行建模的技术。PCA 可以被看作是<strong>线性判别分析</strong>的无监督版本；参见第201页的“判别分析”。</p><h4 id="一个简单的例子"><strong>一个简单的例子</strong></h4><p>对于两个变量 <span class="math inline">\(X_1\)</span> 和 <span class="math inline">\(X_2\)</span>，有两个主成分 <span class="math inline">\(Z_i\)</span>（<span class="math inline">\(i=1\)</span> 或 <span class="math inline">\(2\)</span>）：</p><p><span class="math display">\[Z_i = w_{i,1}X_1 + w_{i,2}X_2\]</span> 权重 <span class="math inline">\(w_{i,1}, w_{i,2}\)</span> 被称为<strong>成分载荷</strong>。这些载荷将原始变量转换为主成分。第一个主成分 <span class="math inline">\(Z_1\)</span> 是<strong>最能解释总变异性</strong>（variation）的线性组合。第二个主成分 <span class="math inline">\(Z_2\)</span> 与第一个<strong>正交</strong>，并尽可能多地解释<strong>剩余的变异性</strong>。（如果还有额外的成分，每个新增的成分都会与其他的正交。）</p><blockquote><p>通用注解：</p><p>通常，也对<strong>偏离预测变量均值</strong>的偏差而不是对变量本身的值来计算主成分。</p></blockquote><p>您可以使用 <code>princomp</code> 函数在 R 中计算主成分。以下代码对雪佛龙（CVX）和埃克森美孚（XOM）的股票价格回报进行了主成分分析（PCA）：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">oil_px <span class="operator">&lt;-</span> sp500_px<span class="punctuation">[</span><span class="punctuation">,</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;CVX&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;XOM&#x27;</span><span class="punctuation">)</span><span class="punctuation">]</span></span><br><span class="line">pca <span class="operator">&lt;-</span> princomp<span class="punctuation">(</span>oil_px<span class="punctuation">)</span></span><br><span class="line">pca<span class="operator">$</span>loadings</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Loadings:</span><br><span class="line">        Comp.1 Comp.2</span><br><span class="line">CVX -0.747  0.665</span><br><span class="line">XOM -0.665 -0.747</span><br><span class="line"></span><br><span class="line">               Comp.1 Comp.2</span><br><span class="line">SS loadings     1.0    1.0</span><br><span class="line">Proportion Var  0.5    0.5</span><br><span class="line">Cumulative Var  0.5    1.0</span><br></pre></td></tr></table></figure><p>在 Python 中，我们可以使用 <code>scikit-learn</code> 实现 <code>sklearn.decomposition.PCA</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pcs = PCA(n_components=<span class="number">2</span>)</span><br><span class="line">pcs.fit(oil_px)</span><br><span class="line">loadings = pd.DataFrame(pcs.components_, columns=oil_px.columns)</span><br><span class="line">loadings</span><br></pre></td></tr></table></figure><p>第一个主成分的 CVX 和 XOM 权重分别为 -0.747 和 -0.665，第二个主成分的权重分别为 0.665 和 -0.747。这该如何解释呢？第一个主成分<strong>本质上是 CVX 和 XOM 的平均值</strong>，反映了这两家能源公司之间的相关性。第二个主成分则衡量了<strong>CVX 和 XOM 的股票价格何时出现分歧</strong>。</p><p>将主成分与数据一起绘制出来是很有启发性的。这里我们用 R 创建一个可视化图：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">loadings <span class="operator">&lt;-</span> pca<span class="operator">$</span>loadings</span><br><span class="line">ggplot<span class="punctuation">(</span>data<span class="operator">=</span>oil_px<span class="punctuation">,</span> aes<span class="punctuation">(</span>x<span class="operator">=</span>CVX<span class="punctuation">,</span> y<span class="operator">=</span>XOM<span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">geom_point<span class="punctuation">(</span>alpha<span class="operator">=</span><span class="number">.3</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">stat_ellipse<span class="punctuation">(</span>type<span class="operator">=</span><span class="string">&#x27;norm&#x27;</span><span class="punctuation">,</span> level<span class="operator">=</span><span class="number">.99</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">geom_abline<span class="punctuation">(</span>intercept <span class="operator">=</span> <span class="number">0</span><span class="punctuation">,</span> slope <span class="operator">=</span> loadings<span class="punctuation">[</span><span class="number">2</span><span class="punctuation">,</span><span class="number">1</span><span class="punctuation">]</span><span class="operator">/</span>loadings<span class="punctuation">[</span><span class="number">1</span><span class="punctuation">,</span><span class="number">1</span><span class="punctuation">]</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">geom_abline<span class="punctuation">(</span>intercept <span class="operator">=</span> <span class="number">0</span><span class="punctuation">,</span> slope <span class="operator">=</span> loadings<span class="punctuation">[</span><span class="number">2</span><span class="punctuation">,</span><span class="number">2</span><span class="punctuation">]</span><span class="operator">/</span>loadings<span class="punctuation">[</span><span class="number">1</span><span class="punctuation">,</span><span class="number">2</span><span class="punctuation">]</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>以下代码在 Python 中创建了一个类似的可视化图：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">abline</span>(<span class="params">slope, intercept, ax</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Calculate coordinates of a line based on slope and intercept&quot;&quot;&quot;</span></span><br><span class="line">    x_vals = np.array(ax.get_xlim())</span><br><span class="line">    <span class="keyword">return</span> (x_vals, intercept + slope * x_vals)</span><br><span class="line">ax = oil_px.plot.scatter(x=<span class="string">&#x27;XOM&#x27;</span>, y=<span class="string">&#x27;CVX&#x27;</span>, alpha=<span class="number">0.3</span>, figsize=(<span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">ax.set_xlim(-<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">ax.set_ylim(-<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">ax.plot(*abline(loadings.loc[<span class="number">0</span>, <span class="string">&#x27;CVX&#x27;</span>] / loadings.loc[<span class="number">0</span>, <span class="string">&#x27;XOM&#x27;</span>], <span class="number">0</span>, ax),</span><br><span class="line">        <span class="string">&#x27;--&#x27;</span></span><br><span class="line">        , color=<span class="string">&#x27;C1&#x27;</span>)</span><br><span class="line">ax.plot(*abline(loadings.loc[<span class="number">1</span>, <span class="string">&#x27;CVX&#x27;</span>] / loadings.loc[<span class="number">1</span>, <span class="string">&#x27;XOM&#x27;</span>], <span class="number">0</span>, ax),</span><br><span class="line">        <span class="string">&#x27;--&#x27;</span></span><br><span class="line">        , color=<span class="string">&#x27;C1&#x27;</span>)</span><br></pre></td></tr></table></figure><p>结果如图7-1所示。 虚线显示了两个主成分的方向：第一个沿着椭圆的长轴，第二个沿着短轴。您可以看到，这两个股票回报的<strong>大部分变异性都由第一个主成分解释</strong>。这是有道理的，因为能源股价格倾向于作为一个群体移动。</p><p><img src="/img3/面向数据科学家的实用统计学/F7.1.png" alt="F7.1" style="zoom:40%;" /></p><blockquote><p>通用注解：</p><p>第一个主成分的权重都是负的，但将所有权重的符号反转并<strong>不会改变主成分</strong>。例如，对于第一个主成分，使用0.747和0.665的权重与使用负权重是等效的，就像由原点和(1,1)定义的无限线与由原点和(-1,-1)定义的线是同一条一样。</p></blockquote><h4 id="计算主成分"><strong>计算主成分</strong></h4><p>Computing the Principal Components</p><p>从两个变量扩展到更多变量是直接的。对于第一个成分，只需将额外的预测变量包含在线性组合中，并分配权重，以<strong>优化从所有预测变量中收集的协变</strong>（ the covariation from all the predictor variables）到这个第一个主成分中（协方差（covariance）是统计术语；参见第202页的“协方差矩阵”）。主成分的计算是一种经典的统计方法，它依赖于<strong>数据的相关矩阵或协方差矩阵</strong>，并且执行迅速，<strong>不依赖于迭代</strong>。如前所述，主成分分析<strong>仅适用于数值变量</strong>，不适用于分类变量。</p><p>完整的流程可以描述如下：</p><ol type="1"><li>在创建第一个主成分时，PCA 得到的预测变量线性组合能<strong>最大化解释的总方差百分比</strong>。</li><li>这个线性组合随后成为第一个“新”预测变量 <span class="math inline">\(Z_1\)</span>。</li><li>PCA 重复此过程，使用相同的变量但采用不同的权重，来创建第二个新预测变量 <span class="math inline">\(Z_2\)</span>。权重的分配使得 <span class="math inline">\(Z_1\)</span> 和 <span class="math inline">\(Z_2\)</span> <strong>不相关</strong>。</li><li>该过程持续进行，直到您拥有与原始变量 <span class="math inline">\(X_i\)</span> 一样多的新变量（或成分）<span class="math inline">\(Z_i\)</span>。</li><li>选择保留<strong>足以解释大部分方差</strong>的成分数量。</li><li>到目前为止，结果是每个成分的一组权重。最后一步是通过将权重应用于原始值，将原始数据转换为新的<strong>主成分得分</strong>。这些新得分随后可以用作<strong>简化后的预测变量集</strong>。</li></ol><h4 id="解释主成分"><strong>解释主成分</strong></h4><p>Interpreting Principal Components</p><p>主成分的性质通常能揭示<strong>数据的结构</strong>信息。有几种标准的<strong>可视化</strong>方式可以帮助您深入了解主成分。其中一种方法是<strong>碎石图</strong>，用于可视化主成分的相对重要性（其名称源于该图与<strong>碎石坡</strong>的相似之处；这里的y轴是<strong>特征值</strong>）。以下 R 代码显示了标普500指数中几家顶尖公司的例子：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">syms <span class="operator">&lt;-</span> <span class="built_in">c</span><span class="punctuation">(</span> <span class="string">&#x27;AAPL&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;MSFT&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;CSCO&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;INTC&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;CVX&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;XOM&#x27;</span><span class="punctuation">,</span></span><br><span class="line"><span class="string">&#x27;SLB&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;COP&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;JPM&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;WFC&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;USB&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;AXP&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;WMT&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;TGT&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;HD&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;COST&#x27;</span><span class="punctuation">)</span></span><br><span class="line">top_sp <span class="operator">&lt;-</span> sp500_px<span class="punctuation">[</span>row.names<span class="punctuation">(</span>sp500_px<span class="punctuation">)</span><span class="operator">&gt;=</span><span class="string">&#x27;2005-01-01&#x27;</span><span class="punctuation">,</span> syms<span class="punctuation">]</span></span><br><span class="line">sp_pca <span class="operator">&lt;-</span> princomp<span class="punctuation">(</span>top_sp<span class="punctuation">)</span></span><br><span class="line">screeplot<span class="punctuation">(</span>sp_pca<span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>从 scikit-learn 的结果中创建加载图的信息可在 <code>explained_variance_</code> 中获得。这里，我们将其转换为一个 pandas 数据框，并用它来制作一个<strong>条形图</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">syms = <span class="built_in">sorted</span>([<span class="string">&#x27;AAPL&#x27;</span>, <span class="string">&#x27;MSFT&#x27;</span>, <span class="string">&#x27;CSCO&#x27;</span>, <span class="string">&#x27;INTC&#x27;</span>, <span class="string">&#x27;CVX&#x27;</span>, <span class="string">&#x27;XOM&#x27;</span>, <span class="string">&#x27;SLB&#x27;</span>, <span class="string">&#x27;COP&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;JPM&#x27;</span>, <span class="string">&#x27;WFC&#x27;</span>, <span class="string">&#x27;USB&#x27;</span>, <span class="string">&#x27;AXP&#x27;</span>, <span class="string">&#x27;WMT&#x27;</span>, <span class="string">&#x27;TGT&#x27;</span>, <span class="string">&#x27;HD&#x27;</span>, <span class="string">&#x27;COST&#x27;</span>])</span><br><span class="line">top_sp = sp500_px.loc[sp500_px.index &gt;= <span class="string">&#x27;2011-01-01&#x27;</span>, syms]</span><br><span class="line">sp_pca = PCA()</span><br><span class="line">sp_pca.fit(top_sp)</span><br><span class="line">explained_variance = pd.DataFrame(sp_pca.explained_variance_)</span><br><span class="line">ax = explained_variance.head(<span class="number">10</span>).plot.bar(legend=<span class="literal">False</span>, figsize=(<span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">ax.set_xlabel(<span class="string">&#x27;Component&#x27;</span>)</span><br></pre></td></tr></table></figure><p>如图7-2所示，第一个主成分的方差相当大（通常如此），但其他靠前的主成分也具有显著性。</p><p><img src="/img3/面向数据科学家的实用统计学/F7.2.png" alt="F7.2" style="zoom:40%;" /></p><p>通常，绘制<strong>顶层主成分的权重</strong>会非常有启发性。在 R 中，一种方法是结合使用 <code>tidyr</code> 包的 <code>gather</code> 函数和 <code>ggplot</code>：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">library<span class="punctuation">(</span>tidyr<span class="punctuation">)</span></span><br><span class="line">loadings <span class="operator">&lt;-</span> sp_pca<span class="operator">$</span>loadings<span class="punctuation">[</span><span class="punctuation">,</span><span class="number">1</span><span class="operator">:</span><span class="number">5</span><span class="punctuation">]</span></span><br><span class="line">loadings<span class="operator">$</span>Symbol <span class="operator">&lt;-</span> row.names<span class="punctuation">(</span>loadings<span class="punctuation">)</span></span><br><span class="line">loadings <span class="operator">&lt;-</span> gather<span class="punctuation">(</span>loadings<span class="punctuation">,</span> <span class="string">&#x27;Component&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;Weight&#x27;</span><span class="punctuation">,</span></span><br><span class="line">ggplot<span class="punctuation">(</span>loadings<span class="punctuation">,</span> aes<span class="punctuation">(</span>x<span class="operator">=</span>Symbol<span class="punctuation">,</span> y<span class="operator">=</span>Weight<span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">geom_bar<span class="punctuation">(</span>stat<span class="operator">=</span><span class="string">&#x27;identity&#x27;</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">facet_grid<span class="punctuation">(</span>Component <span class="operator">~</span></span><br><span class="line">.<span class="punctuation">,</span> scales<span class="operator">=</span><span class="string">&#x27;free_y&#x27;</span><span class="punctuation">)</span></span><br><span class="line"><span class="operator">-</span>Symbol<span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>以下是用于在 Python 中创建相同可视化图的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">loadings = pd.DataFrame(sp_pca.components_[<span class="number">0</span>:<span class="number">5</span>, :], columns=top_sp.columns)</span><br><span class="line">maxPC = <span class="number">1.01</span> * np.<span class="built_in">max</span>(np.<span class="built_in">max</span>(np.<span class="built_in">abs</span>(loadings.loc[<span class="number">0</span>:<span class="number">5</span>, :])))</span><br><span class="line">f, axes = plt.subplots(<span class="number">5</span>, <span class="number">1</span>, figsize=(<span class="number">5</span>, <span class="number">5</span>), sharex=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">for</span> i, ax <span class="keyword">in</span> <span class="built_in">enumerate</span>(axes):</span><br><span class="line">pc_loadings = loadings.loc[i, :]</span><br><span class="line">colors = [<span class="string">&#x27;C0&#x27;</span> <span class="keyword">if</span> l &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="string">&#x27;C1&#x27;</span> <span class="keyword">for</span> l <span class="keyword">in</span> pc_loadings]</span><br><span class="line">ax.axhline(color=<span class="string">&#x27;#888888&#x27;</span>)</span><br><span class="line"></span><br><span class="line">pc_loadings.plot.bar(ax=ax, color=colors)</span><br><span class="line">ax.set_ylabel(<span class="string">f&#x27;PC<span class="subst">&#123;i+<span class="number">1</span>&#125;</span>&#x27;</span>)</span><br><span class="line">ax.set_ylim(-maxPC, maxPC)</span><br></pre></td></tr></table></figure><p><img src="/img3/面向数据科学家的实用统计学/F7.3.png" alt="F7.3" style="zoom:50%;" /></p><p>前五个成分的载荷如图7-3所示。</p><ul><li><strong>第一个主成分</strong>的载荷符号<strong>都相同</strong>：这对于所有列都共享一个共同因素（在本例中为<strong>整体股市趋势</strong>）的数据来说是很典型的。</li><li><strong>第二个成分</strong>捕捉了<strong>能源股</strong>与其他股票相比的价格变化。</li><li><strong>第三个成分</strong>主要是<strong>苹果和CostCo</strong>的走势对比。</li><li><strong>第四个成分</strong>对比了斯伦贝谢（SLB）与其他能源股的走势。</li><li>最后，<strong>第五个成分</strong>主要由<strong>金融公司</strong>主导。</li></ul><blockquote><p>通用注解：如何选择成分数量？How Many Components to Choose? 如果您的目标是<strong>降低数据的维度</strong>，您必须决定选择多少个主成分。最常见的方法是使用<strong>临时规则</strong>来选择那些能够解释“大部分”方差的成分。您可以通过<strong>碎石图</strong>进行直观操作，例如在图7-2中所示。或者，您可以选择<strong>累积方差超过某个阈值</strong>（如80%）的顶层成分。您还可以检查载荷，以确定该成分是否具有<strong>直观的解释</strong>。<strong>交叉验证</strong>为选择显著成分的数量提供了一种更正式的方法（更多内容请参见第155页的“交叉验证”）。</p></blockquote><h4 id="对应分析"><strong>对应分析</strong></h4><p>Correspondence Analysis</p><p><strong>PCA不能用于分类数据</strong>；然而，有一种与其<strong>有所关联的技术</strong>是<strong>对应分析（Correspondence analysis）</strong>。其目标是<strong>识别类别之间或分类特征之间的关联</strong>。对应分析和主成分分析之间的相似之处主要在于<strong>底层</strong>——用于<strong>维度缩放的矩阵代数</strong>。对应分析主要用于<strong>低维分类数据的图形分析</strong>，不像 PCA 那样被用作<strong>大数据背景下的降维准备步骤</strong>。</p><p>输入可以看作一个表格，其中行代表一个变量，列代表另一个变量，而单元格代表记录计数。输出（经过一些矩阵代数处理后）是一个<strong>双标图（biplot）</strong>——一个<strong>散点图</strong>，其坐标轴经过缩放（百分比表示该维度解释了多少方差）。坐标轴上单位的含义与原始数据没有直观的联系，散点图的主要价值在于<strong>以图形方式展示相互关联的变量</strong>（通过在图上的邻近性）。例如，参见图7-4，其中家务任务根据是共同完成还是单独完成（垂直轴）以及主要由妻子还是丈夫负责（水平轴）进行排列。 从任务的分配来看，对应分析和这个示例的精神都已有数十年历史。</p><p><img src="/img3/面向数据科学家的实用统计学/F7.4.png" alt="F7.4" style="zoom:40%;" /></p><p>R 中有多种用于对应分析的软件包。这里，我们使用 <code>ca</code> 包：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ca_analysis <span class="operator">&lt;-</span> ca<span class="punctuation">(</span>housetasks<span class="punctuation">)</span></span><br><span class="line">plot<span class="punctuation">(</span>ca_analysis<span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>在 Python 中，我们可以使用 <code>prince</code> 包，它使用 scikit-learn API 实现了对应分析：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ca = prince.CA(n_components=<span class="number">2</span>)</span><br><span class="line">ca = ca.fit(housetasks)</span><br><span class="line">ca.plot_coordinates(housetasks, figsize=(<span class="number">6</span>, <span class="number">6</span>))</span><br></pre></td></tr></table></figure><p><strong>关键思想</strong></p><ul><li>主成分是预测变量的线性组合（<strong>仅限数值数据</strong>）。</li><li><strong>计算主成分是为了最小化成分之间的相关性，从而减少冗余。</strong></li><li>数量有限的成分通常可以解释因变量的大部分方差。</li><li>这个有限的主成分集可以用来代替（数量更多的）原始预测变量，从而达到降维的目的。</li><li>一种表面上相似的分类数据技术是对应分析，但它在大数据环境中并不实用。</li></ul><h3 id="k-均值聚类"><strong>K-均值聚类</strong></h3><p>K-Means Clustering</p><p><strong>聚类</strong>是一种将数据划分为不同组的技术，其中<strong>每组中的记录彼此相似</strong>。聚类的一个目标是识别<strong>重要且有意义的数据组</strong>。这些组可以直接使用，可以进行更深入的分析，或者作为<strong>特征或结果</strong>传递给预测性回归或分类模型。<strong>K-均值（K-means）</strong>是第一个被开发的聚类方法；由于其算法相对简单且能够扩展到大型数据集，它仍然被广泛使用。</p><blockquote><p>个人注：<strong>K最近邻 (K-Nearest Neighbors, KNN)</strong> 和 <strong>K均值 (K-Means)</strong> 算法虽然名字里都带“K”，但它们在机器学习领域扮演着截然不同的角色。</p><ul><li><strong>KNN</strong> 是一个<strong>监督学习</strong>算法，主要用于<strong>分类</strong>或<strong>回归</strong>。它需要<strong>有标签</strong>的训练数据。</li><li><strong>K均值</strong> 是一个<strong>无监督学习</strong>算法，主要用于<strong>聚类</strong>。它处理<strong>没有标签</strong>的数据。</li></ul><p>简单来说，KNN 是用来<strong>预测</strong>新数据点的类别或值，而 K均值是用来<strong>发现</strong>数据中固有的分组。</p></blockquote><p><strong>K-均值聚类的关键术语</strong></p><ul><li><strong>簇（Cluster）</strong> 一组彼此相似的记录。</li><li><strong>簇均值（Cluster mean）</strong> 簇中记录的变量均值向量。</li><li><strong>K</strong> 簇的数量。</li></ul><p>K-均值通过最小化<strong>每条记录到其所属簇均值的平方距离总和</strong>来将数据划分为 <span class="math inline">\(K\)</span> 个簇。这被称为<strong>簇内平方和（within-cluster sum of squares）</strong>或<strong>簇内SS（within-cluster SS）</strong>。K-均值不保证簇的大小相同，但它会找到<strong>分隔度最佳</strong>的簇。</p><blockquote><p>通用注解：标准化 Normalization</p><p>通常通过减去均值并除以标准差来对连续变量进行<strong>标准化</strong>（归一化）。否则，<strong>尺度较大的变量</strong>会主导聚类过程（参见第243页的“标准化（归一化、z-分数）”）。</p></blockquote><h4 id="一个简单的例子-1"><strong>一个简单的例子</strong></h4><p>从一个包含 <span class="math inline">\(n\)</span> 条记录和仅有两个变量 <span class="math inline">\(x\)</span> 和 <span class="math inline">\(y\)</span> 的数据集开始考虑。假设我们想将数据分为 <span class="math inline">\(K = 4\)</span> 个簇。这意味着将每条记录 <span class="math inline">\((x_i, y_i)\)</span> 分配给一个簇 <span class="math inline">\(k\)</span>。给定将 <span class="math inline">\(n_k\)</span> 条记录分配到簇 <span class="math inline">\(k\)</span> 的任务，该簇的中心 <span class="math inline">\((\bar{x}_k, \bar{y}_k)\)</span> 是簇中所有点的均值：</p><p>簇均值： <span class="math display">\[\bar{x}_k = \frac{1}{n_k} \sum_{i \in \text{Cluster } k} x_i\]</span> <span class="math display">\[\bar{y}_k = \frac{1}{n_k} \sum_{i \in \text{Cluster } k} y_i\]</span></p><blockquote><p>警告：Cluster Mean 在对具有多个变量的记录进行聚类时（这是典型情况），<strong>簇均值</strong>这个术语指的不是一个单一的数字，而是<strong>变量均值的向量</strong>。</p></blockquote><p>一个簇内的平方和由下式给出：</p><p><span class="math display">\[SS_k = \sum_{i \in \text{Cluster } k} (x_i - \bar{x}_k)^2 + (y_i - \bar{y}_k)^2\]</span></p><p>K-均值通过最小化所有四个簇的簇内平方和总和 <span class="math inline">\(SS_1 + SS_2 + SS_3 + SS_4\)</span> 来寻找记录的最佳分配：</p><p><span class="math display">\[\sum_{k=1}^{4} SS_k\]</span></p><p>聚类的一个典型用途是<strong>定位数据中自然、分离的簇</strong>。另一个应用是将数据<strong>划分为预定数量的独立组</strong>，其中聚类用于确保这些组彼此尽可能地不同。</p><p>例如，假设我们想将每日股票回报分为四个组。K-均值聚类可用于将数据分离为最佳分组。请注意，每日股票回报的报告方式实际上是<strong>标准化</strong>的，因此我们不需要对数据进行归一化。在 R 中，K-均值聚类可以使用 <code>kmeans</code> 函数执行。例如，以下代码基于两个变量——埃克森美孚（XOM）和雪佛龙（CVX）的每日股票回报——寻找四个簇：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df <span class="operator">&lt;-</span> sp500_px<span class="punctuation">[</span>row.names<span class="punctuation">(</span>sp500_px<span class="punctuation">)</span><span class="operator">&gt;=</span><span class="string">&#x27;2011-01-01&#x27;</span><span class="punctuation">,</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;XOM&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;CVX&#x27;</span><span class="punctuation">)</span><span class="punctuation">]</span></span><br><span class="line">km <span class="operator">&lt;-</span> kmeans<span class="punctuation">(</span>df<span class="punctuation">,</span> centers<span class="operator">=</span><span class="number">4</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>在 Python 中，我们使用 scikit-learn 的 <code>sklearn.cluster.KMeans</code> 方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df = sp500_px.loc[sp500_px.index &gt;= <span class="string">&#x27;2011-01-01&#x27;</span>, [<span class="string">&#x27;XOM&#x27;</span>, <span class="string">&#x27;CVX&#x27;</span>]]</span><br><span class="line">kmeans = KMeans(n_clusters=<span class="number">4</span>).fit(df)</span><br></pre></td></tr></table></figure><p>每条记录的簇分配以 <code>cluster</code> 组件的形式返回（R）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt; df$cluster &lt;- factor(km$cluster)</span><br><span class="line">&gt; head(df)</span><br><span class="line">                  XOM        CVX cluster</span><br><span class="line">2011-01-03 0.73680496 0.2406809 2</span><br><span class="line">2011-01-04 0.16866845 -0.5845157 1</span><br><span class="line">2011-01-05 0.02663055 0.4469854 2</span><br><span class="line">2011-01-06 0.24855834 -0.9197513 1</span><br><span class="line">2011-01-07 0.33732892 0.1805111 2</span><br><span class="line">2011-01-10 0.00000000 -0.4641675 1</span><br></pre></td></tr></table></figure><p>在 scikit-learn 中，簇标签在 <code>labels_</code> 字段中可用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">&#x27;cluster&#x27;</span>] = kmeans.labels_</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure><p>前六条记录被分配到簇1或簇2。簇的均值也一并返回（R）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt; centers &lt;- data.frame(cluster=factor(1:4), km$centers)</span><br><span class="line">&gt; centers</span><br><span class="line">  cluster         XOM         CVX</span><br><span class="line">1       1 -0.3284864 -0.5669135</span><br><span class="line">2       2  0.2410159  0.3342130</span><br><span class="line">3       3 -1.1439800 -1.7502975</span><br><span class="line">4       4  0.9568628  1.3708892</span><br></pre></td></tr></table></figure><p>在 scikit-learn 中，簇中心在 <code>cluster_centers_</code> 字段中可用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">centers = pd.DataFrame(kmeans.cluster_centers_, columns=[<span class="string">&#x27;XOM&#x27;</span>, <span class="string">&#x27;CVX&#x27;</span>])</span><br><span class="line">centers</span><br></pre></td></tr></table></figure><p>簇1和3代表<strong>“下行”市场</strong>，而簇2和4代表<strong>“上行”市场</strong>。</p><p>由于 K-均值算法使用随机起始点，因此后续的运行和不同的方法实现可能会产生不同的结果。一般来说，您应该检查<strong>波动是否不太大</strong>。</p><p>在这个只有两个变量的例子中，<strong>可视化簇及其均值</strong>是直截了当的：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ggplot<span class="punctuation">(</span>data<span class="operator">=</span>df<span class="punctuation">,</span> aes<span class="punctuation">(</span>x<span class="operator">=</span>XOM<span class="punctuation">,</span> y<span class="operator">=</span>CVX<span class="punctuation">,</span> color<span class="operator">=</span>cluster<span class="punctuation">,</span> shape<span class="operator">=</span>cluster<span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">geom_point<span class="punctuation">(</span>alpha<span class="operator">=</span><span class="number">.3</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">geom_point<span class="punctuation">(</span>data<span class="operator">=</span>centers<span class="punctuation">,</span> aes<span class="punctuation">(</span>x<span class="operator">=</span>XOM<span class="punctuation">,</span> y<span class="operator">=</span>CVX<span class="punctuation">)</span><span class="punctuation">,</span> size<span class="operator">=</span><span class="number">3</span><span class="punctuation">,</span> stroke<span class="operator">=</span><span class="number">2</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p><code>seaborn</code> 的 <code>scatterplot</code> 函数可以很容易地按属性对点进行颜色 (<code>hue</code>) 和样式 (<code>style</code>) 设置：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">ax = sns.scatterplot(x=<span class="string">&#x27;XOM&#x27;</span>, y=<span class="string">&#x27;CVX&#x27;</span>, hue=<span class="string">&#x27;cluster&#x27;</span>, style=<span class="string">&#x27;cluster&#x27;</span>,</span><br><span class="line">ax=ax, data=df)</span><br><span class="line">ax.set_xlim(-<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">ax.set_ylim(-<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">centers.plot.scatter(x=<span class="string">&#x27;XOM&#x27;</span>, y=<span class="string">&#x27;CVX&#x27;</span>, ax=ax, s=<span class="number">50</span>, color=<span class="string">&#x27;black&#x27;</span>)</span><br></pre></td></tr></table></figure><p>最终的图（如图7-5所示）显示了簇分配和簇均值。 请注意，K-均值会将记录分配给簇，即使这些簇<strong>没有很好地分离</strong>（如果您需要将记录最佳地划分为多个组，这可能是有用的）。</p><p><img src="/img3/面向数据科学家的实用统计学/F7.5.png" alt="F7.5" style="zoom:50%;" /></p><h4 id="k-均值算法"><strong>K-均值算法</strong></h4><p>K-Means Algorithm</p><p>通常，K-均值可以应用于具有 <span class="math inline">\(p\)</span> 个变量 <span class="math inline">\(X_1, \dots, X_p\)</span> 的数据集。虽然 K-均值的精确解在计算上非常困难，但<strong>启发式算法</strong>提供了一种有效的方式来计算<strong>局部最优解</strong>。</p><p>该算法从用户指定的 <span class="math inline">\(K\)</span> 值和一组初始簇均值开始，然后<strong>迭代</strong>以下步骤：</p><ol type="1"><li>根据<strong>平方距离</strong>将每条记录分配给最近的簇均值。</li><li>根据记录的分配计算新的簇均值。</li></ol><p>当记录到簇的分配<strong>不再变化</strong>时，算法收敛。对于第一次迭代，您需要指定一组<strong>初始簇均值</strong>。通常，您可以通过将每条记录随机分配给 <span class="math inline">\(K\)</span> 个簇中的一个，然后找到这些簇的均值来实现。</p><p><strong>由于该算法不保证能找到最佳解决方案，因此建议使用不同的随机样本来初始化算法，并多次运行。当使用多组迭代时，K-均值的结果由簇内平方和最低的那次迭代给出。</strong></p><p>R 函数 <code>kmeans</code> 的 <code>nstart</code> 参数允许您指定要尝试的<strong>随机起始点数量</strong>。例如，以下代码运行 K-均值以寻找5个簇，使用了10个不同的起始簇均值：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">syms <span class="operator">&lt;-</span> <span class="built_in">c</span><span class="punctuation">(</span> <span class="string">&#x27;AAPL&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;MSFT&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;CSCO&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;INTC&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;CVX&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;XOM&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;SLB&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;COP&#x27;</span><span class="punctuation">,</span></span><br><span class="line"><span class="string">&#x27;JPM&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;WFC&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;USB&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;AXP&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;WMT&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;TGT&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;HD&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;COST&#x27;</span><span class="punctuation">)</span></span><br><span class="line">df <span class="operator">&lt;-</span> sp500_px<span class="punctuation">[</span>row.names<span class="punctuation">(</span>sp500_px<span class="punctuation">)</span> <span class="operator">&gt;=</span> <span class="string">&#x27;2011-01-01&#x27;</span><span class="punctuation">,</span> syms<span class="punctuation">]</span></span><br><span class="line">km <span class="operator">&lt;-</span> kmeans<span class="punctuation">(</span>df<span class="punctuation">,</span> centers<span class="operator">=</span><span class="number">5</span><span class="punctuation">,</span> nstart<span class="operator">=</span><span class="number">10</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>该函数会自动返回10个不同起始点中的<strong>最佳解决方案</strong>。您可以使用 <code>iter.max</code> 参数来设置算法每次随机开始所允许的最大迭代次数。</p><p>scikit-learn 算法默认重复10次（<code>n_init</code>）。参数 <code>max_iter</code>（默认300）可用于控制迭代次数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">syms = <span class="built_in">sorted</span>([<span class="string">&#x27;AAPL&#x27;</span>, <span class="string">&#x27;MSFT&#x27;</span>, <span class="string">&#x27;CSCO&#x27;</span>, <span class="string">&#x27;INTC&#x27;</span>, <span class="string">&#x27;CVX&#x27;</span>, <span class="string">&#x27;XOM&#x27;</span>, <span class="string">&#x27;SLB&#x27;</span>, <span class="string">&#x27;COP&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;JPM&#x27;</span>, <span class="string">&#x27;WFC&#x27;</span>, <span class="string">&#x27;USB&#x27;</span>, <span class="string">&#x27;AXP&#x27;</span>, <span class="string">&#x27;WMT&#x27;</span>, <span class="string">&#x27;TGT&#x27;</span>, <span class="string">&#x27;HD&#x27;</span>, <span class="string">&#x27;COST&#x27;</span>])</span><br><span class="line">top_sp = sp500_px.loc[sp500_px.index &gt;= <span class="string">&#x27;2011-01-01&#x27;</span>, syms]</span><br><span class="line">kmeans = KMeans(n_clusters=<span class="number">5</span>).fit(top_sp)</span><br></pre></td></tr></table></figure><h4 id="解释簇"><strong>解释簇</strong></h4><p>Interpreting the Clusters</p><p><strong>簇分析</strong>的一个重要部分可能涉及<strong>簇的解释</strong>。来自 <code>kmeans</code> 的两个最重要的输出是<strong>簇的大小</strong>和<strong>簇均值</strong>。对于前一小节中的例子，所得簇的大小由这个 R 命令给出：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">km<span class="operator">$</span>size</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[1] 106 186 285 288 266</span><br></pre></td></tr></table></figure><p>在 Python 中，我们可以使用标准库中的 <code>collections.Counter</code> 类来获取此信息。由于实现上的差异和算法固有的随机性，结果会有所不同：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line">Counter(kmeans.labels_)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Counter(&#123;4: 302, 2: 272, 0: 288, 3: 158, 1: 111&#125;)</span><br></pre></td></tr></table></figure><p>簇的大小相对均衡。<strong>不平衡的簇</strong>可能是由<strong>遥远的异常值</strong>或与数据其余部分<strong>截然不同的记录组</strong>造成的——这两种情况都可能需要进一步检查。</p><p>您可以使用 <code>gather</code> 函数结合 <code>ggplot</code> 绘制簇的中心：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">centers <span class="operator">&lt;-</span> as.data.frame<span class="punctuation">(</span>t<span class="punctuation">(</span>centers<span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line"><span class="built_in">names</span><span class="punctuation">(</span>centers<span class="punctuation">)</span> <span class="operator">&lt;-</span> paste<span class="punctuation">(</span><span class="string">&quot;Cluster&quot;</span><span class="punctuation">,</span> <span class="number">1</span><span class="operator">:</span><span class="number">5</span><span class="punctuation">)</span></span><br><span class="line">centers<span class="operator">$</span>Symbol <span class="operator">&lt;-</span> row.names<span class="punctuation">(</span>centers<span class="punctuation">)</span></span><br><span class="line">centers <span class="operator">&lt;-</span> gather<span class="punctuation">(</span>centers<span class="punctuation">,</span> <span class="string">&#x27;Cluster&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;Mean&#x27;</span><span class="punctuation">,</span></span><br><span class="line"><span class="operator">-</span>Symbol<span class="punctuation">)</span></span><br><span class="line">centers<span class="operator">$</span>Color <span class="operator">=</span> centers<span class="operator">$</span>Mean <span class="operator">&gt;</span> <span class="number">0</span></span><br><span class="line">ggplot<span class="punctuation">(</span>centers<span class="punctuation">,</span> aes<span class="punctuation">(</span>x<span class="operator">=</span>Symbol<span class="punctuation">,</span> y<span class="operator">=</span>Mean<span class="punctuation">,</span> fill<span class="operator">=</span>Color<span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">geom_bar<span class="punctuation">(</span>stat<span class="operator">=</span><span class="string">&#x27;identity&#x27;</span><span class="punctuation">,</span> position<span class="operator">=</span><span class="string">&#x27;identity&#x27;</span><span class="punctuation">,</span> width<span class="operator">=</span><span class="number">.75</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">facet_grid<span class="punctuation">(</span>Cluster <span class="operator">~</span></span><br><span class="line">.<span class="punctuation">,</span> scales<span class="operator">=</span><span class="string">&#x27;free_y&#x27;</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>在 Python 中创建这个可视化图的代码与我们用于 PCA 的代码类似：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">centers = pd.DataFrame(kmeans.cluster_centers_, columns=syms)</span><br><span class="line">f, axes = plt.subplots(<span class="number">5</span>, <span class="number">1</span>, figsize=(<span class="number">5</span>, <span class="number">5</span>), sharex=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">for</span> i, ax <span class="keyword">in</span> <span class="built_in">enumerate</span>(axes):</span><br><span class="line">center = centers.loc[i, :]</span><br><span class="line">maxPC = <span class="number">1.01</span> * np.<span class="built_in">max</span>(np.<span class="built_in">max</span>(np.<span class="built_in">abs</span>(center)))</span><br><span class="line">colors = [<span class="string">&#x27;C0&#x27;</span> <span class="keyword">if</span> l &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="string">&#x27;C1&#x27;</span> <span class="keyword">for</span> l <span class="keyword">in</span> center]</span><br><span class="line">ax.axhline(color=<span class="string">&#x27;#888888&#x27;</span>)</span><br><span class="line">center.plot.bar(ax=ax, color=colors)</span><br><span class="line">ax.set_ylabel(<span class="string">f&#x27;Cluster <span class="subst">&#123;i + <span class="number">1</span>&#125;</span>&#x27;</span>)</span><br><span class="line">ax.set_ylim(-maxPC, maxPC)</span><br></pre></td></tr></table></figure><p>生成的图（如图7-6所示）揭示了每个簇的性质。 例如，<strong>簇4和簇5</strong>分别对应于市场<strong>下行</strong>和<strong>上行</strong>的日子。<strong>簇2和簇3</strong>的特点分别是<strong>消费股</strong>市场<strong>上行</strong>和<strong>能源股</strong>市场<strong>下行</strong>的日子。最后，<strong>簇1</strong>捕捉了<strong>能源股上涨而消费股下跌</strong>的日子。</p><p><img src="/img3/面向数据科学家的实用统计学/F7.6.png" alt="F7.6" style="zoom:50%;" /></p><blockquote><p>通用注解：<strong>簇分析对比 PCA</strong> Cluster Analysis Versus PCA</p><p>簇均值的图在精神上类似于观察<strong>主成分分析 (PCA) 的载荷</strong>；参见第289页的“解释主成分”。一个主要区别是，与 PCA 不同，<strong>簇均值的符号是有意义的</strong>。PCA 识别<strong>变异的主要方向</strong>，而簇分析则寻找<strong>彼此邻近的记录组</strong>。</p></blockquote><h4 id="选择簇的数量"><strong>选择簇的数量</strong></h4><p>Selecting the Number of Clusters</p><p>K-均值算法要求您指定<strong>簇的数量 <span class="math inline">\(K\)</span></strong>。有时，簇的数量是由<strong>应用本身</strong>决定的。例如，一家管理销售团队的公司可能希望将客户聚类为不同的“人物画像”，以集中和指导销售拜访。在这种情况下，<strong>管理层面的考虑</strong>将决定所需客户细分的数量——例如，两个可能无法提供有用的客户区分，而八个可能又太多而难以管理。</p><p>在没有实际或管理考虑来决定簇数量的情况下，可以使用<strong>统计方法</strong>。目前没有一个单一的标准方法来找到“最佳”的簇数量。</p><p>一种常见的方法，称为<strong>“手肘法”（elbow method）</strong>，是识别一组簇何时能解释数据中的“大部分”方差。在此集合之外添加新簇对解释方差的贡献相对较小。手肘是解释方差的<strong>累积曲线在急剧上升后趋于平缓的点</strong>，因此得名。</p><p><img src="/img3/面向数据科学家的实用统计学/F7.7.png" alt="F7.7" style="zoom:50%;" /></p><p>图7-7显示了贷款数据的<strong>累计方差解释百分比</strong>，簇的数量从2到15不等。 在这个例子中，手肘在哪里？没有一个明显的候选点，因为解释方差的<strong>增量增加是逐渐下降的</strong>。这在<strong>没有明确界定簇</strong>的数据中是相当典型的。这也许是手肘法的一个缺点，但它确实揭示了数据的性质。</p><p>在 R 中，<code>kmeans</code> 函数没有提供应用手肘法的单一命令，但可以很容易地从 <code>kmeans</code> 的输出中应用，如下所示：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">pct_var <span class="operator">&lt;-</span> data.frame<span class="punctuation">(</span>pct_var <span class="operator">=</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">num_clusters <span class="operator">=</span> <span class="number">2</span><span class="operator">:</span><span class="number">14</span><span class="punctuation">)</span></span><br><span class="line">totalss <span class="operator">&lt;-</span> kmeans<span class="punctuation">(</span>df<span class="punctuation">,</span> centers<span class="operator">=</span><span class="number">14</span><span class="punctuation">,</span> nstart<span class="operator">=</span><span class="number">50</span><span class="punctuation">,</span> iter.max<span class="operator">=</span><span class="number">100</span><span class="punctuation">)</span><span class="operator">$</span>totss</span><br><span class="line"><span class="keyword">for</span> <span class="punctuation">(</span>i <span class="keyword">in</span> <span class="number">2</span><span class="operator">:</span><span class="number">14</span><span class="punctuation">)</span> <span class="punctuation">&#123;</span></span><br><span class="line">kmCluster <span class="operator">&lt;-</span> kmeans<span class="punctuation">(</span>df<span class="punctuation">,</span> centers<span class="operator">=</span>i<span class="punctuation">,</span> nstart<span class="operator">=</span><span class="number">50</span><span class="punctuation">,</span> iter.max<span class="operator">=</span><span class="number">100</span><span class="punctuation">)</span></span><br><span class="line">pct_var<span class="punctuation">[</span>i<span class="operator">-</span><span class="number">1</span><span class="punctuation">,</span> <span class="string">&#x27;pct_var&#x27;</span><span class="punctuation">]</span> <span class="operator">&lt;-</span> kmCluster<span class="operator">$</span>betweenss <span class="operator">/</span> totalss</span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>对于 KMeans 的结果，我们从 <code>inertia_</code> 属性中获取此信息。转换为 pandas 数据框后，我们可以使用其 <code>plot</code> 方法来创建图表：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">inertia = []</span><br><span class="line"><span class="keyword">for</span> n_clusters <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, <span class="number">14</span>):</span><br><span class="line">kmeans = KMeans(n_clusters=n_clusters, random_state=<span class="number">0</span>).fit(top_sp)</span><br><span class="line">inertia.append(kmeans.inertia_ / n_clusters)</span><br><span class="line">inertias = pd.DataFrame(&#123;<span class="string">&#x27;n_clusters&#x27;</span>: <span class="built_in">range</span>(<span class="number">2</span>, <span class="number">14</span>), <span class="string">&#x27;inertia&#x27;</span>: inertia&#125;)</span><br><span class="line">ax = inertias.plot(x=<span class="string">&#x27;n_clusters&#x27;</span>, y=<span class="string">&#x27;inertia&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Number of clusters(k)&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Average Within-Cluster Squared Distances&#x27;</span>)</span><br><span class="line">plt.ylim((<span class="number">0</span>, <span class="number">1.1</span> * inertias.inertia.<span class="built_in">max</span>()))</span><br><span class="line">ax.legend().set_visible(<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p>在评估要保留多少个簇时，也许最重要的检验是：<strong>这些簇在新数据上复制的可能性有多大？这些簇是否可解释，并且与数据的总体特征相关，还是仅仅反映了一个特定的实例？您可以通过使用交叉验证</strong>来部分评估这一点；参见第155页的“交叉验证”。</p><p>总的来说，<strong>没有一个单一的规则</strong>可以可靠地指导应该产生多少个簇。</p><blockquote><p>通用注解：</p><p>有几种更正式的方法可以根据统计学或信息论来确定簇的数量。例如，Robert Tibshirani、Guenther Walther 和 Trevor Hastie 提出了一种基于统计理论的<strong>“间隙（gap）”统计量</strong>来识别手肘。对于大多数应用来说，理论方法可能不是必需的，甚至不一定是合适的。</p></blockquote><p><strong>关键思想</strong></p><ul><li>所需的簇数量 <strong><span class="math inline">\(K\)</span> 由用户选择</strong>。</li><li>算法通过<strong>迭代地将记录分配给最近的簇均值</strong>来发展簇，直到簇分配不再改变为止。</li><li>通常，实际考虑主导 <span class="math inline">\(K\)</span> 的选择；不存在一个由统计学决定的最佳簇数量。</li></ul><h3 id="层次聚类"><strong>层次聚类</strong></h3><p>Hierarchical Clustering</p><p><strong>层次聚类（Hierarchical clustering）是 K-均值的一种替代方案，它可以产生截然不同</strong>的簇。层次聚类允许用户<strong>可视化</strong>指定不同数量簇的效果。它在发现<strong>异常或离群的群体或记录</strong>方面更为敏感。层次聚类还适合于<strong>直观的图形显示</strong>，从而更容易解释簇。</p><p><strong>层次聚类的关键术语</strong></p><ul><li><strong>树状图（Dendrogram）</strong> 记录及其所属簇层次结构的<strong>视觉表示</strong>。</li><li><strong>距离（Distance）</strong> 衡量一条记录与另一条记录<strong>有多近</strong>的度量。</li><li><strong>相异度（Dissimilarity）</strong> 衡量一个簇与另一个簇<strong>有多近</strong>的度量。</li></ul><p>层次聚类的灵活性是有代价的，它不能很好地扩展到拥有数百万条记录的大型数据集。即使对于只有数万条记录的适度大小的数据，层次聚类也<strong>可能需要密集的计算资源</strong>。事实上，<strong>层次聚类的大多数应用都集中在相对较小的数据集上</strong>。</p><h4 id="一个简单的例子-2"><strong>一个简单的例子</strong></h4><p>层次聚类处理包含 <span class="math inline">\(n\)</span> 条记录和 <span class="math inline">\(p\)</span> 个变量的数据集，并基于两个基本构建块：</p><ul><li>一个<strong>距离度量 <span class="math inline">\(d_{i,j}\)</span></strong>，用于衡量两条记录 <span class="math inline">\(i\)</span> 和 <span class="math inline">\(j\)</span> 之间的距离。</li><li>一个<strong>相异度度量 <span class="math inline">\(D_{A,B}\)</span></strong>，用于根据每个簇成员之间的距离 <span class="math inline">\(d_{i,j}\)</span> 来衡量两个簇 <span class="math inline">\(A\)</span> 和 <span class="math inline">\(B\)</span> 之间的差异。</li></ul><p>对于涉及<strong>数值数据</strong>的应用，最重要的选择是<strong>相异度度量</strong>。层次聚类首先将<strong>每条记录</strong>设置为一个<strong>单独的簇</strong>，然后<strong>迭代地将最不相似的簇合并</strong>。</p><p>在 R 中，<code>hclust</code> 函数可用于执行层次聚类。<code>hclust</code> 与 <code>kmeans</code> 的一个巨大区别是，它作用于<strong>成对距离 <span class="math inline">\(d_{i,j}\)</span></strong>，而不是数据本身。您可以使用 <code>dist</code> 函数来计算这些距离。例如，以下代码对一组公司的股票回报应用层次聚类：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">syms1 <span class="operator">&lt;-</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;GOOGL&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;AMZN&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;AAPL&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;MSFT&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;CSCO&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;INTC&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;CVX&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;XOM&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;SLB&#x27;</span><span class="punctuation">,</span></span><br><span class="line"><span class="string">&#x27;COP&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;JPM&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;WFC&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;USB&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;AXP&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;WMT&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;TGT&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;HD&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;COST&#x27;</span><span class="punctuation">)</span></span><br><span class="line"><span class="comment"># take transpose: to cluster companies, we need the stocks along the rows</span></span><br><span class="line">df <span class="operator">&lt;-</span> t<span class="punctuation">(</span>sp500_px<span class="punctuation">[</span>row.names<span class="punctuation">(</span>sp500_px<span class="punctuation">)</span> <span class="operator">&gt;=</span> <span class="string">&#x27;2011-01-01&#x27;</span><span class="punctuation">,</span> syms1<span class="punctuation">]</span><span class="punctuation">)</span></span><br><span class="line">d <span class="operator">&lt;-</span> dist<span class="punctuation">(</span>df<span class="punctuation">)</span></span><br><span class="line">hcl <span class="operator">&lt;-</span> hclust<span class="punctuation">(</span>d<span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>聚类算法将对数据框的记录（行）进行聚类。因为我们想对<strong>公司</strong>进行聚类，所以我们需要<strong>转置</strong>（<code>t</code>）数据框，将股票放在行中，日期放在列中。</p><p><code>scipy</code> 包在 <code>scipy.cluster.hierarchy</code> 模块中提供了许多不同的层次聚类方法。这里我们使用 <code>linkage</code> 函数和“complete”（完全）方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">syms1 = [<span class="string">&#x27;AAPL&#x27;</span>, <span class="string">&#x27;AMZN&#x27;</span>, <span class="string">&#x27;AXP&#x27;</span>, <span class="string">&#x27;COP&#x27;</span>, <span class="string">&#x27;COST&#x27;</span>, <span class="string">&#x27;CSCO&#x27;</span>, <span class="string">&#x27;CVX&#x27;</span>, <span class="string">&#x27;GOOGL&#x27;</span>, <span class="string">&#x27;HD&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;INTC&#x27;</span>, <span class="string">&#x27;JPM&#x27;</span>, <span class="string">&#x27;MSFT&#x27;</span>, <span class="string">&#x27;SLB&#x27;</span>, <span class="string">&#x27;TGT&#x27;</span>, <span class="string">&#x27;USB&#x27;</span>, <span class="string">&#x27;WFC&#x27;</span>, <span class="string">&#x27;WMT&#x27;</span>, <span class="string">&#x27;XOM&#x27;</span>]</span><br><span class="line">df = sp500_px.loc[sp500_px.index &gt;= <span class="string">&#x27;2011-01-01&#x27;</span>, syms1].transpose()</span><br><span class="line">Z = linkage(df, method=<span class="string">&#x27;complete&#x27;</span>)</span><br></pre></td></tr></table></figure><h4 id="树状图"><strong>树状图</strong></h4><p>The Dendrogram</p><p>层次聚类可以自然地以树的形式进行图形化展示，这被称为<strong>树状图（dendrogram）</strong>。这个名字来源于希腊词根 <em>dendro</em>（树）和 <em>gramma</em>（图画）。在 R 中，您可以使用 <code>plot</code> 命令轻松生成它：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot<span class="punctuation">(</span>hcl<span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>在 Python 中，我们可以使用 <code>dendrogram</code> 方法来绘制 <code>linkage</code> 函数的结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line">dendrogram(Z, labels=df.index, ax=ax, color_threshold=<span class="number">0</span>)</span><br><span class="line">plt.xticks(rotation=<span class="number">90</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&#x27;distance&#x27;</span>)</span><br></pre></td></tr></table></figure><p>结果如图7-8所示（请注意，我们现在绘制的是相互相似的公司，而不是日期）。 树的叶子对应于记录。树枝的长度表示相应簇之间的<strong>相异程度</strong>。Google 和 Amazon 的回报彼此之间以及与其他股票的回报都<strong>非常不相似</strong>。<strong>石油股</strong>（SLB, CVX, XOM, COP）在自己的簇中，<strong>苹果</strong>（AAPL）单独成簇，其余的彼此相似。</p><p><img src="/img3/面向数据科学家的实用统计学/F7.8.png" alt="F7.8" style="zoom:50%;" /></p><p>与 K-均值不同，<strong>不必预先指定簇的数量</strong>。在图形上，您可以通过一条上下滑动的<strong>水平线</strong>来识别不同数量的簇；簇被定义为<strong>水平线与垂直线相交的地方</strong>。要提取特定数量的簇，您可以使用 <code>cutree</code> 函数：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cutree<span class="punctuation">(</span>hcl<span class="punctuation">,</span> k<span class="operator">=</span><span class="number">4</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">GOOGL AMZN AAPL MSFT CSCO INTC CVX XOM SLB COP JPM WFC </span><br><span class="line">   1    2    3    3    3    3    4    4    4    4    3    3 </span><br><span class="line"> USB  AXP  WMT  TGT   HD COST </span><br><span class="line">   3    3    3    3    3    3 </span><br></pre></td></tr></table></figure><p>在 Python 中，您可以使用 <code>fcluster</code> 方法实现相同的功能：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">memb = fcluster(Z, <span class="number">4</span>, criterion=<span class="string">&#x27;maxclust&#x27;</span>)</span><br><span class="line">memb = pd.Series(memb, index=df.index)</span><br><span class="line"><span class="keyword">for</span> key, item <span class="keyword">in</span> memb.groupby(memb):</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;key&#125;</span> : <span class="subst">&#123;<span class="string">&#x27;, &#x27;</span>.join(item.index)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>提取的簇数量被设置为4，您可以看到 Google 和 Amazon 各自属于自己的簇。<strong>石油股</strong>都属于另一个簇。其余的股票都在<strong>第四个簇</strong>中。</p><h4 id="聚类算法"><strong>聚类算法</strong></h4><p>The Agglomerative Algorithm</p><p>层次聚类的主要算法是聚类算法，它迭代地合并相似的簇。聚类算法从每条记录构成其自身的单个记录簇开始，然后逐步建立越来越大的簇。第一步是计算所有记录对之间的距离。</p><p>对于每对记录 <span class="math inline">\(x_1, x_2, \dots, x_p\)</span> 和 <span class="math inline">\(y_1, y_2, \dots, y_p\)</span>，我们使用<strong>距离度量</strong>（参见第241页的“距离度量”）来衡量两条记录之间的距离 <span class="math inline">\(d_{x,y}\)</span>。例如，我们可以使用<strong>欧几里得距离</strong>：</p><p><span class="math display">\[d_{x,y} = \sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2 + \dots + (x_p - y_p)^2}\]</span> 现在我们转向<strong>簇间距离</strong>。考虑两个簇 <span class="math inline">\(A\)</span> 和 <span class="math inline">\(B\)</span>，每个簇都有一组独特的记录，<span class="math inline">\(A=\{a_1, a_2, \dots, a_m\}\)</span> 和 <span class="math inline">\(B=\{b_1, b_2, \dots, b_q\}\)</span>。我们可以通过使用 <span class="math inline">\(A\)</span> 的成员和 <span class="math inline">\(B\)</span> 的成员之间的距离来衡量簇之间的<strong>相异度</strong> <span class="math inline">\(D_{A,B}\)</span>。</p><p>一种相异度度量是<strong>完全链接法（complete-linkage method）</strong>，它是 <span class="math inline">\(A\)</span> 和 <span class="math inline">\(B\)</span> 之间所有记录对的<strong>最大距离</strong>：</p><p><span class="math display">\[D_{A,B} = \max d_{a_i, b_j} \text{ for all pairs } i, j\]</span> 这一定义将相异度定义为所有成对差异中<strong>最大的那个</strong>。</p><p>聚类算法的主要步骤是：</p><ol type="1"><li>创建一个初始簇集，其中<strong>每个簇</strong>由数据中的<strong>一条单一记录</strong>构成。</li><li>计算所有<strong>簇对</strong> <span class="math inline">\(C_k, C_l\)</span> 之间的<strong>相异度</strong> <span class="math inline">\(D_{C_k, C_l}\)</span>。</li><li><strong>合并</strong>根据 <span class="math inline">\(D_{C_k, C_l}\)</span> 测得的<strong>最不相似</strong>的两个簇 <span class="math inline">\(C_k\)</span> 和 <span class="math inline">\(C_l\)</span>。</li><li>如果剩余的簇多于一个，则返回步骤2。否则，完成。</li></ol><h4 id="相异度度量"><strong>相异度度量</strong></h4><p>Measures of Dissimilarity</p><p>有四种常见的相异度度量：<strong>完全链接（complete linkage）</strong>、<strong>单链接（single linkage）</strong>、<strong>平均链接（average linkage）</strong>和<strong>最小方差（minimum variance）</strong>。这些（以及其他度量）都受到大多数层次聚类软件的支持，包括 <code>hclust</code> 和 <code>linkage</code>。</p><ul><li>前面定义的<strong>完全链接法</strong>倾向于产生成员彼此相似的簇。</li><li><strong>单链接法</strong>是两个簇中记录之间的<strong>最小距离</strong>： <span class="math display">\[D_{A,B} = \min d_{a_i, b_j} \text{ for all pairs } i, j\]</span> 这是一种<strong>“贪婪”方法</strong>，产生的簇可能包含<strong>相当不一致的元素</strong>。</li><li><strong>平均链接法</strong>是所有距离对的<strong>平均值</strong>，代表了单链接和完全链接方法之间的一种折中。</li><li>最后，<strong>最小方差法</strong>，也称为<strong>沃德法（Ward's method）</strong>，类似于 <strong>K-均值</strong>，因为它<strong>最小化簇内平方和</strong>（参见第294页的“K-均值聚类”）。</li></ul><p>图7-9对埃克森美孚和雪佛龙的股票回报应用了使用这四种度量的层次聚类。对于每种度量，都保留了四个簇。</p><p><img src="/img3/面向数据科学家的实用统计学/F7.9.png" alt="F7.9" style="zoom:80%;" /></p><p>结果<strong>截然不同</strong>：单链接度量将几乎所有的点都分配给一个单独的簇。除了<strong>最小方差法</strong>（R：<code>Ward.D</code>；Python：<code>ward</code>）之外，所有度量最终都至少有一个簇只包含几个离群点。最小方差法与 K-均值聚类最相似；请与图7-5进行比较。</p><p><strong>关键思想</strong></p><ul><li>层次聚类从<strong>每条记录</strong>都在其自己的簇中开始。</li><li>簇<strong>逐步</strong>与邻近的簇合并，直到所有记录都属于一个<strong>单一的簇</strong>（聚类算法）。</li><li><strong>聚类历史被保留</strong>并绘制出来，用户可以在<strong>不预先指定簇数量</strong>的情况下，在不同阶段<strong>可视化</strong>簇的数量和结构。</li><li>簇间距离以不同的方式计算，所有这些方式都依赖于所有<strong>记录间距离</strong>的集合。</li></ul><h3 id="基于模型的聚类"><strong>基于模型的聚类</strong></h3><p>Model-Based Clustering</p><p>像层次聚类和 K-均值这样的聚类方法是基于<strong>启发式</strong>的，主要依赖于寻找其成员彼此接近的簇，这是通过直接用数据来衡量的（不涉及概率模型）。在过去的20年里，人们投入了大量精力来开发<strong>基于模型的聚类方法</strong>。华盛顿大学的 Adrian Raftery 和其他研究人员对基于模型的聚类做出了重要贡献，包括理论和软件。这些技术以<strong>统计理论</strong>为基础，并提供了更严谨的方法来确定簇的性质和数量。例如，它们可以用于以下情况：可能有一组记录彼此相似但不一定彼此接近（例如，回报方差高的科技股），而另一组记录既相似又接近（例如，方差低的公用事业股）。</p><h4 id="多元正态分布"><strong>多元正态分布</strong></h4><p>Multivariate Normal Distribution</p><p>最广泛使用的基于模型的聚类方法都依赖于<strong>多元正态分布</strong>。多元正态分布是<strong>正态分布</strong>向一组 <span class="math inline">\(p\)</span> 个变量 <span class="math inline">\(X_1, X_2, \dots, X_p\)</span> 的推广。该分布由一组均值 <span class="math inline">\(\mu = (\mu_1, \mu_2, \dots, \mu_p)\)</span> 和一个<strong>协方差矩阵 <span class="math inline">\(\Sigma\)</span></strong> 定义。协方差矩阵是衡量变量之间如何关联的度量（有关协方差的详细信息，请参见第202页的“协方差矩阵”）。协方差矩阵 <span class="math inline">\(\Sigma\)</span> 由 <span class="math inline">\(p\)</span> 个方差 <span class="math inline">\(\sigma_1^2, \sigma_2^2, \dots, \sigma_p^2\)</span> 和所有变量对 <span class="math inline">\(i \neq j\)</span> 的协方差 <span class="math inline">\(\sigma_{i,j}\)</span> 组成。将变量放在行和列中，矩阵如下所示：</p><p><span class="math display">\[\Sigma = \begin{pmatrix} \sigma_1^2 &amp; \sigma_{1,2} &amp; \cdots &amp; \sigma_{1,p} \\ \sigma_{2,1} &amp; \sigma_2^2 &amp; \cdots &amp; \sigma_{2,p} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \sigma_{p,1} &amp; \sigma_{p,2} &amp; \cdots &amp; \sigma_p^2 \end{pmatrix}\]</span></p><p>请注意，协方差矩阵关于从左上到右下的对角线是<strong>对称</strong>的。由于 <span class="math inline">\(\sigma_{i,j} = \sigma_{j,i}\)</span>，所以只有 <span class="math inline">\(p(p-1)/2\)</span> 个协方差项。总的来说，协方差矩阵有 <span class="math inline">\(p(p-1)/2 + p\)</span> 个参数。该分布表示为：</p><p><span class="math display">\[(X_1, X_2, \dots, X_p) \sim N_p(\mu, \Sigma)\]</span></p><p>这是一种符号表示，意思是所有变量都呈<strong>正态分布</strong>，并且整体分布由变量均值向量和协方差矩阵完全描述。</p><p><img src="/img3/面向数据科学家的实用统计学/F7.10.png" alt="F7.10" style="zoom:60%;" /></p><p>图7-10显示了两个变量 <span class="math inline">\(X\)</span> 和 <span class="math inline">\(Y\)</span> 的多元正态分布的<strong>概率等高线</strong>（例如，0.5概率等高线包含50%的分布）。</p><blockquote><p>个人注：<strong>概率等高线</strong>（probability contours) 这些等高线连接着具有<strong>相同概率密度</strong>的点，就像地形图上的等高线连接着具有相同海拔的点一样。</p></blockquote><p>均值为 <span class="math inline">\(\mu_x = 0.5\)</span> 和 <span class="math inline">\(\mu_y = -0.5\)</span>，协方差矩阵为：</p><p><span class="math display">\[\Sigma = \begin{pmatrix} 1 &amp; 1 \\ 1 &amp; 2 \end{pmatrix}\]</span></p><p>由于协方差 <span class="math inline">\(\sigma_{xy}\)</span> 是正的，因此 <span class="math inline">\(X\)</span> 和 <span class="math inline">\(Y\)</span> <strong>正相关</strong>。</p><h4 id="正态分布混合"><strong>正态分布混合</strong></h4><p>Mixtures of Normals</p><p>基于模型的聚类背后的关键思想是，<strong>每条记录</strong>被假定为服从 <span class="math inline">\(K\)</span> 个<strong>多元正态分布</strong>中的一个，其中 <span class="math inline">\(K\)</span> 是簇的数量。每个分布都有<strong>不同的均值 <span class="math inline">\(\mu\)</span> 和协方差矩阵 <span class="math inline">\(\Sigma\)</span></strong>。例如，如果您有两个变量 <span class="math inline">\(X\)</span> 和 <span class="math inline">\(Y\)</span>，那么每行 <span class="math inline">\((X_i, Y_i)\)</span> 被建模为从 <span class="math inline">\(K\)</span> 个多元正态分布中的一个进行采样：<span class="math inline">\(N(\mu_1, \Sigma_1), N(\mu_2, \Sigma_2), \dots, N(\mu_K, \Sigma_K)\)</span>。</p><p>R 有一个非常丰富的基于模型聚类的包，名为 <code>mclust</code>，最初由 Chris Fraley 和 Adrian Raftery 开发。使用这个包，我们可以对之前用 K-均值和层次聚类分析过的股票回报数据应用基于模型的聚类：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">&gt;</span> library<span class="punctuation">(</span>mclust<span class="punctuation">)</span></span><br><span class="line"><span class="operator">&gt;</span> df <span class="operator">&lt;-</span> sp500_px<span class="punctuation">[</span>row.names<span class="punctuation">(</span>sp500_px<span class="punctuation">)</span> <span class="operator">&gt;=</span> <span class="string">&#x27;2011-01-01&#x27;</span><span class="punctuation">,</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;XOM&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;CVX&#x27;</span><span class="punctuation">)</span><span class="punctuation">]</span></span><br><span class="line"><span class="operator">&gt;</span> mcl <span class="operator">&lt;-</span> Mclust<span class="punctuation">(</span>df<span class="punctuation">)</span></span><br><span class="line"><span class="operator">&gt;</span> summary<span class="punctuation">(</span>mcl<span class="punctuation">)</span></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Mclust VEE (ellipsoidal, equal shape and orientation) model with 2 components:</span><br><span class="line">log.likelihood n df BIC ICL</span><br><span class="line">-2255.134 1131 9 -4573.546 -5076.856</span><br><span class="line">Clustering table:</span><br><span class="line">1 2</span><br><span class="line">963 168</span><br></pre></td></tr></table></figure><p>scikit-learn 有 <code>sklearn.mixture.GaussianMixture</code> 类用于基于模型的聚类：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df = sp500_px.loc[sp500_px.index &gt;= <span class="string">&#x27;2011-01-01&#x27;</span>, [<span class="string">&#x27;XOM&#x27;</span>, <span class="string">&#x27;CVX&#x27;</span>]]</span><br><span class="line">mclust = GaussianMixture(n_components=<span class="number">2</span>).fit(df)</span><br><span class="line">mclust.bic(df)</span><br></pre></td></tr></table></figure><p>如果您执行此代码，会注意到计算时间比其他过程长得多。使用 <code>predict</code> 函数提取簇分配，我们可以对簇进行可视化：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cluster <span class="operator">&lt;-</span> factor<span class="punctuation">(</span>predict<span class="punctuation">(</span>mcl<span class="punctuation">)</span><span class="operator">$</span>classification<span class="punctuation">)</span></span><br><span class="line">ggplot<span class="punctuation">(</span>data<span class="operator">=</span>df<span class="punctuation">,</span> aes<span class="punctuation">(</span>x<span class="operator">=</span>XOM<span class="punctuation">,</span> y<span class="operator">=</span>CVX<span class="punctuation">,</span> color<span class="operator">=</span>cluster<span class="punctuation">,</span> shape<span class="operator">=</span>cluster<span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">geom_point<span class="punctuation">(</span>alpha<span class="operator">=</span><span class="number">.8</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>以下是创建类似图形的 Python 代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">colors = [<span class="string">f&#x27;C<span class="subst">&#123;c&#125;</span>&#x27;</span> <span class="keyword">for</span> c <span class="keyword">in</span> mclust.predict(df)]</span><br><span class="line">df.plot.scatter(x=<span class="string">&#x27;XOM&#x27;</span>, y=<span class="string">&#x27;CVX&#x27;</span>, c=colors, alpha=<span class="number">0.5</span>, ax=ax)</span><br><span class="line">ax.set_xlim(-<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">ax.set_ylim(-<span class="number">3</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure><p>结果如图7-11所示。 存在两个簇：一个簇在数据的<strong>中间</strong>，另一个簇在数据的<strong>外缘</strong>。这与使用 K-均值（图7-5）和层次聚类（图7-9）获得的簇<strong>非常不同</strong>，后者找到的是紧凑的簇。</p><p><img src="/img3/面向数据科学家的实用统计学/F7.11.png" alt="F7.11" style="zoom:50%;" /></p><p>您可以使用 <code>summary</code> 函数提取正态分布的参数：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">&gt;</span> summary<span class="punctuation">(</span>mcl<span class="punctuation">,</span> parameters<span class="operator">=</span><span class="literal">TRUE</span><span class="punctuation">)</span><span class="operator">$</span>mean</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">          [,1]       [,2]</span><br><span class="line">XOM 0.05783847 -0.04374944</span><br><span class="line">CVX 0.07363239 -0.21175715</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; summary(mcl, parameters=TRUE)$variance</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">, , 1</span><br><span class="line"></span><br><span class="line">          XOM       CVX</span><br><span class="line">XOM 0.3002049 0.3060989</span><br><span class="line">CVX 0.3060989 0.5496727</span><br><span class="line"></span><br><span class="line">, , 2</span><br><span class="line"></span><br><span class="line">         XOM      CVX</span><br><span class="line">XOM 1.046318 1.066860</span><br><span class="line">CVX 1.066860 1.915799</span><br></pre></td></tr></table></figure><p>在 Python 中，您可以从结果的 <code>means_</code> 和 <code>covariances_</code> 属性中获取此信息：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Mean&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(mclust.means_)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Covariances&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(mclust.covariances_)</span><br></pre></td></tr></table></figure><p>这两个分布有相似的均值和相关性，但第二个分布有大得多的方差和协方差。由于算法的随机性，不同运行的结果可能会略有差异。</p><p><code>mclust</code> 产生的簇可能看起来令人惊讶，但事实上，它们说明了该方法的统计性质。基于模型的聚类的目标是找到最能拟合的多元正态分布集。股票数据似乎具有类似正态的形状：参见图7-10的等高线。然而，<strong>实际上，股票回报的分布比正态分布具有更长的尾部</strong>。为了解决这个问题，<code>mclust</code> 将一个分布拟合到大部分数据上，然后拟合第二个具有更大方差的分布。</p><h4 id="选择簇的数量-1"><strong>选择簇的数量</strong></h4><p>Selecting the Number of Clusters</p><p>与 K-均值和层次聚类不同，<code>mclust</code> 在 R 中自动选择簇的数量（在本例中为两个）。它通过选择贝叶斯信息准则（BIC）具有最大值的簇数量来实现（BIC类似于AIC；参见第156页的“模型选择和逐步回归”）。BIC 通过选择拟合最佳的模型来工作，同时对模型中的参数数量进行惩罚。在基于模型的聚类中，添加更多的簇总是会改善拟合，但代价是引入额外的模型参数。</p><blockquote><p>警告：</p><p>请注意，在大多数情况下，BIC通常是最小化的。<code>mclust</code> 包的作者决定定义 BIC 为相反的符号，以便更容易解释绘图。</p></blockquote><p><code>mclust</code> 拟合14个不同模型，并随着组件数量的增加而自动选择最佳模型。您可以使用 <code>mclust</code> 中的函数绘制这些模型的 BIC 值：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot<span class="punctuation">(</span>mcl<span class="punctuation">,</span> what<span class="operator">=</span><span class="string">&#x27;BIC&#x27;</span><span class="punctuation">,</span> ask<span class="operator">=</span><span class="literal">FALSE</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>簇的数量（或者说不同多元正态模型（组件）的数量）显示在 x 轴上（参见图7-12）。</p><p><img src="/img3/面向数据科学家的实用统计学/F7.12.png" alt="F7.12" style="zoom:60%;" /></p><p>另一方面，<code>GaussianMixture</code> 的实现不会尝试各种组合。如下所示，使用 Python 运行多种组合是直截了当的。这个实现按惯例定义 BIC，因此计算出的 BIC 值为正，我们需要将其最小化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">results = []</span><br><span class="line">covariance_types = [<span class="string">&#x27;full&#x27;</span>, <span class="string">&#x27;tied&#x27;</span>, <span class="string">&#x27;diag&#x27;</span>, <span class="string">&#x27;spherical&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> n_components <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">9</span>):</span><br><span class="line">    <span class="keyword">for</span> covariance_type <span class="keyword">in</span> covariance_types:</span><br><span class="line">        mclust = GaussianMixture(n_components=n_components, warm_start=<span class="literal">True</span>,</span><br><span class="line">                                 covariance_type=covariance_type)</span><br><span class="line">        mclust.fit(df)</span><br><span class="line">        results.append(&#123;</span><br><span class="line">            <span class="string">&#x27;bic&#x27;</span>: mclust.bic(df),</span><br><span class="line">            <span class="string">&#x27;n_components&#x27;</span>: n_components,</span><br><span class="line">            <span class="string">&#x27;covariance_type&#x27;</span>: covariance_type,</span><br><span class="line">        &#125;)</span><br><span class="line"></span><br><span class="line">results = pd.DataFrame(results)</span><br><span class="line">colors = [<span class="string">&#x27;C0&#x27;</span>, <span class="string">&#x27;C1&#x27;</span>, <span class="string">&#x27;C2&#x27;</span>, <span class="string">&#x27;C3&#x27;</span>]</span><br><span class="line">styles = [<span class="string">&#x27;C0-&#x27;</span>,<span class="string">&#x27;C1:&#x27;</span>,<span class="string">&#x27;C0-.&#x27;</span>, <span class="string">&#x27;C1--&#x27;</span>]</span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line"><span class="keyword">for</span> i, covariance_type <span class="keyword">in</span> <span class="built_in">enumerate</span>(covariance_types):</span><br><span class="line">    subset = results.loc[results.covariance_type == covariance_type, :]</span><br><span class="line">    subset.plot(x=<span class="string">&#x27;n_components&#x27;</span>, y=<span class="string">&#x27;bic&#x27;</span>, ax=ax, label=covariance_type,</span><br><span class="line">                kind=<span class="string">&#x27;line&#x27;</span>, style=styles[i])</span><br></pre></td></tr></table></figure><p>使用 <code>warm_start</code> 参数，计算将重用上一次拟合的信息。这将加快后续计算的收敛速度。</p><p>这个图与用于确定 K-均值簇数量的<strong>手肘图</strong>相似，不同之处在于绘制的值是<strong>BIC</strong>而不是<strong>解释方差的百分比</strong>（参见图7-7）。一个很大的区别是，<code>mclust</code> 没有显示一条线，而是显示<strong>14条不同的线</strong>！这是因为 <code>mclust</code> 实际上为每种簇大小拟合了14个不同的模型，并最终选择了<strong>最拟合的模型</strong>。<code>GaussianMixture</code> 实现的方法较少，因此线条数量只有四条。</p><p>为什么 <code>mclust</code> 要拟合这么多模型来确定最佳多元正态集呢？这是因为有<strong>不同的方法来参数化协方差矩阵 <span class="math inline">\(\Sigma\)</span></strong> 以进行模型拟合。在大多数情况下，您不需要担心模型的细节，只需使用 <code>mclust</code> 选择的模型即可。在本例中，根据 BIC，三个不同的模型（称为 VEE、VEV 和 VVE）使用两个组件提供了最佳拟合。</p><blockquote><p>通用注解：</p><p>基于模型的聚类是一个丰富且<strong>快速发展的研究领域</strong>，本书的涵盖范围只占该领域的一小部分。事实上，<code>mclust</code> 的帮助文件目前长达154页。对于数据科学家遇到的<strong>大多数问题</strong>来说，深入研究基于模型的聚类的细微差别<strong>可能超出了需要</strong>。</p></blockquote><p>基于模型的聚类技术确实存在一些局限性。这些方法需要一个<strong>潜在的数据模型假设</strong>，而聚类结果<strong>非常依赖于这个假设</strong>。其计算要求<strong>甚至高于层次聚类</strong>，这使得它难以扩展到大数据。最后，该算法<strong>比其他方法更复杂，更难理解</strong>。</p><p><strong>关键思想</strong></p><ul><li>假设簇源自<strong>不同的数据生成过程</strong>，具有<strong>不同的概率分布</strong>。</li><li>拟合不同的模型，假设有不同数量的（通常为<strong>正态</strong>）分布。</li><li>该方法选择<strong>拟合数据良好</strong>且<strong>不使用过多参数</strong>（即，不过拟合）的模型（以及相关的簇数量）。</li></ul><h3 id="缩放和分类变量"><strong>缩放和分类变量</strong></h3><p>Scaling and Categorical Variables</p><p>无监督学习技术通常要求<strong>数据经过适当的缩放</strong>。这与许多回归和分类技术不同，在这些技术中缩放并不重要（一个例外是 K-近邻；参见第238页的“K-近邻”）。</p><p><strong>数据缩放的关键术语</strong></p><ul><li><strong>缩放（Scaling）</strong> 压缩或扩展数据，通常是为了将多个变量带到<strong>相同的尺度</strong>。</li><li><strong>归一化（Normalization）</strong> 一种缩放方法——<strong>减去均值并除以标准差</strong>。 同义词：<strong>标准化（Standardization）</strong></li><li><strong>Gower's distance</strong> 一种应用于<strong>混合数值和分类数据</strong>的缩放算法，将所有变量带到<strong>0-1的范围</strong>。</li></ul><p>例如，对于个人贷款数据，变量具有<strong>截然不同的单位和量级</strong>。有些变量的值相对较小（例如，工作年限），而另一些变量的值则非常大（例如，以美元计的贷款金额）。如果数据没有经过缩放，那么 PCA、K-均值和其他聚类方法将<strong>被大值的变量主导</strong>，而<strong>忽略小值的变量</strong>。</p><p>对于某些聚类过程，<strong>分类数据</strong>可能会带来特殊问题。与 K-近邻一样，<strong>无序因子变量</strong>通常使用<strong>独热编码</strong>（one hot encoding）转换为一组<strong>二元（0/1）变量</strong>（参见第242页的“独热编码”）。二元变量不仅可能与其他数据处于不同的尺度，而且二元变量只有两个值这一事实，对于 PCA 和 K-均值等技术来说也可能存在问题。</p><h4 id="变量缩放"><strong>变量缩放</strong></h4><p>Scaling the Variables</p><p>在应用聚类过程之前，需要对具有<strong>截然不同尺度和单位</strong>的变量进行<strong>适当的归一化</strong>。例如，让我们在不进行归一化的情况下，将 <code>kmeans</code> 应用于一组贷款违约数据：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">defaults <span class="operator">&lt;-</span> loan_data<span class="punctuation">[</span>loan_data<span class="operator">$</span>outcome<span class="operator">==</span><span class="string">&#x27;default&#x27;</span><span class="punctuation">,</span><span class="punctuation">]</span></span><br><span class="line">df <span class="operator">&lt;-</span> defaults<span class="punctuation">[</span><span class="punctuation">,</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;loan_amnt&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;annual_inc&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;revol_bal&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;open_acc&#x27;</span><span class="punctuation">,</span></span><br><span class="line"><span class="string">&#x27;dti&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;revol_util&#x27;</span><span class="punctuation">)</span><span class="punctuation">]</span></span><br><span class="line">km <span class="operator">&lt;-</span> kmeans<span class="punctuation">(</span>df<span class="punctuation">,</span> centers<span class="operator">=</span><span class="number">4</span><span class="punctuation">,</span> nstart<span class="operator">=</span><span class="number">10</span><span class="punctuation">)</span></span><br><span class="line">centers <span class="operator">&lt;-</span> data.frame<span class="punctuation">(</span>size<span class="operator">=</span>km<span class="operator">$</span>size<span class="punctuation">,</span> km<span class="operator">$</span>centers<span class="punctuation">)</span></span><br><span class="line"><span class="built_in">round</span><span class="punctuation">(</span>centers<span class="punctuation">,</span> digits<span class="operator">=</span><span class="number">2</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">   size loan_amnt annual_inc revol_bal open_acc  dti revol_util</span><br><span class="line">1    52  22570.19  489783.40  85161.35    13.33 6.91      59.65</span><br><span class="line">2  1192  21856.38  165473.54  38935.88    12.61 13.48      63.67</span><br><span class="line">3 13902  10606.48   42500.30  10280.52     9.59 17.71      58.11</span><br><span class="line">4  7525  18282.25   83458.11  19653.82    11.66 16.77      62.27</span><br></pre></td></tr></table></figure><p>以下是相应的 Python 代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">defaults = loan_data.loc[loan_data[<span class="string">&#x27;outcome&#x27;</span>] == <span class="string">&#x27;default&#x27;</span>,]</span><br><span class="line">columns = [<span class="string">&#x27;loan_amnt&#x27;</span>, <span class="string">&#x27;annual_inc&#x27;</span>, <span class="string">&#x27;revol_bal&#x27;</span>, <span class="string">&#x27;open_acc&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;dti&#x27;</span>, <span class="string">&#x27;revol_util&#x27;</span>]</span><br><span class="line">df = defaults[columns]</span><br><span class="line">kmeans = KMeans(n_clusters=<span class="number">4</span>, random_state=<span class="number">1</span>).fit(df)</span><br><span class="line">counts = Counter(kmeans.labels_)</span><br><span class="line">centers = pd.DataFrame(kmeans.cluster_centers_, columns=columns)</span><br><span class="line">centers[<span class="string">&#x27;size&#x27;</span>] = [counts[i] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>)]</span><br><span class="line">centers</span><br></pre></td></tr></table></figure><p>变量 <code>annual_inc</code> 和 <code>revol_bal</code> <strong>主导了簇</strong>，并且簇的大小<strong>差异很大</strong>。簇1只有52个成员，其收入和循环信贷余额相对较高。</p><p>一种常见的变量缩放方法是将它们<strong>转换为z-分数</strong>，即<strong>减去均值并除以标准差</strong>。这被称为<strong>标准化</strong>或<strong>归一化</strong>（关于使用 z-分数的更多讨论，请参见第243页的“标准化（归一化，z-分数）”）：</p><p><span class="math display">\[z = \frac{x-\bar{x}}{s}\]</span></p><p>让我们看看将 <code>kmeans</code> 应用于<strong>归一化数据</strong>后簇发生了什么：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">df0 <span class="operator">&lt;-</span> scale<span class="punctuation">(</span>df<span class="punctuation">)</span></span><br><span class="line">km0 <span class="operator">&lt;-</span> kmeans<span class="punctuation">(</span>df0<span class="punctuation">,</span> centers<span class="operator">=</span><span class="number">4</span><span class="punctuation">,</span> nstart<span class="operator">=</span><span class="number">10</span><span class="punctuation">)</span></span><br><span class="line">centers0 <span class="operator">&lt;-</span> scale<span class="punctuation">(</span>km0<span class="operator">$</span>centers<span class="punctuation">,</span> center<span class="operator">=</span><span class="literal">FALSE</span><span class="punctuation">,</span></span><br><span class="line">scale<span class="operator">=</span><span class="number">1</span> <span class="operator">/</span> <span class="built_in">attr</span><span class="punctuation">(</span>df0<span class="punctuation">,</span> <span class="string">&#x27;scaled:scale&#x27;</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">centers0 <span class="operator">&lt;-</span> scale<span class="punctuation">(</span>centers0<span class="punctuation">,</span> center<span class="operator">=</span><span class="operator">-</span><span class="built_in">attr</span><span class="punctuation">(</span>df0<span class="punctuation">,</span> <span class="string">&#x27;scaled:center&#x27;</span><span class="punctuation">)</span><span class="punctuation">,</span> scale<span class="operator">=</span><span class="literal">FALSE</span><span class="punctuation">)</span></span><br><span class="line">centers0 <span class="operator">&lt;-</span> data.frame<span class="punctuation">(</span>size<span class="operator">=</span>km0<span class="operator">$</span>size<span class="punctuation">,</span> centers0<span class="punctuation">)</span></span><br><span class="line"><span class="built_in">round</span><span class="punctuation">(</span>centers0<span class="punctuation">,</span> digits<span class="operator">=</span><span class="number">2</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">   size loan_amnt annual_inc revol_bal open_acc  dti revol_util</span><br><span class="line">1  7355  10467.65   51134.87  11523.31     7.48 15.78      77.73</span><br><span class="line">2  5309  10363.43   53523.09   6038.26     8.68 11.32      30.70</span><br><span class="line">3  3713  25894.07  116185.91  32797.67    12.41 16.22      66.14</span><br><span class="line">4  6294  13361.61   55596.65  16375.27    14.25 24.23      59.61</span><br></pre></td></tr></table></figure><p>在 Python 中，我们可以使用 scikit-learn 的 <code>StandardScaler</code>。<code>inverse_transform</code> 方法允许将簇中心<strong>转换回原始尺度</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scaler = preprocessing.StandardScaler()</span><br><span class="line">df0 = scaler.fit_transform(df * <span class="number">1.0</span>)</span><br><span class="line">kmeans = KMeans(n_clusters=<span class="number">4</span>, random_state=<span class="number">1</span>).fit(df0)</span><br><span class="line">counts = Counter(kmeans.labels_)</span><br><span class="line">centers = pd.DataFrame(scaler.inverse_transform(kmeans.cluster_centers_),</span><br><span class="line">columns=columns)</span><br><span class="line">centers[<span class="string">&#x27;size&#x27;</span>] = [counts[i] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>)]</span><br><span class="line">centers</span><br></pre></td></tr></table></figure><p>簇的大小<strong>更加均衡</strong>，并且簇不再被 <code>annual_inc</code> 和 <code>revol_bal</code> 主导，从而揭示了数据中<strong>更有趣的结构</strong>。请注意，在上述代码中，中心被<strong>重新缩放回原始单位</strong>。如果我们没有对它们进行缩放，得到的值将是 z-分数，因此可解释性会降低。</p><blockquote><p>通用注解：</p><p>缩放对于 <strong>PCA</strong> 也非常重要。使用 z-分数等同于在计算主成分时使用<strong>相关矩阵</strong>（参见第30页的“相关性”），而不是协方差矩阵。计算 PCA 的软件通常都有一个使用相关矩阵的选项（在 R 中，<code>princomp</code> 函数有一个 <code>cor</code> 参数）。</p></blockquote><h4 id="主导变量"><strong>主导变量</strong></h4><p>Dominant Variables</p><p>即使变量以相同的尺度测量并准确反映相对重要性（例如，对股票价格的走势），有时<strong>重新缩放变量</strong>仍然是有用的。</p><p>假设我们在第289页的“解释主成分”中添加了 Google (GOOGL) 和 Amazon (AMZN) 到分析中。下面我们来看看如何在 R 中完成：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">syms <span class="operator">&lt;-</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;GOOGL&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;AMZN&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;AAPL&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;MSFT&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;CSCO&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;INTC&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;CVX&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;XOM&#x27;</span><span class="punctuation">,</span></span><br><span class="line"><span class="string">&#x27;SLB&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;COP&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;JPM&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;WFC&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;USB&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;AXP&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;WMT&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;TGT&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;HD&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;COST&#x27;</span><span class="punctuation">)</span></span><br><span class="line">top_sp1 <span class="operator">&lt;-</span> sp500_px<span class="punctuation">[</span>row.names<span class="punctuation">(</span>sp500_px<span class="punctuation">)</span> <span class="operator">&gt;=</span> <span class="string">&#x27;2005-01-01&#x27;</span><span class="punctuation">,</span> syms<span class="punctuation">]</span></span><br><span class="line">sp_pca1 <span class="operator">&lt;-</span> princomp<span class="punctuation">(</span>top_sp1<span class="punctuation">)</span></span><br><span class="line">screeplot<span class="punctuation">(</span>sp_pca1<span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>在 Python 中，我们按如下方式获取碎石图：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">syms = [<span class="string">&#x27;GOOGL&#x27;</span>, <span class="string">&#x27;AMZN&#x27;</span>, <span class="string">&#x27;AAPL&#x27;</span>, <span class="string">&#x27;MSFT&#x27;</span>, <span class="string">&#x27;CSCO&#x27;</span>, <span class="string">&#x27;INTC&#x27;</span>, <span class="string">&#x27;CVX&#x27;</span>, <span class="string">&#x27;XOM&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;SLB&#x27;</span>, <span class="string">&#x27;COP&#x27;</span>, <span class="string">&#x27;JPM&#x27;</span>, <span class="string">&#x27;WFC&#x27;</span>, <span class="string">&#x27;USB&#x27;</span>, <span class="string">&#x27;AXP&#x27;</span>, <span class="string">&#x27;WMT&#x27;</span>, <span class="string">&#x27;TGT&#x27;</span>, <span class="string">&#x27;HD&#x27;</span>, <span class="string">&#x27;COST&#x27;</span>]</span><br><span class="line">top_sp1 = sp500_px.loc[sp500_px.index &gt;= <span class="string">&#x27;2005-01-01&#x27;</span>, syms]</span><br><span class="line">sp_pca1 = PCA()</span><br><span class="line">sp_pca1.fit(top_sp1)</span><br><span class="line">explained_variance = pd.DataFrame(sp_pca1.explained_variance_)</span><br><span class="line">ax = explained_variance.head(<span class="number">10</span>).plot.bar(legend=<span class="literal">False</span>, figsize=(<span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">ax.set_xlabel(<span class="string">&#x27;Component&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="/img3/面向数据科学家的实用统计学/F7.13.png" alt="F7.13" style="zoom:50%;" /></p><p>碎石图显示了前几个主成分的方差。在本例中，图7-13中的碎石图显示<strong>第一和第二成分的方差比其他成分大得多</strong>。 这通常表明<strong>一个或两个变量主导了载荷</strong>。在这个例子中，情况确实如此：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">round(sp_pca1$loadings[,1:2], 3)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">       Comp.1 Comp.2</span><br><span class="line">GOOGL   0.781  0.609</span><br><span class="line">AMZN    0.593 -0.792</span><br><span class="line">AAPL    0.078  0.004</span><br><span class="line">MSFT    0.029  0.002</span><br><span class="line">CSCO    0.017 -0.001</span><br><span class="line">INTC    0.020 -0.001</span><br><span class="line">CVX     0.068 -0.021</span><br><span class="line">XOM     0.053 -0.005</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>在 Python 中，我们使用以下代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loadings = pd.DataFrame(sp_pca1.components_[<span class="number">0</span>:<span class="number">2</span>, :], columns=top_sp1.columns)</span><br><span class="line">loadings.transpose()</span><br></pre></td></tr></table></figure><p>前两个主成分几乎完全由 <strong>GOOGL</strong> 和 <strong>AMZN</strong> 主导。这是因为 GOOGL 和 AMZN 的<strong>股价波动主导了变异性</strong>。</p><p>为了处理这种情况，您可以<strong>保持现状</strong>、<strong>重新缩放变量</strong>（参见第319页的“变量缩放”），或者<strong>将主导变量从分析中排除并单独处理</strong>。没有“正确”的方法，处理方式取决于具体的应用。</p><h4 id="分类数据和-gowers-距离"><strong>分类数据和 Gower's 距离</strong></h4><p>Categorical Data and Gower’s Distance</p><p>对于<strong>分类数据</strong>，您必须将其转换为数值数据，可以通过<strong>排序</strong>（对于有序因子）或<strong>编码为一组二元（虚拟）变量</strong>来完成。如果数据由<strong>混合的连续和二元变量</strong>组成，您通常会希望<strong>缩放变量</strong>，使其范围相似；参见第319页的“变量缩放”。一种流行的方法是使用 <strong>Gower's 距离</strong>。</p><p>Gower's 距离的基本思想是根据<strong>数据的类型</strong>对<strong>每个变量</strong>应用<strong>不同的距离度量</strong>：</p><ul><li>对于<strong>数值变量</strong>和<strong>有序因子</strong>，距离被计算为两条记录之间<strong>差值的绝对值</strong>（曼哈顿距离）。</li><li>对于<strong>分类变量</strong>，如果两条记录的类别<strong>不同</strong>，距离为<strong>1</strong>；如果类别<strong>相同</strong>，距离为<strong>0</strong>。</li></ul><p>Gower's 距离的计算过程如下：</p><ol type="1"><li>计算<strong>每条记录</strong>所有<strong>变量对 <span class="math inline">\(i\)</span> 和 <span class="math inline">\(j\)</span></strong> 的距离 <span class="math inline">\(d_{i,j}\)</span>。</li><li>将每对距离 <span class="math inline">\(d_{i,j}\)</span> 进行缩放，使其最小值<strong>为0</strong>，最大值<strong>为1</strong>。</li><li>使用简单或加权平均值，将变量之间的成对缩放距离相加，以创建<strong>距离矩阵</strong>。</li></ol><p>为了说明 Gower's 距离，我们从 R 中的贷款数据中提取几行：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">&gt;</span> x <span class="operator">&lt;-</span> loan_data<span class="punctuation">[</span><span class="number">1</span><span class="operator">:</span><span class="number">5</span><span class="punctuation">,</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;dti&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;payment_inc_ratio&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;home_&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;purpose_&#x27;</span><span class="punctuation">)</span><span class="punctuation">]</span></span><br><span class="line"><span class="operator">&gt;</span> x</span><br><span class="line"><span class="comment"># A tibble: 5 × 4</span></span><br><span class="line">    dti payment_inc_ratio home_     purpose_</span><br><span class="line">  <span class="operator">&lt;</span>dbl<span class="operator">&gt;</span>             <span class="operator">&lt;</span>dbl<span class="operator">&gt;</span> <span class="operator">&lt;</span>fct<span class="operator">&gt;</span>     <span class="operator">&lt;</span>fct<span class="operator">&gt;</span></span><br><span class="line"><span class="number">1</span>  <span class="number">1.00</span>             <span class="number">2.97</span>  OWN       credit_card</span><br><span class="line"><span class="number">2</span>  <span class="number">5.55</span>             <span class="number">3.83</span>  RENT      debt_consolidation</span><br><span class="line"><span class="number">3</span> <span class="number">18.08</span>             <span class="number">7.69</span>  MORTGAGE  debt_consolidation</span><br><span class="line"><span class="number">4</span> <span class="number">10.08</span>             <span class="number">5.58</span>  RENT      home_improvement</span><br><span class="line"><span class="number">5</span>  <span class="number">6.56</span>             <span class="number">2.92</span>  MORTGAGE  major_purchase</span><br></pre></td></tr></table></figure><p>R 的 <code>cluster</code> 包中的 <code>daisy</code> 函数可用于计算 Gower's 距离：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">library<span class="punctuation">(</span>cluster<span class="punctuation">)</span></span><br><span class="line">daisy<span class="punctuation">(</span>x<span class="punctuation">,</span> metric<span class="operator">=</span><span class="string">&#x27;gower&#x27;</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Dissimilarities :</span><br><span class="line">          1         2         3         4</span><br><span class="line">2 0.6220479                              </span><br><span class="line">3 0.6863877 0.8143398                    </span><br><span class="line">4 0.6329040 0.7608561 0.4307083          </span><br><span class="line">5 0.3772789 0.5389727 0.3091088 0.5056250</span><br><span class="line"></span><br><span class="line">Metric :  mixed ; Types = I, I, N, N </span><br><span class="line">Number of objects : 5</span><br></pre></td></tr></table></figure><p>截至撰写本文时，Gower's 距离在任何流行的 Python 包中都不可用。然而，正在进行将其包含在 <code>scikit-learn</code> 中的活动。一旦实现发布，我们将更新随附的源代码。</p><p>所有距离都在<strong>0到1之间</strong>。距离最大的记录对是<strong>2和3</strong>：它们的 <code>home</code> 和 <code>purpose</code> 值都不同，并且 <code>dti</code>（债务收入比）和 <code>payment_inc_ratio</code> 的水平也大相径庭。记录<strong>3和5</strong>的距离最小，因为它们的 <code>home</code> 和 <code>purpose</code> 值相同。</p><p>您可以将从 <code>daisy</code> 计算出的 Gower's 距离矩阵传递给 <code>hclust</code> 进行<strong>层次聚类</strong>（参见第304页的“层次聚类”）：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">df <span class="operator">&lt;-</span> defaults<span class="punctuation">[</span>sample<span class="punctuation">(</span>nrow<span class="punctuation">(</span>defaults<span class="punctuation">)</span><span class="punctuation">,</span> <span class="number">250</span><span class="punctuation">)</span><span class="punctuation">,</span></span><br><span class="line"><span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;dti&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;payment_inc_ratio&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;home&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;purpose&#x27;</span><span class="punctuation">)</span><span class="punctuation">]</span></span><br><span class="line">d <span class="operator">=</span> daisy<span class="punctuation">(</span>df<span class="punctuation">,</span> metric<span class="operator">=</span><span class="string">&#x27;gower&#x27;</span><span class="punctuation">)</span></span><br><span class="line">hcl <span class="operator">&lt;-</span> hclust<span class="punctuation">(</span>d<span class="punctuation">)</span></span><br><span class="line">dnd <span class="operator">&lt;-</span> as.dendrogram<span class="punctuation">(</span>hcl<span class="punctuation">)</span></span><br><span class="line">plot<span class="punctuation">(</span>dnd<span class="punctuation">,</span> leaflab<span class="operator">=</span><span class="string">&#x27;none&#x27;</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p><img src="/img3/面向数据科学家的实用统计学/F7.14.png" alt="F7.14" style="zoom:67%;" /></p><p>生成的<strong>树状图</strong>如图7-14所示。 单个记录在 x 轴上无法区分，但我们可以将树状图在0.5处水平切割，并使用此代码检查其中一个子树中的记录：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dnd_cut <span class="operator">&lt;-</span> cut<span class="punctuation">(</span>dnd<span class="punctuation">,</span> h<span class="operator">=</span><span class="number">0.5</span><span class="punctuation">)</span></span><br><span class="line">df<span class="punctuation">[</span>labels<span class="punctuation">(</span>dnd_cut<span class="operator">$</span>lower<span class="punctuation">[[</span><span class="number">1</span><span class="punctuation">]</span><span class="punctuation">]</span><span class="punctuation">)</span><span class="punctuation">,</span><span class="punctuation">]</span></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">      dti payment_inc_ratio home_          purpose_</span><br><span class="line">44532 21.22          8.37694   OWN debt_consolidation</span><br><span class="line">39826 22.59          6.22827   OWN debt_consolidation</span><br><span class="line">13282 31.00          9.64200   OWN debt_consolidation</span><br><span class="line">31510 26.21         11.94380   OWN debt_consolidation</span><br><span class="line">6693  26.96          9.45600   OWN debt_consolidation</span><br><span class="line">7356  25.81          9.39257   OWN debt_consolidation</span><br><span class="line">9278  21.00         14.71850   OWN debt_consolidation</span><br><span class="line">13520 29.00         18.86670   OWN debt_consolidation</span><br><span class="line">14668 25.75         17.53440   OWN debt_consolidation</span><br><span class="line">19975 22.70         17.12170   OWN debt_consolidation</span><br><span class="line">23492 22.68         18.50250   OWN debt_consolidation</span><br></pre></td></tr></table></figure><p>这个子树完全由<strong>拥有房产</strong>且贷款目的标记为<strong>“债务合并”</strong>的记录组成。虽然并非所有子树都存在严格的分离，但这表明分类变量倾向于在簇中被分组在一起。</p><h4 id="混合数据的聚类问题"><strong>混合数据的聚类问题</strong></h4><p>Problems with Clustering Mixed Data</p><p>K-均值和 PCA <strong>最适合连续变量</strong>。对于较小的数据集，最好使用带有 Gower's 距离的<strong>层次聚类</strong>。原则上，K-均值也可以应用于<strong>二元或分类数据</strong>。通常会使用“独热编码”表示法（参见第242页的“独热编码”）将分类数据转换为数值。然而，在实践中，将 K-均值和 PCA 与二元数据一起使用可能会很困难。</p><p>如果使用标准的 z-分数，<strong>二元变量将主导簇的定义</strong>。这是因为 0/1 变量只取两个值，而 K-均值可以通过将所有值为0或1的记录分配给单个簇来获得较小的簇内平方和。例如，将 <code>kmeans</code> 应用于包含因子变量 <code>home</code> 和 <code>pub_rec_zero</code> 的贷款违约数据，如R中所示：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">df <span class="operator">&lt;-</span> model.matrix<span class="punctuation">(</span><span class="operator">~</span> <span class="operator">-</span><span class="number">1</span> <span class="operator">+</span> dti <span class="operator">+</span> payment_inc_ratio <span class="operator">+</span> home_ <span class="operator">+</span> pub_rec_zero<span class="punctuation">,</span></span><br><span class="line">data<span class="operator">=</span>defaults<span class="punctuation">)</span></span><br><span class="line">df0 <span class="operator">&lt;-</span> scale<span class="punctuation">(</span>df<span class="punctuation">)</span></span><br><span class="line">km0 <span class="operator">&lt;-</span> kmeans<span class="punctuation">(</span>df0<span class="punctuation">,</span> centers<span class="operator">=</span><span class="number">4</span><span class="punctuation">,</span> nstart<span class="operator">=</span><span class="number">10</span><span class="punctuation">)</span></span><br><span class="line">centers0 <span class="operator">&lt;-</span> scale<span class="punctuation">(</span>km0<span class="operator">$</span>centers<span class="punctuation">,</span> center<span class="operator">=</span><span class="literal">FALSE</span><span class="punctuation">,</span></span><br><span class="line">scale<span class="operator">=</span><span class="number">1</span><span class="operator">/</span><span class="built_in">attr</span><span class="punctuation">(</span>df0<span class="punctuation">,</span> <span class="string">&#x27;scaled:scale&#x27;</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line"><span class="built_in">round</span><span class="punctuation">(</span>scale<span class="punctuation">(</span>centers0<span class="punctuation">,</span> center<span class="operator">=</span><span class="operator">-</span><span class="built_in">attr</span><span class="punctuation">(</span>df0<span class="punctuation">,</span> <span class="string">&#x27;scaled:center&#x27;</span><span class="punctuation">)</span><span class="punctuation">,</span> scale<span class="operator">=</span><span class="literal">FALSE</span><span class="punctuation">)</span><span class="punctuation">,</span> <span class="number">2</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">   dti payment_inc_ratio home_MORTGAGE home_OWN home_RENT pub_rec_zero</span><br><span class="line">1 17.20              9.27          0.00     1.00      0.00         0.92</span><br><span class="line">2 16.99              9.11          0.00     0.00      1.00         1.00</span><br><span class="line">3 16.50              8.06          0.52     0.00      0.48         1.00</span><br><span class="line">4 17.46              8.42          1.00     0.00      0.00         0.00</span><br></pre></td></tr></table></figure><p>在 Python 中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">columns = [<span class="string">&#x27;dti&#x27;</span>, <span class="string">&#x27;payment_inc_ratio&#x27;</span>, <span class="string">&#x27;home_&#x27;</span>, <span class="string">&#x27;pub_rec_zero&#x27;</span>]</span><br><span class="line">df = pd.get_dummies(defaults[columns])</span><br><span class="line">scaler = preprocessing.StandardScaler()</span><br><span class="line">df0 = scaler.fit_transform(df * <span class="number">1.0</span>)</span><br><span class="line">kmeans = KMeans(n_clusters=<span class="number">4</span>, random_state=<span class="number">1</span>).fit(df0)</span><br><span class="line">centers = pd.DataFrame(scaler.inverse_transform(kmeans.cluster_centers_),</span><br><span class="line">columns=df.columns)</span><br><span class="line">centers</span><br></pre></td></tr></table></figure><p>前四个簇本质上是<strong>因子变量不同水平的代理</strong>。为了避免这种行为，您可以将二元变量<strong>缩放</strong>，使其方差小于其他变量。另外，对于非常大的数据集，您可以将聚类应用于<strong>具有特定分类值的不同数据子集</strong>。例如，您可以将聚类分别应用于那些有房贷的人、拥有房产的人或租房的人所获得的贷款。</p><p><strong>关键思想</strong></p><ul><li>以不同尺度测量的变量需要<strong>转换为相似的尺度</strong>，以确保它们对算法的影响不主要由其尺度决定。</li><li>一种常见的缩放方法是<strong>归一化（标准化）</strong>——减去均值并除以标准差。</li><li>另一种方法是 <strong>Gower's 距离</strong>，它将所有变量缩放到0-1的范围（常用于混合数值和分类数据）。</li></ul><h3 id="总结"><strong>总结</strong></h3><p>对于数值数据的<strong>降维</strong>，主要工具是<strong>主成分分析</strong>或 <strong>K-均值聚类</strong>。两者都需要注意<strong>数据的正确缩放</strong>，以确保有意义的数据缩减。</p><p>对于<strong>高度结构化</strong>且簇<strong>分隔良好</strong>的数据进行聚类时，所有方法可能都会产生相似的结果。每种方法都有其自身的优势。<strong>K-均值</strong>可以扩展到非常大的数据，且易于理解。<strong>层次聚类</strong>可以应用于混合数据类型——数值和分类——并易于进行直观的显示（树状图）。<strong>基于模型的聚类</strong>建立在统计理论基础上，提供了一种比启发式方法更严谨的方法。然而，对于<strong>非常大的数据</strong>，K-均值是主要使用的方法。</p><p>对于像贷款和股票数据这样的<strong>嘈杂数据</strong>（以及数据科学家将面临的大部分数据），选择就更加<strong>严峻</strong>了。K-均值、层次聚类，尤其是基于模型的聚类，都会产生<strong>截然不同</strong>的解决方案。数据科学家应该如何进行？不幸的是，<strong>没有简单的经验法则来指导选择</strong>。最终，使用的方法将取决于<strong>数据大小</strong>和<strong>应用的目标</strong>。</p>]]></content>
      
      
      <categories>
          
          <category> 翻译 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> AI </tag>
            
            <tag> 统计 </tag>
            
            <tag> 数据科学 </tag>
            
            <tag> R </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第6章 统计机器学习</title>
      <link href="/2025/09/25/%E7%AC%AC6%E7%AB%A0%20%E7%BB%9F%E8%AE%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
      <url>/2025/09/25/%E7%AC%AC6%E7%AB%A0%20%E7%BB%9F%E8%AE%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<p><a href="/img3/面向数据科学家的实用统计学/Practical-Statistics-for-Data-Scientists.pdf">《Practical Statistics for Data Scientists》书籍英文版</a><br /><a href="/img3/面向数据科学家的实用统计学/面向数据科学家的实用统计学.pdf">《面向数据科学家的实用统计学》中文版书籍</a></p><h2 id="第6章-统计机器学习"><strong>第6章 统计机器学习</strong></h2><p>Statistical Machine Learning</p><p>统计学在近期发展中，致力于开发更强大、更自动化的<strong>预测建模技术</strong>，涵盖了<strong>回归</strong>和<strong>分类</strong>。这些方法与上一章讨论的方法一样，都是<strong>有监督学习</strong>——它们通过在已知结果的数据上进行训练，来学习如何预测新数据的结果。它们属于<strong>统计机器学习</strong>的范畴，与经典统计方法不同之处在于，它们是<strong>数据驱动</strong>的，并且不试图对数据强加线性的或其他整体结构。例如，<strong>K-最近邻</strong>(K-Nearest Neighbors)方法非常简单：根据相似记录的分类方式来对一条记录进行分类。最成功和应用最广泛的技术是基于<strong>集成学习</strong>(ensemble learning)并应用于<strong>决策树</strong>(decision trees.)的方法。集成学习的基本思想是<strong>使用多个模型</strong>来形成预测，而不是仅仅使用一个单一模型。决策树是一种灵活且自动化的技术，用于学习预测变量和结果变量之间关系的规则。事实证明，将集成学习与决策树相结合，可以产生一些性能最佳的现成预测建模技术。</p><p>许多统计机器学习技术的发展，可以追溯到加州大学伯克利分校的统计学家 Leo Breiman（参见图6-1）和斯坦福大学的 Jerry Friedman。 他们的工作，以及伯克利和斯坦福其他研究人员的工作，始于1984年对<strong>树模型</strong>的开发。随后在20世纪90年代开发的<strong>装袋法（bagging）</strong>和<strong>提升法（boosting）</strong>等集成方法，奠定了统计机器学习的基础。 <span id="more"></span></p><p><img src="/img3/面向数据科学家的实用统计学/F6.1.png" alt="F6.1" style="zoom:50%;" /></p><blockquote><p>通用注解：</p><p><strong>机器学习与统计学</strong>（Machine Learning Versus Statistics）</p><p>在预测建模的背景下，<strong>机器学习和统计学有什么区别</strong>？这两个学科之间没有明确的界限。机器学习更倾向于开发<strong>高效算法</strong>，以处理大规模数据来优化预测模型。而统计学通常更关注<strong>概率理论</strong>和模型的<strong>底层结构</strong>。<strong>装袋法</strong>和<strong>随机森林</strong>（参见第259页的“装袋法与随机森林”）稳固地属于统计学阵营。另一方面，<strong>提升法</strong>（参见第270页的“提升法”）在这两个学科中都有发展，但在机器学习方面获得了更多关注。不管历史如何，提升法所展现的潜力确保了它将在统计学和机器学习中都蓬勃发展。</p></blockquote><h3 id="k-最近邻"><strong>K-最近邻</strong></h3><p>K-Nearest Neighbors</p><p>K-最近邻（KNN）背后的思想非常简单。对于每一条待分类或预测的记录：</p><ol type="1"><li>找到<strong>K</strong>个特征相似（即，预测变量值相似）的记录。</li><li>对于<strong>分类</strong>，找出这些相似记录中<strong>哪一类是多数</strong>，并将该类别分配给新记录。</li><li>对于<strong>预测</strong>（也称作 <strong>KNN 回归</strong>），求出这些相似记录的<strong>平均值</strong>，并用该平均值作为新记录的预测值。</li></ol><p><strong>K-最近邻的关键术语</strong></p><ul><li><p><strong>邻居（Neighbor）</strong> 与另一条记录有相似预测变量值的记录。</p></li><li><p><strong>距离度量（Distance metrics）</strong> 用一个单一数值来衡量一条记录与另一条记录相距多远。</p></li><li><p><strong>标准化（Standardization）</strong> 减去均值并除以标准差。 同义词：<strong>归一化（Normalization）</strong></p></li><li><p><strong>z分数（z-score）</strong> 标准化后得到的值。</p></li><li><p><strong>K</strong> 在最近邻计算中考虑的邻居数量。</p></li></ul><p>KNN 是最简单的预测/分类技术之一：它<strong>不需要拟合模型</strong>（如回归中那样）。但这并不意味着使用 KNN 是一个自动化的过程。预测结果取决于<strong>特征如何缩放</strong>、<strong>如何衡量相似性</strong>以及<strong>K设置得有多大</strong>。此外，所有预测变量都必须是<strong>数值形式</strong>。我们将通过一个分类示例来演示如何使用 KNN 方法。</p><h4 id="一个小例子预测贷款违约"><strong>一个小例子：预测贷款违约</strong></h4><p>表6-1显示了来自 LendingClub 的一些个人贷款数据记录。LendingClub 是P2P借贷领域的领导者，投资者群体向个人提供贷款。分析的目标是预测一笔新的潜在贷款的结果：<strong>已还清</strong>还是<strong>违约</strong>。</p><p><img src="/img3/面向数据科学家的实用统计学/T6.1.png" alt="T6.1" style="zoom:50%;" /></p><p>考虑一个非常简单的模型，只有两个预测变量：<strong>dti</strong>，即债务支付（不包括抵押贷款）与收入的比率；以及 <strong>payment_inc_ratio</strong>，即贷款还款额与收入的比率。两个比率都乘以100。</p><p>使用一个包含200笔贷款的小数据集 <code>loan200</code>，其中包含已知的二元结果（违约或未违约，在预测变量 <code>outcome200</code> 中指定），并将 K 设置为20。对于一笔待预测的新贷款 <code>newloan</code>，其 dti=22.5，payment_inc_ratio=9，KNN 估计值在 R 中可计算如下：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">newloan <span class="operator">&lt;-</span> loan200<span class="punctuation">[</span><span class="number">1</span><span class="punctuation">,</span> <span class="number">2</span><span class="operator">:</span><span class="number">3</span><span class="punctuation">,</span> drop<span class="operator">=</span><span class="literal">FALSE</span><span class="punctuation">]</span></span><br><span class="line">knn_pred <span class="operator">&lt;-</span> knn<span class="punctuation">(</span>train<span class="operator">=</span>loan200<span class="punctuation">[</span><span class="operator">-</span><span class="number">1</span><span class="punctuation">,</span> <span class="number">2</span><span class="operator">:</span><span class="number">3</span><span class="punctuation">]</span><span class="punctuation">,</span> test<span class="operator">=</span>newloan<span class="punctuation">,</span> cl<span class="operator">=</span>loan200<span class="punctuation">[</span><span class="operator">-</span><span class="number">1</span><span class="punctuation">,</span> <span class="number">1</span><span class="punctuation">]</span><span class="punctuation">,</span> k<span class="operator">=</span><span class="number">20</span><span class="punctuation">)</span></span><br><span class="line">knn_pred <span class="operator">==</span> <span class="string">&#x27;paid off&#x27;</span></span><br><span class="line"><span class="punctuation">[</span><span class="number">1</span><span class="punctuation">]</span> <span class="literal">TRUE</span></span><br></pre></td></tr></table></figure><p>KNN 的预测结果是该贷款将会违约。</p><p>虽然 R 有一个原生的 <code>knn</code> 函数，但贡献的 R 包 <strong><code>FNN</code></strong>（意为 Fast Nearest Neighbor，快速最近邻）能更有效地扩展到大数据，并提供更大的灵活性。</p><p><code>scikit-learn</code> 包在 Python 中提供了 KNN 的快速高效实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">predictors = [<span class="string">&#x27;payment_inc_ratio&#x27;</span>, <span class="string">&#x27;dti&#x27;</span>]</span><br><span class="line">outcome = <span class="string">&#x27;outcome&#x27;</span></span><br><span class="line">newloan = loan200.loc[<span class="number">0</span>:<span class="number">0</span>, predictors]</span><br><span class="line">X = loan200.loc[<span class="number">1</span>:, predictors]</span><br><span class="line">y = loan200.loc[<span class="number">1</span>:, outcome]</span><br><span class="line">knn = KNeighborsClassifier(n_neighbors=<span class="number">20</span>)</span><br><span class="line">knn.fit(X, y)</span><br><span class="line">knn.predict(newloan)</span><br></pre></td></tr></table></figure><p>图6-2给出了这个例子的可视化展示。 . 中间的<strong>十字</strong>是待预测的新贷款。<strong>方块</strong>（已还清）和<strong>圆圈</strong>（违约）是训练数据。<strong>大黑圈</strong>显示了最近20个点的边界。在这个例子中，圆圈内有9笔违约贷款和11笔已还清贷款。因此，预测的贷款结果是<strong>已还清</strong>。注意，如果我们只考虑3个最近邻，预测结果将是贷款违约。</p><p><img src="/img3/面向数据科学家的实用统计学/F6.2.png" alt="F6.2" style="zoom:50%;" /></p><blockquote><p>通用注解：</p><p>KNN 分类的输出通常是一个二元决策，例如贷款数据中的<strong>违约</strong>或<strong>已还清</strong>，但KNN程序通常也提供输出一个介于0到1之间的<strong>概率（倾向性）</strong>。这个概率是基于K个最近邻居中某一类别所占的比例。在前面的例子中，违约的概率估计为<span class="math inline">\({9}/{20}\)</span>，即0.45。</p><p>使用概率得分可以让您使用<strong>不同于简单多数投票（概率0.5）的分类规则</strong>。这在处理<strong>不平衡类别</strong>问题时尤为重要；参见第230页的“不平衡数据的策略”。例如，如果目标是识别一个<strong>罕见类别</strong>的成员，截止点通常会设置在50%以下。一种常见的方法是将截止点设置为罕见事件的发生概率。</p></blockquote><h4 id="距离度量"><strong>距离度量</strong></h4><p>Distance Metrics</p><p><strong>相似性（接近度）</strong>是使用<strong>距离度量</strong>来确定的，距离度量是一个函数，它测量两条记录 <span class="math inline">\((x_1, x_2, \dots, x_p)\)</span> 和 <span class="math inline">\((u_1, u_2, \dots, u_p)\)</span> 之间的距离。</p><p>两个向量之间最流行的距离度量是<strong>欧几里得距离</strong>（Euclidean distance）。要测量两个向量之间的欧几里得距离，需要将一个向量的对应分量减去另一个向量的对应分量，将差值平方，然后求和，最后取平方根：</p><p><span class="math display">\[\sqrt{(x_1 - u_1)^2 + (x_2 - u_2)^2 + \cdots + (x_p - u_p)^2}\]</span> 另一种用于数值数据的常见距离度量是<strong>曼哈顿距离</strong>（Manhattan distance）：</p><p><span class="math display">\[|x_1 - u_1| + |x_2 - u_2| + \cdots + |x_p - u_p|\]</span> <strong>欧几里得距离</strong>对应于两点之间的直线距离（例如，像乌鸦飞行的距离）。<strong>曼哈顿距离</strong>是沿着单一方向一次移动一段距离来遍历两点之间的距离（例如，沿着矩形城市街区行进）。因此，如果相似性被定义为点到点的旅行时间，曼哈顿距离是一个有用的近似值。</p><p>在测量两个向量之间的距离时，<strong>测量尺度相对较大的变量（特征）将主导整个度量</strong>。例如，对于贷款数据，距离几乎完全取决于以成千上万计的<strong>收入</strong>和<strong>贷款金额</strong>变量。相比之下，比率变量的作用几乎可以忽略不计。我们通过<strong>标准化数据</strong>来解决这个问题；参见第243页的“标准化（归一化、z-分数）”。</p><blockquote><p>通用注解：</p><p><strong>其他距离度量</strong>（Other Distance Metrics）</p><p>有许多其他的度量方法可以用来测量向量之间的距离。<strong>对于数值数据，马氏距离（Mahalanobis distance）很有吸引力，因为它考虑了两个变量之间的相关性。这一点很有用，因为如果两个变量高度相关，马氏距离在计算时会本质上将它们视为一个单一变量。而欧几里得距离和曼哈顿距离不考虑相关性</strong>，实际上会更多地加权那些作为特征基础的属性。马氏距离是主成分（参见第284页的“主成分分析”）之间的欧几里得距离。使用马氏距离的缺点是<strong>增加了计算量和复杂性</strong>；它需要使用<strong>协方差矩阵</strong>进行计算（参见第202页的“协方差矩阵”）。</p></blockquote><h4 id="独热编码"><strong>独热编码</strong></h4><p>One Hot Encoder</p><p>表6-1中的贷款数据包含几个因子（字符串）变量。大多数统计和机器学习模型要求将这类变量转换为一系列传达相同信息的<strong>二元虚拟变量</strong>，如表6-2所示。</p><p><img src="/img3/面向数据科学家的实用统计学/T6.2.png" alt="T6.2" style="zoom:50%;" /></p><p>原本一个表示<strong>房产居住状态</strong>的单一变量，可以是“有抵押贷款自有房”、“无抵押贷款自有房”、“租房”或“其他”，现在我们得到了四个二元变量。第一个将是“有抵押贷款自有房 - 是/否”，第二个将是“无抵押贷款自有房 - 是/否”，以此类推。因此，这个名为“房产居住状态”的预测变量会产生一个包含一个1和三个0的向量，可用于统计和机器学习算法。独热编码（one hot encoding）这个短语源于数字电路术语，它描述的是一种电路设置，其中只有一个位被允许为正值（热）。</p><blockquote><p>通用注解：</p><p>在线性回归和逻辑回归中，独热编码会引起<strong>多重共线性</strong>问题；参见第172页的“多重共线性”。在这种情况下，一个虚拟变量会被省略（它的值可以从其他变量推断出来）。但在KNN和本书讨论的其他方法中，这不是一个问题。</p></blockquote><h4 id="标准化归一化z-分数"><strong>标准化（归一化，z-分数）</strong></h4><p>Standardization (Normalization, z-Scores)</p><p>在测量中，我们通常不那么关心“有多少”，而更关心“<strong>与平均值有多大差异</strong>”。<strong>标准化（Standardization）</strong>，也称为<strong>归一化（Normalization）</strong>，通过减去均值并除以标准差，将所有变量置于相似的尺度上；这样，我们确保一个变量不会仅仅因为其原始测量的尺度而过度影响模型：</p><p><span class="math display">\[z = \frac{x - \bar{x}}{s}\]</span> 这种转换的结果通常被称为 <strong>z-分数</strong>。测量值随后以“<strong>偏离均值的标准差</strong>”来表示。</p><blockquote><p>警告：</p><p>在这种统计背景下的“归一化”不应与数据库归一化相混淆，后者是移除冗余数据和验证数据依赖关系的过程。</p></blockquote><p>对于KNN和一些其他程序（例如主成分分析和聚类），在应用程序之前<strong>对数据进行标准化至关重要</strong>。为了说明这个想法，我们使用 <code>dti</code> 和 <code>payment_inc_ratio</code>（参见第239页的“一个小例子：预测贷款违约”）以及另外两个变量来对贷款数据应用KNN：<code>revol_bal</code>，申请人可用的总循环信贷余额（以美元计），以及 <code>revol_util</code>，已使用的信贷百分比。待预测的新记录如下所示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">newloan</span><br><span class="line">payment_inc_ratio dti revol_bal revol_util</span><br><span class="line">1 2.3932 1 1687 9.4</span><br></pre></td></tr></table></figure><p>在<code>revol_bal</code>这个以美元计量的变量上，其量级远大于其他变量。<strong><code>knn</code></strong>函数返回最近邻居的索引作为一个属性<strong><code>nn.index</code></strong>，我们可以用它来展示<code>loan_df</code>中五个最接近的行：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">loan_df <span class="operator">&lt;-</span> model.matrix<span class="punctuation">(</span><span class="operator">~</span> <span class="operator">-</span><span class="number">1</span> <span class="operator">+</span> payment_inc_ratio <span class="operator">+</span> dti <span class="operator">+</span> revol_bal <span class="operator">+</span></span><br><span class="line">revol_util<span class="punctuation">,</span> data<span class="operator">=</span>loan_data<span class="punctuation">)</span></span><br><span class="line">newloan <span class="operator">&lt;-</span> loan_df<span class="punctuation">[</span><span class="number">1</span><span class="punctuation">,</span> <span class="punctuation">,</span> drop<span class="operator">=</span><span class="literal">FALSE</span><span class="punctuation">]</span></span><br><span class="line">loan_df <span class="operator">&lt;-</span> loan_df<span class="punctuation">[</span><span class="operator">-</span><span class="number">1</span><span class="punctuation">,</span><span class="punctuation">]</span></span><br><span class="line">outcome <span class="operator">&lt;-</span> loan_data<span class="punctuation">[</span><span class="operator">-</span><span class="number">1</span><span class="punctuation">,</span> <span class="number">1</span><span class="punctuation">]</span></span><br><span class="line">knn_pred <span class="operator">&lt;-</span> knn<span class="punctuation">(</span>train<span class="operator">=</span>loan_df<span class="punctuation">,</span> test<span class="operator">=</span>newloan<span class="punctuation">,</span> cl<span class="operator">=</span>outcome<span class="punctuation">,</span> k<span class="operator">=</span><span class="number">5</span><span class="punctuation">)</span></span><br><span class="line">loan_df<span class="punctuation">[</span><span class="built_in">attr</span><span class="punctuation">(</span>knn_pred<span class="punctuation">,</span> <span class="string">&quot;nn.index&quot;</span><span class="punctuation">)</span><span class="punctuation">,</span><span class="punctuation">]</span></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">   payment_inc_ratio   dti   revol_bal   revol_util</span><br><span class="line">35537          1.47212  1.46        1686       10.0</span><br><span class="line">33652          3.38178  6.37        1688        8.4</span><br><span class="line">25864          2.36303  1.39        1691        3.5</span><br><span class="line">42954          1.28160  7.14        1684        3.9</span><br><span class="line">43600          4.12244  8.98        1684        7.2</span><br></pre></td></tr></table></figure><p>在模型拟合后，我们可以使用<strong><code>scikit-learn</code></strong>的<strong><code>kneighbors</code></strong>方法来识别训练集中五个最接近的行：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">predictors = [<span class="string">&#x27;payment_inc_ratio&#x27;</span>, <span class="string">&#x27;dti&#x27;</span>, <span class="string">&#x27;revol_bal&#x27;</span>, <span class="string">&#x27;revol_util&#x27;</span>]</span><br><span class="line">outcome = <span class="string">&#x27;outcome&#x27;</span></span><br><span class="line">newloan = loan_data.loc[<span class="number">0</span>:<span class="number">0</span>, predictors]</span><br><span class="line">X = loan_data.loc[<span class="number">1</span>:, predictors]</span><br><span class="line">y = loan_data.loc[<span class="number">1</span>:, outcome]</span><br><span class="line">knn = KNeighborsClassifier(n_neighbors=<span class="number">5</span>)</span><br><span class="line">knn.fit(X, y)</span><br><span class="line">nbrs = knn.kneighbors(newloan)</span><br><span class="line">X.iloc[nbrs[<span class="number">1</span>][<span class="number">0</span>], :]</span><br></pre></td></tr></table></figure><p>在这些邻居中，<code>revol_bal</code>的值与新记录中的值非常接近，但其他预测变量的值则完全不相干，基本上没有在确定邻居的过程中起到任何作用。</p><p>现在，我们比较一下将KNN应用于<strong>标准化后</strong>的数据，使用R的<strong><code>scale</code></strong>函数，该函数计算每个变量的z-分数：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">loan_df <span class="operator">&lt;-</span> model.matrix<span class="punctuation">(</span><span class="operator">~</span> <span class="operator">-</span><span class="number">1</span> <span class="operator">+</span> payment_inc_ratio <span class="operator">+</span> dti <span class="operator">+</span> revol_bal <span class="operator">+</span></span><br><span class="line">revol_util<span class="punctuation">,</span> data<span class="operator">=</span>loan_data<span class="punctuation">)</span></span><br><span class="line">loan_std <span class="operator">&lt;-</span> scale<span class="punctuation">(</span>loan_df<span class="punctuation">)</span></span><br><span class="line">newloan_std <span class="operator">&lt;-</span> loan_std<span class="punctuation">[</span><span class="number">1</span><span class="punctuation">,</span> <span class="punctuation">,</span> drop<span class="operator">=</span><span class="literal">FALSE</span><span class="punctuation">]</span></span><br><span class="line">loan_std <span class="operator">&lt;-</span> loan_std<span class="punctuation">[</span><span class="operator">-</span><span class="number">1</span><span class="punctuation">,</span><span class="punctuation">]</span></span><br><span class="line">loan_df <span class="operator">&lt;-</span> loan_df<span class="punctuation">[</span><span class="operator">-</span><span class="number">1</span><span class="punctuation">,</span><span class="punctuation">]</span></span><br><span class="line">outcome <span class="operator">&lt;-</span> loan_data<span class="punctuation">[</span><span class="operator">-</span><span class="number">1</span><span class="punctuation">,</span> <span class="number">1</span><span class="punctuation">]</span></span><br><span class="line">knn_pred <span class="operator">&lt;-</span> knn<span class="punctuation">(</span>train<span class="operator">=</span>loan_std<span class="punctuation">,</span> test<span class="operator">=</span>newloan_std<span class="punctuation">,</span> cl<span class="operator">=</span>outcome<span class="punctuation">,</span> k<span class="operator">=</span><span class="number">5</span><span class="punctuation">)</span></span><br><span class="line">loan_df<span class="punctuation">[</span><span class="built_in">attr</span><span class="punctuation">(</span>knn_pred<span class="punctuation">,</span> <span class="string">&quot;nn.index&quot;</span><span class="punctuation">)</span><span class="punctuation">,</span><span class="punctuation">]</span></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">   payment_inc_ratio    dti    revol_bal   revol_util</span><br><span class="line">2081          2.61091   1.03         1218        9.7</span><br><span class="line">1439          2.34343   0.51          278        9.9</span><br><span class="line">30216         2.71200   1.34         1075        8.5</span><br><span class="line">28543         2.39760   0.74         2917        7.4</span><br><span class="line">44738         2.34309   1.37          488        7.2</span><br></pre></td></tr></table></figure><p>我们还需要移除<strong><code>loan_df</code></strong>的第一行，以使行号相互对应。</p><p><strong><code>sklearn.preprocessing.StandardScaler</code></strong>方法首先使用预测变量进行训练，然后用于在训练KNN模型之前转换数据集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">newloan = loan_data.loc[<span class="number">0</span>:<span class="number">0</span>, predictors]</span><br><span class="line">X = loan_data.loc[<span class="number">1</span>:, predictors]</span><br><span class="line">y = loan_data.loc[<span class="number">1</span>:, outcome]</span><br><span class="line">scaler = preprocessing.StandardScaler()</span><br><span class="line">scaler.fit(X * <span class="number">1.0</span>)</span><br><span class="line">X_std = scaler.transform(X * <span class="number">1.0</span>)</span><br><span class="line">newloan_std = scaler.transform(newloan * <span class="number">1.0</span>)</span><br><span class="line">knn = KNeighborsClassifier(n_neighbors=<span class="number">5</span>)</span><br><span class="line">knn.fit(X_std, y)</span><br><span class="line">nbrs = knn.kneighbors(newloan_std)</span><br><span class="line">X.iloc[nbrs[<span class="number">1</span>][<span class="number">0</span>], :]</span><br></pre></td></tr></table></figure><p>五个最近的邻居在所有变量上都更加相似，这提供了一个更合理的结果。请注意，结果是<strong>以原始尺度显示</strong>的，但KNN是<strong>应用于缩放后的数据</strong>和待预测的新贷款的。</p><blockquote><p>知识点：</p><p>使用z-分数只是重新缩放变量的一种方式。除了均值，我们还可以使用更稳健的<strong>位置估计量</strong>，例如<strong>中位数</strong>。同样，可以使用<strong>四分位距</strong>等不同的<strong>尺度估计量</strong>来代替标准差。有时，变量会被“压扁”到0-1的范围内。同样重要的是要意识到，将每个变量缩放到具有单位方差在某种程度上是<strong>武断</strong>的。这暗示着每个变量在预测能力上被认为具有相同的<strong>重要性</strong>。如果你主观上知道某些变量比其他变量更重要，那么可以对这些变量进行放大。例如，对于贷款数据，可以合理地预期<strong>还款与收入的比率</strong>非常重要。</p></blockquote><blockquote><p>通用注解：</p><p><strong>标准化（归一化）并不会改变数据的分布形状</strong>；如果数据本身不呈正态分布，标准化也不会使其变为正态分布（参见第69页的“正态分布”）。</p></blockquote><h4 id="k的选择"><strong>K的选择</strong></h4><p>Choosing K</p><p><strong>K的选择对于KNN的性能非常重要</strong>。最简单的选择是设置<strong>K = 1</strong>，这被称为<strong>1-最近邻分类器</strong>。它的预测很直观：基于在训练集中找到与待预测新记录最相似的数据记录。但是，将K设置为1很少是最佳选择；通常情况下，使用K&gt;1的最近邻居会获得更好的性能。</p><p>一般来说，如果<strong>K过低</strong>，我们可能会<strong>过拟合</strong>：包含了数据中的噪声。<strong>较高的K值</strong>提供了平滑效果，可以降低训练数据过拟合的风险。另一方面，如果<strong>K过高</strong>，我们可能会<strong>过度平滑</strong>数据，并错过KNN捕捉数据<strong>局部结构</strong>的能力，而这正是其主要优势之一。</p><p>在<strong>过拟合和过度平滑之间取得最佳平衡的K值</strong>通常通过<strong>准确率指标</strong>来确定，特别是使用<strong>保留（holdout）或验证数据</strong>时的准确率。关于最佳K值没有普适的规则——它很大程度上取决于数据的性质。对于<strong>结构高度清晰且噪声较小</strong>的数据，较小的K值效果最好。借用信号处理领域的一个术语，这类数据有时被称为具有<strong>高信噪比（SNR）</strong>。手写和语音识别数据集就是通常具有高信噪比的例子。对于<strong>噪声大且结构较少</strong>的数据（低信噪比数据），例如贷款数据，较大的K值更合适。通常，<strong>K值在1到20之间</strong>。为了避免平局，通常会选择一个奇数。</p><blockquote><p>通用注解：</p><p><strong>偏差-方差权衡</strong>（Bias-Variance Trade-off）</p><p><strong>过度平滑与过拟合之间的矛盾</strong>是<strong>偏差-方差权衡</strong>的一个例子，这是统计模型拟合中一个普遍存在的问题。</p><ul><li><strong>方差</strong>指的是由于选择<strong>训练数据</strong>而产生的建模误差；也就是说，如果你选择一组不同的训练数据，得到的模型也会不同。</li><li><strong>偏差</strong>指的是由于你没有正确识别底层的<strong>真实世界情景</strong>而产生的建模误差；即使你简单地增加更多的训练数据，这种误差也不会消失。</li></ul><p>当一个灵活的模型<strong>过拟合</strong>时，<strong>方差会增加</strong>。你可以通过使用一个更简单的模型来减少方差，但由于模型在模拟真实底层情况时失去了灵活性，<strong>偏差可能会增加</strong>。处理这种权衡的一种通用方法是<strong>交叉验证</strong>。有关更多细节，请参见第155页的“交叉验证”。</p></blockquote><h4 id="knn作为特征引擎"><strong>KNN作为特征引擎</strong></h4><p>KNN as a Feature Engine</p><p>KNN因其简单和直观的特性而受到欢迎。但在性能方面，<strong>KNN本身通常无法与更复杂的分类技术竞争</strong>。然而，在实际模型拟合中，可以通过<strong>分阶段的过程</strong>将KNN与其他分类技术结合，以添加<strong>“局部知识”</strong>：</p><ol type="1"><li>对数据运行KNN，为每条记录得出一个分类（或一个类别的准概率）。</li><li>将这个结果作为一个<strong>新特征</strong>添加到记录中，然后用另一种分类方法对数据进行处理。这样，原始的预测变量就被使用了两次。</li></ol><p>你可能首先会想，这个过程是否会因为某些预测变量被使用两次而导致<strong>多重共线性</strong>问题（参见第172页的“多重共线性”）。这不是一个问题，因为被纳入第二阶段模型的信息是<strong>高度局部的</strong>，它只来自少数几条附近的记录，因此是<strong>附加信息而非冗余信息</strong>。</p><blockquote><p>通用注解：</p><p>你可以将这种分阶段使用KNN的方式看作是<strong>集成学习</strong>的一种形式，其中多个预测建模方法被结合在一起使用。它也可以被认为是<strong>特征工程</strong>的一种形式，其目的是推导出具有预测能力的特征（预测变量）。这通常需要手动审查数据；而KNN提供了一种<strong>相当自动化的方法</strong>来完成此任务。</p></blockquote><p>例如，考虑金县（King County）的住房数据。在为待售房屋定价时，房地产经纪人会根据最近售出的类似房屋的价格来定价，这被称为“<strong>comps</strong>”（可比房屋）。从本质上讲，房地产经纪人正在进行<strong>手动版本的KNN</strong>：通过查看类似房屋的销售价格，他们可以估算出某一套房屋的售价。我们可以通过对最近的销售数据应用KNN，来为统计模型创建一个新特征，以模仿房地产专业人士的做法。预测值是销售价格，现有的预测变量可以包括位置、总平方英尺、结构类型、地块大小以及卧室和浴室的数量。我们通过KNN添加的新预测变量（特征）是每条记录的KNN预测值（类似于房地产经纪人的“comps”）。由于我们正在预测一个数值，这里使用的是K-最近邻的<strong>平均值</strong>而不是多数投票（这被称为<strong>KNN回归</strong>）。</p><p>类似地，对于贷款数据，我们可以创建代表贷款流程不同方面的新特征。例如，以下R代码将构建一个代表借款人<strong>信用度</strong>的特征：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">borrow_df <span class="operator">&lt;-</span> model.matrix<span class="punctuation">(</span><span class="operator">~</span> <span class="operator">-</span><span class="number">1</span> <span class="operator">+</span> dti <span class="operator">+</span> revol_bal <span class="operator">+</span> revol_util <span class="operator">+</span> open_acc <span class="operator">+</span></span><br><span class="line">delinq_2yrs_zero <span class="operator">+</span> pub_rec_zero<span class="punctuation">,</span> data<span class="operator">=</span>loan_data<span class="punctuation">)</span></span><br><span class="line">borrow_knn <span class="operator">&lt;-</span> knn<span class="punctuation">(</span>borrow_df<span class="punctuation">,</span> test<span class="operator">=</span>borrow_df<span class="punctuation">,</span> cl<span class="operator">=</span>loan_data<span class="punctuation">[</span><span class="punctuation">,</span> <span class="string">&#x27;outcome&#x27;</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">prob<span class="operator">=</span><span class="literal">TRUE</span><span class="punctuation">,</span> k<span class="operator">=</span><span class="number">20</span><span class="punctuation">)</span></span><br><span class="line">prob <span class="operator">&lt;-</span> <span class="built_in">attr</span><span class="punctuation">(</span>borrow_knn<span class="punctuation">,</span> <span class="string">&quot;prob&quot;</span><span class="punctuation">)</span></span><br><span class="line">borrow_feature <span class="operator">&lt;-</span> ifelse<span class="punctuation">(</span>borrow_knn <span class="operator">==</span> <span class="string">&#x27;default&#x27;</span><span class="punctuation">,</span> prob<span class="punctuation">,</span> <span class="number">1</span> <span class="operator">-</span> prob<span class="punctuation">)</span></span><br><span class="line">summary<span class="punctuation">(</span>borrow_feature<span class="punctuation">)</span></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">   Min. 1st Qu. Median Mean 3rd Qu. Max.</span><br><span class="line">0.000  0.400  0.500 0.501  0.600  0.950</span><br></pre></td></tr></table></figure><p>使用 <strong><code>scikit-learn</code></strong>，我们使用训练模型的<strong><code>predict_proba</code></strong>方法来获取概率：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">predictors = [<span class="string">&#x27;dti&#x27;</span>, <span class="string">&#x27;revol_bal&#x27;</span>, <span class="string">&#x27;revol_util&#x27;</span>, <span class="string">&#x27;open_acc&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;delinq_2yrs_zero&#x27;</span>, <span class="string">&#x27;pub_rec_zero&#x27;</span>]</span><br><span class="line">outcome = <span class="string">&#x27;outcome&#x27;</span></span><br><span class="line">X = loan_data[predictors]</span><br><span class="line">y = loan_data[outcome]</span><br><span class="line">knn = KNeighborsClassifier(n_neighbors=<span class="number">20</span>)</span><br><span class="line">knn.fit(X, y)</span><br><span class="line">loan_data[<span class="string">&#x27;borrower_score&#x27;</span>] = knn.predict_proba(X)[:, <span class="number">1</span>]</span><br><span class="line">loan_data[<span class="string">&#x27;borrower_score&#x27;</span>].describe()</span><br></pre></td></tr></table></figure><p>其结果是一个新特征，它根据借款人的信用历史来预测其违约的可能性。</p><p><strong>关键思想</strong></p><ul><li><strong>K-最近邻（KNN）</strong>通过将一条记录分配给<strong>与其相似的记录所属的类别</strong>来进行分类。</li><li>相似性（距离）是通过<strong>欧几里得距离</strong>或其他相关度量来确定的。</li><li>用于比较记录的最近邻居数量 <strong>K</strong>，是通过使用不同的K值来衡量算法在训练数据上的表现来确定的。</li><li>通常，预测变量会进行<strong>标准化</strong>，以确保尺度较大的变量不会主导距离度量。</li><li>KNN常被用作<strong>预测建模的第一阶段</strong>，其预测值作为<strong>第二阶段（非KNN）建模的预测变量</strong>重新添加到数据中。</li></ul><h3 id="树模型"><strong>树模型</strong></h3><p>Tree Models</p><p><strong>树模型</strong>，也称为<strong>分类与回归树（CART）</strong>、<strong>决策树</strong>，或简称<strong>树</strong>，是由 Leo Breiman 等人在1984年首次开发的一种有效且流行的分类（和回归）方法。树模型及其更强大的后代——<strong>随机森林</strong>和<strong>提升树</strong>（参见第259页的“装袋法与随机森林”和第270页的“提升法”）——构成了数据科学中最广泛使用和最强大的回归与分类预测建模工具的基础。</p><p><strong>树的关键术语</strong></p><ul><li><p><strong>递归划分（Recursive partitioning）</strong> 反复地将数据进行划分和再划分，目的是使每个最终子划分中的结果尽可能<strong>同质（homogeneous）</strong>。</p></li><li><p><strong>分割值（Split value）</strong> 一个预测变量的值，它将记录分为两组：一组是该预测变量的值<strong>小于</strong>分割值的记录，另一组是<strong>大于</strong>分割值的记录。</p></li><li><p><strong>节点（Node）</strong> 在决策树或相应的分支规则集中，节点是分割值的图形或规则表示。</p></li><li><p><strong>叶子（Leaf）</strong> 一组if-then规则或树的分支的末端——将你带到该叶子的规则为树中的任何记录提供了一条分类规则。</p></li><li><p><strong>损失（Loss）</strong> 在分割过程的某个阶段中，错误分类的数量；损失越多，<strong>不纯度（impurity）</strong>越高。</p></li><li><p><strong>不纯度（Impurity）</strong> 数据子分区中各类别的混合程度（混合程度越高，不纯度越高）。 同义词：<strong>异质性（Heterogeneity）</strong> 反义词：<strong>同质性（Homogeneity）</strong>、<strong>纯度（purity）</strong></p></li><li><p><strong>剪枝（Pruning）</strong> 对一棵完全生长的树进行逐步修剪其分支的过程，以<strong>减少过拟合</strong>。</p></li></ul><p>树模型是一组“如果-那么-否则”的规则，易于理解和实现。与线性和逻辑回归相比，树模型能够<strong>发现数据中对应于复杂交互的隐藏模式</strong>。然而，与KNN或朴素贝叶斯不同的是，简单的树模型可以用易于解释的预测变量关系来表达。</p><blockquote><p>警告：</p><p><strong>运筹学中的决策树</strong>（Decision Trees in Operations Research）</p><p>在<strong>决策科学</strong>和<strong>运筹学</strong>中，“决策树”这个术语有一个不同（且更古老）的含义，它指的是一种<strong>人类决策分析过程</strong>。在这个含义下，决策点、可能的结果及其估计概率被呈现在一个分支图中，并选择具有<strong>最大预期价值</strong>的决策路径。</p></blockquote><h4 id="一个简单示例"><strong>一个简单示例</strong></h4><p>A Simple Example</p><p>在 R 中拟合树模型主要有两个包：<code>rpart</code> 和 <code>tree</code>。使用 <code>rpart</code> 包，我们可以对3000条贷款数据记录的样本进行模型拟合，使用 <code>payment_inc_ratio</code> 和 <code>borrower_score</code> 变量（数据描述参见第238页的“K-最近邻”）：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">library<span class="punctuation">(</span>rpart<span class="punctuation">)</span></span><br><span class="line">loan_tree <span class="operator">&lt;-</span> rpart<span class="punctuation">(</span>outcome <span class="operator">~</span> borrower_score <span class="operator">+</span> payment_inc_ratio<span class="punctuation">,</span></span><br><span class="line">data<span class="operator">=</span>loan3000<span class="punctuation">,</span> control<span class="operator">=</span>rpart.control<span class="punctuation">(</span>cp<span class="operator">=</span><span class="number">0.005</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">plot<span class="punctuation">(</span>loan_tree<span class="punctuation">,</span> uniform<span class="operator">=</span><span class="literal">TRUE</span><span class="punctuation">,</span> margin<span class="operator">=</span><span class="number">0.05</span><span class="punctuation">)</span></span><br><span class="line">text<span class="punctuation">(</span>loan_tree<span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p><code>sklearn.tree.DecisionTreeClassifier</code> 提供了决策树的实现。<code>dmba</code> 包提供了一个方便的函数，用于在Jupyter Notebook中创建可视化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">predictors = [<span class="string">&#x27;borrower_score&#x27;</span>, <span class="string">&#x27;payment_inc_ratio&#x27;</span>]</span><br><span class="line">outcome = <span class="string">&#x27;outcome&#x27;</span></span><br><span class="line">X = loan3000[predictors]</span><br><span class="line">y = loan3000[outcome]</span><br><span class="line">loan_tree = DecisionTreeClassifier(random_state=<span class="number">1</span>, criterion=<span class="string">&#x27;entropy&#x27;</span>,</span><br><span class="line">min_impurity_decrease=<span class="number">0.003</span>)</span><br><span class="line">loan_tree.fit(X, y)</span><br><span class="line">plotDecisionTree(loan_tree, feature_names=predictors,</span><br><span class="line">class_names=loan_tree.classes_)</span><br></pre></td></tr></table></figure><p>生成的树如图6-3所示。由于实现方式不同，你会发现 R 和 Python 的结果不完全相同；这是正常的。 . 这些分类规则是通过遍历一棵<strong>分层树</strong>来确定的，从<strong>根节点</strong>开始，如果节点条件为真则向左移动，否则向右移动，直到到达<strong>叶子节点</strong>。通常，树是倒置绘制的，根节点在顶部，叶子节点在底部。例如，如果一笔贷款的 <code>borrower_score</code> 为0.6，<code>payment_inc_ratio</code> 为8.0，我们最终会到达最左边的叶子节点，并预测该贷款将<strong>已还清</strong>。</p><p><img src="/img3/面向数据科学家的实用统计学/F6.3.png" alt="F6.3" style="zoom:50%;" /></p><p>在 R 中也可以轻松生成一个格式优美的树文本版本：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loan_tree</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">n= 3000</span><br><span class="line"></span><br><span class="line">node), split, n, loss, yval, (yprob)</span><br><span class="line">      * denotes terminal node</span><br><span class="line">1) root 3000 1445 paid off (0.5183333 0.4816667)</span><br><span class="line">2) borrower_score&gt;=0.575 878 261 paid off (0.7027335 0.2972665) *</span><br><span class="line">3) borrower_score&lt; 0.575 2122 938 default (0.4420358 0.5579642)</span><br><span class="line">6) borrower_score&gt;=0.375 1639 802 default (0.4893228 0.5106772)</span><br><span class="line">12) payment_inc_ratio&lt; 10.42265 1157 547 paid off (0.5272256 0.4727744)</span><br><span class="line">24) payment_inc_ratio&lt; 4.42601 334 139 paid off (0.5838323 0.4161677) *</span><br><span class="line">25) payment_inc_ratio&gt;=4.42601 823 408 paid off (0.5042527 0.4957473)</span><br><span class="line">50) borrower_score&gt;=0.475 418 190 paid off (0.5454545 0.4545455) *</span><br><span class="line">51) borrower_score&lt; 0.475 405 187 default (0.4617284 0.5382716) *</span><br><span class="line">13) payment_inc_ratio&gt;=10.42265 482 192 default (0.3983402 0.6016598) *</span><br><span class="line">7) borrower_score&lt; 0.375 483 136 default (0.2815735 0.7184265) *</span><br></pre></td></tr></table></figure><p>树的深度由缩进表示。每个节点对应于由该分区中主要结果决定的<strong>临时分类</strong>。<strong>“损失”（loss）</strong>是在一个分区中，由该临时分类产生的错误分类数量。例如，在节点2中，总共878条记录中有261条是错误分类的。括号中的值分别对应于已还清贷款和违约贷款的比例。例如，在预测为违约的节点13中，超过60%的记录是违约贷款。</p><p><code>scikit-learn</code> 文档描述了如何创建决策树模型的文本表示。我们在 <code>dmba</code> 包中包含了一个方便的函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(textDecisionTree(loan_tree))</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">--</span><br><span class="line">node=0 test node: go to node 1 if 0 &lt;= 0.5750000178813934 else to node 6</span><br><span class="line">node=1 test node: go to node 2 if 0 &lt;= 0.32500000298023224 else to node 3</span><br><span class="line">node=2 leaf node: [[0.785, 0.215]]</span><br><span class="line">node=3 test node: go to node 4 if 1 &lt;= 10.42264986038208 else to node 5</span><br><span class="line">node=4 leaf node: [[0.488, 0.512]]</span><br><span class="line">node=5 leaf node: [[0.613, 0.387]]</span><br><span class="line">node=6 test node: go to node 7 if 1 &lt;= 9.19082498550415 else to node 10</span><br><span class="line">node=7 test node: go to node 8 if 0 &lt;= 0.7249999940395355 else to node 9</span><br><span class="line">node=8 leaf node: [[0.247, 0.753]]</span><br><span class="line">node=9 leaf node: [[0.073, 0.927]]</span><br><span class="line">node=10 leaf node: [[0.457, 0.543]]</span><br></pre></td></tr></table></figure><h4 id="递归划分算法"><strong>递归划分算法</strong></h4><p>The Recursive Partitioning Algorithm</p><p>用于构建决策树的算法，称为<strong>递归划分</strong>，简单直观。该算法反复使用<strong>最能将数据划分为相对同质分区的预测变量值</strong>来对数据进行划分。图6-4展示了图6-3中树创建的分区。第一个规则（用规则1表示）是 <code>borrower_score &gt;= 0.575</code>，它将图的右侧部分分割出来。第二个规则是 <code>borrower_score &lt; 0.375</code>，它将左侧部分分割出来。</p><p><img src="/img3/面向数据科学家的实用统计学/F6.4.png" alt="F6.4" style="zoom:50%;" /></p><p>假设我们有一个响应变量 <span class="math inline">\(Y\)</span> 和一组 <span class="math inline">\(P\)</span> 个预测变量 <span class="math inline">\(X_j\)</span> (其中 <span class="math inline">\(j=1, \dots, P\)</span>)。对于一个记录分区 <span class="math inline">\(A\)</span>，递归划分会找到最佳方式将其划分为两个子分区：</p><ol type="1"><li>对于每个预测变量 <span class="math inline">\(X_j\)</span>：<ol type="a"><li>对于 <span class="math inline">\(X_j\)</span> 的每个值 <span class="math inline">\(s_j\)</span>：<ol type="i"><li>将分区 <span class="math inline">\(A\)</span> 中 <span class="math inline">\(X_j\)</span> 值小于 <span class="math inline">\(s_j\)</span> 的记录作为一个分区，其余 <span class="math inline">\(X_j\)</span> 值大于或等于 <span class="math inline">\(s_j\)</span> 的记录作为另一个分区。</li><li>测量 <span class="math inline">\(A\)</span> 的每个子分区内类别的<strong>同质性</strong>。</li></ol></li><li>选择能产生<strong>最大分区内类别同质性</strong>的 <span class="math inline">\(s_j\)</span> 值。</li></ol></li><li>选择能产生<strong>最大分区内类别同质性</strong>的变量 <span class="math inline">\(X_j\)</span> 和分割值 <span class="math inline">\(s_j\)</span>。</li></ol><p>现在是<strong>递归</strong>部分： 1. 用整个数据集初始化 <span class="math inline">\(A\)</span>。 2. 应用划分算法将 <span class="math inline">\(A\)</span> 划分为两个子分区 <span class="math inline">\(A_1\)</span> 和 <span class="math inline">\(A_2\)</span>。 3. 对子分区 <span class="math inline">\(A_1\)</span> 和 <span class="math inline">\(A_2\)</span> 重复步骤2。 4. 当无法再进行能够充分改善分区同质性的划分时，算法终止。</p><p>最终结果是对数据的划分，如在<span class="math inline">\(P\)</span>维空间中的图6-4所示，每个分区根据该分区中响应变量的<strong>多数投票</strong>来预测结果为0或1。</p><blockquote><p>通用注解：</p><p>除了二元0/1预测，树模型还可以根据分区中0和1的数量来产生<strong>概率估计</strong>。该估计值就是分区中1的数量除以该分区中的观察记录总数： <span class="math display">\[P(Y=1) = \frac{分区中1的数量}{分区的大小}\]</span> 然后，估计的 <span class="math inline">\(P(Y=1)\)</span> 可以转换为二元决策；例如，如果 <span class="math inline">\(P(Y=1) &gt; 0.5\)</span>，则将估计值设为1。</p></blockquote><h4 id="测量同质性或不纯度"><strong>测量同质性或不纯度</strong></h4><p>Measuring Homogeneity or Impurity</p><p>树模型会递归地创建分区（记录集）<span class="math inline">\(A\)</span>，并预测结果 <span class="math inline">\(Y=0\)</span> 或 <span class="math inline">\(Y=1\)</span>。从前面的算法中可以看出，我们需要一种方法来测量分区内的<strong>同质性</strong>，也称为<strong>类别纯度</strong>。或者等价地，我们需要测量分区中的<strong>不纯度</strong>。预测的准确率是该分区内被错误分类的记录比例 <span class="math inline">\(p\)</span>，其范围从0（完美）到0.5（纯随机猜测）。</p><p>事实证明，<strong>准确率不是一个好的不纯度度量</strong>。相反，两个常见的不纯度度量是 <strong>Gini不纯度（Gini impurity）</strong>和<strong>信息熵（entropy of information）</strong>。虽然这些（以及其他）不纯度度量也适用于具有两个以上类别的分类问题，但我们这里重点关注<strong>二元情况</strong>。</p><p>一个记录集 <span class="math inline">\(A\)</span> 的 Gini不纯度是：</p><p><span class="math display">\[I_G(A) = p(1 - p)\]</span></p><p>熵度量由下式给出：</p><p><span class="math display">\[I_E(A) = -p \log_2 p - (1-p) \log_2 (1-p)\]</span></p><p>图6-5显示，<strong>基尼不纯度</strong>（重新缩放后）和<strong>熵</strong>度量是相似的，但<strong>熵</strong>对于中等到高准确率会给出<strong>更高</strong>的不纯度分数。</p><p><img src="/img3/面向数据科学家的实用统计学/F6.5.png" alt="F6.5" style="zoom:33%;" /></p><blockquote><p>警告：</p><p><strong>Gini系数</strong></p><p>Gini不纯度不应与<strong>Gini系数</strong>相混淆。它们代表相似的概念，但<strong>Gini系数</strong>仅限于二元分类问题，并且与<strong>AUC指标</strong>相关（参见第226页的“AUC”）。</p></blockquote><p>不纯度度量被用于前面描述的<strong>分割算法</strong>中。对于数据的每一个提议分区，都会测量由该分割所产生的每个分区的不纯度。然后计算一个<strong>加权平均值</strong>，并在每个阶段选择产生最低加权平均值的分区。</p><h4 id="阻止树继续生长"><strong>阻止树继续生长</strong></h4><p>Stopping the Tree from Growing</p><p>随着树变得越来越大，其分割规则也变得越来越详细，树会逐渐从识别数据中真实可靠的“大”规则，转变为反映噪声的“微小”规则。一棵<strong>完全生长的树</strong>会导致<strong>完全纯净的叶子</strong>，因此在分类其所训练的数据时会达到100%的准确率。当然，这种准确率是<strong>虚幻的</strong>——我们<strong>过拟合</strong>了数据（参见第247页的“偏差-方差权衡”），拟合的是训练数据中的噪声，而不是我们想要在新数据中识别的信号。</p><p>我们需要某种方法来决定<strong>何时停止树的生长</strong>，使其处于一个能够<strong>泛化到新数据</strong>的阶段。在R和Python中有多种方法可以阻止分割：</p><ul><li><strong>如果生成的子分区太小，或终端叶子太小，就避免进行分割。</strong> 在 <code>rpart</code> (R) 中，这些约束分别由参数 <code>minsplit</code> 和 <code>minbucket</code> 控制，默认值分别为20和7。在 Python 的 <code>DecisionTreeClassifier</code> 中，我们可以使用参数 <code>min_samples_split</code> (默认2) 和 <code>min_samples_leaf</code> (默认1) 来控制。</li><li><strong>如果新的分区不能“显著”减少不纯度，则不进行分割。</strong> 在 <code>rpart</code> 中，这由<strong>复杂度参数</strong> <code>cp</code> 控制，<code>cp</code> 是衡量树复杂度的指标——复杂度越高，<code>cp</code> 值越大。实际上，<code>cp</code> 被用来通过对树中额外的复杂度（分割）施加惩罚来限制树的生长。<code>DecisionTreeClassifier</code> (Python) 有一个参数 <code>min_impurity_decrease</code>，它根据加权不纯度减少值来限制分割。在这个参数中，较小的值将导致更复杂的树。</li></ul><p>这些方法涉及<strong>主观规则</strong>，可以用于探索性工作，但我们<strong>不容易确定其最佳值</strong>（即，能最大化在新数据上的预测准确率的值）。我们需要将<strong>交叉验证</strong>与<strong>系统性地改变模型参数</strong>或通过<strong>剪枝</strong>来修改树的方法相结合。</p><p><strong>在 R 中控制树的复杂度</strong></p><p>利用复杂度参数 <strong>cp</strong>，我们可以估计出哪种大小的树在新数据上表现最佳。如果 <strong>cp</strong> 太小，树就会<strong>过拟合</strong>数据，拟合了噪声而非信号。另一方面，如果 <strong>cp</strong> 太大，树就会太小，<strong>预测能力</strong>也会很弱。<code>rpart</code> 的默认值为0.01，不过对于大型数据集来说，这个值可能太大。在前面的例子中，我们将 <strong>cp</strong> 设置为0.005，因为默认值导致了只有一个分割的树。在探索性分析中，简单地尝试几个值就足够了。</p><p>确定最佳 <strong>cp</strong> 值是<strong>偏差-方差权衡</strong>的一个例子。估算一个好的 <strong>cp</strong> 值的最常用方法是通过<strong>交叉验证</strong>（参见第155页的“交叉验证”）： 1. 将数据划分为<strong>训练集</strong>和<strong>验证集</strong>（保留集）。 2. 用训练数据生长树。 3. 逐步修剪它，在每一步记录 <strong>cp</strong> 值（使用训练数据）。 4. 记录对应于验证数据上<strong>最小误差（损失）</strong>的 <strong>cp</strong> 值。 5. 重新将数据划分为训练集和验证集，并重复树的生长、修剪和 <strong>cp</strong> 值记录过程。 6. 反复执行此操作，并对每棵树反映最小误差的 <strong>cp</strong> 值取平均。 7. 回到原始数据或未来的数据上，生长一棵树，并在最佳 <strong>cp</strong> 值处停止。</p><p>在 <code>rpart</code> 中，您可以使用参数 <code>cptable</code> 生成一个包含 <strong>cp</strong> 值及其相关<strong>交叉验证误差</strong>（在 R 中为 <strong>xerror</strong>）的表格，您可以从中确定具有最低交叉验证误差的 <strong>cp</strong> 值。</p><p><strong>在 Python 中控制树的复杂度</strong></p><p>在 <code>scikit-learn</code> 的决策树实现中，<strong>既没有复杂度参数，也没有剪枝功能</strong>。解决方案是使用<strong>网格搜索</strong>来组合不同的参数值。例如，我们可以将 <code>max_depth</code> 的范围设为5到30，<code>min_samples_split</code> 在20到100之间变化。<code>scikit-learn</code> 中的 <strong><code>GridSearchCV</code></strong> 方法是一种方便的方式，可以将穷尽搜索所有组合与交叉验证结合起来。然后，根据交叉验证的模型性能选择<strong>最优参数集</strong>。</p><h4 id="预测连续值"><strong>预测连续值</strong></h4><p>Predicting a Continuous Value</p><p>用树模型预测连续值（也称为<strong>回归</strong>）遵循同样的逻辑和程序，只是<strong>不纯度</strong>的测量方式不同。在每个子分区中，不纯度是通过与<strong>均值的平方偏差</strong>（平方误差）来测量的，并且预测性能是根据每个分区中<strong>均方根误差（RMSE）</strong>的平方根来评估的（参见第153页的“评估模型”）。</p><p><code>scikit-learn</code> 提供了 <code>sklearn.tree.DecisionTreeRegressor</code> 方法来训练决策树回归模型。</p><h4 id="树模型的应用方式"><strong>树模型的应用方式</strong></h4><p>How Trees Are Used</p><p>组织中预测建模人员面临的一大障碍是，他们所使用的方法被认为是<strong>“黑箱”</strong>，这会引起组织内其他部门的反对。在这方面，树模型有两个吸引人的优点：</p><ul><li><strong>树模型提供了一个可视化工具</strong>来探索数据，从而了解哪些变量是重要的，以及它们之间是如何相互关联的。树可以捕捉预测变量之间的<strong>非线性关系</strong>。</li><li><strong>树模型提供了一套规则</strong>，可以有效地传达给非专业人士，以便进行实施或“推销”数据挖掘项目。</li></ul><p>然而，在预测方面，<strong>利用多个树的结果通常比只使用单个树更强大</strong>。特别是，<strong>随机森林</strong>和<strong>提升树</strong>算法几乎总是提供卓越的预测准确性和性能（参见第259页的“装袋法与随机森林”和第270页的“提升法”），但<strong>单个树的上述优点</strong>也随之<strong>丧失</strong>了。</p><p><strong>关键思想</strong></p><ul><li><strong>决策树</strong>生成一系列规则，用于分类或预测结果。</li><li>这些规则对应于对数据进行<strong>连续的分区</strong>。</li><li>每个分区或分割都引用一个特定的预测变量值，并将数据分为该预测变量值<strong>高于或低于</strong>该分割值的记录。</li><li>在每个阶段，树算法选择能够<strong>最小化每个子分区内结果不纯度</strong>的分割点。</li><li>当无法再进行分割时，树就<strong>完全生长</strong>了，每个<strong>终端节点</strong>或<strong>叶子节点</strong>都只包含单一类别的记录；遵循该规则（分割）路径的新案例将被分配到该类别。</li><li>一棵完全生长的树会<strong>过拟合</strong>数据，必须进行<strong>剪枝</strong>，以使其捕捉<strong>信号</strong>而非<strong>噪声</strong>。</li><li><strong>多树算法</strong>，如随机森林和提升树，能提供更好的预测性能，但它们<strong>失去了单棵树基于规则的可沟通能力</strong>。</li></ul><h3 id="装袋法与随机森林"><strong>装袋法与随机森林</strong></h3><p>Bagging and the Random Forest</p><p>1906年，统计学家弗朗西斯·高尔顿爵士参观英格兰的一个乡村集市，那里正在举行一场猜测展览公牛净重的比赛。共有800个猜测，尽管单个猜测差异很大，但其平均值和中位数与公牛的真实重量相差不到1%。詹姆斯·苏洛维基在他的著作《群体的智慧》（The Wisdom of Crowds, Doubleday, 2004）中探讨了这一现象。这一原理也适用于预测模型：<strong>对多个模型进行平均（或多数投票）</strong>——即<strong>模型集成</strong>——结果证明比仅选择一个模型<strong>更为准确</strong>。</p><p><strong>装袋法与随机森林的关键术语</strong></p><ul><li><p><strong>集成（Ensemble）</strong> 通过使用一组模型来形成预测。 同义词：<strong>模型平均（Model averaging）</strong></p></li><li><p><strong>装袋法（Bagging）</strong> 一种通过对数据进行自举来形成一组模型的通用技术。 同义词：<strong>自举聚合（Bootstrap aggregation）</strong></p></li><li><p><strong>随机森林（Random forest）</strong> 一种基于决策树模型的装袋估计方法。 同义词：<strong>装袋决策树（Bagged decision trees）</strong></p></li><li><p><strong>变量重要性（Variable importance）</strong> 衡量预测变量在模型性能中的重要性的指标。</p></li></ul><p>集成方法已被应用于许多不同的建模方法，最著名的例子是<strong>Netflix Prize</strong>，Netflix曾悬赏100万美元，奖励任何能够将预测顾客对电影评分的准确性提高10%的参赛者。集成的简单版本如下： 1. 开发一个预测模型，并记录给定数据集的预测结果。 2. 在相同数据上为多个模型重复此步骤。 3. 对于每条待预测的记录，对其预测结果取<strong>平均值</strong>（或加权平均值，或多数投票）。</p><p>集成方法最系统且最有效地应用于<strong>决策树</strong>。集成树模型非常强大，可以帮助我们以相对较少的努力构建出优秀的预测模型。</p><p>除了简单的集成算法，集成模型还有两个主要变体：<strong>装袋法</strong>和<strong>提升法</strong>。在集成树模型中，它们分别被称为<strong>随机森林模型</strong>和<strong>提升树模型</strong>。本节重点介绍<strong>装袋法</strong>；提升法将在第270页的“提升法”中进行讨论。</p><h4 id="bagging-方法"><strong>Bagging 方法</strong></h4><p><strong>Bagging</strong>，是“bootstrap aggregating”（自助聚合）的缩写，由 Leo Breiman 于1994年提出。假设我们有一个响应变量 <span class="math inline">\(Y\)</span> 和 <span class="math inline">\(P\)</span> 个预测变量 <span class="math inline">\(\mathbf{X} = (X_1, X_2, \dots, X_P)\)</span>，以及 <span class="math inline">\(N\)</span> 条记录。</p><p>Bagging 类似于集成学习的基本算法，不同之处在于，<strong>不是将不同的模型拟合到相同的数据上，而是将每个新模型拟合到一个自助（bootstrap）重采样的数据上</strong>。这里更正式地介绍该算法： 1. 初始化要拟合的模型数量 <span class="math inline">\(M\)</span> 和要选择的记录数量 <span class="math inline">\(n\)</span>（<span class="math inline">\(n &lt; N\)</span>）。设置迭代计数器 <span class="math inline">\(m=1\)</span>。 2. 从训练数据中<strong>有放回地</strong>抽取 <span class="math inline">\(n\)</span> 条记录，形成一个子样本 <span class="math inline">\(Y_m\)</span> 和 <span class="math inline">\(\mathbf{X}_m\)</span>（即“<strong>包</strong>”）。 3. 使用 <span class="math inline">\(Y_m\)</span> 和 <span class="math inline">\(\mathbf{X}_m\)</span> 训练一个模型，以创建一组决策规则 <span class="math inline">\(\hat f_m(\mathbf{X})\)</span>。 4. 增加模型计数器 <span class="math inline">\(m = m + 1\)</span>。如果 <span class="math inline">\(m \le M\)</span>，返回步骤2。</p><p>在 <span class="math inline">\(\hat f_m\)</span> 预测 <span class="math inline">\(Y=1\)</span> 的概率的情况下，bagging 估计量由下式给出：</p><p><span class="math display">\[\hat f(\mathbf{X}) = \frac{1}{M} (\hat f_1(\mathbf{X}) + \hat f_2(\mathbf{X}) + \dots + \hat f_M(\mathbf{X}))\]</span></p><h4 id="随机森林"><strong>随机森林</strong></h4><p>Random Forest</p><p>随机森林是在决策树上应用<strong>装袋法</strong>的一种重要扩展：除了对记录进行抽样，该算法<strong>也对变量进行抽样</strong>。在传统的决策树中，为了确定如何创建一个子分区 <span class="math inline">\(A\)</span>，算法会通过最小化<strong>Gini不纯度</strong>等标准来选择变量和分割点（参见第254页的“测量同质性或不纯度”）。而在随机森林中，在算法的每个阶段，变量的选择<strong>被限制在一个随机的变量子集中</strong>。</p><p>与基本的树算法（参见第252页的“递归划分算法”）相比，随机森林算法增加了两个步骤：前面讨论过的装袋法（参见第259页的“装袋法与随机森林”），以及在每次分割时对变量进行的<strong>自举抽样</strong>：</p><ol type="1"><li>从记录中进行<strong>自举（有放回）抽样</strong>。</li><li>对于第一次分割，<strong>无放回地随机抽取 <span class="math inline">\(p &lt; P\)</span> 个变量</strong>。</li><li>对于每个抽样的变量 <span class="math inline">\(X_{j_1}, X_{j_2}, \dots, X_{j_p}\)</span>，应用分割算法：<ol type="a"><li>对于每个 <span class="math inline">\(X_{j_k}\)</span> 的值 <span class="math inline">\(s_{j_k}\)</span>：</li><li>将分区 <span class="math inline">\(A\)</span> 中 <span class="math inline">\(X_{j_k}\)</span> 值小于 <span class="math inline">\(s_{j_k}\)</span> 的记录划分为一个分区，其余 <span class="math inline">\(X_{j_k}\)</span> 值大于或等于 <span class="math inline">\(s_{j_k}\)</span> 的记录作为另一个分区。</li></ol><ol start="2" type="i"><li>测量 <span class="math inline">\(A\)</span> 的每个子分区内的类别<strong>同质性</strong>。</li></ol><ol start="2" type="a"><li>选择能产生<strong>最大分区内类别同质性</strong>的 <span class="math inline">\(s_{j_k}\)</span> 值。</li></ol></li><li>选择能产生<strong>最大分区内类别同质性</strong>的变量 <span class="math inline">\(X_{j_k}\)</span> 和分割值 <span class="math inline">\(s_{j_k}\)</span>。</li><li>继续进行下一次分割，从步骤2开始重复之前的步骤。</li><li>继续进行额外的分割，遵循相同的程序，直到树完全生长。</li><li>回到步骤1，进行另一次自举子抽样，并重新开始整个过程。</li></ol><p>在每一步中要抽取多少变量？经验法则是选择 <span class="math inline">\(\sqrt{P}\)</span>，其中 <span class="math inline">\(P\)</span> 是预测变量的数量。<code>randomForest</code> 包在 R 中实现了随机森林。以下代码将此包应用于贷款数据（数据描述参见第238页的“K-最近邻”）：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rf <span class="operator">&lt;-</span> randomForest<span class="punctuation">(</span>outcome <span class="operator">~</span> borrower_score <span class="operator">+</span> payment_inc_ratio<span class="punctuation">,</span></span><br><span class="line">data<span class="operator">=</span>loan3000<span class="punctuation">)</span></span><br><span class="line">rf</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Call:</span><br><span class="line"> randomForest(formula = outcome ~ borrower_score + payment_inc_ratio,</span><br><span class="line"> data = loan3000)</span><br><span class="line">               Type of random forest: classification</span><br><span class="line">                     Number of trees: 500</span><br><span class="line">No. of variables tried at each split: 1</span><br><span class="line"></span><br><span class="line">        OOB estimate of  error rate: 39.17%</span><br><span class="line">Confusion matrix:</span><br><span class="line">         default paid off class.error</span><br><span class="line">default      873      572   0.39584775</span><br><span class="line">paid off     603      952   0.38778135</span><br></pre></td></tr></table></figure><p>在 Python 中，我们使用 <code>sklearn.ensemble.RandomForestClassifier</code> 方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">predictors = [<span class="string">&#x27;borrower_score&#x27;</span>, <span class="string">&#x27;payment_inc_ratio&#x27;</span>]</span><br><span class="line">outcome = <span class="string">&#x27;outcome&#x27;</span></span><br><span class="line">X = loan3000[predictors]</span><br><span class="line">y = loan3000[outcome]</span><br><span class="line">rf = RandomForestClassifier(n_estimators=<span class="number">500</span>, random_state=<span class="number">1</span>, oob_score=<span class="literal">True</span>)</span><br><span class="line">rf.fit(X, y)</span><br></pre></td></tr></table></figure><p>默认情况下，会训练500棵树。由于预测变量集中只有两个变量，算法在每个阶段随机选择一个变量进行分割（即，大小为1的自举子样本）。</p><p><strong>袋外误差（Out-of-bag, OOB）</strong> 是指训练好的模型应用于<strong>未被用于该树训练</strong>的数据时的<strong>误差率</strong>。利用模型的输出，可以在 R 中绘制<strong>OOB误差</strong>与随机森林中<strong>树的数量</strong>之间的关系图：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">error_df <span class="operator">=</span> data.frame<span class="punctuation">(</span>error_rate<span class="operator">=</span>rf<span class="operator">$</span>err.rate<span class="punctuation">[</span><span class="punctuation">,</span><span class="string">&#x27;OOB&#x27;</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">num_trees<span class="operator">=</span><span class="number">1</span><span class="operator">:</span>rf<span class="operator">$</span>ntree<span class="punctuation">)</span></span><br><span class="line">ggplot<span class="punctuation">(</span>error_df<span class="punctuation">,</span> aes<span class="punctuation">(</span>x<span class="operator">=</span>num_trees<span class="punctuation">,</span> y<span class="operator">=</span>error_rate<span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">geom_line<span class="punctuation">(</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p><code>RandomForestClassifier</code> 的实现没有简单的方法来获取作为随机森林中树数量函数的袋外估计。我们可以训练一系列树数量递增的分类器，并跟踪 <code>oob_score_</code> 值。然而，这种方法效率不高：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">n_estimator = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">20</span>, <span class="number">510</span>, <span class="number">5</span>))</span><br><span class="line">oobScores = []</span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> n_estimator:</span><br><span class="line">rf = RandomForestClassifier(n_estimators=n, criterion=<span class="string">&#x27;entropy&#x27;</span>,</span><br><span class="line">max_depth=<span class="number">5</span>, random_state=<span class="number">1</span>, oob_score=<span class="literal">True</span>)</span><br><span class="line">rf.fit(X, y)</span><br><span class="line">oobScores.append(rf.oob_score_)</span><br><span class="line">df = pd.DataFrame(&#123; <span class="string">&#x27;n&#x27;</span>: n_estimator, <span class="string">&#x27;oobScore&#x27;</span>: oobScores &#125;)</span><br><span class="line">df.plot(x=<span class="string">&#x27;n&#x27;</span>, y=<span class="string">&#x27;oobScore&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="/img3/面向数据科学家的实用统计学/F6.6.png" alt="F6.6" style="zoom:33%;" /></p><p>结果如图6-6所示。 误差率从超过0.44迅速下降，随后稳定在0.385左右。预测值可以通过 <code>predict</code> 函数获得，并在 R 中绘制如下：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">pred <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>rf<span class="punctuation">,</span> prob<span class="operator">=</span><span class="literal">TRUE</span><span class="punctuation">)</span></span><br><span class="line">rf_df <span class="operator">&lt;-</span> cbind<span class="punctuation">(</span>loan3000<span class="punctuation">,</span> pred <span class="operator">=</span> pred<span class="punctuation">)</span></span><br><span class="line">ggplot<span class="punctuation">(</span>data<span class="operator">=</span>rf_df<span class="punctuation">,</span> aes<span class="punctuation">(</span>x<span class="operator">=</span>borrower_score<span class="punctuation">,</span> y<span class="operator">=</span>payment_inc_ratio<span class="punctuation">,</span></span><br><span class="line">shape<span class="operator">=</span>pred<span class="punctuation">,</span> color<span class="operator">=</span>pred<span class="punctuation">,</span> size<span class="operator">=</span>pred<span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">geom_point<span class="punctuation">(</span>alpha<span class="operator">=</span><span class="number">.8</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">scale_color_manual<span class="punctuation">(</span>values <span class="operator">=</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;paid off&#x27;</span><span class="operator">=</span><span class="string">&#x27;#b8e186&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;default&#x27;</span><span class="operator">=</span><span class="string">&#x27;#d95f02&#x27;</span><span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">scale_shape_manual<span class="punctuation">(</span>values <span class="operator">=</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;paid off&#x27;</span><span class="operator">=</span><span class="number">0</span><span class="punctuation">,</span> <span class="string">&#x27;default&#x27;</span><span class="operator">=</span><span class="number">1</span><span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">scale_size_manual<span class="punctuation">(</span>values <span class="operator">=</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;paid off&#x27;</span><span class="operator">=</span><span class="number">0.5</span><span class="punctuation">,</span> <span class="string">&#x27;default&#x27;</span><span class="operator">=</span><span class="number">2</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>在 Python 中，我们可以创建类似的图：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">predictions = X.copy()</span><br><span class="line">predictions[<span class="string">&#x27;prediction&#x27;</span>] = rf.predict(X)</span><br><span class="line">predictions.head()</span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">predictions.loc[predictions.prediction==<span class="string">&#x27;paid off&#x27;</span>].plot(</span><br><span class="line">x=<span class="string">&#x27;borrower_score&#x27;</span>, y=<span class="string">&#x27;payment_inc_ratio&#x27;</span>, style=<span class="string">&#x27;.&#x27;</span>,</span><br><span class="line">markerfacecolor=<span class="string">&#x27;none&#x27;</span>, markeredgecolor=<span class="string">&#x27;C1&#x27;</span>, ax=ax)</span><br><span class="line">predictions.loc[predictions.prediction==<span class="string">&#x27;default&#x27;</span>].plot(</span><br><span class="line">x=<span class="string">&#x27;borrower_score&#x27;</span>, y=<span class="string">&#x27;payment_inc_ratio&#x27;</span>, style=<span class="string">&#x27;o&#x27;</span>,</span><br><span class="line">markerfacecolor=<span class="string">&#x27;none&#x27;</span>, markeredgecolor=<span class="string">&#x27;C0&#x27;</span>, ax=ax)</span><br><span class="line">ax.legend([<span class="string">&#x27;paid off&#x27;</span>, <span class="string">&#x27;default&#x27;</span>]);</span><br><span class="line">ax.set_xlim(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">ax.set_ylim(<span class="number">0</span>, <span class="number">25</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">&#x27;borrower_score&#x27;</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&#x27;payment_inc_ratio&#x27;</span>)</span><br></pre></td></tr></table></figure><p>该图（如图6-7所示）很好地揭示了随机森林的本质。</p><p><img src="/img3/面向数据科学家的实用统计学/F6.7.png" alt="F6.7" style="zoom:40%;" /></p><p>随机森林方法是一种“<strong>黑箱</strong>”方法。它比单个树产生更准确的预测，但<strong>单个树直观的决策规则却丢失了</strong>。随机森林的预测也有些<strong>噪声</strong>：请注意，一些<strong>借款人得分非常高</strong>（表明信用度高）的贷款，<strong>最终仍被预测为违约</strong>。这是数据中一些不寻常记录的结果，也展示了随机森林<strong>过拟合</strong>的危险（参见第247页的“偏差-方差权衡”）。</p><h4 id="变量重要性"><strong>变量重要性</strong></h4><p>Variable Importance</p><p>当您为具有许多特征和记录的数据构建预测模型时，<strong>随机森林算法的强大之处</strong>就显现出来了。它能够自动确定哪些预测变量是重要的，并发现与<strong>交互项</strong>相对应的预测变量之间的复杂关系（参见第174页的“交互项与主效应”）。例如，使用<strong>所有列</strong>对贷款违约数据进行模型拟合。以下代码在 R 中展示了这一点：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rf_all <span class="operator">&lt;-</span> randomForest<span class="punctuation">(</span>outcome <span class="operator">~</span></span><br><span class="line">.<span class="punctuation">,</span> data<span class="operator">=</span>loan_data<span class="punctuation">,</span> importance<span class="operator">=</span><span class="literal">TRUE</span><span class="punctuation">)</span></span><br><span class="line">rf_all</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Call:</span><br><span class="line"> randomForest(formula = outcome ~</span><br><span class="line">., data = loan_data, importance = TRUE)</span><br><span class="line">               Type of random forest: classification</span><br><span class="line">                     Number of trees: 500</span><br><span class="line">No. of variables tried at each split: 4</span><br><span class="line"></span><br><span class="line">        OOB estimate of  error rate: 33.79%</span><br><span class="line">Confusion matrix:</span><br><span class="line">         paid off default class.error</span><br><span class="line">paid off    14676    7995  0.3526532</span><br><span class="line">default      7325   15346  0.3231000</span><br></pre></td></tr></table></figure><p>在 Python 中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">predictors = [<span class="string">&#x27;loan_amnt&#x27;</span>, <span class="string">&#x27;term&#x27;</span>, <span class="string">&#x27;annual_inc&#x27;</span>, <span class="string">&#x27;dti&#x27;</span>, <span class="string">&#x27;payment_inc_ratio&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;revol_bal&#x27;</span>, <span class="string">&#x27;revol_util&#x27;</span>, <span class="string">&#x27;purpose&#x27;</span>, <span class="string">&#x27;delinq_2yrs_zero&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;pub_rec_zero&#x27;</span>, <span class="string">&#x27;open_acc&#x27;</span>, <span class="string">&#x27;grade&#x27;</span>, <span class="string">&#x27;emp_length&#x27;</span>, <span class="string">&#x27;purpose_&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;home_&#x27;</span>, <span class="string">&#x27;emp_len_&#x27;</span>, <span class="string">&#x27;borrower_score&#x27;</span>]</span><br><span class="line">outcome = <span class="string">&#x27;outcome&#x27;</span></span><br><span class="line">X = pd.get_dummies(loan_data[predictors], drop_first=<span class="literal">True</span>)</span><br><span class="line">y = loan_data[outcome]</span><br><span class="line">rf_all = RandomForestClassifier(n_estimators=<span class="number">500</span>, random_state=<span class="number">1</span>)</span><br><span class="line">rf_all.fit(X, y)</span><br></pre></td></tr></table></figure><p><code>importance=TRUE</code> 参数要求 <code>randomForest</code> 存储关于不同变量重要性的额外信息。<code>varImpPlot</code> 函数将绘制变量的相对性能（相对于排列该变量）：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">varImpPlot<span class="punctuation">(</span>rf_all<span class="punctuation">,</span> type<span class="operator">=</span><span class="number">1</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p><strong><code>mean decrease in accuracy</code></strong></p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">varImpPlot<span class="punctuation">(</span>rf_all<span class="punctuation">,</span> type<span class="operator">=</span><span class="number">2</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p><strong><code>mean decrease in node impurity</code></strong></p><p>在 Python 中，<code>RandomForestClassifier</code> 在训练期间会收集特征重要性的信息，并通过 <code>feature_importances_</code> 字段使其可用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">importances = rf_all.feature_importances_</span><br></pre></td></tr></table></figure><p>分类器的 <code>feature_importance_</code> 属性提供了<strong>“Gini 减少量”（Gini decrease）</strong>。然而，Python 中<strong>“准确率减少量”（Accuracy decrease）</strong>并非开箱即用。我们可以使用以下代码来计算它：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">rf = RandomForestClassifier(n_estimators=<span class="number">500</span>)</span><br><span class="line">scores = defaultdict(<span class="built_in">list</span>)</span><br><span class="line"><span class="comment"># cross-validate the scores on a number of different random splits of the data</span></span><br><span class="line"><span class="keyword">for</span></span><br><span class="line">_</span><br><span class="line"><span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">train_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size=<span class="number">0.3</span>)</span><br><span class="line">rf.fit(train_X, train_y)</span><br><span class="line">acc = metrics.accuracy_score(valid_y, rf.predict(valid_X))</span><br><span class="line"><span class="keyword">for</span> column <span class="keyword">in</span> X.columns:</span><br><span class="line">X_t = valid_X.copy()</span><br><span class="line">X_t[column] = np.random.permutation(X_t[column].values)</span><br></pre></td></tr></table></figure><p><img src="/img3/面向数据科学家的实用统计学/F6.8.png" alt="F6.8" style="zoom:50%;" /></p><p>结果如图6-8所示。 类似的图表可以使用这段Python代码创建：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">df = pd.DataFrame(&#123;</span><br><span class="line"><span class="string">&#x27;feature&#x27;</span>: X.columns,</span><br><span class="line"><span class="string">&#x27;Accuracy decrease&#x27;</span>: [np.mean(scores[column]) <span class="keyword">for</span> column <span class="keyword">in</span> X.columns],</span><br><span class="line"><span class="string">&#x27;Gini decrease&#x27;</span>: rf_all.feature_importances_,</span><br><span class="line">&#125;)</span><br><span class="line">df = df.sort_values(<span class="string">&#x27;Accuracy decrease&#x27;</span>)</span><br><span class="line">fig, axes = plt.subplots(ncols=<span class="number">2</span>, figsize=(<span class="number">8</span>, <span class="number">4.5</span>))</span><br><span class="line">ax = df.plot(kind=<span class="string">&#x27;barh&#x27;</span>, x=<span class="string">&#x27;feature&#x27;</span>, y=<span class="string">&#x27;Accuracy decrease&#x27;</span>,</span><br><span class="line">legend=<span class="literal">False</span>, ax=axes[<span class="number">0</span>])</span><br><span class="line">ax.set_ylabel(<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">ax = df.plot(kind=<span class="string">&#x27;barh&#x27;</span>, x=<span class="string">&#x27;feature&#x27;</span>, y=<span class="string">&#x27;Gini decrease&#x27;</span>,</span><br><span class="line">legend=<span class="literal">False</span>, ax=axes[<span class="number">1</span>])</span><br><span class="line">ax.set_ylabel(<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">ax.get_yaxis().set_visible(<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p>有两种方法来衡量<strong>变量重要性</strong>：</p><ul><li>通过<strong>模型准确率的减少量</strong>来衡量（<code>type=1</code>），当一个变量的值被随机排列时。随机排列这些值的作用是<strong>消除该变量的所有预测能力</strong>。准确率是从<strong>袋外数据（out-of-bag data）计算得出的（因此这个度量实际上是一个交叉验证估计</strong>）。</li><li>通过<strong>所有节点 Gini不纯度评分的平均减少量</strong>来衡量（<code>type=2</code>），这些节点都曾根据该变量进行分割（参见第254页的“测量同质性或不纯度”）。这衡量了包含该变量能<strong>在多大程度上提高节点纯度</strong>。这个度量是<strong>基于训练集</strong>的，因此比在袋外数据上计算的度量<strong>更不可靠</strong>。</li></ul><p>图6-8的顶部和底部面板分别显示了根据<strong>准确率减少量</strong>和<strong>Gini不纯度减少量</strong>计算出的变量重要性。两个面板中的变量都按准确率减少量进行了排名。这两种方法产生的变量重要性得分<strong>差异很大</strong>。</p><p>既然准确率减少量是一个更可靠的指标，为什么我们还要使用 Gini不纯度减少量呢？默认情况下，<code>randomForest</code> 只计算 Gini不纯度：Gini不纯度是<strong>算法的副产品</strong>，而按变量计算的模型准确率需要<strong>额外的计算</strong>（随机排列数据并对这些数据进行预测）。在<strong>计算复杂度</strong>很重要的场景中，例如在生产环境中需要拟合数千个模型时，额外的计算工作可能不值得。此外，Gini减少量能揭示<strong>随机森林使用了哪些变量来制定其分割规则</strong>（回想一下，这些信息在单棵树中很容易看到，但在随机森林中实际上丢失了）。</p><h4 id="超参数"><strong>超参数</strong></h4><p>Hyperparameters</p><p><strong>随机森林</strong>与许多统计机器学习算法一样，可以被视为一个<strong>“黑箱算法”</strong>，其内部有一些<strong>可调整的“旋钮”</strong>。这些“旋钮”被称为<strong>超参数（hyperparameters）</strong>，它们是您在拟合模型之前需要设定的参数；它们<strong>不作为训练过程的一部分进行优化</strong>。虽然传统的统计模型也需要选择（例如，在回归模型中选择要使用的预测变量），但随机森林的超参数更为关键，尤其是在<strong>避免过拟合</strong>方面。特别是，随机森林的两个最重要的超参数是：</p><ul><li><p><strong><code>nodesize/min_samples_leaf</code></strong> 终端节点（树中的叶子）的<strong>最小大小</strong>。在 R 中，分类的默认值为1，回归的默认值为5。Python 的 scikit-learn 实现中，两者默认值均为1。</p></li><li><p><strong><code>maxnodes/max_leaf_nodes</code></strong> 每个决策树中的<strong>最大节点数</strong>。默认情况下没有限制，将在 <code>nodesize</code> 约束下拟合最大尺寸的树。请注意，在 Python 中，您指定的是<strong>最大终端节点数</strong>。这两个参数之间存在关系：</p><p><code>maxnodes = 2 * max_leaf_nodes - 1</code></p></li></ul><p>您可能会很想忽略这些参数，直接使用默认值。然而，当您将随机森林应用于<strong>噪声数据</strong>时，使用默认值可能导致<strong>过拟合</strong>。当您增加 <code>nodesize/min_samples_leaf</code> 或设置 <code>maxnodes/max_leaf_nodes</code> 时，算法会拟合较小的树，从而<strong>更不容易创建虚假的预测规则</strong>。可以使用<strong>交叉验证</strong>（参见第155页的“交叉验证”）来测试设置不同超参数值所带来的影响。</p><p><strong>关键思想</strong></p><ul><li><strong>集成模型</strong>通过结合多个模型的结果来提高模型准确率。</li><li><strong>装袋法</strong>是一种特殊的集成模型，它基于对数据的<strong>自举样本</strong>拟合多个模型并进行平均。</li><li><strong>随机森林</strong>是应用于决策树的一种特殊类型的装袋法。除了对数据进行重采样外，随机森林算法在<strong>分割树时还会对预测变量进行抽样</strong>。</li><li>随机森林的一个有用输出是<strong>变量重要性</strong>的度量，它根据预测变量对模型准确率的贡献进行排名。</li><li>随机森林有一组<strong>超参数</strong>，应使用交叉验证进行<strong>调优</strong>以避免过拟合。</li></ul><h3 id="提升法"><strong>提升法</strong></h3><p>Boosting</p><p>集成模型已成为预测建模的标准工具。<strong>提升法（Boosting）</strong>是一种创建模型集成的通用技术。它与<strong>装袋法（bagging）</strong>差不多是同时开发的（参见第259页的“装袋法与随机森林”）。与装袋法一样，提升法最常用于<strong>决策树</strong>。尽管它们有相似之处，但提升法采取了截然不同的方法——这种方法带有更多的“花哨功能”。因此，装袋法只需相对较少的调优即可完成，而提升法在其应用中需要更多的关注。<strong>如果将这两种方法比作汽车，装袋法可以被看作是本田雅阁（可靠且稳定），而提升法则可以被看作是保时捷（强大但需要更细心的呵护）</strong>。</p><p>在线性回归模型中，通常会检查<strong>残差</strong>以看是否可以改进拟合（参见第185页的“偏残差图和非线性”）。提升法将这个概念推向了更远，它<strong>拟合了一系列模型，其中每个后续模型都试图最小化前一个模型的误差</strong>。通常使用该算法的几个变体：<strong>Adaboost</strong>、<strong>梯度提升（gradient boosting）</strong>和<strong>随机梯度提升（stochastic gradient boosting）</strong>。后者，即随机梯度提升，是最通用和应用最广泛的。事实上，通过正确的参数选择，该算法可以模拟随机森林。</p><p><strong>提升法的关键术语</strong></p><ul><li><p><strong>集成（Ensemble）</strong> 通过使用一系列模型来形成预测。 同义词：<strong>模型平均（Model averaging）</strong></p></li><li><p><strong>提升法（Boosting）</strong> 一种通用技术，通过在每个连续轮次中对具有较大残差的记录赋予更多权重来拟合一系列模型。</p></li><li><p><strong>Adaboost</strong> 提升法的早期版本，根据残差对数据进行重新加权。</p></li><li><p><strong>梯度提升（Gradient boosting）</strong> 一种更通用的提升形式，被定义为<strong>最小化成本函数</strong>。</p></li><li><p><strong>随机梯度提升（Stochastic gradient boosting）</strong> 最通用的提升算法，在每一轮中都包含了<strong>记录和列的重采样</strong>。</p></li><li><p><strong>正则化（Regularization）</strong> 一种通过在成本函数中<strong>添加惩罚项</strong>以避免模型过拟合的技术，该惩罚项与模型中的参数数量相关。</p></li><li><p><strong>超参数（Hyperparameters）</strong> 在拟合算法之前需要设定的参数。</p></li></ul><h4 id="提升算法"><strong>提升算法</strong></h4><p>The Boosting Algorithm</p><p>提升算法有多种，但它们的基本思想本质上是相同的。最容易理解的是 <strong>Adaboost</strong>，它的过程如下：</p><ol type="1"><li>初始化<strong>最大拟合模型数 <span class="math inline">\(M\)</span></strong>，并设置迭代计数器 <span class="math inline">\(m = 1\)</span>。初始化<strong>观测权重</strong> <span class="math inline">\(w_i = 1/N\)</span>（对于 <span class="math inline">\(i=1, 2, \dots, N\)</span>）。初始化<strong>集成模型</strong> <span class="math inline">\(F_0 = 0\)</span>。</li><li>使用观测权重 <span class="math inline">\(w_1, w_2, \dots, w_N\)</span> 训练模型 <span class="math inline">\(\hat f_m\)</span>，使其<strong>最小化加权误差</strong> <span class="math inline">\(e_m\)</span>，该误差由错误分类观测的权重总和定义。</li><li>将模型添加到集成中：<span class="math inline">\(\hat F_m = \hat F_{m-1} + \alpha_m \hat f_m\)</span>，其中 <span class="math inline">\(\alpha_m = \log\frac{1 - e_m}{e_m}\)</span>。</li><li>更新权重 <span class="math inline">\(w_1, w_2, \dots, w_N\)</span>，以便<strong>增加被错误分类的观测的权重</strong>。增加的幅度取决于 <span class="math inline">\(\alpha_m\)</span>，<span class="math inline">\(\alpha_m\)</span> 的值越大，权重增加得越多。</li><li>递增模型计数器 <span class="math inline">\(m = m + 1\)</span>。如果 <span class="math inline">\(m \le M\)</span>，则返回步骤2。</li></ol><p>提升后的估计值由下式给出：</p><p><span class="math display">\[\hat F = \alpha_1 \hat f_1 + \alpha_2 \hat f_2 + \cdots + \alpha_M \hat f_M\]</span> 通过增加被错误分类的观测的权重，该算法<strong>迫使模型更侧重于对其表现不佳的数据进行训练</strong>。因子 <span class="math inline">\(\alpha_m\)</span> 确保<strong>误差较低的模型拥有更大的权重</strong>。</p><p><strong>梯度提升</strong>与 Adaboost 类似，但它将问题视为<strong>成本函数的优化</strong>。梯度提升不是调整权重，而是<strong>拟合模型以适应伪残差</strong>，这起到了更侧重于较大残差进行训练的作用。</p><p>本着随机森林的精神，<strong>随机梯度提升</strong>通过在每个阶段对观测和预测变量进行抽样，为算法增加了<strong>随机性</strong>。</p><h4 id="xgboost"><strong>XGBoost</strong></h4><p>XGBoost</p><p>最广泛使用的用于提升法的开源软件是 <strong>XGBoost</strong>，它是由华盛顿大学的<strong>陈天奇</strong>和<strong>Carlos Guestrin</strong>最初开发的随机梯度提升的一种实现。它是一个计算高效且具有许多选项的实现，作为软件包可用于大多数主要的数据科学编程语言。在 R 中，XGBoost 可作为 <code>xgboost</code> 包使用，在 Python 中也使用相同的名称。</p><p><code>xgboost</code> 方法有许多可以且应该调整的参数（参见第279页的“超参数与交叉验证”）。两个非常重要的参数是 <strong><code>subsample</code></strong>，它控制在每次迭代中应该采样的观测记录比例；以及 <strong><code>eta</code></strong>，一个应用于提升算法中 <span class="math inline">\(\alpha_m\)</span> 的<strong>收缩因子</strong>（参见第271页的“提升算法”）。</p><p>使用 <code>subsample</code> 会让提升法的行为类似于随机森林，只是采样是<strong>无放回</strong>的。收缩参数 <code>eta</code> 有助于通过<strong>减少权重的变化</strong>来防止过拟合（权重的变化越小，算法就越不容易过拟合训练集）。</p><p>以下代码在 R 中将 <code>xgboost</code> 应用于只有两个预测变量的贷款数据：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">predictors <span class="operator">&lt;-</span> data.matrix<span class="punctuation">(</span>loan3000<span class="punctuation">[</span><span class="punctuation">,</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;borrower_score&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;payment_inc_ratio&#x27;</span><span class="punctuation">)</span><span class="punctuation">]</span><span class="punctuation">)</span></span><br><span class="line">label <span class="operator">&lt;-</span> <span class="built_in">as.numeric</span><span class="punctuation">(</span>loan3000<span class="punctuation">[</span><span class="punctuation">,</span><span class="string">&#x27;outcome&#x27;</span><span class="punctuation">]</span><span class="punctuation">)</span> <span class="operator">-</span> <span class="number">1</span></span><br><span class="line">xgb <span class="operator">&lt;-</span> xgboost<span class="punctuation">(</span>data<span class="operator">=</span>predictors<span class="punctuation">,</span> label<span class="operator">=</span>label<span class="punctuation">,</span> objective<span class="operator">=</span><span class="string">&quot;binary:logistic&quot;</span><span class="punctuation">,</span></span><br><span class="line">params<span class="operator">=</span><span class="built_in">list</span><span class="punctuation">(</span>subsample<span class="operator">=</span><span class="number">0.63</span><span class="punctuation">,</span> eta<span class="operator">=</span><span class="number">0.1</span><span class="punctuation">)</span><span class="punctuation">,</span> nrounds<span class="operator">=</span><span class="number">100</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[1] train-error:0.358333</span><br><span class="line">[2] train-error:0.346333</span><br><span class="line">[3] train-error:0.347333</span><br><span class="line">...</span><br><span class="line">[99] train-error:0.239333</span><br><span class="line">[100] train-error:0.241000</span><br></pre></td></tr></table></figure><p>请注意，<code>xgboost</code> <strong>不支持公式语法</strong>，因此预测变量需要转换为 <code>data.matrix</code>，响应变量需要转换为0/1变量。<code>objective</code> 参数告诉 <code>xgboost</code> 这是哪种类型的问题；基于此，<code>xgboost</code> 会选择一个要优化的指标。</p><p>在 Python 中，<code>xgboost</code> 有两种不同的接口：<strong>scikit-learn API</strong> 和一个更像 R 的<strong>函数式接口</strong>。为了与其他 scikit-learn 方法保持一致，一些参数被重新命名了。例如，<code>eta</code> 被重命名为 <code>learning_rate</code>；使用 <code>eta</code> 虽然不会导致失败，但也不会产生预期的效果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">predictors = [<span class="string">&#x27;borrower_score&#x27;</span>, <span class="string">&#x27;payment_inc_ratio&#x27;</span>]</span><br><span class="line">outcome = <span class="string">&#x27;outcome&#x27;</span></span><br><span class="line">X = loan3000[predictors]</span><br><span class="line">y = loan3000[outcome]</span><br><span class="line">xgb = XGBClassifier(objective=<span class="string">&#x27;binary:logistic&#x27;</span>, subsample=<span class="number">0.63</span>)</span><br><span class="line">xgb.fit(X, y)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">--</span><br><span class="line">XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, colsample_bylevel=1,</span><br><span class="line">colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.1,</span><br><span class="line">max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,</span><br><span class="line">n_estimators=100, n_jobs=1, nthread=None, objective=&#x27;binary:logistic&#x27;,</span><br><span class="line">random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,</span><br><span class="line">silent=None, subsample=0.63, verbosity=1)</span><br></pre></td></tr></table></figure><p>预测值可以从 R 中的 <code>predict</code> 函数获得，并且因为只有两个变量，可以针对预测变量进行绘制：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">pred <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>xgb<span class="punctuation">,</span> newdata<span class="operator">=</span>predictors<span class="punctuation">)</span></span><br><span class="line">xgb_df <span class="operator">&lt;-</span> cbind<span class="punctuation">(</span>loan3000<span class="punctuation">,</span> pred_default <span class="operator">=</span> pred <span class="operator">&gt;</span> <span class="number">0.5</span><span class="punctuation">,</span> prob_default <span class="operator">=</span> pred<span class="punctuation">)</span></span><br><span class="line">ggplot<span class="punctuation">(</span>data<span class="operator">=</span>xgb_df<span class="punctuation">,</span> aes<span class="punctuation">(</span>x<span class="operator">=</span>borrower_score<span class="punctuation">,</span> y<span class="operator">=</span>payment_inc_ratio<span class="punctuation">,</span></span><br><span class="line">color<span class="operator">=</span>pred_default<span class="punctuation">,</span> shape<span class="operator">=</span>pred_default<span class="punctuation">,</span> size<span class="operator">=</span>pred_default<span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">geom_point<span class="punctuation">(</span>alpha<span class="operator">=</span><span class="number">.8</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">scale_color_manual<span class="punctuation">(</span>values <span class="operator">=</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;FALSE&#x27;</span><span class="operator">=</span><span class="string">&#x27;#b8e186&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;TRUE&#x27;</span><span class="operator">=</span><span class="string">&#x27;#d95f02&#x27;</span><span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">scale_shape_manual<span class="punctuation">(</span>values <span class="operator">=</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;FALSE&#x27;</span><span class="operator">=</span><span class="number">0</span><span class="punctuation">,</span> <span class="string">&#x27;TRUE&#x27;</span><span class="operator">=</span><span class="number">1</span><span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">scale_size_manual<span class="punctuation">(</span>values <span class="operator">=</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;FALSE&#x27;</span><span class="operator">=</span><span class="number">0.5</span><span class="punctuation">,</span> <span class="string">&#x27;TRUE&#x27;</span><span class="operator">=</span><span class="number">2</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>使用以下代码可以在 Python 中创建相同的图：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">6</span>, <span class="number">4</span>))</span><br><span class="line">xgb_df.loc[xgb_df.prediction==<span class="string">&#x27;paid off&#x27;</span>].plot(</span><br><span class="line">x=<span class="string">&#x27;borrower_score&#x27;</span>, y=<span class="string">&#x27;payment_inc_ratio&#x27;</span>, style=<span class="string">&#x27;.&#x27;</span>,</span><br><span class="line">markerfacecolor=<span class="string">&#x27;none&#x27;</span>, markeredgecolor=<span class="string">&#x27;C1&#x27;</span>, ax=ax)</span><br><span class="line"></span><br><span class="line">xgb_df.loc[xgb_df.prediction==<span class="string">&#x27;default&#x27;</span>].plot(</span><br><span class="line">x=<span class="string">&#x27;borrower_score&#x27;</span>, y=<span class="string">&#x27;payment_inc_ratio&#x27;</span>, style=<span class="string">&#x27;o&#x27;</span>,</span><br><span class="line">markerfacecolor=<span class="string">&#x27;none&#x27;</span>, markeredgecolor=<span class="string">&#x27;C0&#x27;</span>, ax=ax)</span><br><span class="line">ax.legend([<span class="string">&#x27;paid off&#x27;</span>, <span class="string">&#x27;default&#x27;</span>]);</span><br><span class="line">ax.set_xlim(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">ax.set_ylim(<span class="number">0</span>, <span class="number">25</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">&#x27;borrower_score&#x27;</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&#x27;payment_inc_ratio&#x27;</span>)</span><br></pre></td></tr></table></figure><p>结果如图6-9所示。 从定性上看，这与随机森林的预测结果相似；参见图6-7。预测结果有些<strong>噪声</strong>，因为一些<strong>借款人得分非常高</strong>（表明信用度高）的贷款，最终仍被预测为违约。</p><p><img src="/img3/面向数据科学家的实用统计学/F6.9.png" alt="F6.9" style="zoom:50%;" /></p><h4 id="正则化避免过拟合"><strong>正则化：避免过拟合</strong></h4><p>Regularization: Avoiding Overfitting</p><p>盲目应用 <code>xgboost</code> 可能会因<strong>过拟合训练数据</strong>而导致模型不稳定。过拟合问题是双重的：</p><ul><li>模型在<strong>不属于训练集的新数据</strong>上的准确率会<strong>降低</strong>。</li><li>模型的预测<strong>高度可变</strong>，导致结果不稳定。</li></ul><p>任何建模技术都可能容易过拟合。例如，如果回归方程中包含过多变量，模型最终可能会产生虚假预测。然而，对于大多数统计技术，通过<strong>明智地选择预测变量</strong>可以避免过拟合。即使是随机森林，通常在不调整参数的情况下也能产生一个合理的模型。</p><p>但是，<code>xgboost</code> 的情况并非如此。使用模型中包含的所有变量来拟合训练集上的贷款数据。在 R 中，您可以这样做：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">seed <span class="operator">&lt;-</span> 400820</span><br><span class="line">predictors <span class="operator">&lt;-</span> data.matrix<span class="punctuation">(</span>loan_data<span class="punctuation">[</span><span class="punctuation">,</span> <span class="operator">-</span>which<span class="punctuation">(</span><span class="built_in">names</span><span class="punctuation">(</span>loan_data<span class="punctuation">)</span> <span class="operator">%in%</span></span><br><span class="line"><span class="string">&#x27;outcome&#x27;</span><span class="punctuation">)</span><span class="punctuation">]</span><span class="punctuation">)</span></span><br><span class="line">label <span class="operator">&lt;-</span> <span class="built_in">as.numeric</span><span class="punctuation">(</span>loan_data<span class="operator">$</span>outcome<span class="punctuation">)</span> <span class="operator">-</span> <span class="number">1</span></span><br><span class="line">test_idx <span class="operator">&lt;-</span> sample<span class="punctuation">(</span>nrow<span class="punctuation">(</span>loan_data<span class="punctuation">)</span><span class="punctuation">,</span> <span class="number">10000</span><span class="punctuation">)</span></span><br><span class="line">xgb_default <span class="operator">&lt;-</span> xgboost<span class="punctuation">(</span>data<span class="operator">=</span>predictors<span class="punctuation">[</span><span class="operator">-</span>test_idx<span class="punctuation">,</span><span class="punctuation">]</span><span class="punctuation">,</span> label<span class="operator">=</span>label<span class="punctuation">[</span><span class="operator">-</span>test_idx<span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">objective<span class="operator">=</span><span class="string">&#x27;binary:logistic&#x27;</span><span class="punctuation">,</span> nrounds<span class="operator">=</span><span class="number">250</span><span class="punctuation">,</span> verbose<span class="operator">=</span><span class="number">0</span><span class="punctuation">)</span></span><br><span class="line">pred_default <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>xgb_default<span class="punctuation">,</span> predictors<span class="punctuation">[</span>test_idx<span class="punctuation">,</span><span class="punctuation">]</span><span class="punctuation">)</span></span><br><span class="line">error_default <span class="operator">&lt;-</span> <span class="built_in">abs</span><span class="punctuation">(</span>label<span class="punctuation">[</span>test_idx<span class="punctuation">]</span> <span class="operator">-</span> pred_default<span class="punctuation">)</span> <span class="operator">&gt;</span> <span class="number">0.5</span></span><br><span class="line">xgb_default<span class="operator">$</span>evaluation_log<span class="punctuation">[</span><span class="number">250</span><span class="punctuation">,</span><span class="punctuation">]</span></span><br><span class="line">mean<span class="punctuation">(</span>error_default<span class="punctuation">)</span></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">-</span><br><span class="line">iter train_error</span><br><span class="line">1: 250 0.133043</span><br><span class="line">[1] 0.3529</span><br></pre></td></tr></table></figure><p>我们使用 Python 中的 <code>train_test_split</code> 函数将数据集分为训练集和测试集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">predictors = [<span class="string">&#x27;loan_amnt&#x27;</span>, <span class="string">&#x27;term&#x27;</span>, <span class="string">&#x27;annual_inc&#x27;</span>, <span class="string">&#x27;dti&#x27;</span>, <span class="string">&#x27;payment_inc_ratio&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;revol_bal&#x27;</span>, <span class="string">&#x27;revol_util&#x27;</span>, <span class="string">&#x27;purpose&#x27;</span>, <span class="string">&#x27;delinq_2yrs_zero&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;pub_rec_zero&#x27;</span>, <span class="string">&#x27;open_acc&#x27;</span>, <span class="string">&#x27;grade&#x27;</span>, <span class="string">&#x27;emp_length&#x27;</span>, <span class="string">&#x27;purpose_&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;home_&#x27;</span>, <span class="string">&#x27;emp_len_&#x27;</span>, <span class="string">&#x27;borrower_score&#x27;</span>]</span><br><span class="line">outcome = <span class="string">&#x27;outcome&#x27;</span></span><br><span class="line">X = pd.get_dummies(loan_data[predictors], drop_first=<span class="literal">True</span>)</span><br><span class="line">y = pd.Series([<span class="number">1</span> <span class="keyword">if</span> o == <span class="string">&#x27;default&#x27;</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> o <span class="keyword">in</span> loan_data[outcome]])</span><br><span class="line">train_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size=<span class="number">10000</span>)</span><br><span class="line">xgb_default = XGBClassifier(objective=<span class="string">&#x27;binary:logistic&#x27;</span>, n_estimators=<span class="number">250</span>,</span><br><span class="line">max_depth=<span class="number">6</span>, reg_lambda=<span class="number">0</span>, learning_rate=<span class="number">0.3</span>,</span><br><span class="line">subsample=<span class="number">1</span>)</span><br><span class="line">xgb_default.fit(train_X, train_y)</span><br><span class="line">pred_default = xgb_default.predict_proba(valid_X)[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">error_default = <span class="built_in">abs</span>(valid_y - pred_default) &gt; <span class="number">0.5</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;default: &#x27;</span>, np.mean(error_default))</span><br></pre></td></tr></table></figure><p>测试集由从完整数据中随机抽样的10,000条记录组成，训练集则由剩余的记录组成。提升法在训练集上的<strong>错误率</strong>仅为13.3%。然而，<strong>测试集</strong>的错误率高达35.3%。这是<strong>过拟合</strong>的结果：虽然提升法可以很好地解释训练集中的可变性，但其预测规则<strong>不适用于新数据</strong>。</p><p>提升法提供了几个参数来<strong>避免过拟合</strong>，包括 <code>eta</code> (或 <code>learning_rate</code>) 和 <code>subsample</code> (参见第272页的“XGBoost”)。另一种方法是<strong>正则化</strong>，该技术通过添加一个<strong>惩罚模型复杂度的惩罚项</strong>来修改成本函数。决策树通过最小化 Gini不纯度评分等成本标准来进行拟合（参见第254页的“测量同质性或不纯度”）。在 <code>xgboost</code> 中，可以通过<strong>添加一个衡量模型复杂度的项</strong>来修改成本函数。</p><p><code>xgboost</code> 中有两个用于<strong>正则化模型</strong>的参数：<strong><code>alpha</code></strong> 和 <strong><code>lambda</code></strong>，它们分别对应于<strong>曼哈顿距离（L1-正则化）和欧几里得距离平方（L2-正则化）</strong>（参见第241页的“距离度量”）。增加这些参数会<strong>惩罚更复杂的模型</strong>并<strong>减小拟合树的大小</strong>。例如，看看我们在 R 中将 <code>lambda</code> 设置为1000时会发生什么：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">xgb_penalty <span class="operator">&lt;-</span> xgboost<span class="punctuation">(</span>data<span class="operator">=</span>predictors<span class="punctuation">[</span><span class="operator">-</span>test_idx<span class="punctuation">,</span><span class="punctuation">]</span><span class="punctuation">,</span> label<span class="operator">=</span>label<span class="punctuation">[</span><span class="operator">-</span>test_idx<span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">params<span class="operator">=</span><span class="built_in">list</span><span class="punctuation">(</span>eta<span class="operator">=</span><span class="number">.1</span><span class="punctuation">,</span> subsample<span class="operator">=</span><span class="number">.63</span><span class="punctuation">,</span> lambda<span class="operator">=</span><span class="number">1000</span><span class="punctuation">)</span><span class="punctuation">,</span></span><br><span class="line">objective<span class="operator">=</span><span class="string">&#x27;binary:logistic&#x27;</span><span class="punctuation">,</span> nrounds<span class="operator">=</span><span class="number">250</span><span class="punctuation">,</span> verbose<span class="operator">=</span><span class="number">0</span><span class="punctuation">)</span></span><br><span class="line">pred_penalty <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>xgb_penalty<span class="punctuation">,</span> predictors<span class="punctuation">[</span>test_idx<span class="punctuation">,</span><span class="punctuation">]</span><span class="punctuation">)</span></span><br><span class="line">error_penalty <span class="operator">&lt;-</span> <span class="built_in">abs</span><span class="punctuation">(</span>label<span class="punctuation">[</span>test_idx<span class="punctuation">]</span> <span class="operator">-</span> pred_penalty<span class="punctuation">)</span> <span class="operator">&gt;</span> <span class="number">0.5</span></span><br><span class="line">xgb_penalty<span class="operator">$</span>evaluation_log<span class="punctuation">[</span><span class="number">250</span><span class="punctuation">,</span><span class="punctuation">]</span></span><br><span class="line">mean<span class="punctuation">(</span>error_penalty<span class="punctuation">)</span></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">-</span><br><span class="line">iter train_error</span><br><span class="line">1: 250 0.30966</span><br><span class="line">[1] 0.3286</span><br></pre></td></tr></table></figure><p>在 scikit-learn API 中，参数被称为 <code>reg_alpha</code> 和 <code>reg_lambda</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">xgb_penalty = XGBClassifier(objective=<span class="string">&#x27;binary:logistic&#x27;</span>, n_estimators=<span class="number">250</span>,</span><br><span class="line">max_depth=<span class="number">6</span>, reg_lambda=<span class="number">1000</span>, learning_rate=<span class="number">0.1</span>,</span><br><span class="line">subsample=<span class="number">0.63</span>)</span><br><span class="line">xgb_penalty.fit(train_X, train_y)</span><br><span class="line">pred_penalty = xgb_penalty.predict_proba(valid_X)[:, <span class="number">1</span>]</span><br><span class="line">error_penalty = <span class="built_in">abs</span>(valid_y - pred_penalty) &gt; <span class="number">0.5</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;penalty: &#x27;</span>, np.mean(error_penalty))</span><br></pre></td></tr></table></figure><p>现在，<strong>训练误差仅略低于测试集上的误差</strong>。</p><p>在 R 中，<code>predict</code> 方法提供了一个方便的参数 <code>ntreelimit</code>，它强制仅使用前 <span class="math inline">\(i\)</span> 棵树进行预测。这使我们能够随着模型的增加，直接比较<strong>样本内（in-sample）</strong>和<strong>样本外（out-of-sample）</strong>的错误率：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">error_default <span class="operator">&lt;-</span> <span class="built_in">rep</span><span class="punctuation">(</span><span class="number">0</span><span class="punctuation">,</span> <span class="number">250</span><span class="punctuation">)</span></span><br><span class="line">error_penalty <span class="operator">&lt;-</span> <span class="built_in">rep</span><span class="punctuation">(</span><span class="number">0</span><span class="punctuation">,</span> <span class="number">250</span><span class="punctuation">)</span></span><br><span class="line"><span class="keyword">for</span><span class="punctuation">(</span>i <span class="keyword">in</span> <span class="number">1</span><span class="operator">:</span><span class="number">250</span><span class="punctuation">)</span><span class="punctuation">&#123;</span></span><br><span class="line">pred_def <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>xgb_default<span class="punctuation">,</span> predictors<span class="punctuation">[</span>test_idx<span class="punctuation">,</span><span class="punctuation">]</span><span class="punctuation">,</span> ntreelimit<span class="operator">=</span>i<span class="punctuation">)</span></span><br><span class="line">error_default<span class="punctuation">[</span>i<span class="punctuation">]</span> <span class="operator">&lt;-</span> mean<span class="punctuation">(</span><span class="built_in">abs</span><span class="punctuation">(</span>label<span class="punctuation">[</span>test_idx<span class="punctuation">]</span> <span class="operator">-</span> pred_def<span class="punctuation">)</span> <span class="operator">&gt;=</span> <span class="number">0.5</span><span class="punctuation">)</span></span><br><span class="line">pred_pen <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>xgb_penalty<span class="punctuation">,</span> predictors<span class="punctuation">[</span>test_idx<span class="punctuation">,</span><span class="punctuation">]</span><span class="punctuation">,</span> ntreelimit<span class="operator">=</span>i<span class="punctuation">)</span></span><br><span class="line">error_penalty<span class="punctuation">[</span>i<span class="punctuation">]</span> <span class="operator">&lt;-</span> mean<span class="punctuation">(</span><span class="built_in">abs</span><span class="punctuation">(</span>label<span class="punctuation">[</span>test_idx<span class="punctuation">]</span> <span class="operator">-</span> pred_pen<span class="punctuation">)</span> <span class="operator">&gt;=</span> <span class="number">0.5</span><span class="punctuation">)</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>在 Python 中，我们可以用 <code>ntree_limit</code> 参数调用 <code>predict_proba</code> 方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">results = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">250</span>):</span><br><span class="line">train_default = xgb_default.predict_proba(train_X, ntree_limit=i)[:, <span class="number">1</span>]</span><br><span class="line">train_penalty = xgb_penalty.predict_proba(train_X, ntree_limit=i)[:, <span class="number">1</span>]</span><br><span class="line">pred_default = xgb_default.predict_proba(valid_X, ntree_limit=i)[:, <span class="number">1</span>]</span><br><span class="line">pred_penalty = xgb_penalty.predict_proba(valid_X, ntree_limit=i)[:, <span class="number">1</span>]</span><br><span class="line">results.append(&#123;</span><br><span class="line"><span class="string">&#x27;iterations&#x27;</span>: i,</span><br><span class="line"><span class="string">&#x27;default train&#x27;</span>: np.mean(<span class="built_in">abs</span>(train_y - train_default) &gt; <span class="number">0.5</span>),</span><br><span class="line"><span class="string">&#x27;penalty train&#x27;</span>: np.mean(<span class="built_in">abs</span>(train_y - train_penalty) &gt; <span class="number">0.5</span>),</span><br><span class="line"><span class="string">&#x27;default test&#x27;</span>: np.mean(<span class="built_in">abs</span>(valid_y - pred_default) &gt; <span class="number">0.5</span>),</span><br><span class="line"><span class="string">&#x27;penalty test&#x27;</span>: np.mean(<span class="built_in">abs</span>(valid_y - pred_penalty) &gt; <span class="number">0.5</span>),</span><br><span class="line">&#125;)</span><br><span class="line">results = pd.DataFrame(results)</span><br><span class="line">results.head()</span><br></pre></td></tr></table></figure><p>模型的输出将训练集的误差返回到 <code>xgb_default$evaluation_log</code> 组件中。通过将其与样本外误差结合，我们可以绘制<strong>误差与迭代次数</strong>的关系图：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">errors <span class="operator">&lt;-</span> rbind<span class="punctuation">(</span>xgb_default<span class="operator">$</span>evaluation_log<span class="punctuation">,</span></span><br><span class="line">xgb_penalty<span class="operator">$</span>evaluation_log<span class="punctuation">,</span></span><br><span class="line">ata.frame<span class="punctuation">(</span>iter<span class="operator">=</span><span class="number">1</span><span class="operator">:</span><span class="number">250</span><span class="punctuation">,</span> train_error<span class="operator">=</span>error_default<span class="punctuation">)</span><span class="punctuation">,</span></span><br><span class="line">data.frame<span class="punctuation">(</span>iter<span class="operator">=</span><span class="number">1</span><span class="operator">:</span><span class="number">250</span><span class="punctuation">,</span> train_error<span class="operator">=</span>error_penalty<span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">errors<span class="operator">$</span>type <span class="operator">&lt;-</span> <span class="built_in">rep</span><span class="punctuation">(</span><span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;default train&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;penalty train&#x27;</span><span class="punctuation">,</span></span><br><span class="line"><span class="string">&#x27;default test&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;penalty test&#x27;</span><span class="punctuation">)</span><span class="punctuation">,</span> <span class="built_in">rep</span><span class="punctuation">(</span><span class="number">250</span><span class="punctuation">,</span> <span class="number">4</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">ggplot<span class="punctuation">(</span>errors<span class="punctuation">,</span> aes<span class="punctuation">(</span>x<span class="operator">=</span>iter<span class="punctuation">,</span> y<span class="operator">=</span>train_error<span class="punctuation">,</span> group<span class="operator">=</span>type<span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">geom_line<span class="punctuation">(</span>aes<span class="punctuation">(</span>linetype<span class="operator">=</span>type<span class="punctuation">,</span> color<span class="operator">=</span>type<span class="punctuation">)</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>我们可以使用 pandas 的 <code>plot</code> 方法创建折线图。从第一个图返回的坐标轴允许我们在同一个图上叠加额外的线条。这是许多 Python 图形包支持的模式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ax = results.plot(x=<span class="string">&#x27;iterations&#x27;</span>, y=<span class="string">&#x27;default test&#x27;</span>)</span><br><span class="line">results.plot(x=<span class="string">&#x27;iterations&#x27;</span>, y=<span class="string">&#x27;penalty test&#x27;</span>, ax=ax)</span><br><span class="line">results.plot(x=<span class="string">&#x27;iterations&#x27;</span>, y=<span class="string">&#x27;default train&#x27;</span>, ax=ax)</span><br><span class="line">results.plot(x=<span class="string">&#x27;iterations&#x27;</span>, y=<span class="string">&#x27;penalty train&#x27;</span>, ax=ax)</span><br></pre></td></tr></table></figure><p>结果如图6-10所示。 这表明<strong>默认模型</strong>在训练集上的准确率稳步提高，但<strong>在测试集上的表现实际上却变差了</strong>。而<strong>正则化后的模型</strong>没有表现出这种行为。</p><p><img src="/img3/面向数据科学家的实用统计学/F6.10.png" alt="F6.10" style="zoom:50%;" /></p><p><strong>岭回归和Lasso</strong></p><p>通过对模型的复杂度施加惩罚来帮助避免过拟合的技术可以追溯到20世纪70年代。<strong>最小二乘回归</strong>最小化<strong>残差平方和（RSS）</strong>；参见第148页的“最小二乘”。<strong>岭回归（Ridge regression）</strong>最小化<strong>残差平方和</strong>加上一个<strong>惩罚项</strong>，该惩罚项是系数数量和大小的函数：</p><p><span class="math display">\[\sum_{i=1}^{n} (Y_i - b_0 - b_1X_1 - \dots - b_pX_p)^2 + \lambda \sum_{j=1}^{p} b_j^2\]</span> <span class="math inline">\(\lambda\)</span> 的值决定了对系数的惩罚程度；值越大，产生的模型就<strong>越不容易过拟合数据</strong>。<strong>Lasso</strong> 与此类似，不同之处在于它使用<strong>曼哈顿距离</strong>而不是<strong>欧几里得距离</strong>作为惩罚项：</p><p><span class="math display">\[\sum_{i=1}^{n} (Y_i - b_0 - b_1X_1 - \dots - b_pX_p)^2 + \alpha \sum_{j=1}^{p} |b_j|\]</span> 使用欧几里得距离也称为 <strong>L2 正则化</strong>，使用曼哈顿距离则称为 <strong>L1 正则化</strong>。<code>xgboost</code> 的参数 <code>lambda</code> (<code>reg_lambda</code>) 和 <code>alpha</code> (<code>reg_alpha</code>) 的作用与此类似。</p><h4 id="超参数和交叉验证"><strong>超参数和交叉验证</strong></h4><p>Hyperparameters and Cross-Validation</p><p><code>xgboost</code> 具有一系列<strong>令人望而生畏</strong>的超参数；关于讨论，请参见第281页的“XGBoost超参数”。正如在第274页的“正则化：避免过拟合”中所看到的，具体的选择可以<strong>显著改变模型拟合</strong>。面对如此多的超参数组合可供选择，我们应该如何做出指导性选择呢？解决这个问题的标准方案是使用<strong>交叉验证</strong>；参见第155页的“交叉验证”。</p><p><strong>交叉验证</strong>将数据随机分成 <span class="math inline">\(K\)</span> 个不同的组，也称为<strong>折叠（folds）</strong>。对于每个折叠，模型在<strong>不包含该折叠数据</strong>的其余数据上进行训练，然后在该折叠的数据上进行评估。这能得到一个模型在<strong>样本外数据</strong>上的<strong>准确率度量</strong>。最佳的超参数集由<strong>通过对每个折叠的误差取平均</strong>计算出的<strong>总体误差最低</strong>的模型所决定。</p><p>为了说明这项技术，我们将其应用于 <code>xgboost</code> 的参数选择。在这个例子中，我们探讨了两个参数：<strong>收缩参数 <code>eta</code> (<code>learning_rate</code>)</strong>（参见第272页的“XGBoost”）和树的<strong>最大深度 <code>max_depth</code></strong>。参数 <code>max_depth</code> 是从叶子节点到树根的最大深度，默认值为6。这给了我们另一种控制<strong>过拟合</strong>的方法：<strong>深层树往往更复杂，可能导致数据过拟合</strong>。</p><p>首先，我们设置折叠和参数列表。在 R 中，操作如下：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">N <span class="operator">&lt;-</span> nrow<span class="punctuation">(</span>loan_data<span class="punctuation">)</span></span><br><span class="line">fold_number <span class="operator">&lt;-</span> sample<span class="punctuation">(</span><span class="number">1</span><span class="operator">:</span><span class="number">5</span><span class="punctuation">,</span> N<span class="punctuation">,</span> replace<span class="operator">=</span><span class="literal">TRUE</span><span class="punctuation">)</span></span><br><span class="line">params <span class="operator">&lt;-</span> data.frame<span class="punctuation">(</span>eta <span class="operator">=</span> <span class="built_in">rep</span><span class="punctuation">(</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">.1</span><span class="punctuation">,</span> <span class="number">.5</span><span class="punctuation">,</span> <span class="number">.9</span><span class="punctuation">)</span><span class="punctuation">,</span> <span class="number">3</span><span class="punctuation">)</span><span class="punctuation">,</span></span><br><span class="line">max_depth <span class="operator">=</span> <span class="built_in">rep</span><span class="punctuation">(</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">3</span><span class="punctuation">,</span> <span class="number">6</span><span class="punctuation">,</span> <span class="number">12</span><span class="punctuation">)</span><span class="punctuation">,</span> <span class="built_in">rep</span><span class="punctuation">(</span><span class="number">3</span><span class="punctuation">,</span><span class="number">3</span><span class="punctuation">)</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>现在，我们使用五个折叠，应用前面描述的算法来计算每个模型和每个折叠的误差：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">error <span class="operator">&lt;-</span> matrix<span class="punctuation">(</span><span class="number">0</span><span class="punctuation">,</span> nrow<span class="operator">=</span><span class="number">9</span><span class="punctuation">,</span> ncol<span class="operator">=</span><span class="number">5</span><span class="punctuation">)</span></span><br><span class="line"><span class="keyword">for</span><span class="punctuation">(</span>i <span class="keyword">in</span> <span class="number">1</span><span class="operator">:</span>nrow<span class="punctuation">(</span>params<span class="punctuation">)</span><span class="punctuation">)</span><span class="punctuation">&#123;</span></span><br><span class="line"><span class="keyword">for</span><span class="punctuation">(</span>k <span class="keyword">in</span> <span class="number">1</span><span class="operator">:</span><span class="number">5</span><span class="punctuation">)</span><span class="punctuation">&#123;</span></span><br><span class="line">fold_idx <span class="operator">&lt;-</span> <span class="punctuation">(</span><span class="number">1</span><span class="operator">:</span>N<span class="punctuation">)</span><span class="punctuation">[</span>fold_number <span class="operator">==</span> k<span class="punctuation">]</span></span><br><span class="line">xgb <span class="operator">&lt;-</span> xgboost<span class="punctuation">(</span>data<span class="operator">=</span>predictors<span class="punctuation">[</span><span class="operator">-</span>fold_idx<span class="punctuation">,</span><span class="punctuation">]</span><span class="punctuation">,</span> label<span class="operator">=</span>label<span class="punctuation">[</span><span class="operator">-</span>fold_idx<span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">params<span class="operator">=</span><span class="built_in">list</span><span class="punctuation">(</span>eta<span class="operator">=</span>params<span class="punctuation">[</span>i<span class="punctuation">,</span> <span class="string">&#x27;eta&#x27;</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">max_depth<span class="operator">=</span>params<span class="punctuation">[</span>i<span class="punctuation">,</span> <span class="string">&#x27;max_depth&#x27;</span><span class="punctuation">]</span><span class="punctuation">)</span><span class="punctuation">,</span></span><br><span class="line">objective<span class="operator">=</span><span class="string">&#x27;binary:logistic&#x27;</span><span class="punctuation">,</span> nrounds<span class="operator">=</span><span class="number">100</span><span class="punctuation">,</span> verbose<span class="operator">=</span><span class="number">0</span><span class="punctuation">)</span></span><br><span class="line">pred <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>xgb<span class="punctuation">,</span> predictors<span class="punctuation">[</span>fold_idx<span class="punctuation">,</span><span class="punctuation">]</span><span class="punctuation">)</span></span><br><span class="line">error<span class="punctuation">[</span>i<span class="punctuation">,</span> k<span class="punctuation">]</span> <span class="operator">&lt;-</span> mean<span class="punctuation">(</span><span class="built_in">abs</span><span class="punctuation">(</span>label<span class="punctuation">[</span>fold_idx<span class="punctuation">]</span> <span class="operator">-</span> pred<span class="punctuation">)</span> <span class="operator">&gt;=</span> <span class="number">0.5</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>在下面的 Python 代码中，我们创建了所有可能的超参数组合，并使用每种组合来拟合和评估模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">idx = np.random.choice(<span class="built_in">range</span>(<span class="number">5</span>), size=<span class="built_in">len</span>(X), replace=<span class="literal">True</span>)</span><br><span class="line">error = []</span><br><span class="line"><span class="keyword">for</span> eta, max_depth <span class="keyword">in</span> product([<span class="number">0.1</span>, <span class="number">0.5</span>, <span class="number">0.9</span>], [<span class="number">3</span>, <span class="number">6</span>, <span class="number">9</span>]):</span><br><span class="line">xgb = XGBClassifier(objective=<span class="string">&#x27;binary:logistic&#x27;</span>, n_estimators=<span class="number">250</span>,</span><br><span class="line">max_depth=max_depth, learning_rate=eta)</span><br><span class="line">cv_error = []</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">fold_idx = idx == k</span><br><span class="line">train_X = X.loc[~fold_idx]; train_y = y[~fold_idx]</span><br><span class="line">valid_X = X.loc[fold_idx]; valid_y = y[fold_idx]</span><br><span class="line">xgb.fit(train_X, train_y)</span><br><span class="line">pred = xgb.predict_proba(valid_X)[:, <span class="number">1</span>]</span><br><span class="line">cv_error.append(np.mean(<span class="built_in">abs</span>(valid_y - pred) &gt; <span class="number">0.5</span>))</span><br><span class="line">error.append(&#123;</span><br><span class="line"><span class="string">&#x27;eta&#x27;</span>: eta,</span><br><span class="line"><span class="string">&#x27;max_depth&#x27;</span>: max_depth,</span><br><span class="line"><span class="string">&#x27;avg_error&#x27;</span>: np.mean(cv_error)</span><br><span class="line">&#125;)</span><br><span class="line"><span class="built_in">print</span>(error[-<span class="number">1</span>])</span><br><span class="line">errors = pd.DataFrame(error)</span><br></pre></td></tr></table></figure><p>我们使用 Python 标准库中的 <code>itertools.product</code> 函数来创建这两个超参数的所有可能组合。</p><p>由于我们总共要拟合45个模型，这可能需要一些时间。误差以矩阵形式存储，行代表模型，列代表折叠。使用 <code>rowMeans</code> 函数，我们可以比较不同参数集的误差率：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">avg_error <span class="operator">&lt;-</span> 100 <span class="operator">*</span> <span class="built_in">round</span><span class="punctuation">(</span>rowMeans<span class="punctuation">(</span>error<span class="punctuation">)</span><span class="punctuation">,</span> <span class="number">4</span><span class="punctuation">)</span></span><br><span class="line">cbind<span class="punctuation">(</span>params<span class="punctuation">,</span> avg_error<span class="punctuation">)</span></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">eta max_depth avg_error</span><br><span class="line">1 0.1 3 32.90</span><br><span class="line">2 0.5 3 33.43</span><br><span class="line">3 0.9 3 34.36</span><br><span class="line">4 0.1 6 33.08</span><br><span class="line">5 0.5 6 35.60</span><br><span class="line">6 0.9 6 37.82</span><br><span class="line">7 0.1 12 34.56</span><br><span class="line">8 0.5 12 36.83</span><br><span class="line">9 0.9 12 38.18</span><br></pre></td></tr></table></figure><p><strong>交叉验证</strong>表明，使用<strong>较浅的树</strong>和<strong>较小的 <code>eta/learning_rate</code> 值</strong>可以得到更准确的结果。由于这些模型也更稳定，因此最佳参数是 <code>eta=0.1</code> 和 <code>max_depth=3</code>（或者可能是 <code>max_depth=6</code>）。</p><p><strong>XGBoost 超参数</strong></p><p><code>xgboost</code> 的超参数主要用于在<strong>准确率</strong>、<strong>计算复杂度</strong>与<strong>过拟合</strong>之间取得平衡。有关参数的完整讨论，请参阅 <code>xgboost</code> 文档。</p><ul><li><p><strong><code>eta</code>/<code>learning_rate</code></strong> 应用于提升算法中 <span class="math inline">\(\alpha\)</span> 的<strong>收缩因子</strong>，取值范围在0到1之间。默认值为0.3，但对于噪声数据，建议使用较小的值（例如0.1）。在 Python 中，默认值为0.1。</p></li><li><p><strong><code>nrounds</code>/<code>n_estimators</code></strong> <strong>提升轮数</strong>。如果 <code>eta</code> 被设置为一个较小的值，增加轮数很重要，因为算法学习得更慢了。只要包含了一些参数来防止过拟合，更多的轮次并不会带来坏处。</p></li><li><p><strong><code>max_depth</code></strong> 树的<strong>最大深度</strong>（默认值为6）。与拟合非常深的树的<strong>随机森林</strong>相反，提升法通常拟合<strong>较浅的树</strong>。这有一个好处，即可以避免因噪声数据而在模型中产生<strong>虚假复杂的交互作用</strong>。在 Python 中，默认值为3。</p></li><li><p><strong><code>subsample</code> 和 <code>colsample_bytree</code></strong> <strong>无放回抽样</strong>的记录比例和用于拟合树的<strong>预测变量抽样</strong>比例。这些参数类似于随机森林中的参数，有助于<strong>避免过拟合</strong>。默认值为1.0。</p></li><li><p><strong><code>lambda</code>/<code>reg_lambda</code> 和 <code>alpha</code>/<code>reg_alpha</code></strong> 用于帮助控制<strong>过拟合</strong>的<strong>正则化参数</strong>（参见第274页的“正则化：避免过拟合”）。Python 的默认值为 <code>reg_lambda=1</code> 和 <code>reg_alpha=0</code>。在 R 中，这两个值的默认值均为0。</p></li></ul><p><strong>关键思想</strong></p><ul><li><strong>提升法</strong>是一类集成模型，它基于拟合一系列模型，并在连续轮次中对具有较大误差的记录赋予更多的权重。</li><li><strong>随机梯度提升</strong>是最通用的提升类型，并提供最佳性能。最常见的随机梯度提升形式使用树模型。</li><li><strong>XGBoost</strong> 是一种流行且计算高效的<strong>随机梯度提升</strong>软件包；它可用于数据科学中所有常用语言。</li><li>提升法<strong>容易过拟合</strong>数据，因此需要<strong>调优超参数</strong>以避免这种情况。</li><li><strong>正则化</strong>是一种避免过拟合的方法，它通过在模型的参数数量（例如，树的大小）上包含一个惩罚项来实现。</li><li>由于需要设置大量的超参数，<strong>交叉验证</strong>对于提升法尤其重要。</li></ul><h3 id="小结"><strong>小结</strong></h3><p>本章描述了两种分类和预测方法，它们<strong>灵活且局部地</strong>从数据中“学习”，而不是像线性回归那样，从一个对整个数据集进行拟合的结构化模型开始。<strong>K-近邻</strong>是一种简单的过程，它通过查看周围相似的记录，并将它们的<strong>多数类别</strong>（或平均值）分配给被预测的记录。<strong>树模型</strong>则通过尝试各种预测变量的截止（分割）值，迭代地将数据划分为<strong>越来越同质</strong>的区域和子区域。最有效的分割值形成一条路径，也形成一条通往分类或预测的“规则”。树模型是一种非常强大且流行的预测工具，通常优于其他方法。它们催生了各种<strong>集成方法</strong>（随机森林、提升法、装袋法），这些方法增强了树的预测能力。</p>]]></content>
      
      
      <categories>
          
          <category> 翻译 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> AI </tag>
            
            <tag> 统计 </tag>
            
            <tag> 数据科学 </tag>
            
            <tag> R </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第5章 分类</title>
      <link href="/2025/09/25/%E7%AC%AC5%E7%AB%A0%20%E5%88%86%E7%B1%BB/"/>
      <url>/2025/09/25/%E7%AC%AC5%E7%AB%A0%20%E5%88%86%E7%B1%BB/</url>
      
        <content type="html"><![CDATA[<blockquote><p>个人注：以下使用gemini翻译 20250916</p></blockquote><p><a href="/img3/面向数据科学家的实用统计学/Practical-Statistics-for-Data-Scientists.pdf">《Practical Statistics for Data Scientists》书籍英文版</a><br /><a href="/img3/面向数据科学家的实用统计学/面向数据科学家的实用统计学.pdf">《面向数据科学家的实用统计学》中文版书籍</a></p><h2 id="第5章-分类"><strong>第5章 分类</strong></h2><p>数据科学家经常需要为商业问题提供自动化决策。一封电子邮件是钓鱼邮件吗？一个客户是否可能流失？一个网络用户是否可能点击广告？这些都是<strong>分类问题</strong>，一种<strong>监督学习</strong>形式。我们首先在已知结果的数据上训练一个模型，然后将该模型应用于结果未知的数据。分类也许是<strong>预测</strong>最重要的形式：其目标是预测一条记录是1还是0（例如，钓鱼/非钓鱼、点击/不点击、流失/不流失），或者在某些情况下，预测它属于几个类别中的一个（例如，Gmail 将你的收件箱过滤为“主要”、“社交”、“推广”或“论坛”）。</p><p>很多时候，我们需要的不仅仅是一个简单的二元分类，我们还想知道一个案例属于某个类别的<strong>预测概率</strong>。大多数算法都可以返回一个属于目标类别的<strong>概率分数（probability score）（倾向性）</strong>（propensity），而不仅仅是简单地分配一个二元分类。事实上，对于<strong>逻辑回归</strong>，R 的默认输出是<strong>对数几率</strong>（log-odds）尺度，这必须被转换为倾向性。在 Python 的 <code>scikit-learn</code> 中，逻辑回归与大多数分类方法一样，提供了两种预测方法：<code>predict</code>（返回类别）和 <code>predict_proba</code>（返回每个类别的概率）。然后，可以使用一个<strong>滑动截止点</strong>（ sliding cutoff）将倾向性分数转换为决策。一般方法如下：</p><ol type="1"><li><strong>设定一个截止概率</strong>：为目标类别设定一个截止概率，如果记录的概率高于这个截止点，我们就认为它属于该类别。</li><li><strong>估算概率</strong>：使用任何模型估算一条记录属于目标类别的概率。</li><li><strong>做出决策</strong>：如果这个概率高于截止概率，则将新记录分配给目标类别。</li></ol><p>截止点越高，被预测为1的记录就越少；截止点越低，被预测为1的记录就越多。</p><p>本章将介绍几种用于分类和估算倾向性的关键技术；下一章将描述既可用于分类也可用于数值预测的其他方法。</p><span id="more"></span><p><strong>多于两个类别的情况</strong>：</p><p>绝大多数问题涉及二元响应。然而，一些分类问题涉及的响应变量可能有多个可能的结果。例如，在客户订阅合同的周年纪念日，可能会有三种结果：客户流失（Y=2）、转为按月合同（Y=1），或签订新的长期合同（Y=0）。目标是预测 <span class="math inline">\(j=0, 1, 2\)</span> 中的 <span class="math inline">\(Y=j\)</span>。本章中的大多数分类方法都可以直接或经过简单调整后应用于具有多于两个结果的响应变量。</p><p>即使在结果多于两个的情况下，问题通常也可以通过使用<strong>条件概率</strong>重新定义为一系列二元问题。例如，为了预测合同结果，你可以解决两个二元预测问题：</p><ul><li>预测 <span class="math inline">\(Y=0\)</span> 还是 <span class="math inline">\(Y&gt;0\)</span>。</li><li>在给定 <span class="math inline">\(Y&gt;0\)</span> 的条件下，预测 <span class="math inline">\(Y=1\)</span> 还是 <span class="math inline">\(Y=2\)</span>。</li></ul><p>在这种情况下，将问题分解为两个案例是有意义的：（1）客户是否流失；（2）如果他们不流失，他们会选择哪种类型的合同。从模型拟合的角度来看，将多类别问题转换为一系列二元问题通常是有利的，当某个类别比其他类别常见得多时，这种方法尤其有效。</p><h3 id="朴素贝叶斯"><strong>朴素贝叶斯</strong></h3><p>Naive Bayes</p><p>朴素贝叶斯算法利用<strong>在给定结果下观察到预测变量的概率</strong>，来估计我们真正感兴趣的：<strong>在给定一组预测变量值下，观察到结果 <span class="math inline">\(Y=i\)</span> 的概率</strong>。</p><p><strong>朴素贝叶斯的关键术语</strong></p><ul><li><p><strong>条件概率（Conditional probability）</strong> 在给定某个其他事件（例如，<span class="math inline">\(Y=i\)</span>）的情况下，观察到某个事件（例如，<span class="math inline">\(X=i\)</span>）的概率，写作 <span class="math inline">\(P(X_i | Y_i)\)</span>。</p></li><li><p><strong>后验概率（Posterior probability）</strong> 在结合了预测变量信息之后，某个结果出现的概率（与不考虑预测变量信息的<strong>先验概率</strong>（prior probability）相反）。</p></li></ul><p>为了理解朴素贝叶斯分类，我们可以从想象<strong>完整或精确的贝叶斯分类</strong>开始。对于每一条要分类的记录：</p><ol type="1"><li>找到所有具有相同<strong>预测变量配置文件</strong>（即，预测变量值完全相同）的其他记录。</li><li>确定这些记录属于哪些类别，并找出<strong>最普遍（即概率最高）的类别</strong>。</li><li>将该类别分配给新记录。</li></ol><p>上述方法相当于在样本中找到所有与待分类的新记录完全相似的记录，其所有预测变量值都相同。</p><blockquote><p><strong>通用注解：</strong></p><p>在标准的朴素贝叶斯算法中，预测变量必须是<strong>分类（因子）变量</strong>。对于如何使用连续变量，请参阅第200页的“数值预测变量”中的两种变通方法。</p></blockquote><h4 id="为什么精确贝叶斯分类不切实际"><strong>为什么精确贝叶斯分类不切实际</strong></h4><p>Why Exact Bayesian Classification Is Impractical</p><p>当预测变量的数量超过少数几个时，许多待分类的记录将找不到精确匹配。考虑一个基于人口统计变量来预测投票的模型。即使是一个相当大的样本，也可能找不到一个完全匹配的新记录，例如：一个来自美国中西部的、高收入的、男性、西班牙裔，在上次选举中投了票，但在上上次选举中没有投票，有三个女儿和一个儿子，并且离了婚。这还只有八个变量，对于大多数分类问题来说，这数量非常小。如果只增加一个有五个同样频繁的类别的新变量，匹配的概率就会<strong>降低五倍</strong>。</p><h4 id="朴素解法">朴素解法</h4><p>The Naive Solution</p><p>在<strong>朴素贝叶斯</strong>解法中，我们不再将概率计算局限于与待分类记录完全匹配的记录。相反，我们使用<strong>整个数据集</strong>。朴素贝叶斯算法的步骤如下：</p><ol type="1"><li><p>对于一个二元响应 <span class="math inline">\(Y=i\)</span>（<span class="math inline">\(i=0\)</span> 或 <span class="math inline">\(1\)</span>），估计每个预测变量的<strong>个体条件概率</strong> <span class="math inline">\(P(X_j | Y=i)\)</span>；这些是在观察到 <span class="math inline">\(Y=i\)</span> 时，预测变量值出现在记录中的概率。该概率通过训练集中属于 <span class="math inline">\(Y=i\)</span> 记录的 <span class="math inline">\(X_j\)</span> 值的比例来估计。</p></li><li><p>将这些概率相乘，然后乘以属于 <span class="math inline">\(Y=i\)</span> 的记录比例。</p></li><li><p>为所有类别重复步骤1和2。</p></li><li><p>通过将步骤2为类别 <span class="math inline">\(i\)</span> 计算出的值除以所有类别此类值的总和，来估计类别 <span class="math inline">\(i\)</span> 的概率。</p></li><li><p>将该记录分配给在此组预测变量值下具有最高概率的类别。</p></li></ol><p>这个朴素贝叶斯算法也可以用方程来表示，用于计算在给定一组预测变量 <span class="math inline">\(X_1, \dots, X_p\)</span> 时，观察到结果 <span class="math inline">\(Y=i\)</span> 的概率：</p><p><span class="math display">\[P(Y=i | X_1, X_2, \dots, X_p)\]</span> 以下是使用精确贝叶斯分类计算类别概率的完整公式：</p><p><span class="math display">\[P(Y=i | X_1, X_2, \dots, X_p) = \frac{P(Y=i)P(X_1, \dots, X_p | Y=i)}{P(Y=0)P(X_1, \dots, X_p | Y=0) + P(Y=1)P(X_1, \dots, X_p | Y=1)}\]</span></p><p>在<strong>朴素贝叶斯条件独立性</strong>的假设下，这个方程变为：</p><p><span class="math display">\[P(Y=i | X_1, X_2, \dots, X_p) = \frac{P(Y=i)P(X_1 | Y=i) \dots P(X_p | Y=i)}{P(Y=0)P(X_1 | Y=0) \dots P(X_p | Y=0) + P(Y=1)P(X_1 | Y=1) \dots P(X_p | Y=1)}\]</span></p><p><strong>为什么这个公式被称为“朴素”？</strong>我们做了一个简化的假设：<strong>在观察到某个结果时，预测变量向量的精确条件概率，可以由个体条件概率的乘积来很好地估计</strong>。换句话说，在估计 <span class="math inline">\(P(X_j | Y=i)\)</span> 而不是 <span class="math inline">\(P(X_1, X_2, \dots, X_p | Y=i)\)</span> 时，我们假设 <span class="math inline">\(X_j\)</span> <strong>独立于</strong>所有其他预测变量 <span class="math inline">\(X_k\)</span>（<span class="math inline">\(k \neq j\)</span>）。</p><p>在 R 语言中，可以使用几个包来估计朴素贝叶斯模型。以下代码使用 <code>klaR</code> 包对贷款支付数据进行模型拟合：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">library<span class="punctuation">(</span>klaR<span class="punctuation">)</span></span><br><span class="line">naive_model <span class="operator">&lt;-</span> NaiveBayes<span class="punctuation">(</span>outcome <span class="operator">~</span> purpose_ <span class="operator">+</span> home_ <span class="operator">+</span> emp_len_<span class="punctuation">,</span></span><br><span class="line">data <span class="operator">=</span> na.omit<span class="punctuation">(</span>loan_data<span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">naive_model<span class="operator">$</span>table</span><br></pre></td></tr></table></figure><p>模型的输出是条件概率 <span class="math inline">\(P(X_j | Y=i)\)</span>。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">$purpose_</span><br><span class="line">var</span><br><span class="line">grouping credit_card debt_consolidation home_improvement major_purchase</span><br><span class="line">paid off 0.18759649 0.55215915 0.07150104 0.05359270</span><br><span class="line">default 0.15151515 0.57571347 0.05981209 0.03727229</span><br><span class="line">var</span><br><span class="line">grouping medical other small_business</span><br><span class="line">paid off 0.01424728 0.09990737 0.02099599</span><br><span class="line">default 0.01433549 0.11561025 0.04574126</span><br><span class="line">$home_</span><br><span class="line">var</span><br><span class="line">grouping MORTGAGE OWN RENT</span><br><span class="line">paid off 0.4894800 0.0808963 0.4296237</span><br><span class="line">default 0.4313440 0.0832782 0.4853778</span><br><span class="line">$emp_len_</span><br><span class="line">var</span><br><span class="line">grouping &lt; 1 Year &gt; 1 Year</span><br><span class="line">paid off 0.03105289 0.96894711</span><br><span class="line">default 0.04728508 0.95271492</span><br></pre></td></tr></table></figure><p>在 Python 中，我们可以使用 <code>scikit-learn</code> 中的 <code>sklearn.naive_bayes.MultinomialNB</code>。在拟合模型之前，我们需要将分类特征转换为虚拟变量：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">predictors = [<span class="string">&#x27;purpose_&#x27;</span>, <span class="string">&#x27;home_&#x27;</span>, <span class="string">&#x27;emp_len_&#x27;</span>]</span><br><span class="line">outcome = <span class="string">&#x27;outcome&#x27;</span></span><br><span class="line">X = pd.get_dummies(loan_data[predictors], prefix=</span><br><span class="line">y = loan_data[outcome]</span><br><span class="line"><span class="string">&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#x27;&#x27;</span>)</span><br><span class="line">naive_model = MultinomialNB(alpha=<span class="number">0.01</span>, fit_prior=<span class="literal">True</span>)</span><br><span class="line">naive_model.fit(X, y)</span><br></pre></td></tr></table></figure><p>可以通过 <code>feature_log_prob_</code> 属性从拟合模型中获得条件概率。</p><p>该模型可用于预测一笔新贷款的结果。我们使用数据集的最后一条记录进行测试：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">new_loan <span class="operator">&lt;-</span> loan_data<span class="punctuation">[</span><span class="number">147</span><span class="punctuation">,</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;purpose_&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;home_&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;emp_len_&#x27;</span><span class="punctuation">)</span><span class="punctuation">]</span></span><br><span class="line">row.names<span class="punctuation">(</span>new_loan<span class="punctuation">)</span> <span class="operator">&lt;-</span> <span class="literal">NULL</span></span><br><span class="line">new_loan</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">purpose_ home_ emp_len_</span><br><span class="line">1 small_business MORTGAGE &gt; 1 Year</span><br></pre></td></tr></table></figure><p>在 Python 中，我们按如下方式获取此值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">new_loan = X.loc[<span class="number">146</span>:<span class="number">146</span>, :]</span><br></pre></td></tr></table></figure><p>在这种情况下，模型预测为<strong>违约</strong>（R）：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predict<span class="punctuation">(</span>naive_model<span class="punctuation">,</span> new_loan<span class="punctuation">)</span></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$class</span><br><span class="line">[1] default</span><br><span class="line">Levels: paid off default</span><br><span class="line">$posterior</span><br><span class="line">paid off default</span><br><span class="line">[1,] 0.3463013 0.6536987</span><br></pre></td></tr></table></figure><p>正如我们所讨论的，<code>scikit-learn</code> 的分类模型有两个方法：<code>predict</code>，返回预测的类别；以及 <code>predict_proba</code>，返回类别概率：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;predicted class: &#x27;</span>, naive_model.predict(new_loan)[<span class="number">0</span>])</span><br><span class="line">probabilities = pd.DataFrame(naive_model.predict_proba(new_loan),</span><br><span class="line">columns=loan_data[outcome].cat.categories)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;predicted probabilities&#x27;</span>, probabilities)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">predicted class: default</span><br><span class="line">predicted probabilities</span><br><span class="line">   default  paid off</span><br><span class="line">0  0.653696  0.346304</span><br></pre></td></tr></table></figure><p>预测还返回了违约概率的后验估计。众所周知，朴素贝叶斯分类器会产生<strong>有偏的估计</strong>（biased estimates.）。然而，当目标是根据 <span class="math inline">\(Y=1\)</span> 的概率对记录进行<strong>排序</strong>时，不需要无偏的概率估计，并且朴素贝叶斯会产生良好的结果。</p><h4 id="数值预测变量"><strong>数值预测变量</strong></h4><p>Numeric Predictor Variables</p><p>贝叶斯分类器只适用于<strong>分类（因子）预测变量</strong>，例如在垃圾邮件分类中，预测任务的核心在于单词、短语、字符等的出现与否。如果想将朴素贝叶斯应用于<strong>数值预测变量</strong>，必须采取以下两种方法之一：</p><ul><li><strong>分箱和转换</strong>：将数值预测变量分箱并转换为分类预测变量，然后应用上一节介绍的算法。</li><li><strong>使用概率模型</strong>：例如，使用<strong>正态分布</strong>（参见第69页的“正态分布”）来估计条件概率 <span class="math inline">\(P(X_j | Y=i)\)</span>。</li></ul><blockquote><p><strong>警告：</strong></p><p>当训练数据中某个预测变量类别<strong>不存在</strong>时，算法会将新数据中相应结果变量的概率赋值为零，而不是像其他方法那样简单地忽略此变量并利用其他变量的信息。大多数朴素贝叶斯的实现都使用一个<strong>平滑参数</strong>（拉普拉斯平滑）来防止这种情况发生。</p></blockquote><p><strong>关键思想</strong></p><ul><li><strong>适用范围</strong>：朴素贝叶斯适用于分类（因子）预测变量和结果。</li><li><strong>核心问题</strong>：它会问，“在每个结果类别中，哪个预测变量类别最有可能出现？”</li><li><strong>反向推断</strong>：然后，该信息被反向用于估计在给定预测变量值的情况下，结果类别的概率。</li></ul><h3 id="判别分析"><strong>判别分析</strong></h3><p>Discriminant Analysis</p><p>判别分析是最早出现的统计分类器，由R. A. Fisher于1936年在《优生学年鉴》杂志上发表的一篇文章中提出。</p><p><strong>判别分析的关键术语</strong></p><ul><li><p><strong>协方差 (Covariance)</strong> 衡量一个变量与另一个变量<strong>共同变化</strong>的程度（即，相似的大小和方向）。</p></li><li><p><strong>判别函数 (Discriminant function)</strong> 应用于预测变量时，能<strong>最大化类别间分离度</strong>的函数。</p></li><li><p><strong>判别权重 (Discriminant weights)</strong> 应用判别函数后得到的分数，用于估计记录属于某个类别的概率。</p></li></ul><p>尽管判别分析涵盖了多种技术，但最常用的是<strong>线性判别分析（Linear Discriminant Analysis, LDA）</strong>。Fisher最初提出的方法与今天的LDA略有不同，但其机制基本相同。随着树模型和逻辑回归等更复杂技术的出现，LDA的应用现在已不如从前广泛。</p><p>然而，在某些应用中你仍然可能遇到LDA，而且它与其他更广泛使用的方法（如<strong>主成分分析</strong>；参见第284页的“主成分分析”）存在联系。</p><blockquote><p><strong>警告：</strong></p><p>线性判别分析（LDA）不应与<strong>潜在狄利克雷分配（Latent Dirichlet Allocation, LDA）</strong>混淆。后者用于文本和自然语言处理，与线性判别分析无关。</p></blockquote><h4 id="协方差矩阵"><strong>协方差矩阵</strong></h4><p>Covariance Matrix</p><p>要理解判别分析，首先有必要引入两个或多个变量之间的<strong>协方差</strong>概念。协方差衡量了两个变量 <code>x</code> 和 <code>z</code> 之间的关系。用 <span class="math inline">\(\bar{x}\)</span> 和 <span class="math inline">\(\bar{z}\)</span> 表示每个变量的均值（参见第9页的“均值”）。<span class="math inline">\(x\)</span> 和 <span class="math inline">\(z\)</span> 之间的协方差 <span class="math inline">\(s_{x, z}\)</span> 由以下公式给出：</p><p><span class="math display">\[s_{x, z} = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(z_i - \bar{z})}{n - 1}\]</span></p><p>其中，<span class="math inline">\(n\)</span> 是记录的数量（注意我们用 <span class="math inline">\(n-1\)</span> 而不是 <span class="math inline">\(n\)</span> 来除；参见第15页的“自由度，以及 <span class="math inline">\(n\)</span> 还是 <span class="math inline">\(n-1\)</span>？”）。</p><p>与<strong>相关系数</strong>（参见第30页的“相关性”）一样，正值表示正向关系，负值表示负向关系。然而，相关系数被限制在-1和1之间，而协方差的尺度取决于变量 <span class="math inline">\(x\)</span> 和 <span class="math inline">\(z\)</span> 的尺度。由 <span class="math inline">\(x\)</span> 和 <span class="math inline">\(z\)</span> 组成的<strong>协方差矩阵</strong> <span class="math inline">\(\Sigma\)</span> 的对角线上（行和列是同一变量）是各个变量的方差 <span class="math inline">\(s_x^2\)</span> 和 <span class="math inline">\(s_z^2\)</span>，非对角线上是变量对之间的协方差：</p><p><span class="math display">\[\Sigma = \begin{pmatrix} s_x^2 &amp; s_{x,z} \\ s_{z,x} &amp; s_z^2 \end{pmatrix}\]</span></p><blockquote><p><strong>通用注解：</strong></p><p>回想一下，<strong>标准差</strong>用于将一个变量标准化为<strong>z分数</strong>；协方差矩阵则用于这种标准化过程的<strong>多元扩展</strong>。这被称为<strong>马哈拉诺比斯距离</strong>（Mahalanobis distance）（参见第242页的“其他距离度量”），并与 LDA 函数相关。</p></blockquote><h4 id="费舍尔线性判别"><strong>费舍尔线性判别</strong></h4><p>Fisher’s Linear Discriminant</p><p>为了简化，我们关注一个分类问题：使用两个连续数值变量 <span class="math inline">\(x\)</span> 和 <span class="math inline">\(z\)</span> 来预测一个二元结果 <span class="math inline">\(y\)</span>。从技术上讲，判别分析假定预测变量是正态分布的连续变量，但实际上，即使在非极端的非正态性或二元预测变量情况下，该方法也表现良好。费舍尔线性判别将<strong>组间变异</strong>与<strong>组内变异</strong>区分开来。具体来说，为了将记录分为两组，线性判别分析（LDA）侧重于最大化“组间”平方和 <span class="math inline">\(SS_{between}\)</span>（衡量两组之间的变异）相对于“组内”平方和 <span class="math inline">\(SS_{within}\)</span>（衡量组内变异）的比率。在这里，这两组对应于 <span class="math inline">\(y=0\)</span> 的记录（<span class="math inline">\(x_0, z_0\)</span>）和 <span class="math inline">\(y=1\)</span> 的记录（<span class="math inline">\(x_1, z_1\)</span>）。</p><p>该方法找到能够最大化平方和比率的线性组合 <span class="math inline">\(w_x x + w_z z\)</span>：</p><p><span class="math display">\[\frac{SS_{between}}{SS_{within}}\]</span></p><p><strong>组间平方和</strong>是两个组均值之间的平方距离，而<strong>组内平方和</strong>是每组内部围绕均值的扩散程度，并由协方差矩阵加权。直观地，通过最大化组间平方和并最小化组内平方和，该方法实现了两组之间最大的分离度。</p><h4 id="一个简单的例子"><strong>一个简单的例子</strong></h4><p>与书籍《Modern Applied Statistics with S》（作者：W. N. Venables 和 B. D. Ripley，1994年，Springer出版社）相关的 <code>MASS</code> 包提供了在 R 中进行 LDA 的函数。以下代码将该函数应用于一个贷款样本数据，使用了两个预测变量 <code>borrower_score</code> 和 <code>payment_inc_ratio</code>，并打印出估计的线性判别权重：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">library<span class="punctuation">(</span>MASS<span class="punctuation">)</span></span><br><span class="line">loan_lda <span class="operator">&lt;-</span> lda<span class="punctuation">(</span>outcome <span class="operator">~</span> borrower_score <span class="operator">+</span> payment_inc_ratio<span class="punctuation">,</span></span><br><span class="line">data<span class="operator">=</span>loan3000<span class="punctuation">)</span></span><br><span class="line">loan_lda<span class="operator">$</span>scaling</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">LD1</span><br><span class="line">borrower_score     7.17583880</span><br><span class="line">payment_inc_ratio -0.09967559</span><br></pre></td></tr></table></figure><p>在 Python 中，我们可以使用 <code>sklearn.discriminant_analysis</code> 中的 <code>LinearDiscriminantAnalysis</code>。<code>scalings_</code> 属性给出了估计的权重：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">loan3000.outcome = loan3000.outcome.astype(<span class="string">&#x27;category&#x27;</span>)</span><br><span class="line">predictors = [<span class="string">&#x27;borrower_score&#x27;</span>, <span class="string">&#x27;payment_inc_ratio&#x27;</span>]</span><br><span class="line">outcome = <span class="string">&#x27;outcome&#x27;</span></span><br><span class="line">X = loan3000[predictors]</span><br><span class="line">y = loan3000[outcome]</span><br><span class="line">loan_lda = LinearDiscriminantAnalysis()</span><br><span class="line">loan_lda.fit(X, y)</span><br><span class="line">pd.DataFrame(loan_lda.scalings_, index=X.columns)</span><br></pre></td></tr></table></figure><blockquote><p><strong>通用注解：</strong></p><p><strong>使用判别分析进行特征选择</strong>：如果在运行 LDA 之前对预测变量进行标准化，那么判别权重可以作为<strong>变量重要性</strong>的度量，从而提供一种计算高效的<strong>特征选择</strong>方法。</p></blockquote><p><code>lda</code> 函数可以预测“违约”与“已还清”的概率：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pred <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>loan_lda<span class="punctuation">)</span></span><br><span class="line">head<span class="punctuation">(</span>pred<span class="operator">$</span>posterior<span class="punctuation">)</span></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">paid off  default</span><br><span class="line">1 0.4464563 0.5535437</span><br><span class="line">2 0.4410466 0.5589534</span><br><span class="line">3 0.7273038 0.2726962</span><br><span class="line">4 0.4937462 0.5062538</span><br><span class="line">5 0.3900475 0.6099525</span><br><span class="line">6 0.5892594 0.4107406</span><br></pre></td></tr></table></figure><p>拟合模型的 <code>predict_proba</code> 方法返回“违约”和“已还清”结果的概率：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pred = pd.DataFrame(loan_lda.predict_proba(loan3000[predictors]),</span><br><span class="line">columns=loan_lda.classes_)</span><br><span class="line">pred.head()</span><br></pre></td></tr></table></figure><p>绘制预测结果的图表有助于说明 LDA 的工作原理。使用 <code>predict</code> 函数的输出，可以按如下方式生成违约概率的估计图：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">center <span class="operator">&lt;-</span> 0.5 <span class="operator">*</span> <span class="punctuation">(</span>loan_lda<span class="operator">$</span>mean<span class="punctuation">[</span><span class="number">1</span><span class="punctuation">,</span> <span class="punctuation">]</span> <span class="operator">+</span> loan_lda<span class="operator">$</span>mean<span class="punctuation">[</span><span class="number">2</span><span class="punctuation">,</span> <span class="punctuation">]</span><span class="punctuation">)</span></span><br><span class="line">slope <span class="operator">&lt;-</span> <span class="operator">-</span>loan_lda<span class="operator">$</span>scaling<span class="punctuation">[</span><span class="number">1</span><span class="punctuation">]</span> <span class="operator">/</span> loan_lda<span class="operator">$</span>scaling<span class="punctuation">[</span><span class="number">2</span><span class="punctuation">]</span></span><br><span class="line">intercept <span class="operator">&lt;-</span> center<span class="punctuation">[</span><span class="number">2</span><span class="punctuation">]</span> <span class="operator">-</span> center<span class="punctuation">[</span><span class="number">1</span><span class="punctuation">]</span> <span class="operator">*</span> slope</span><br><span class="line">ggplot<span class="punctuation">(</span>data<span class="operator">=</span>lda_df<span class="punctuation">,</span> aes<span class="punctuation">(</span>x<span class="operator">=</span>borrower_score<span class="punctuation">,</span> y<span class="operator">=</span>payment_inc_ratio<span class="punctuation">,</span></span><br><span class="line">color<span class="operator">=</span>prob_default<span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">geom_point<span class="punctuation">(</span>alpha<span class="operator">=</span><span class="number">.6</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">scale_color_gradientn<span class="punctuation">(</span>colors<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;#ca0020&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;#f7f7f7&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;#0571b0&#x27;</span><span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">scale_x_continuous<span class="punctuation">(</span>expand<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">0</span><span class="punctuation">,</span><span class="number">0</span><span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">scale_y_continuous<span class="punctuation">(</span>expand<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">0</span><span class="punctuation">,</span><span class="number">0</span><span class="punctuation">)</span><span class="punctuation">,</span> lim<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">0</span><span class="punctuation">,</span> <span class="number">20</span><span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">geom_abline<span class="punctuation">(</span>slope<span class="operator">=</span>slope<span class="punctuation">,</span> intercept<span class="operator">=</span>intercept<span class="punctuation">,</span> color<span class="operator">=</span><span class="string">&#x27;darkgreen&#x27;</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>在 Python 中，使用以下代码创建类似的图：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Use scalings and center of means to determine decision boundary</span></span><br><span class="line">center = np.mean(loan_lda.means_, axis=<span class="number">0</span>)</span><br><span class="line">slope = - loan_lda.scalings_[<span class="number">0</span>] / loan_lda.scalings_[<span class="number">1</span>]</span><br><span class="line">intercept = center[<span class="number">1</span>] - center[<span class="number">0</span>] * slope</span><br><span class="line"><span class="comment"># payment_inc_ratio for borrower_score of 0 and 20</span></span><br><span class="line">x_0 = (<span class="number">0</span> - intercept) / slope</span><br><span class="line">x_20 = (<span class="number">20</span> - intercept) / slope</span><br><span class="line">lda_df = pd.concat([loan3000, pred[<span class="string">&#x27;default&#x27;</span>]], axis=<span class="number">1</span>)</span><br><span class="line">lda_df.head()</span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">g = sns.scatterplot(x=<span class="string">&#x27;borrower_score&#x27;</span>, y=<span class="string">&#x27;payment_inc_ratio&#x27;</span>,</span><br><span class="line">hue=<span class="string">&#x27;default&#x27;</span>, data=lda_df,</span><br><span class="line">palette=sns.diverging_palette(<span class="number">240</span>, <span class="number">10</span>, n=<span class="number">9</span>, as_cmap=<span class="literal">True</span>),</span><br><span class="line">ax=ax, legend=<span class="literal">False</span>)</span><br><span class="line">ax.set_ylim(<span class="number">0</span>, <span class="number">20</span>)</span><br><span class="line">ax.set_xlim(<span class="number">0.15</span>, <span class="number">0.8</span>)</span><br><span class="line">ax.plot((x_0, x_20), (<span class="number">0</span>, <span class="number">20</span>), linewidth=<span class="number">3</span>)</span><br><span class="line">ax.plot(*loan_lda.means_.transpose())</span><br></pre></td></tr></table></figure><p>由此产生的图表如图5-1所示。对角线左侧的数据点被预测为违约（概率大于0.5）。</p><p><img src="/img3/面向数据科学家的实用统计学/F5.1.png" alt="F5.1" style="zoom:50%;" /></p><p>使用判别函数权重，LDA 将预测变量空间分为两个区域，如图中的实线所示。离这条线越远（两个方向上），预测的置信度越高（即，概率越远离0.5）。</p><blockquote><p>通用注解：</p><p><strong>判别分析的扩展</strong>Extensions of Discriminant Analysis<strong>更多预测变量</strong>：尽管本节的文字和示例只使用了两个预测变量，但 LDA 对于多于两个预测变量的情况同样有效。唯一的限制因素是<strong>记录的数量</strong>（估计协方差矩阵需要每个变量有足够的记录，这在数据科学应用中通常不是问题）。</p><p>判别分析还有其他变体。其中最著名的是<strong>二次判别分析（Quadratic Discriminant Analysis, QDA）</strong>。尽管其名称如此，QDA 仍然是一种线性判别函数。主要区别在于，在 LDA 中，我们假定对应于 <span class="math inline">\(Y=0\)</span> 和 <span class="math inline">\(Y=1\)</span> 的两组拥有<strong>相同的协方差矩阵</strong>。而在 QDA 中，允许这两组拥有<strong>不同的协方差矩阵</strong>。在实践中，这种差异在大多数应用中并不重要。</p></blockquote><p><strong>关键思想</strong></p><ul><li><strong>适用范围</strong>：判别分析适用于连续或分类预测变量，以及分类结果。</li><li><strong>工作原理</strong>：它使用<strong>协方差矩阵</strong>计算一个<strong>线性判别函数</strong>，该函数用于区分属于一个类别的记录和属于另一个类别的记录。</li><li><strong>结果</strong>：该函数应用于每条记录，以推导出<strong>权重或分数</strong>（每个可能的类别一个），从而确定其估计的类别。</li></ul><h3 id="逻辑回归"><strong>逻辑回归</strong></h3><p>Logistic Regression</p><p>逻辑回归与多元线性回归（参见第4章）类似，但其<strong>结果是二元的</strong>。它采用各种转换方法，将问题转化为一个可以拟合线性模型的形式。与K-最近邻和朴素贝叶斯不同，逻辑回归是一种<strong>结构化模型方法</strong>（a structured model approach），而非以数据为中心的方法（a data-centric approach）。由于其计算速度快，且输出的模型有助于快速对新数据进行评分，因此它是一种非常流行的方法。</p><p><strong>逻辑回归的关键术语</strong></p><ul><li><p><strong>Logit（逻辑函数）</strong> 将类别成员资格的概率映射到 <span class="math inline">\(\pm \infty\)</span> 范围（而不是0到1）的函数。 同义词：<strong>对数几率</strong>（见下文）</p></li><li><p><strong>Odds（几率）</strong> “成功”（1）与“不成功”（0）的比率。</p></li><li><p><strong>Log odds（对数几率）</strong> 转换后模型中的响应变量（现在是线性的），该值可以被映射回概率。</p></li></ul><h4 id="逻辑响应函数与-logit"><strong>逻辑响应函数与 Logit</strong></h4><p>Logistic Response Function and Logit</p><p>逻辑回归的关键要素是<strong>逻辑响应函数</strong>和 <strong>logit</strong>，通过它们我们将概率（在0-1的尺度上）映射到一个更广的、适合线性建模的尺度。</p><p>第一步是将结果变量视为<strong>标签为“1”的概率 <span class="math inline">\(p\)</span></strong>。直观地，我们可能会试图将 <span class="math inline">\(p\)</span> 建模为预测变量的线性函数：</p><p><span class="math display">\[p = \beta_0 + \beta_1x_1 + \beta_2x_2 + \dots + \beta_qx_q\]</span> 然而，拟合这个模型并不能确保 <span class="math inline">\(p\)</span> 最终会落在0到1之间，而概率必须如此。</p><p>相反，我们通过对预测变量应用<strong>逻辑响应</strong>或<strong>逆 Logit 函数</strong>来建模 <span class="math inline">\(p\)</span>：</p><p><span class="math display">\[p = \frac{1}{1 + e^{-(\beta_0 + \beta_1x_1 + \beta_2x_2 + \dots + \beta_qx_q)}}\]</span> 这个转换确保了 <span class="math inline">\(p\)</span> 始终在0到1之间。</p><p>为了摆脱分母中的指数表达式，我们考虑<strong>几率（Odds）</strong>而非概率。几率是“成功”（1）与“不成功”（0）的比率，这对于任何地方的押注者来说都很熟悉。用概率表示，几率是一个事件发生的概率除以该事件不发生的概率。例如，如果一匹马获胜的概率是0.5，那么“不会获胜”的概率是 (1-0.5)=0.5，几率就是1.0：</p><p><span class="math display">\[\text{Odds}(Y=1) = \frac{p}{1-p}\]</span> 我们可以使用<strong>逆几率函数</strong>从几率中获得概率：</p><p><span class="math display">\[p = \frac{\text{Odds}}{1 + \text{Odds}}\]</span> 我们将此与前面展示的逻辑响应函数结合起来，得到：</p><p><span class="math display">\[\text{Odds}(Y=1) = e^{\beta_0 + \beta_1x_1 + \beta_2x_2 + \dots + \beta_qx_q}\]</span> 最后，对两边取对数，我们得到一个涉及预测变量线性函数的表达式：</p><p><span class="math display">\[\log(\text{Odds}(Y=1)) = \beta_0 + \beta_1x_1 + \beta_2x_2 + \dots + \beta_qx_q\]</span> <strong>对数几率函数</strong>，也称为 <strong>logit 函数</strong>，将概率 <span class="math inline">\(p\)</span> 从 [0, 1] 映射到任意值 <span class="math inline">\([-\infty, +\infty]\)</span>（见图5-2）。转换的循环完成；我们使用一个<strong>线性模型来预测一个概率</strong>，然后我们可以通过应用一个截止规则，将该概率映射回一个<strong>类别标签</strong>——任何概率大于截止点的记录都被归类为1。</p><p><img src="/img3/面向数据科学家的实用统计学/F5.2.png" alt="F5.2" style="zoom:33%;" /></p><h4 id="逻辑回归与广义线性模型-glm"><strong>逻辑回归与广义线性模型 (GLM)</strong></h4><p>Logistic Regression and the GLM</p><p>逻辑回归公式中的响应变量是二元结果为1的<strong>对数几率</strong>。我们只观察到二元结果本身，而不是对数几率，因此需要特殊的统计方法来拟合方程。逻辑回归是广义线性模型（GLM）的一个特例，该模型旨在将线性回归扩展到其他情境。</p><p>在 R 中，要拟合逻辑回归，需要使用 <code>glm</code> 函数并将 <code>family</code> 参数设置为 <code>binomial</code>。以下代码使用第238页“K-最近邻”中引入的个人贷款数据来拟合逻辑回归：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">logistic_model <span class="operator">&lt;-</span> glm<span class="punctuation">(</span>outcome <span class="operator">~</span> payment_inc_ratio <span class="operator">+</span> purpose_ <span class="operator">+</span></span><br><span class="line">home_ <span class="operator">+</span> emp_len_ <span class="operator">+</span> borrower_score<span class="punctuation">,</span></span><br><span class="line">data<span class="operator">=</span>loan_data<span class="punctuation">,</span> family<span class="operator">=</span><span class="string">&#x27;binomial&#x27;</span><span class="punctuation">)</span></span><br><span class="line">logistic_model</span><br><span class="line">Call<span class="operator">:</span> glm<span class="punctuation">(</span>formula <span class="operator">=</span> outcome <span class="operator">~</span> payment_inc_ratio <span class="operator">+</span> purpose_ <span class="operator">+</span> home_ <span class="operator">+</span></span><br><span class="line">emp_len_ <span class="operator">+</span> borrower_score<span class="punctuation">,</span> family <span class="operator">=</span> <span class="string">&quot;binomial&quot;</span><span class="punctuation">,</span> data <span class="operator">=</span> loan_data<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line">Coefficients<span class="operator">:</span></span><br><span class="line"><span class="punctuation">(</span>Intercept<span class="punctuation">)</span> payment_inc_ratio</span><br><span class="line"><span class="number">1.63809</span> <span class="number">0.07974</span></span><br><span class="line">purpose_debt_consolidation purpose_home_improvement</span><br><span class="line"><span class="number">0.24937</span> <span class="number">0.40774</span></span><br><span class="line">purpose_major_purchase purpose_medical</span><br><span class="line"><span class="number">0.22963</span> <span class="number">0.51048</span></span><br><span class="line">purpose_other purpose_small_business</span><br><span class="line"><span class="number">0.62066</span> <span class="number">1.21526</span></span><br><span class="line">home_OWN home_RENT</span><br><span class="line"><span class="number">0.04833</span> <span class="number">0.15732</span></span><br><span class="line">emp_len_ <span class="operator">&gt;</span> <span class="number">1</span> Year borrower_score</span><br><span class="line"><span class="operator">-</span><span class="number">0.35673</span> <span class="operator">-</span><span class="number">4.61264</span></span><br><span class="line">Degrees of Freedom<span class="operator">:</span> <span class="number">45341</span> Total <span class="punctuation">(</span>i.e. Null<span class="punctuation">)</span>; <span class="number">45330</span> Residual</span><br><span class="line">Null Deviance<span class="operator">:</span> <span class="number">62860</span></span><br><span class="line">Residual Deviance<span class="operator">:</span> <span class="number">57510</span> AIC<span class="operator">:</span> <span class="number">57540</span></span><br></pre></td></tr></table></figure><p>响应变量是 <code>outcome</code>，如果贷款已还清则为0，如果违约则为1。<code>purpose_</code> 和 <code>home_</code> 是因子变量，分别代表贷款目的和房屋所有权状态。与线性回归一样，一个有 P 个水平的因子变量用 P-1 列来表示。在 R 中，默认使用<strong>参考编码</strong>，所有水平都与参考水平进行比较（参见第163页的“回归中的因子变量”）。这些因子的参考水平分别是 <code>credit_card</code> 和 <code>MORTGAGE</code>。<code>borrower_score</code> 变量是一个从0到1的分数，代表借款人的信用度（从差到优秀）。这个变量是使用K-最近邻从其他几个变量创建的——参见第247页的“KNN 作为特征工程”。</p><p>在 Python 中，我们使用 <code>sklearn.linear_model</code> 中的 <code>LogisticRegression</code> 类。<code>penalty</code> 和 <code>C</code> 参数用于通过 <strong>L1 或 L2 正则化</strong>防止过拟合。正则化默认是开启的。为了在不进行正则化的情况下拟合，我们将 <code>C</code> 设置为一个非常大的值。<code>solver</code> 参数选择使用的优化器；<code>liblinear</code> 方法是默认的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">predictors = [<span class="string">&#x27;payment_inc_ratio&#x27;</span>, <span class="string">&#x27;purpose_&#x27;</span>, <span class="string">&#x27;home_&#x27;</span>, <span class="string">&#x27;emp_len_&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;borrower_score&#x27;</span>]</span><br><span class="line">outcome = <span class="string">&#x27;outcome&#x27;</span></span><br><span class="line">X = pd.get_dummies(loan_data[predictors], prefix=</span><br><span class="line"><span class="string">&#x27;&#x27;</span></span><br><span class="line">, prefix_sep=</span><br><span class="line"><span class="string">&#x27;&#x27;</span></span><br><span class="line">,</span><br><span class="line">drop_first=<span class="literal">True</span>)</span><br><span class="line">y = loan_data[outcome]</span><br><span class="line">logit_reg = LogisticRegression(penalty=<span class="string">&#x27;l2&#x27;</span>, C=<span class="number">1e42</span>, solver=<span class="string">&#x27;liblinear&#x27;</span>)</span><br><span class="line">logit_reg.fit(X, y)</span><br></pre></td></tr></table></figure><p>与 R 不同，<code>scikit-learn</code> 从 <code>y</code> 中的唯一值（<code>paid off</code> 和 <code>default</code>）派生出类别。在内部，这些类别按字母顺序排序。由于这与 R 中使用的因子顺序相反，你会发现系数是相反的。<code>predict</code> 方法返回类别标签，而 <code>predict_proba</code> 返回从 <code>logit_reg.classes_</code> 属性中可用的顺序的概率。</p><h4 id="广义线性模型"><strong>广义线性模型</strong></h4><p>Generalized Linear Models</p><p>广义线性模型（Generalized Linear Models, GLMs）由两个主要部分构成：</p><ul><li><strong>一个概率分布或族</strong>：在逻辑回归中是二项分布。</li><li><strong>一个链接函数</strong>：将响应变量映射到预测变量的转换函数，在逻辑回归中是 <strong>logit</strong>。</li></ul><p>逻辑回归是迄今为止最常见的 GLM 形式。数据科学家也会遇到其他类型的 GLM。有时会使用 <strong>log 链接函数</strong>而不是 logit；在实践中，对于大多数应用，使用 log 链接函数不太可能导致非常不同的结果。<strong>泊松分布</strong>通常用于建模计数数据（例如，一个用户在特定时间内访问网页的次数）。其他族包括<strong>负二项分布</strong>和<strong>伽马分布</strong>，它们常用于建模经过的时间（例如，到故障的时间）。与逻辑回归相比，使用这些模型的 GLM 应用更为细致，需要更谨慎。除非你熟悉并理解这些方法的效用和陷阱，否则最好避免使用它们。</p><h4 id="逻辑回归的预测值"><strong>逻辑回归的预测值</strong></h4><p>Predicted Values from Logistic Regression</p><p>逻辑回归的预测值是<strong>对数几率</strong>：<span class="math inline">\(\hat{Y }= \log(\text{Odds}(Y=1))\)</span>。预测概率由<strong>逻辑响应函数</strong>给出：</p><p><span class="math display">\[\hat{p} = \frac{1}{1 + e^{- \hat{Y}}}\]</span> 例如，查看 R 中 <code>logistic_model</code> 的预测：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pred <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>logistic_model<span class="punctuation">)</span></span><br><span class="line">summary<span class="punctuation">(</span>pred<span class="punctuation">)</span></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Min. 1st Qu. Median Mean 3rd Qu. Max.</span><br><span class="line">-2.704774 -0.518825 -0.008539 0.002564 0.505061 3.509606</span><br></pre></td></tr></table></figure><p>在 Python 中，我们可以将概率转换为数据框，并使用 <code>describe</code> 方法来获取分布的这些特征：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pred = pd.DataFrame(logit_reg.predict_log_proba(X),</span><br><span class="line">columns=loan_data[outcome].cat.categories)</span><br><span class="line">pred.describe()</span><br></pre></td></tr></table></figure><p>将这些值转换为概率是一个简单的转换：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">prob <span class="operator">&lt;-</span> 1<span class="operator">/</span><span class="punctuation">(</span><span class="number">1</span> <span class="operator">+</span> <span class="built_in">exp</span><span class="punctuation">(</span><span class="operator">-</span>pred<span class="punctuation">)</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; summary(prob)</span><br><span class="line">Min. 1st Qu. Median Mean 3rd Qu. Max.</span><br><span class="line">0.06269 0.37313 0.49787 0.50000 0.62365 0.97096</span><br></pre></td></tr></table></figure><p><code>scikit-learn</code> 中的 <code>predict_proba</code> 方法可以直接获得概率：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pred = pd.DataFrame(logit_reg.predict_proba(X),</span><br><span class="line">columns=loan_data[outcome].cat.categories)</span><br><span class="line">pred.describe()</span><br></pre></td></tr></table></figure><p>这些值在0到1的范围内，但它们尚未声明预测值是违约还是已还清。我们可以将任何大于0.5的值声明为违约。在实践中，如果目标是识别<strong>罕见类别</strong>的成员，通常采用较低的截止点是合适的（参见第223页的“罕见类别问题”）。</p><h4 id="系数与几率比的解释"><strong>系数与几率比的解释</strong></h4><p>Interpreting the Coefficients and Odds Ratios</p><p>逻辑回归的一个优点是，它产生的模型可以快速地在新数据上进行评分，无需重新计算。另一个优点是，与其他分类方法相比，它相对更容易解释。其核心概念是理解<strong>几率比</strong>(odds ratio)。对于一个二元因子变量 <span class="math inline">\(X\)</span> 来说，几率比最容易理解：</p><p><span class="math display">\[\text{几率比} = \frac{\text{Odds}(Y=1 | X=1)}{\text{Odds}(Y=1 | X=0)}\]</span> 这被解释为当 <span class="math inline">\(X=1\)</span> 时的几率与当 <span class="math inline">\(X=0\)</span> 时的几率之比。如果几率比是2，那么当 <span class="math inline">\(X=1\)</span> 时 <span class="math inline">\(Y=1\)</span> 的几率是当 <span class="math inline">\(X=0\)</span> 时的两倍。</p><p>为什么我们要用几率比而不是概率？我们使用几率是因为，在逻辑回归中，系数 <span class="math inline">\(\beta_j\)</span> 是 <span class="math inline">\(X_j\)</span> <strong>几率比的对数</strong>。</p><p>一个例子可以更清楚地说明这一点。对于第210页“逻辑回归与 GLM”中拟合的模型，<code>purpose_small_business</code> 的回归系数是1.21526。这意味着，与用于偿还信用卡债务的贷款相比，用于创建或扩展小企业的贷款，其违约与已还清的几率比<strong>减少了</strong> <span class="math inline">\(e^{1.21526} \approx 3.4\)</span> 倍。显然，用于创建或扩展小企业的贷款比其他类型的贷款风险高得多。</p><p><img src="/img3/面向数据科学家的实用统计学/F5.3.png" alt="F5.3" style="zoom:33%;" /></p><p>图5-3显示了当几率比大于1时，几率比和对数几率比之间的关系。由于系数在对数尺度上，系数增加1，会导致几率比增加 <span class="math inline">\(e^1 \approx 2.72\)</span> 倍。</p><p>数值变量 <span class="math inline">\(X\)</span> 的几率比可以类似地解释：它们衡量 <span class="math inline">\(X\)</span> 变化一个单位时几率比的变化。例如，付款收入比从5增加到6，将使贷款违约的几率增加 <span class="math inline">\(e^{0.08244} \approx 1.09\)</span> 倍。变量 <code>borrower_score</code> 是衡量借款人信用度的分数，范围从0（低）到1（高）。信用度最好的借款人相对于最差的借款人违约的几率比，要小 <span class="math inline">\(e^{-4.61264} \approx 0.01\)</span> 倍。换句话说，信用度最差的借款人的违约风险是信用度最好的借款人的100倍！</p><h4 id="线性回归与逻辑回归相似点与差异"><strong>线性回归与逻辑回归：相似点与差异</strong></h4><p>Linear and Logistic Regression: Similarities and Differences</p><p>线性回归和逻辑回归有许多共同点。两者都假设预测变量与响应变量之间存在<strong>参数化的线性形式</strong>。探索和寻找最佳模型的方式也十分相似。线性模型的扩展，例如使用样条变换预测变量（参见第189页的“样条回归”），同样适用于逻辑回归。</p><p>然而，逻辑回归在两个根本方面有所不同： * <strong>模型的拟合方式</strong>（不适用最小二乘法）。 * <strong>模型残差的性质和分析</strong>。</p><p><strong>模型拟合</strong></p><p>线性回归使用<strong>最小二乘法</strong>进行拟合，其拟合质量通过 <strong>RMSE</strong> 和 <strong>R-squared</strong> 统计量进行评估。在逻辑回归中（与线性回归不同），没有闭合形式的解，模型必须使用<strong>最大似然估计（MLE）</strong>进行拟合。最大似然估计是一个试图找到最有可能产生我们所观察到的数据的模型的过程。在逻辑回归方程中，响应变量不是0或1，而是对响应为1的对数几率的估计。最大似然估计找到的解，使得估计的对数几率能够最好地描述观察到的结果。该算法的机制涉及一种<strong>拟牛顿优化</strong>，它在基于当前参数的评分步骤（费舍尔评分）和更新参数以改善拟合之间进行迭代。</p><p><strong>最大似然估计（MLE）</strong></p><p>如果你对统计符号感兴趣，这里有更多细节：假设有一组数据 <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> 和一个依赖于一组参数 <span class="math inline">\(\theta\)</span> 的概率模型 <span class="math inline">\(P_\theta(X_1, X_2, \dots, X_n)\)</span>。<strong>MLE 的目标是找到能够最大化 <span class="math inline">\(P_\theta(X_1, X_2, \dots, X_n)\)</span> 值的参数集 <span class="math inline">\(\theta\)</span></strong>；也就是说，它最大化在给定模型 <span class="math inline">\(P\)</span> 的情况下观察到 <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> 的概率。在拟合过程中，模型使用一个称为<strong>偏差（deviance）</strong>的度量进行评估：</p><p><span class="math display">\[\text{偏差} = -2 \log P_\theta(X_1, X_2, \dots, X_n)\]</span> 偏差越低 (个人注：因为有负号) ，拟合效果越好。</p><p>幸运的是，大多数实践者不需要关注拟合算法的细节，因为这些都由软件自动处理。大多数数据科学家无需担心拟合方法，只需理解它是在特定假设下找到一个好模型的方式即可。</p><blockquote><p>警告：</p><p><strong>处理因子变量</strong>（Handling Factor Variables）</p><p>在逻辑回归中，因子变量应像在线性回归中一样进行编码；参见第163页的“回归中的因子变量”。在 R 和其他软件中，这通常是自动处理的，并且通常使用<strong>参考编码</strong>。</p><p>本章涵盖的所有其他分类方法通常使用<strong>独热编码</strong>表示（参见第242页的“独热编码”）。在 Python 的 <code>scikit-learn</code> 中，最容易使用独热编码，这意味着在回归中只能使用由此产生的 <span class="math inline">\(n-1\)</span> 个虚拟变量。</p></blockquote><h4 id="模型评估"><strong>模型评估</strong></h4><p>Assessing the Model</p><p>像其他分类方法一样，逻辑回归模型的评估标准是其对新数据的分类准确性（参见第219页的“评估分类模型”）。与线性回归一样，一些标准的统计工具可用于检查和改进模型。除了估计的系数外，R 还会报告系数的标准误（SE）、z 值和 p 值：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">summary<span class="punctuation">(</span>logistic_model<span class="punctuation">)</span></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">Call:</span><br><span class="line">glm(formula = outcome ~ payment_inc_ratio + purpose_ + home_ +</span><br><span class="line">emp_len_ + borrower_score, family = &quot;binomial&quot;, data = loan_data)</span><br><span class="line">Deviance Residuals:</span><br><span class="line">Min       1Q   Median       3Q      Max</span><br><span class="line">-2.51951 -1.06908 -0.05853  1.07421  2.15528</span><br><span class="line">Coefficients:</span><br><span class="line">                           Estimate  Std. Error z value Pr(&gt;|z|)</span><br><span class="line">(Intercept)                 1.638092   0.073708  22.224  &lt; 2e-16 ***</span><br><span class="line">payment_inc_ratio           0.079737   0.002487  32.058  &lt; 2e-16 ***</span><br><span class="line">purpose_debt_consolidation  0.249373   0.027615   9.030  &lt; 2e-16 ***</span><br><span class="line">purpose_home_improvement    0.407743   0.046615   8.747  &lt; 2e-16 ***</span><br><span class="line">purpose_major_purchase      0.229628   0.053683   4.277  1.89e-05 ***</span><br><span class="line">purpose_medical             0.510479   0.086780   5.882  4.04e-09 ***</span><br><span class="line">purpose_other               0.620663   0.039436  15.738  &lt; 2e-16 ***</span><br><span class="line">purpose_small_business      1.215261   0.063320  19.192  &lt; 2e-16 ***</span><br><span class="line">home_OWN                    0.048330   0.038036   1.271  0.204</span><br><span class="line">home_RENT                   0.157320   0.021203   7.420  1.17e-13 ***</span><br><span class="line">emp_len_ &gt; 1 Year           -0.356731   0.052622  -6.779  1.21e-11 ***</span><br><span class="line">borrower_score              -4.612638   0.083558 -55.203  &lt; 2e-16 ***</span><br><span class="line">---</span><br><span class="line">Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1</span><br><span class="line">(Dispersion parameter for binomial family taken to be 1)</span><br><span class="line">Null deviance: 62857  on 45341  degrees of freedom</span><br><span class="line">Residual deviance: 57515  on 45330  degrees of freedom</span><br><span class="line">AIC: 57539</span><br><span class="line">Number of Fisher Scoring iterations: 4</span><br></pre></td></tr></table></figure><p><code>statsmodels</code> 包有一个广义线性模型（GLM）的实现，它提供了类似详细的信息：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">y_numbers = [<span class="number">1</span> <span class="keyword">if</span> yi == <span class="string">&#x27;default&#x27;</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> yi <span class="keyword">in</span> y]</span><br><span class="line">logit_reg_sm = sm.GLM(y_numbers, X.assign(const=<span class="number">1</span>),</span><br><span class="line">family=sm.families.Binomial())</span><br><span class="line">logit_result = logit_reg_sm.fit()</span><br><span class="line">logit_result.summary()</span><br></pre></td></tr></table></figure><p><strong>p 值的解释与回归中的情况具有相同的警告，应将其更多地视为变量重要性的相对指标（参见第153页的“评估模型”），而不是作为统计显著性的正式度量。</strong>一个具有二元响应的逻辑回归模型<strong>没有</strong>相关的 RMSE 或 R-squared。相反，逻辑回归模型通常使用更通用的<strong>分类指标</strong>进行评估；参见第219页的“评估分类模型”。</p><p>许多线性回归的概念也适用于逻辑回归（以及其他 GLM）。例如，你可以使用<strong>逐步回归</strong>、拟合<strong>交互项</strong>，或包含<strong>样条项</strong>。关于<strong>混杂变量</strong>和<strong>相关变量</strong>的相同问题也适用于逻辑回归（参见第169页的“解释回归方程”）。你可以使用 R 中的 <code>mgcv</code> 包拟合广义加性模型（参见第192页的“广义加性模型”）：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">logistic_gam <span class="operator">&lt;-</span> gam<span class="punctuation">(</span>outcome <span class="operator">~</span> s<span class="punctuation">(</span>payment_inc_ratio<span class="punctuation">)</span> <span class="operator">+</span> purpose_ <span class="operator">+</span></span><br><span class="line">home_ <span class="operator">+</span> emp_len_ <span class="operator">+</span> s<span class="punctuation">(</span>borrower_score<span class="punctuation">)</span><span class="punctuation">,</span></span><br><span class="line">data<span class="operator">=</span>loan_data<span class="punctuation">,</span> family<span class="operator">=</span><span class="string">&#x27;binomial&#x27;</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p><code>statsmodels</code> 的公式接口在 Python 中也支持这些扩展：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> statsmodels.formula.api <span class="keyword">as</span> smf</span><br><span class="line">formula = (<span class="string">&#x27;outcome ~ bs(payment_inc_ratio, df=4) + purpose_ + &#x27;</span> +</span><br><span class="line"><span class="string">&#x27;home_ + emp_len_ + bs(borrower_score, df=4)&#x27;</span>)</span><br><span class="line">model = smf.glm(formula=formula, data=loan_data, family=sm.families.Binomial())</span><br><span class="line">results = model.fit()</span><br></pre></td></tr></table></figure><p><strong>残差分析</strong></p><p>Analysis of residuals</p><p>残差分析是逻辑回归与线性回归的一个不同之处。与线性回归一样（参见图4-9），在 R 中计算<strong>偏残差</strong>非常简单：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">terms <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>logistic_gam<span class="punctuation">,</span> type<span class="operator">=</span><span class="string">&#x27;terms&#x27;</span><span class="punctuation">)</span></span><br><span class="line">partial_resid <span class="operator">&lt;-</span> resid<span class="punctuation">(</span>logistic_model<span class="punctuation">)</span> <span class="operator">+</span> terms</span><br><span class="line">df <span class="operator">&lt;-</span> data.frame<span class="punctuation">(</span>payment_inc_ratio <span class="operator">=</span> loan_data<span class="punctuation">[</span><span class="punctuation">,</span> <span class="string">&#x27;payment_inc_ratio&#x27;</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">terms <span class="operator">=</span> terms<span class="punctuation">[</span><span class="punctuation">,</span> <span class="string">&#x27;s(payment_inc_ratio)&#x27;</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">partial_resid <span class="operator">=</span> partial_resid<span class="punctuation">[</span><span class="punctuation">,</span> <span class="string">&#x27;s(payment_inc_ratio)&#x27;</span><span class="punctuation">]</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line">ggplot<span class="punctuation">(</span>df<span class="punctuation">,</span> aes<span class="punctuation">(</span>x<span class="operator">=</span>payment_inc_ratio<span class="punctuation">,</span> y<span class="operator">=</span>partial_resid<span class="punctuation">,</span> solid <span class="operator">=</span> <span class="literal">FALSE</span><span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">geom_point<span class="punctuation">(</span>shape<span class="operator">=</span><span class="number">46</span><span class="punctuation">,</span> alpha<span class="operator">=</span><span class="number">0.4</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">geom_line<span class="punctuation">(</span>aes<span class="punctuation">(</span>x<span class="operator">=</span>payment_inc_ratio<span class="punctuation">,</span> y<span class="operator">=</span>terms<span class="punctuation">)</span><span class="punctuation">,</span></span><br><span class="line">color<span class="operator">=</span><span class="string">&#x27;red&#x27;</span><span class="punctuation">,</span> alpha<span class="operator">=</span><span class="number">0.5</span><span class="punctuation">,</span> size<span class="operator">=</span><span class="number">1.5</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">labs<span class="punctuation">(</span>y<span class="operator">=</span><span class="string">&#x27;Partial Residual&#x27;</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>由此产生的图表如图5-4所示。</p><p><img src="/img3/面向数据科学家的实用统计学/F5.4.png" alt="F5.4" style="zoom:50%;" /></p><p>图中所示的估计拟合线穿过两组点云之间。顶部的点云对应于响应为1（违约贷款），底部的点云对应于响应为0（已还清贷款）。这对于逻辑回归的残差来说是非常典型的，因为其输出是二元的。预测值是以 <strong>logit</strong>（对数几率）来衡量的，它始终是一个有限值。而实际值（绝对的0或1）对应于<strong>无限的 logit</strong>（正或负），因此残差（被加到拟合值上）永远不会等于0。因此，在偏残差图中，绘制的点云始终位于拟合线上方或下方。尽管逻辑回归中的偏残差不如线性回归中那么有价值，但它们仍可用于确认非线性行为并识别<strong>高影响力的记录</strong>。</p><p>目前，在任何主要的 Python 包中都没有偏残差的实现。我们在随附的源代码仓库中提供了用于创建偏残差图的 Python 代码。</p><blockquote><p>警告：</p><p><code>summary</code> 函数的部分输出可以被忽略。<strong>离散参数</strong>不适用于逻辑回归，它是为其他类型的 GLM 准备的。<strong>残差偏差</strong>和<strong>评分迭代次数</strong>与最大似然拟合方法相关；参见第215页的“最大似然估计”。</p></blockquote><p><strong>关键思想</strong></p><ul><li><strong>相似性</strong>：逻辑回归与线性回归类似，但其结果变量是<strong>二元</strong>的。</li><li><strong>模型转换</strong>：需要进行几种转换，才能将模型转换为可以作为线性模型进行拟合的形式，其中响应变量是<strong>几率比的对数</strong>。</li><li><strong>反向映射</strong>：在模型（通过迭代过程）拟合后，对数几率被映射回一个<strong>概率</strong>。</li><li><strong>流行原因</strong>：逻辑回归之所以流行，是因为它计算速度快，并且产生的模型只需少量算术运算即可在新数据上进行评分。</li></ul><h3 id="评估分类模型">评估分类模型</h3><p>Evaluating Classification Models</p><p>对分类模型进行评估，通常是训练几个不同的模型，将它们分别应用于一个<strong>保留样本</strong>，并评估其性能。有时，在评估和调优了多个模型后，如果数据足够，会使用<strong>第三个</strong>、之前未使用的保留样本来估计所选模型在新数据上的表现。不同的学科和从业者也会使用<strong>验证（validation）</strong>和<strong>测试（test）</strong>这两个术语来指代保留样本。从根本上说，评估过程旨在确定哪个模型能产生最准确和最有用的预测。</p><p><strong>评估分类模型的关键术语</strong></p><ul><li><p><strong>准确率（Accuracy）</strong> 正确分类的案例所占的百分比（或比例）。</p></li><li><p><strong>混淆矩阵（Confusion matrix）</strong> 一个表格显示（二元情况下为2×2），按预测和实际分类状态列出记录数量。</p></li><li><p><strong>灵敏度（Sensitivity）</strong> 所有实际为1的案例中，被正确分类为1的百分比（或比例）。 同义词：<strong>召回率（Recall）</strong></p></li><li><p><strong>特异度（Specificity）</strong> 所有实际为0的案例中，被正确分类为0的百分比（或比例）。</p></li><li><p><strong>精确率（Precision）</strong> 所有被预测为1的案例中，实际为1的百分比（或比例）。</p></li><li><p><strong>ROC曲线（ROC curve）</strong> 绘制灵敏度与特异度关系的图。</p></li><li><p><strong>提升度（Lift）</strong> 衡量模型在不同概率截止点下识别（相对罕见的）1的能力有多有效。</p></li></ul><p>衡量分类性能的一个简单方法是计算正确预测的比例，即测量<strong>准确率（Accuracy）</strong>。准确率只是一个总误差的度量：</p><p><span class="math display">\[\text{准确率} = \frac{\sum\text{True Positive} + \sum\text{True Negative}}{\text{样本量}}\]</span> 在大多数分类算法中，每个案例都被分配一个“<strong>估计为1的概率</strong>”。默认的决策点，或<strong>截止点（cutoff）</strong>，通常是0.50或50%。如果概率高于0.5，则分类为“1”；否则为“0”。另一个替代的默认截止点是数据中1的<strong>普遍概率</strong>。</p><h4 id="混淆矩阵"><strong>混淆矩阵</strong></h4><p>Confusion Matrix</p><p>混淆矩阵是分类指标的核心。它是一个表格，按预测和实际结果的类别显示正确和不正确预测的数量。在 R 和 Python 中有许多可用的包来计算混淆矩阵，但在二元情况下，手动计算一个也很简单。</p><p>为了说明混淆矩阵，考虑在包含相同数量的违约和已还清贷款的平衡数据集上训练的 <code>logistic_gam</code> 模型（参见图5-4）。按照惯例，<strong><span class="math inline">\(Y=1\)</span> 对应于目标事件</strong>（例如，违约），而 <strong><span class="math inline">\(Y=0\)</span> 对应于负面（或常规）事件</strong>（例如，已还清）。以下代码在 R 中计算了应用于整个（不平衡）训练集的 <code>logistic_gam</code> 模型的混淆矩阵：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">pred <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>logistic_gam<span class="punctuation">,</span> newdata<span class="operator">=</span>train_set<span class="punctuation">)</span></span><br><span class="line">pred_y <span class="operator">&lt;-</span> <span class="built_in">as.numeric</span><span class="punctuation">(</span>pred <span class="operator">&gt;</span> <span class="number">0</span><span class="punctuation">)</span></span><br><span class="line">true_y <span class="operator">&lt;-</span> <span class="built_in">as.numeric</span><span class="punctuation">(</span>train_set<span class="operator">$</span>outcome<span class="operator">==</span><span class="string">&#x27;default&#x27;</span><span class="punctuation">)</span></span><br><span class="line">true_pos <span class="operator">&lt;-</span> <span class="punctuation">(</span>true_y<span class="operator">==</span><span class="number">1</span><span class="punctuation">)</span> <span class="operator">&amp;</span> <span class="punctuation">(</span>pred_y<span class="operator">==</span><span class="number">1</span><span class="punctuation">)</span></span><br><span class="line">true_neg <span class="operator">&lt;-</span> <span class="punctuation">(</span>true_y<span class="operator">==</span><span class="number">0</span><span class="punctuation">)</span> <span class="operator">&amp;</span> <span class="punctuation">(</span>pred_y<span class="operator">==</span><span class="number">0</span><span class="punctuation">)</span></span><br><span class="line">false_pos <span class="operator">&lt;-</span> <span class="punctuation">(</span>true_y<span class="operator">==</span><span class="number">0</span><span class="punctuation">)</span> <span class="operator">&amp;</span> <span class="punctuation">(</span>pred_y<span class="operator">==</span><span class="number">1</span><span class="punctuation">)</span></span><br><span class="line">false_neg <span class="operator">&lt;-</span> <span class="punctuation">(</span>true_y<span class="operator">==</span><span class="number">1</span><span class="punctuation">)</span> <span class="operator">&amp;</span> <span class="punctuation">(</span>pred_y<span class="operator">==</span><span class="number">0</span><span class="punctuation">)</span></span><br><span class="line">conf_mat <span class="operator">&lt;-</span> matrix<span class="punctuation">(</span><span class="built_in">c</span><span class="punctuation">(</span><span class="built_in">sum</span><span class="punctuation">(</span>true_pos<span class="punctuation">)</span><span class="punctuation">,</span> <span class="built_in">sum</span><span class="punctuation">(</span>false_pos<span class="punctuation">)</span><span class="punctuation">,</span></span><br><span class="line"><span class="built_in">sum</span><span class="punctuation">(</span>false_neg<span class="punctuation">)</span><span class="punctuation">,</span> <span class="built_in">sum</span><span class="punctuation">(</span>true_neg<span class="punctuation">)</span><span class="punctuation">)</span><span class="punctuation">,</span> <span class="number">2</span><span class="punctuation">,</span> <span class="number">2</span><span class="punctuation">)</span></span><br><span class="line">colnames<span class="punctuation">(</span>conf_mat<span class="punctuation">)</span> <span class="operator">&lt;-</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;Yhat = 1&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;Yhat = 0&#x27;</span><span class="punctuation">)</span></span><br><span class="line">rownames<span class="punctuation">(</span>conf_mat<span class="punctuation">)</span> <span class="operator">&lt;-</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;Y = 1&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;Y = 0&#x27;</span><span class="punctuation">)</span></span><br><span class="line">conf_mat</span><br><span class="line"></span><br><span class="line">Yhat <span class="operator">=</span> <span class="number">1</span> Yhat <span class="operator">=</span> <span class="number">0</span></span><br><span class="line">Y <span class="operator">=</span> <span class="number">1</span> <span class="number">14295</span>    <span class="number">8376</span></span><br><span class="line">Y <span class="operator">=</span> <span class="number">0</span> <span class="number">8052</span>    <span class="number">14619</span></span><br></pre></td></tr></table></figure><p>在 Python 中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">pred = logit_reg.predict(X)</span><br><span class="line">pred_y = logit_reg.predict(X) == <span class="string">&#x27;default&#x27;</span></span><br><span class="line">true_y = y == <span class="string">&#x27;default&#x27;</span></span><br><span class="line">true_pos = true_y &amp; pred_y</span><br><span class="line">true_neg = ~true_y &amp; ~pred_y</span><br><span class="line">false_pos = ~true_y &amp; pred_y</span><br><span class="line">false_neg = true_y &amp; ~pred_y</span><br><span class="line">conf_mat = pd.DataFrame([[np.<span class="built_in">sum</span>(true_pos), np.<span class="built_in">sum</span>(false_neg)],</span><br><span class="line">[np.<span class="built_in">sum</span>(false_pos), np.<span class="built_in">sum</span>(true_neg)]],</span><br><span class="line">index=[<span class="string">&#x27;Y = default&#x27;</span>, <span class="string">&#x27;Y = paid off&#x27;</span>],</span><br><span class="line">columns=[<span class="string">&#x27;Yhat = default&#x27;</span>, <span class="string">&#x27;Yhat = paid off&#x27;</span>])</span><br><span class="line">conf_mat</span><br></pre></td></tr></table></figure><p>预测结果是列，实际结果是行。矩阵的对角线元素显示了<strong>正确预测的数量</strong>，非对角线元素显示了<strong>不正确预测的数量</strong>。例如，有14,295笔违约贷款被正确预测为违约，但有8,376笔违约贷款被错误地预测为已还清。</p><p>图5-5显示了二元响应 <span class="math inline">\(Y\)</span> 的混淆矩阵与不同指标之间的关系（关于这些指标的更多信息，请参阅第223页的“精确率、召回率和特异度”）。与贷款数据的示例一样，<strong>实际响应沿行排列，预测响应沿列排列</strong>。对角线上的框（左上、右下）显示了预测 <span class="math inline">\(Y\)</span> 正确预测响应的情况。</p><p><img src="/img3/面向数据科学家的实用统计学/F5.5.png" alt="F5.5" style="zoom:50%;" /></p><p>一个没有明确提及的重要指标是<strong>假阳性率</strong>（与精确率镜像）。当1s是<strong>罕见</strong>的类别时，假阳性与所有预测为阳性的比率可能很高，这导致一种不直观的情况：被预测为1的案例很可能实际上是0。这个问题困扰着广泛应用的医疗筛查测试（例如乳房X光检查）：由于病症相对罕见，阳性测试结果<strong>很可能并不意味着患有乳腺癌</strong>。这给公众带来了很大的困惑。</p><blockquote><p>警告：</p><p>在这里，我们将实际响应沿行排列，预测响应沿列排列，但将其反转也很常见。一个值得注意的例子是 R 中流行的 <code>caret</code> 包。</p></blockquote><h4 id="罕见类别问题"><strong>罕见类别问题</strong></h4><p>The Rare Class Problem</p><p>在许多情况下，待预测的类别存在<strong>不平衡</strong>，其中一个类别比另一个更普遍，例如，合法的保险索赔与欺诈性索赔，或网站上的浏览者与购买者。<strong>罕见类别</strong>（例如，欺诈性索赔）通常是更受关注的类别，通常被指定为1，与更普遍的0形成对比。在典型场景中，1是更重要的案例，因为将其错误地分类为0的成本比将0错误地分类为1的成本更高。例如，正确识别一次欺诈性保险索赔可能节省数千美元。另一方面，正确识别一次非欺诈性索赔仅节省了你手动进行更仔细审查的成本和精力（如果该索赔被标记为“欺诈性”，你就会这样做）。</p><p>在这种情况下，除非类别很容易分离，否则<strong>最准确的分类模型可能是一个简单地将所有东西都分类为0的模型</strong>。例如，如果一个网站上只有0.1%的浏览者最终购买，那么一个预测每个浏览者都会离开而不会购买的模型将有99.9%的准确率。然而，它将毫无用处。相反，我们会乐于接受一个整体准确率较低，但善于识别购买者的模型，即使它在这个过程中错误地分类了一些非购买者。</p><h4 id="精确率召回率和特异度"><strong>精确率、召回率和特异度</strong></h4><p>Precision, Recall, and Specificity</p><p>除了纯粹的准确率之外，还有一些更细致的指标常用于评估分类模型。其中一些在统计学——特别是生物统计学——中有很长的历史，它们被用来描述诊断测试的预期性能。<strong>精确率（Precision）衡量预测为正向结果的准确性</strong>（参见图5-5）： <span class="math display">\[\text{精确率} = \frac{\sum\text{True Positive}}{\sum\text{True Positive} + \sum\text{False Positive}}\]</span> <strong>召回率（Recall）</strong>，也称为<strong>灵敏度（Sensitivity）</strong>，衡量模型<strong>预测正向结果的能力</strong>——它正确识别的1的比例（参见图5-5）。在生物统计学和医学诊断中，“灵敏度”这个术语使用得更多，而在机器学习社区中，“召回率”使用得更多。召回率的定义是： <span class="math display">\[\text{召回率} = \frac{\sum\text{True Positive}}{\sum\text{True Positive} + \sum\text{False Negative}}\]</span> 另一个使用的指标是<strong>特异度（Specificity）</strong>，它衡量模型<strong>预测负向结果的能力</strong>： <span class="math display">\[\text{特异度} = \frac{\sum\text{True Negative}}{\sum\text{True Negative} + \sum\text{False Positive}}\]</span> 我们可以从 R 中的 <code>conf_mat</code> 计算这三个指标：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># precision</span></span><br><span class="line">conf_mat<span class="punctuation">[</span><span class="number">1</span><span class="punctuation">,</span> <span class="number">1</span><span class="punctuation">]</span> <span class="operator">/</span> <span class="built_in">sum</span><span class="punctuation">(</span>conf_mat<span class="punctuation">[</span><span class="punctuation">,</span><span class="number">1</span><span class="punctuation">]</span><span class="punctuation">)</span></span><br><span class="line"><span class="comment"># recall</span></span><br><span class="line">conf_mat<span class="punctuation">[</span><span class="number">1</span><span class="punctuation">,</span> <span class="number">1</span><span class="punctuation">]</span> <span class="operator">/</span> <span class="built_in">sum</span><span class="punctuation">(</span>conf_mat<span class="punctuation">[</span><span class="number">1</span><span class="punctuation">,</span><span class="punctuation">]</span><span class="punctuation">)</span></span><br><span class="line"><span class="comment"># specificity</span></span><br><span class="line">conf_mat<span class="punctuation">[</span><span class="number">2</span><span class="punctuation">,</span> <span class="number">2</span><span class="punctuation">]</span> <span class="operator">/</span> <span class="built_in">sum</span><span class="punctuation">(</span>conf_mat<span class="punctuation">[</span><span class="number">2</span><span class="punctuation">,</span><span class="punctuation">]</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>以下是 Python 中计算这些指标的等效代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">conf_mat = confusion_matrix(y, logit_reg.predict(X))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Precision&#x27;</span>, conf_mat[<span class="number">0</span>, <span class="number">0</span>] / <span class="built_in">sum</span>(conf_mat[:, <span class="number">0</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Recall&#x27;</span>, conf_mat[<span class="number">0</span>, <span class="number">0</span>] / <span class="built_in">sum</span>(conf_mat[<span class="number">0</span>, :]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Specificity&#x27;</span>, conf_mat[<span class="number">1</span>, <span class="number">1</span>] / <span class="built_in">sum</span>(conf_mat[<span class="number">1</span>, :]))</span><br><span class="line">precision_recall_fscore_support(y, logit_reg.predict(X),</span><br><span class="line">labels=[<span class="string">&#x27;default&#x27;</span>, <span class="string">&#x27;paid off&#x27;</span>])</span><br></pre></td></tr></table></figure><p><code>scikit-learn</code> 有一个自定义方法 <code>precision_recall_fscore_support</code>，可以一次性计算出精确率和召回率/特异度。</p><h4 id="roc曲线"><strong>ROC曲线</strong></h4><p>ROC Curve</p><p>你可以看到，召回率（recall）<strong>和</strong>特异度（specificity）之间存在一个权衡。捕获更多的1通常意味着将更多的0错误分类为1。一个理想的分类器应该在很好地分类1的同时，不会将更多的0错误分类为1。</p><p>捕捉这种权衡的指标是“<strong>受试者工作特征（Receiver Operating Characteristics）</strong>”曲线，通常简称为<strong>ROC曲线</strong>。ROC曲线将召回率（灵敏度）绘制在y轴上，特异度绘制在x轴上。ROC曲线显示了当你改变截止点来决定如何分类一条记录时，召回率和特异度之间的权衡关系。灵敏度（召回率）绘制在y轴上，而你可能会遇到两种形式的x轴标注：</p><ul><li><strong>x轴绘制特异度</strong>，左边是1，右边是0。</li><li><strong>x轴绘制1-特异度</strong>，左边是0，右边是1。</li></ul><p>无论哪种方式，曲线看起来都完全相同。计算ROC曲线的过程是：</p><ol type="1"><li><strong>排序</strong>：根据预测为1的概率对记录进行排序，从最有可能的开始，到最不可能的结束。</li><li><strong>计算</strong>：基于排序后的记录，计算累积的特异度和召回率。</li></ol><p>在 R 中计算 ROC 曲线非常简单。以下代码计算贷款数据的 ROC：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">idx <span class="operator">&lt;-</span> order<span class="punctuation">(</span><span class="operator">-</span>pred<span class="punctuation">)</span></span><br><span class="line">recall <span class="operator">&lt;-</span> <span class="built_in">cumsum</span><span class="punctuation">(</span>true_y<span class="punctuation">[</span>idx<span class="punctuation">]</span> <span class="operator">==</span> <span class="number">1</span><span class="punctuation">)</span> <span class="operator">/</span> <span class="built_in">sum</span><span class="punctuation">(</span>true_y <span class="operator">==</span> <span class="number">1</span><span class="punctuation">)</span></span><br><span class="line">specificity <span class="operator">&lt;-</span> <span class="punctuation">(</span><span class="built_in">sum</span><span class="punctuation">(</span>true_y <span class="operator">==</span> <span class="number">0</span><span class="punctuation">)</span> <span class="operator">-</span> <span class="built_in">cumsum</span><span class="punctuation">(</span>true_y<span class="punctuation">[</span>idx<span class="punctuation">]</span> <span class="operator">==</span> <span class="number">0</span><span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">/</span> <span class="built_in">sum</span><span class="punctuation">(</span>true_y <span class="operator">==</span> <span class="number">0</span><span class="punctuation">)</span></span><br><span class="line">roc_df <span class="operator">&lt;-</span> data.frame<span class="punctuation">(</span>recall <span class="operator">=</span> recall<span class="punctuation">,</span> specificity <span class="operator">=</span> specificity<span class="punctuation">)</span></span><br><span class="line">ggplot<span class="punctuation">(</span>roc_df<span class="punctuation">,</span> aes<span class="punctuation">(</span>x<span class="operator">=</span>specificity<span class="punctuation">,</span> y<span class="operator">=</span>recall<span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">geom_line<span class="punctuation">(</span>color<span class="operator">=</span><span class="string">&#x27;blue&#x27;</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">scale_x_reverse<span class="punctuation">(</span>expand<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">0</span><span class="punctuation">,</span> <span class="number">0</span><span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">scale_y_continuous<span class="punctuation">(</span>expand<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">0</span><span class="punctuation">,</span> <span class="number">0</span><span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">geom_line<span class="punctuation">(</span>data<span class="operator">=</span>data.frame<span class="punctuation">(</span>x<span class="operator">=</span><span class="punctuation">(</span><span class="number">0</span><span class="operator">:</span><span class="number">100</span><span class="punctuation">)</span> <span class="operator">/</span> <span class="number">100</span><span class="punctuation">)</span><span class="punctuation">,</span> aes<span class="punctuation">(</span>x<span class="operator">=</span>x<span class="punctuation">,</span> y<span class="operator">=</span><span class="number">1</span><span class="operator">-</span>x<span class="punctuation">)</span><span class="punctuation">,</span></span><br><span class="line">linetype<span class="operator">=</span><span class="string">&#x27;dotted&#x27;</span><span class="punctuation">,</span> color<span class="operator">=</span><span class="string">&#x27;red&#x27;</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>在 Python 中，我们可以使用 <code>scikit-learn</code> 函数 <code>sklearn.metrics.roc_curve</code> 来计算 ROC 曲线所需的信息。你也可以找到类似的 R 包，例如 <code>ROCR</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">fpr, tpr, thresholds = roc_curve(y, logit_reg.predict_proba(X)[:,<span class="number">0</span>],</span><br><span class="line">pos_label=<span class="string">&#x27;default&#x27;</span>)</span><br><span class="line">roc_df = pd.DataFrame(&#123;<span class="string">&#x27;recall&#x27;</span>: tpr, <span class="string">&#x27;specificity&#x27;</span>: <span class="number">1</span> - fpr&#125;)</span><br><span class="line">ax = roc_df.plot(x=<span class="string">&#x27;specificity&#x27;</span>, y=<span class="string">&#x27;recall&#x27;</span>, figsize=(<span class="number">4</span>, <span class="number">4</span>), legend=<span class="literal">False</span>)</span><br><span class="line">ax.set_ylim(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">ax.set_xlim(<span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">ax.plot((<span class="number">1</span>, <span class="number">0</span>), (<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">ax.set_xlabel(<span class="string">&#x27;specificity&#x27;</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&#x27;recall&#x27;</span>)</span><br></pre></td></tr></table></figure><p>结果如图5-6所示。虚线对角线对应于<strong>不比随机猜测好</strong>的分类器。一个<strong>极其有效</strong>的分类器（或者在医疗情境中，一个极其有效的诊断测试）的 ROC 曲线将<strong>紧贴左上角</strong>——它将正确识别大量的1，同时不会将大量的0错误分类为1。对于这个模型，如果我们想要一个特异度至少为50%的分类器，那么召回率约为75%。</p><p><img src="/img3/面向数据科学家的实用统计学/F5.6.png" alt="F5.6" style="zoom:33%;" /></p><blockquote><p>通用注解：</p><p><strong>精确率-召回率曲线</strong>(Precision-Recall Curve)</p><p>除了 ROC 曲线之外，检查<strong>精确率-召回率（Precision-Recall, PR）曲线</strong>也很有启发性。PR 曲线以类似的方式计算，不同之处在于数据从最不可能到最有可能进行排序，并计算累积的精确率和召回率统计数据。PR 曲线在评估具有<strong>高度不平衡结果</strong>的数据时特别有用。</p></blockquote><h4 id="auc-曲线下面积"><strong>AUC (曲线下面积)</strong></h4><p>ROC曲线是一个有价值的图形工具，但它本身并不能构成衡量分类器性能的单一指标。然而，ROC曲线可以用来计算曲线下面积（Area Under the Curve, AUC）指标。AUC就是ROC曲线下的总面积。<strong>AUC的值越大，分类器越有效</strong>。一个AUC为1的分类器是完美的：它能正确分类所有1，并且不会将任何0错误地分类为1。一个完全无效的分类器——对角线——的AUC值为0.5。</p><p><img src="/img3/面向数据科学家的实用统计学/F5.7.png" alt="F5.7" style="zoom:33%;" /></p><p>图5-7显示了贷款模型的ROC曲线下面积。在 R 中可以通过数值积分计算AUC值：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sum</span><span class="punctuation">(</span>roc_df<span class="operator">$</span>recall<span class="punctuation">[</span><span class="operator">-</span><span class="number">1</span><span class="punctuation">]</span> <span class="operator">*</span> diff<span class="punctuation">(</span><span class="number">1</span> <span class="operator">-</span> roc_df<span class="operator">$</span>specificity<span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line"><span class="punctuation">[</span><span class="number">1</span><span class="punctuation">]</span> <span class="number">0.6926172</span></span><br></pre></td></tr></table></figure><p>在 Python 中，我们可以像 R 中那样计算准确率，也可以使用 <code>scikit-learn</code> 的 <code>sklearn.metrics.roc_auc_score</code> 函数。你需要提供0或1的期望值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(np.<span class="built_in">sum</span>(roc_df.recall[:-<span class="number">1</span>] * np.diff(<span class="number">1</span> - roc_df.specificity)))</span><br><span class="line"><span class="built_in">print</span>(roc_auc_score([<span class="number">1</span> <span class="keyword">if</span> yi == <span class="string">&#x27;default&#x27;</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> yi <span class="keyword">in</span> y],</span><br><span class="line">logit_reg.predict_proba(X)[:, <span class="number">0</span>]))</span><br></pre></td></tr></table></figure><p>该模型的AUC约为0.69，这表明它是一个相对较弱的分类器。</p><blockquote><p>警告：</p><p><strong>假阳性率的混淆</strong>（False Positive Rate Confusion）</p><p>假阳性/假阴性率常常与特异度或灵敏度混淆或混为一谈（甚至在出版物和软件中也是如此！）。有时，假阳性率被定义为<strong>测试结果为阳性的真阴性样本的比例</strong>。在许多情况下（例如网络入侵检测），该术语被用来指代<strong>真阴性的阳性信号的比例</strong>。</p></blockquote><h4 id="提升度"><strong>提升度</strong></h4><p>Lift</p><p>使用 AUC 作为评估模型的指标，比简单的准确率有所改进，因为它能评估分类器在整体准确率和识别更重要的 1 之间的权衡处理得如何。但它没有完全解决<strong>罕见类别问题</strong>，在这种情况下，你需要将模型的概率截止点降低到0.5以下，以避免所有记录都被分类为0。在这种情况下，一条记录被分类为1的概率可能低至0.4、0.3甚至更低就足够了。实际上，我们最终会过度识别1，以体现它们更高的重要性。</p><p>改变这个截止点会增加你捕捉到1的机会（代价是将更多的0错误地分类为1）。但最佳的截止点在哪里呢？</p><p><strong>提升度（Lift）</strong>的概念让你能够<strong>推迟回答这个问题</strong>。相反，它让你按照记录被预测为1的概率顺序来考虑它们。例如，在被分类为1的记录中，如果取概率最高的10%作为子集，那么与<strong>盲目随机挑选</strong>的基准相比，算法的表现要好多少？如果你在这个最高的十分位数中能得到0.3%的响应率，而不是随机挑选得到的0.1%的整体响应率，那么该算法在这个十分位数中的提升度（也称为<strong>增益</strong>）就是3。<strong>提升度图（增益图）</strong>将这种效果量化到整个数据范围内。它可以按十分位数生成，也可以在数据的整个范围内连续生成。</p><p>要计算提升度图，首先要生成一个<strong>累积增益图（cumulative gains chart）</strong>，该图将召回率绘制在y轴上，记录总数绘制在x轴上。<strong>提升度曲线是累积增益与代表随机选择的对角线之间的比率</strong>。十分位增益图是预测建模中最古老的技术之一，其历史可以追溯到互联网商业出现之前。它们在直邮营销专业人士中特别受欢迎。如果无差别地应用，直邮是一种昂贵的广告方式，因此广告商会使用预测模型（在早期非常简单）来识别最有可能获得回报的潜在客户。</p><blockquote><p>警告：</p><p><strong>Uplift</strong></p><p>有时，<strong>uplift</strong> 一词与 <strong>lift</strong> 含义相同。但在一个更受限的情境中，它有一个不同的含义，即当进行 A/B 测试，并将处理方式（A或B）用作预测模型中的预测变量时。<strong>Uplift 是预测模型对于单个案例使用处理A与处理B相比，其响应的改进程度</strong>。这是通过对单个案例进行两次评分来确定的：第一次将预测变量设置为A，然后再次将其切换为B。营销人员和政治竞选顾问使用这种方法来决定哪种信息处理方式应该用于哪些客户或选民。</p></blockquote><p>提升度曲线让你能够观察<strong>设定不同概率截止点来将记录分类为1所带来的后果</strong>。它可以作为确定适当截止水平的中间步骤。例如，税务机关可能只有一定数量的资源用于税务审计，并希望将这些资源用于最有可能偷税漏税的人。考虑到资源限制，该机构会使用提升度图来估算在哪里划定界限，决定哪些报税表将被选中进行审计，哪些将被放过。</p><p><strong>关键思想</strong></p><ul><li><strong>准确率（正确分类的百分比）</strong>仅仅是评估模型的第一步。</li><li>其他指标（召回率、特异度和精确率）侧重于更具体的性能特征（例如，<strong>召回率</strong>衡量模型正确识别1的能力）。</li><li><strong>AUC（ROC曲线下面积）</strong>是衡量模型区分1和0能力的一个常用指标。</li><li>同样，<strong>提升度</strong>衡量模型识别1的有效性，通常按十分位数计算，从最有可能的1开始。</li></ul><h3 id="不平衡数据的策略"><strong>不平衡数据的策略</strong></h3><p>Strategies for Imbalanced Data</p><p>上一节讨论了使用<strong>超越简单准确率</strong>的指标来评估分类模型，这些指标适用于<strong>不平衡数据</strong>——即目标结果（网站购买、保险欺诈等）非常罕见的数据。本节将探讨可用于<strong>改善不平衡数据下预测建模性能的其他策略</strong>。</p><p><strong>不平衡数据的关键术语</strong></p><ul><li><p><strong>欠采样（Undersample）</strong> 在分类模型中使用较少数量的普遍类别记录。 同义词：<strong>降采样（Downsample）</strong></p></li><li><p><strong>过采样（Oversample）</strong> 在分类模型中使用更多数量的罕见类别记录，如果需要，可通过自举法实现。 同义词：<strong>升采样（Upsample）</strong></p></li><li><p><strong>上加权或下加权（Up weight or down weight）</strong> 在模型中对罕见（或普遍）类别赋予更多（或更少）的权重。</p></li><li><p><strong>数据生成（Data generation）</strong> 类似于自举法，但每个新的自举记录都与原始记录略有不同。</p></li><li><p><strong>z分数（z-score）</strong> 标准化后得到的值。</p></li><li><p><strong>K</strong> 在最近邻计算中考虑的邻居数量。</p></li></ul><h4 id="欠采样"><strong>欠采样</strong></h4><p>Undersampling</p><p>如果你有足够的数据，就像贷款数据一样，一种解决方案是<strong>对普遍类别进行欠采样（或降采样）</strong>，这样建模数据在0和1之间会更平衡。欠采样的基本思想是，主导类的数据包含许多<strong>冗余记录</strong>。处理一个更小、更平衡的数据集，有助于提高模型性能，并使数据准备、模型探索和试运行变得更容易。</p><p>多少数据才算足够？这取决于应用，但通常来说，对于较不主导的类别，拥有数万条记录就足够了。<strong>1与0之间越容易区分，所需的数据就越少</strong>。</p><p>第208页“逻辑回归”中分析的贷款数据基于一个<strong>平衡的训练集</strong>：一半贷款已还清，另一半已违约。预测值也类似：一半的概率小于0.5，另一半大于0.5。在完整的（不平衡）数据集中，只有大约19%的贷款是违约的，如R代码所示：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mean<span class="punctuation">(</span>full_train_set<span class="operator">$</span>outcome<span class="operator">==</span><span class="string">&#x27;default&#x27;</span><span class="punctuation">)</span></span><br><span class="line"><span class="punctuation">[</span><span class="number">1</span><span class="punctuation">]</span> <span class="number">0.1889455</span></span><br></pre></td></tr></table></figure><p>在Python中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;percentage of loans in default: &#x27;</span>,</span><br><span class="line"><span class="number">100</span> * np.mean(full_train_set.outcome == <span class="string">&#x27;default&#x27;</span>))</span><br></pre></td></tr></table></figure><p>如果我们使用完整的数据集来训练模型，会发生什么？让我们看看在R中会是什么样子：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">full_model <span class="operator">&lt;-</span> glm<span class="punctuation">(</span>outcome <span class="operator">~</span> payment_inc_ratio <span class="operator">+</span> purpose_ <span class="operator">+</span> home_ <span class="operator">+</span></span><br><span class="line">emp_len_<span class="operator">+</span> dti <span class="operator">+</span> revol_bal <span class="operator">+</span> revol_util<span class="punctuation">,</span></span><br><span class="line">data<span class="operator">=</span>full_train_set<span class="punctuation">,</span> family<span class="operator">=</span><span class="string">&#x27;binomial&#x27;</span><span class="punctuation">)</span></span><br><span class="line">pred <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>full_model<span class="punctuation">)</span></span><br><span class="line">mean<span class="punctuation">(</span>pred <span class="operator">&gt;</span> <span class="number">0</span><span class="punctuation">)</span></span><br><span class="line"><span class="punctuation">[</span><span class="number">1</span><span class="punctuation">]</span> <span class="number">0.003942094</span></span><br></pre></td></tr></table></figure><p>在Python中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">predictors = [<span class="string">&#x27;payment_inc_ratio&#x27;</span>, <span class="string">&#x27;purpose_&#x27;</span>, <span class="string">&#x27;home_&#x27;</span>, <span class="string">&#x27;emp_len_&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;dti&#x27;</span>, <span class="string">&#x27;revol_bal&#x27;</span>, <span class="string">&#x27;revol_util&#x27;</span>]</span><br><span class="line">outcome = <span class="string">&#x27;outcome&#x27;</span></span><br><span class="line">X = pd.get_dummies(full_train_set[predictors], prefix=</span><br><span class="line">drop_first=<span class="literal">True</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;</span></span><br><span class="line">, prefix_sep=</span><br><span class="line"><span class="string">&#x27;&#x27;</span></span><br><span class="line">,</span><br><span class="line">y = full_train_set[outcome]</span><br><span class="line">full_model = LogisticRegression(penalty=<span class="string">&#x27;l2&#x27;</span>, C=<span class="number">1e42</span>, solver=<span class="string">&#x27;liblinear&#x27;</span>)</span><br><span class="line">full_model.fit(X, y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;percentage of loans predicted to default: &#x27;</span>,</span><br><span class="line"><span class="number">100</span> * np.mean(full_model.predict(X) == <span class="string">&#x27;default&#x27;</span>))</span><br></pre></td></tr></table></figure><p>只有0.39%的贷款被预测为违约，不到预期数量的1/47。已还清的贷款<strong>压倒性地盖过了</strong>违约贷款，因为模型在训练时对所有数据给予了同等的权重。直观地思考，存在如此多的未违约贷款，加上预测变量数据中不可避免的变异性，这意味着，即使对于一笔违约贷款，模型也可能偶然找到一些与它相似的未违约贷款。当使用平衡样本时，大约50%的贷款被预测为违约。</p><h4 id="过采样和上下加权"><strong>过采样和上/下加权</strong></h4><p>Oversampling and Up/Down Weighting</p><p>对<strong>欠采样</strong>方法的一种批评是，它<strong>丢弃了数据</strong>，没有利用手头的所有信息。如果你的数据集相对较小，且稀有类别只包含几百或几千条记录，那么对主导类别进行欠采样可能会丢失有用的信息。在这种情况下，你应该过采样（或升采样）<strong>稀有类别，通过</strong>带放回抽样（自举法）来增加额外的行。</p><p>你也可以通过<strong>加权数据</strong>来达到类似的效果。许多分类算法都接受一个权重参数，允许你对数据进行上/下加权。例如，在 R 中使用 <code>glm</code> 函数的 <code>weight</code> 参数，对贷款数据应用一个权重向量：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">wt <span class="operator">&lt;-</span> ifelse<span class="punctuation">(</span>full_train_set<span class="operator">$</span>outcome<span class="operator">==</span><span class="string">&#x27;default&#x27;</span><span class="punctuation">,</span></span><br><span class="line"><span class="number">1</span> <span class="operator">/</span> mean<span class="punctuation">(</span>full_train_set<span class="operator">$</span>outcome <span class="operator">==</span> <span class="string">&#x27;default&#x27;</span><span class="punctuation">)</span><span class="punctuation">,</span> <span class="number">1</span><span class="punctuation">)</span></span><br><span class="line">full_model <span class="operator">&lt;-</span> glm<span class="punctuation">(</span>outcome <span class="operator">~</span> payment_inc_ratio <span class="operator">+</span> purpose_ <span class="operator">+</span> home_ <span class="operator">+</span></span><br><span class="line">emp_len_<span class="operator">+</span> dti <span class="operator">+</span> revol_bal <span class="operator">+</span> revol_util<span class="punctuation">,</span></span><br><span class="line">data<span class="operator">=</span>full_train_set<span class="punctuation">,</span> weight<span class="operator">=</span>wt<span class="punctuation">,</span> family<span class="operator">=</span><span class="string">&#x27;quasibinomial&#x27;</span><span class="punctuation">)</span></span><br><span class="line">pred <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>full_model<span class="punctuation">)</span></span><br><span class="line">mean<span class="punctuation">(</span>pred <span class="operator">&gt;</span> <span class="number">0</span><span class="punctuation">)</span></span><br><span class="line"><span class="punctuation">[</span><span class="number">1</span><span class="punctuation">]</span> <span class="number">0.5767208</span></span><br></pre></td></tr></table></figure><p>大多数 <code>scikit-learn</code> 方法允许在 <code>fit</code> 函数中使用 <code>sample_weight</code> 关键字参数来指定权重：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">default_wt = <span class="number">1</span> / np.mean(full_train_set.outcome == <span class="string">&#x27;default&#x27;</span>)</span><br><span class="line">wt = [default_wt <span class="keyword">if</span> outcome == <span class="string">&#x27;default&#x27;</span> <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> outcome <span class="keyword">in</span> full_train_set.outcome]</span><br><span class="line">full_model = LogisticRegression(penalty=<span class="string">&quot;l2&quot;</span>, C=<span class="number">1e42</span>, solver=<span class="string">&#x27;liblinear&#x27;</span>)</span><br><span class="line">full_model.fit(X, y, sample_weight=wt)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;percentage of loans predicted to default (weighting): &#x27;</span>,</span><br><span class="line"><span class="number">100</span> * np.mean(full_model.predict(X) == <span class="string">&#x27;default&#x27;</span>))</span><br></pre></td></tr></table></figure><p>对于违约贷款，权重被设置为 <span class="math inline">\(1/p\)</span>，其中 <span class="math inline">\(p\)</span> 是违约的概率。未违约贷款的权重为1。违约贷款和未违约贷款的<strong>权重总和大致相等</strong>。现在预测值的均值约为58%，而不是0.39%。</p><p>请注意，加权提供了<strong>过采样稀有类别和欠采样主导类别</strong>的替代方案。</p><blockquote><p>通用注解：</p><p><strong>调整损失函数</strong>（Adapting the Loss Function）</p><p>许多分类和回归算法都优化某个特定标准或<strong>损失函数</strong>。例如，逻辑回归试图最小化<strong>偏差（deviance）</strong>。在文献中，有人提出修改损失函数以避免由稀有类别引起的问题。在实践中，这很难做到：分类算法可能很复杂且难以修改。<strong>加权</strong>是一种改变损失函数的简单方法，它<strong>降低了低权重记录的错误对模型的影响，从而倾向于高权重记录</strong>。</p></blockquote><h4 id="数据生成"><strong>数据生成</strong></h4><p>Data Generation</p><p><strong>数据生成</strong>是<strong>通过扰动现有记录来创建新记录</strong>，是利用自举法进行过采样的一种变体（参见第232页的“过采样和上/下加权”）。这一想法的直觉在于，由于我们只观察到有限的实例集，算法没有足够丰富的信息来构建分类“规则”。通过创建<strong>与现有记录相似但不完全相同</strong>的新记录，算法有机会学习到一套更健壮的规则。这种思想在精神上类似于<strong>集成统计模型</strong>（ensemble statistical models），例如<strong>提升法（boosting）</strong>和<strong>装袋法（bagging）</strong>（参见第6章）。</p><p>随着 SMOTE 算法（<strong>“合成少数类过采样技术”</strong>的缩写）的发布，这一想法得到了推广。SMOTE 算法会找到一条与被过采样的记录相似的记录（参见第238页的“K-最近邻”），并创建一个<strong>合成记录</strong>，该记录是原始记录和相邻记录的随机加权平均值，其中权重是针对每个预测变量单独生成的。创建的合成过采样记录的数量取决于所需的过采样比率，以使数据集在结果类别上达到大致平衡。</p><p>在 R 中有几个 SMOTE 的实现。处理不平衡数据最全面的包是 <code>unbalanced</code>。它提供了多种技术，包括一个用于选择最佳方法的“Racing”算法。然而，SMOTE 算法本身足够简单，可以使用 <code>FNN</code> 包在 R 中直接实现。</p><p>Python 包 <code>imbalanced-learn</code> 实现了一系列方法，其 API 与 <code>scikit-learn</code> 兼容。它提供了各种过采样和欠采样的方法，并支持将这些技术与提升法和装袋法分类器结合使用。</p><h4 id="基于成本的分类"><strong>基于成本的分类</strong></h4><p>Cost-Based Classification</p><p>在实践中，准确率和 AUC 是选择分类规则的<strong>一种简陋方式</strong>。通常，可以为<strong>假阳性与假阴性</strong>分配一个<strong>估计成本</strong>，更合适的方法是结合这些成本来确定分类1和0时的<strong>最佳截止点</strong>。例如，假设一笔新贷款违约的预期成本为 <span class="math inline">\(C\)</span>，而一笔已还清贷款的预期回报为 <span class="math inline">\(R\)</span>。那么该笔贷款的预期回报（expected return）是：</p><p><span class="math display">\[预期回报 = P(Y=0) \times R + P(Y=1) \times C\]</span> 与其简单地将一笔贷款标记为违约或已还清，或者确定违约概率，更有意义的是<strong>确定该笔贷款是否具有正的预期回报</strong>。预测的违约概率是一个中间步骤，必须将其与贷款的总价值结合起来，以确定<strong>预期利润</strong>，这才是业务的最终规划指标。例如，一笔价值较小的贷款可能会被放弃，而选择一笔价值较大但预测违约概率稍高的贷款。</p><h4 id="探索预测结果"><strong>探索预测结果</strong></h4><p>Exploring the Predictions</p><p>仅仅一个单一的指标，例如AUC，无法评估模型对特定情况的适用性的所有方面。图5-8展示了针对贷款数据，仅使用两个预测变量：<code>borrower_score</code> 和 <code>payment_inc_ratio</code>，拟合的四种不同模型的决策规则。这些模型包括：线性判别分析（LDA）、逻辑线性回归、使用广义加性模型（GAM）拟合的逻辑回归，以及一个树模型（参见第249页的“树模型”）。图中线左上方的区域对应于<strong>预测违约</strong>。在这种情况下，LDA 和逻辑线性回归的结果几乎相同。树模型产生了最不规则的规则，呈现出两个台阶。最后，逻辑回归的 GAM 拟合代表了树模型和线性模型之间的折中。</p><p><img src="/img3/面向数据科学家的实用统计学/F5.8.png" alt="F5.8" style="zoom:50%;" /></p><p>在更高维度中，或者在 GAM 和树模型的情况下，即使是生成这些规则的区域，也<strong>不容易可视化</strong>。</p><p>无论如何，对预测值进行探索性分析总是值得的。</p><p><strong>关键思想</strong></p><ul><li><strong>高度不平衡的数据</strong>（即，感兴趣的结果，即1，很罕见）对于分类算法来说是<strong>有问题的</strong>。</li><li>处理不平衡数据的一种策略是，通过<strong>欠采样</strong>多数类别（或<strong>过采样</strong>稀有类别）来平衡训练数据。</li><li>如果使用了所有1s后仍然太少，你可以对稀有类别进行<strong>自举</strong>，或使用 <strong>SMOTE</strong> 来创建与现有稀有案例相似的<strong>合成数据</strong>。</li><li>不平衡数据通常意味着正确分类某个类别（即1s）具有更高的价值，而这种价值比率应该被纳入评估指标中。</li></ul><h3 id="总结"><strong>总结</strong></h3><p><strong>分类</strong>，即预测一条记录属于两个或多个类别中的哪一个，是预测分析的一个基本工具。一笔贷款会违约吗（是或否）？它会提前还款吗？一个网站访问者会点击链接吗？他们会购买东西吗？一笔保险索赔是欺诈性的吗？在分类问题中，通常有一个类别是主要关注的（例如，欺诈性的保险索赔），在二元分类中，这个类别被指定为1，而另一个更普遍的类别为0。通常，这个过程的一个关键部分是<strong>估计倾向得分</strong>，即属于目标类别的概率。一个常见的情景是，感兴趣的类别相对<strong>罕见</strong>。在评估分类器时，有多种<strong>超越简单准确率</strong>的模型评估指标；当所有记录都分类为0也能产生高准确率时，这些指标在稀有类别情境下尤为重要。</p>]]></content>
      
      
      <categories>
          
          <category> 翻译 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> AI </tag>
            
            <tag> 统计 </tag>
            
            <tag> 数据科学 </tag>
            
            <tag> R </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第4章 回归与预测</title>
      <link href="/2025/09/25/%E7%AC%AC4%E7%AB%A0%20%E5%9B%9E%E5%BD%92%E4%B8%8E%E9%A2%84%E6%B5%8B/"/>
      <url>/2025/09/25/%E7%AC%AC4%E7%AB%A0%20%E5%9B%9E%E5%BD%92%E4%B8%8E%E9%A2%84%E6%B5%8B/</url>
      
        <content type="html"><![CDATA[<p><a href="/img3/面向数据科学家的实用统计学/Practical-Statistics-for-Data-Scientists.pdf">《Practical Statistics for Data Scientists》书籍英文版</a><br /><a href="/img3/面向数据科学家的实用统计学/面向数据科学家的实用统计学.pdf">《面向数据科学家的实用统计学》中文版书籍</a></p><h2 id="第-4-章-回归与预测">第 4 章 回归与预测</h2><p>在统计学中，也许最常见的目标就是回答这样的问题：“变量 X（或者更常见地，X₁, …, Xₚ）是否与变量 Y 有关联？如果有，这种关系是什么，我们能否利用它来预测 Y？”</p><p>在预测领域——特别是基于其他“预测变量”的值来预测一个结果（目标）变量——<strong>统计学与数据科学</strong>的联系最为紧密。这一过程是在结果已知的数据上训练模型，以便随后将其应用于结果未知的数据，称为<strong>监督学习</strong>。数据科学与统计学的另一个重要交叉领域是<strong>异常检测</strong>：最初用于数据分析和改进回归模型的回归诊断方法，也可用来检测异常记录。</p><span id="more"></span><h3 id="简单线性回归">简单线性回归</h3><p>简单线性回归提供了一个描述两个变量大小之间关系的模型。例如，随着 X 增大，Y 也增大；或者随着 X 增大，Y 减小。相关系数是衡量两个变量如何相关的另一种方式（见第 30 页“相关”一节）。不同的是，相关系数衡量的是两个变量之间联系的强度，而回归则量化了这种关系的具体形式。</p><p><strong>简单线性回归的关键术语</strong></p><p><strong>响应变量（Response）</strong> 我们试图预测的变量。 同义词：因变量、Y 变量、目标、结果</p><p>​ dependent variable, Y variable, target, outcome</p><p><strong>自变量（Independent variable）</strong> 用于预测响应变量的变量。 同义词：X 变量、特征、属性、预测变量</p><p>​ X variable, feature, attribute, predictor</p><p><strong>记录（Record）</strong> 某个具体个体或案例的预测变量与结果变量值构成的向量。 同义词：行、案例、实例、样本</p><p><strong>截距（Intercept）</strong> 回归直线的截距，即当 X = 0 时预测的值。 同义词：b₀、β₀</p><p><strong>回归系数（Regression coefficient）</strong> 回归直线的斜率。 同义词：斜率、b₁、β₁、参数估计、权重</p><p><strong>拟合值（Fitted values）</strong> 从回归直线得到的 Ŷᵢ 的估计值。 同义词：预测值</p><p><strong>残差（Residuals）</strong> 观测值与拟合值之间的差异。 同义词：误差</p><p>​ errors</p><p><strong>最小二乘法（Least squares）</strong> 通过最小化残差平方和来拟合回归的方法。 同义词：普通最小二乘、OLS</p><h4 id="回归方程">回归方程</h4><p>The Regression Equation</p><p>简单线性回归估计当 X 发生一定变化时，Y 会改变多少。在相关系数中，变量 X 和 Y 是可以互换的；而在回归分析中，我们试图利用线性关系（即一条直线）由 X 预测 Y 变量： <span class="math display">\[Y = b_0 + b_1X\]</span></p><p>我们读作：“Y 等于 b₁ 乘以 X，再加上一个常数 b₀”。符号 <strong>b₀</strong> 称为截距（或常数），符号 <strong>b₁</strong> 称为 X 的斜率。在 R 的输出中，两者都显示为系数，不过在一般用法中，“系数”这一术语通常只指 b₁。Y 变量称为<strong>响应变量</strong>或<strong>因变量</strong>，因为它依赖于 X；X 变量称为<strong>预测变量</strong>或<strong>自变量</strong>。机器学习领域倾向于用其他术语，把 Y 称为“目标（target）”，把 X 称为“特征向量（feature vector）”。在本书中，我们将“预测变量（predictor）”和“特征（feature）”两个词交替使用。</p><p><img src="/img3/面向数据科学家的实用统计学/F4.1.png" alt="F4.1" style="zoom:33%;" /></p><p>请看图 4-1 中的散点图，显示工人接触棉尘的年数（Exposure）与肺活量指标（PEFR 或“呼气峰流速”）之间的关系。PEFR 与 Exposure 的关系如何？仅从图上很难判断。</p><p>简单线性回归试图找到一条“最佳”的直线，用来预测响应变量 PEFR 作为预测变量 Exposure 的函数：</p><p><span class="math display">\[\text{PEFR} = b_0 + b_1\text{Exposure}\]</span></p><p>在 R 中可以用 <strong>lm</strong> 函数拟合线性回归模型：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model <span class="operator">&lt;-</span> lm<span class="punctuation">(</span>PEFR <span class="operator">~</span> Exposure<span class="punctuation">,</span> data<span class="operator">=</span>lung<span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>lm 表示 “linear model”，波浪号 <code>~</code> 表示“由 Exposure 预测 PEFR”。在这种模型定义下，截距会自动包含并拟合。如果想排除截距，需要这样写：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PEFR <span class="operator">~</span> Exposure <span class="operator">-</span> <span class="number">1</span></span><br></pre></td></tr></table></figure><p>打印模型对象得到如下输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Call:</span><br><span class="line">lm(formula = PEFR ~ Exposure, data = lung)</span><br><span class="line"></span><br><span class="line">Coefficients:</span><br><span class="line">(Intercept)   Exposure</span><br><span class="line">   424.583     -4.185</span><br></pre></td></tr></table></figure><p>其中截距 <strong>b₀ = 424.583</strong>，可以解释为“工人接触棉尘年数为 0 时预测的 PEFR”。回归系数 <strong>b₁ = -4.185</strong> 可以解释为：“工人每多接触棉尘一年，PEFR 测量值平均减少 4.185”。</p><p>在 Python 中，可以使用 scikit-learn 包的 <strong>LinearRegression</strong>。（statsmodels 包也有一个与 R 类似的线性回归实现 sm.OLS，我们将在本章后面使用它）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">predictors = [<span class="string">&#x27;Exposure&#x27;</span>]</span><br><span class="line">outcome = <span class="string">&#x27;PEFR&#x27;</span></span><br><span class="line">model = LinearRegression()</span><br><span class="line">model.fit(lung[predictors], lung[outcome])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Intercept: <span class="subst">&#123;model.intercept_:<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Coefficient Exposure: <span class="subst">&#123;model.coef_[<span class="number">0</span>]:<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p>该模型得到的回归直线如图 4-2 所示。</p><p><img src="/img3/面向数据科学家的实用统计学/F4.2.png" alt="F4.2" style="zoom:33%;" /></p><h4 id="拟合值与残差">拟合值与残差</h4><p>Fitted Values and Residuals</p><p>回归分析中的重要概念是<strong>拟合值</strong>（预测值the predictions）和<strong>残差</strong>（预测误差 prediction errors）。一般情况下，数据不会完全落在一条直线上，所以回归方程应包含一个显式的误差项 <span class="math inline">\(e_i\)</span>：</p><p><span class="math display">\[Y_i = b_0 + b_1X_i + e_i\]</span></p><p><strong>拟合值</strong>（也称<strong>预测值</strong>）通常记作 <span class="math inline">\(\hat{Y}_i\)</span>（Y-hat），其表达式为：</p><p><span class="math display">\[\hat{Y}_i = b_0 + b_1X_i\]</span></p><p>这里的 <span class="math inline">\(b_0\)</span> 和 <span class="math inline">\(b_1\)</span> 表示系数是估计值，而不是已知值。</p><blockquote><p><strong>知识点：</strong></p><p>“帽子”符号（hat notation）用来区分<strong>估计值</strong>与<strong>真实值</strong>。因此，符号 <span class="math inline">\(\hat{b}\)</span>（“b-hat”）是未知参数 <span class="math inline">\(b\)</span> 的估计值。统计学家之所以要区分估计值和真实值，是因为<strong>估计值有不确定性，而真实值是固定的</strong>。</p></blockquote><p>我们通过原始数据减去预测值来计算残差 <span class="math inline">\(e_i\)</span>：</p><p><span class="math display">\[e_i = Y_i - \hat{Y}_i\]</span></p><p>在 R 中，可以用 <strong>predict</strong> 和 <strong>residuals</strong> 函数获得拟合值和残差：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fitted <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>model<span class="punctuation">)</span></span><br><span class="line">resid <span class="operator">&lt;-</span> residuals<span class="punctuation">(</span>model<span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>在 scikit-learn 的 <strong>LinearRegression</strong> 模型中，可以用 <strong>predict</strong> 方法在训练数据上获得拟合值，再计算残差。这是 scikit-learn 中所有模型的通用模式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fitted = model.predict(lung[predictors])</span><br><span class="line">residuals = lung[outcome] - fitted</span><br></pre></td></tr></table></figure><p>图 4-3 展示了肺活量数据回归直线的残差。残差就是从数据点到回归直线的垂直虚线的长度。</p><p><img src="/img3/面向数据科学家的实用统计学/F4.3.png" alt="F4.3" style="zoom:50%;" /></p><h4 id="最小二乘法">最小二乘法</h4><p>Least Squares</p><p>那么模型是如何拟合数据的？当关系很明显时，你可以想象手工拟合一条直线。实际上，<strong>回归直线是通过最小化残差平方和（RSS, residual sum of squares）来估计的</strong>：</p><p><span class="math display">\[\text{RSS}=\sum_{i=1}^{n}(Y_i - \hat{Y}_i)^2=\sum_{i=1}^{n}(Y_i - b_0 - b_1X_i)^2\]</span></p><p><span class="math inline">\(b_0\)</span> 和 <span class="math inline">\(b_1\)</span> 的估计值是使 RSS 最小的数值。</p><p>最小化残差平方和的方法称为<strong>最小二乘回归</strong>（least squares regression），或<strong>普通最小二乘回归</strong>（ordinary least squares, OLS）。这种方法常被归功于德国数学家高斯（Carl Friedrich Gauss），但实际上是由法国数学家勒让德（Adrien-Marie Legendre）在 1805 年首次发表的。最小二乘回归可以通过任何标准统计软件快速、方便地计算。</p><p>从历史上看，计算便利性是最小二乘法在回归中广泛应用的原因之一。在大数据时代，计算速度仍然是重要因素。<strong>最小二乘法对异常值敏感</strong>（就像均值一样，见第 10 页的“中位数与稳健估计”），不过这通常只在小型或中等规模数据集里才是个显著问题。有关回归中异常值的讨论见第 177 页“异常值”。</p><blockquote><p><strong>通用注解</strong></p><p>回归术语<strong>：当分析师和研究人员单独使用“回归”这个词时，通常指的是</strong>线性回归<strong>，重点是建立一个线性模型来解释预测变量与数值型结果变量之间的关系。在正式的统计学意义上，</strong>回归还包括非线性模型<strong>，即预测变量与结果变量之间存在某种函数关系的模型。在机器学习领域，“回归”这个词有时也被宽泛地用来指任何</strong>输出连续数值型预测<strong>的模型（而相对于</strong>分类**方法，分类预测的是二元或类别型结果）。</p></blockquote><h4 id="预测与解释画像分析"><strong>预测与解释（画像分析）</strong></h4><p>Prediction Versus Explanation (Profiling)</p><p>从<strong>历史上</strong>看，回归分析的主要用途是揭示预测变量与结果变量之间的线性关系。<strong>目标是理解这种关系，并用拟合回归的数据来解释它</strong>。在这种情况下，主要关注的是回归方程中估计的斜率 <span class="math inline">\(b\)</span>。经济学家想知道消费者支出与 GDP 增长之间的关系；公共卫生官员可能想了解一项公共宣传活动是否在促进安全性行为方面有效。在这些例子中，重点不是预测单个案例，而是理解变量之间的总体关系。</p><p>随着<strong>大数据</strong>的出现，回归被广泛<strong>用于为新数据预测个体结果（即建立预测模型），而不是解释手头的数据</strong>。在这种情况下，主要关注的是拟合值 <span class="math inline">\(\hat{Y}\)</span>。在市场营销中，回归可用于预测广告投放规模变化带来的收入变化；大学利用回归预测学生的 GPA 与其 SAT 分数之间的关系。</p><p>一个拟合良好的回归模型表明，当 <span class="math inline">\(X\)</span> 发生变化时，<span class="math inline">\(Y\)</span> 也会随之变化。然而，<strong>仅凭回归方程并不能证明因果方向</strong>。因果推论必须基于对关系更广泛的理解。例如，回归方程可能显示网页广告点击数与转化数之间存在显著关系。得出“点击广告导致销售”而不是“销售导致点击”的结论，<strong>是基于我们对营销过程的知识，而非回归方程本身。</strong></p><p><strong>关键要点</strong></p><ul><li>回归方程将响应变量 <span class="math inline">\(Y\)</span> 与预测变量 <span class="math inline">\(X\)</span> 的关系建模为一条直线。</li><li>回归模型产生拟合值和残差——即响应的预测值以及预测误差。</li><li>回归模型通常通过最小二乘法来拟合。</li><li>回归既可用于预测，也可用于解释。</li></ul><h3 id="多元线性回归"><strong>多元线性回归</strong></h3><p>Multiple Linear Regression</p><p>当有多个预测变量时，方程可以扩展为： <span class="math display">\[Y = b_0 + b_1 X_1 + b_2 X_2 + \dots + b_p X_p + e\]</span></p><p>此时不再是一条直线，而是一个线性模型——每个系数与其变量（特征）之间的关系是线性的。</p><p><strong>多元线性回归关键术语</strong></p><ul><li><p><strong>均方根误差（Root mean squared error, RMSE）</strong> 回归平均平方误差的平方根（这是比较回归模型最广泛使用的指标）。</p></li><li><p><strong>残差标准误（Residual standard error, RSE）</strong> 与均方根误差相同，但对自由度进行了调整。</p></li><li><p><strong>R 平方（R-squared, <span class="math inline">\(R^2\)</span>）</strong> 模型解释的方差比例，从 0 到 1。 同义词：决定系数（coefficient of determination）。</p></li><li><p><strong>t 统计量（t-statistic）</strong> 某个预测变量系数除以其标准误差，提供衡量模型中变量重要性的指标（参见第 110 页 “t 检验”）。</p></li><li><p><strong>加权回归（Weighted regression）</strong> 对不同记录赋予不同权重的回归。</p></li></ul><p>简单线性回归中的其他概念（如最小二乘拟合、拟合值和残差的定义）都可以推广到多元线性回归。例如，拟合值为：</p><p><span class="math display">\[\hat{Y}_i = b_0 + b_1 X_{1,i} + b_2 X_{2,i} + \dots + b_p X_{p,i}\]</span></p><h4 id="示例"><strong>示例</strong></h4><p>King County房屋数据</p><p>多元线性回归的一个示例是估计房屋价值。县评估员需要估算房屋的价值以评税；房地产专业人士和购房者会参考 Zillow 等流行网站来判断一个公平价格。下面是来自美国华盛顿州金县（西雅图）的房屋数据（<code>house</code> 数据框）的一些行：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">head<span class="punctuation">(</span>house<span class="punctuation">[</span><span class="punctuation">,</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;AdjSalePrice&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;SqFtTotLiving&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;SqFtLot&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;Bathrooms&#x27;</span><span class="punctuation">,</span></span><br><span class="line"><span class="string">&#x27;Bedrooms&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;BldgGrade&#x27;</span><span class="punctuation">)</span><span class="punctuation">]</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>数据来源：本地数据框 [6 x 6]</p><table><colgroup><col style="width: 19%" /><col style="width: 20%" /><col style="width: 13%" /><col style="width: 15%" /><col style="width: 14%" /><col style="width: 15%" /></colgroup><thead><tr class="header"><th>AdjSalePrice (int)</th><th>SqFtTotLiving (dbl)</th><th>SqFtLot (int)</th><th>Bathrooms (int)</th><th>Bedrooms (dbl)</th><th>BldgGrade (int)</th></tr></thead><tbody><tr class="odd"><td>1</td><td>300805</td><td>2400</td><td>9373</td><td>3.00</td><td>6</td></tr><tr class="even"><td>2</td><td>1076162</td><td>3764</td><td>20156</td><td>3.75</td><td>4</td></tr><tr class="odd"><td>3</td><td>761805</td><td>2060</td><td>26036</td><td>1.75</td><td>4</td></tr><tr class="even"><td>4</td><td>442065</td><td>3200</td><td>8618</td><td>3.75</td><td>5</td></tr><tr class="odd"><td>5</td><td>297065</td><td>1720</td><td>8620</td><td>1.75</td><td>4</td></tr><tr class="even"><td>6</td><td>411781</td><td>930</td><td>1012</td><td>1.50</td><td>2</td></tr></tbody></table><p>在 pandas 中，<code>head</code> 方法同样列出前几行：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">subset = [<span class="string">&#x27;AdjSalePrice&#x27;</span>, <span class="string">&#x27;SqFtTotLiving&#x27;</span>, <span class="string">&#x27;SqFtLot&#x27;</span>, <span class="string">&#x27;Bathrooms&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;Bedrooms&#x27;</span>, <span class="string">&#x27;BldgGrade&#x27;</span>]</span><br><span class="line">house[subset].head()</span><br></pre></td></tr></table></figure><p>我们的目标是用其它变量预测销售价格。R 语言的 <code>lm</code> 函数通过在公式右边加入更多变量就可以处理多元回归；参数 <code>na.action=na.omit</code> 表示模型会丢弃包含缺失值的记录：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">house_lm <span class="operator">&lt;-</span> lm<span class="punctuation">(</span>AdjSalePrice <span class="operator">~</span> SqFtTotLiving <span class="operator">+</span> SqFtLot <span class="operator">+</span> Bathrooms <span class="operator">+</span></span><br><span class="line">Bedrooms <span class="operator">+</span> BldgGrade<span class="punctuation">,</span></span><br><span class="line">data<span class="operator">=</span>house<span class="punctuation">,</span> na.action<span class="operator">=</span>na.omit<span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>scikit-learn 的 <code>LinearRegression</code> 也可以用于多元线性回归：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">predictors = [<span class="string">&#x27;SqFtTotLiving&#x27;</span>, <span class="string">&#x27;SqFtLot&#x27;</span>, <span class="string">&#x27;Bathrooms&#x27;</span>, <span class="string">&#x27;Bedrooms&#x27;</span>, <span class="string">&#x27;BldgGrade&#x27;</span>]</span><br><span class="line">outcome = <span class="string">&#x27;AdjSalePrice&#x27;</span></span><br><span class="line">house_lm = LinearRegression()</span><br><span class="line">house_lm.fit(house[predictors], house[outcome])</span><br></pre></td></tr></table></figure><p>在 R 中打印 <code>house_lm</code> 对象会产生以下输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Call:</span><br><span class="line">lm(formula = AdjSalePrice ~ SqFtTotLiving + SqFtLot + Bathrooms +</span><br><span class="line">Bedrooms + BldgGrade, data = house, na.action = na.omit)</span><br><span class="line"></span><br><span class="line">Coefficients:</span><br><span class="line">(Intercept) SqFtTotLiving SqFtLot Bathrooms</span><br><span class="line">-5.219e+05 2.288e+02 -6.047e-02 -1.944e+04</span><br><span class="line">Bedrooms BldgGrade</span><br><span class="line">-4.777e+04 1.061e+05</span><br></pre></td></tr></table></figure><p>在 scikit-learn 的 <code>LinearRegression</code> 模型中，截距（intercept）和系数（coefficients）分别存储在拟合模型的 <code>intercept_</code> 和 <code>coef_</code> 属性中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Intercept: <span class="subst">&#123;house_lm.intercept_:<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Coefficients:&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> name, coef <span class="keyword">in</span> <span class="built_in">zip</span>(predictors, house_lm.coef_):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27; <span class="subst">&#123;name&#125;</span>: <span class="subst">&#123;coef&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p>这些系数的解释与简单线性回归相同：在其他变量 <span class="math inline">\(X_k\)</span>（当 <span class="math inline">\(k \neq j\)</span>）保持不变的情况下，预测值 <span class="math inline">\(Y\)</span> 每当 <span class="math inline">\(X_j\)</span> 变化 1 个单位时，便按系数 <span class="math inline">\(b_j\)</span> 的大小变化。例如，在房屋中增加一平方英尺的可用面积，估计价值大约增加 229 美元；增加 1,000 平方英尺的可用面积则意味着价值增加约 228,800 美元。</p><h4 id="评估模型">评估模型</h4><p>Assessing the Model</p><p>从数据科学角度来看，最重要的性能指标是<strong>均方根误差（Root Mean Squared Error, RMSE）</strong>。RMSE 是预测值 <span class="math inline">\(\hat y_i\)</span> 与真实值 <span class="math inline">\(y_i\)</span> 的误差平方的平均值再开方： <span class="math display">\[\text{RMSE}=\sqrt{\frac{\sum_{i=1}^{n}(y_i-\hat y_i)^2}{n}}\]</span></p><p>它衡量模型的整体精度，也是将该模型与其他模型（包括用机器学习方法拟合的模型）进行比较的基础。与 RMSE 类似的是<strong>残差标准误差（Residual Standard Error, RSE）</strong>。在这种情况下有 <span class="math inline">\(p\)</span> 个自变量，RSE 公式为：</p><p><span class="math display">\[\text{RSE}=\sqrt{\frac{\sum_{i=1}^{n}(y_i-\hat y_i)^2}{n-p-1}}\]</span></p><p>唯一的区别在于分母——RSE 用自由度而不是样本数（参见本书第116页“自由度”部分）。在实践中，对于线性回归，RMSE 与 RSE 的差别通常非常小，尤其是在大数据场景下。</p><p>R 语言的 <code>summary</code> 函数可以计算 RSE 以及回归模型的其他指标：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">summary<span class="punctuation">(</span>house_lm<span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">Call:</span><br><span class="line">lm(formula = AdjSalePrice ~ SqFtTotLiving + SqFtLot + Bathrooms +</span><br><span class="line">Bedrooms + BldgGrade, data = house, na.action = na.omit)</span><br><span class="line"></span><br><span class="line">Residuals:</span><br><span class="line">    Min       1Q   Median       3Q      Max</span><br><span class="line">-1199479 -118908  -20977   87435  9473035</span><br><span class="line"></span><br><span class="line">Coefficients:</span><br><span class="line">               Estimate  Std. Error  t value  Pr(&gt;|t|)</span><br><span class="line">(Intercept)    -5.219e+05  1.565e+04 -33.342  &lt; 2e-16 ***</span><br><span class="line">SqFtTotLiving   2.288e+02  3.899e+00  58.694  &lt; 2e-16 ***</span><br><span class="line">SqFtLot        -6.047e-02  6.118e-02  -0.988   0.323</span><br><span class="line">Bathrooms      -1.944e+04  3.625e+03  -5.363  8.27e-08 ***</span><br><span class="line">Bedrooms       -4.777e+04  2.490e+03 -19.187  &lt; 2e-16 ***</span><br><span class="line">BldgGrade       1.061e+05  2.396e+03  44.277  &lt; 2e-16 ***</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1</span><br><span class="line"></span><br><span class="line">Residual standard error: 261300 on 22681 degrees of freedom</span><br><span class="line">Multiple R-squared: 0.5406, Adjusted R-squared: 0.5405</span><br><span class="line">F-statistic: 5338 on 5 and 22681 DF, p-value: &lt; 2.2e-16</span><br></pre></td></tr></table></figure><p>scikit-learn 提供了许多用于回归和分类的评估指标。这里我们用 <code>mean_squared_error</code> 计算 RMSE，用 <code>r2_score</code> 计算决定系数（R²）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">fitted = house_lm.predict(house[predictors])</span><br><span class="line">RMSE = np.sqrt(mean_squared_error(house[outcome], fitted))</span><br><span class="line">r2 = r2_score(house[outcome], fitted)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;RMSE: <span class="subst">&#123;RMSE:<span class="number">.0</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;r2: <span class="subst">&#123;r2:<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p>在 Python 中可以使用 <code>statsmodels</code> 对回归模型做更详细的分析：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = sm.OLS(house[outcome], house[predictors].assign(const=<span class="number">1</span>))</span><br><span class="line">results = model.fit()</span><br><span class="line">results.summary()</span><br></pre></td></tr></table></figure><p>这里用到的 pandas 方法 <code>assign</code> 会在自变量中添加一个值为 1 的常数列，这是为了在模型中包含截距项所必需的。</p><p><strong>另一个有用的指标：决定系数（R²）</strong>：你在软件输出中还会看到另一个有用的指标：<strong>决定系数</strong>（coefficient of determination），也叫 <strong>R² 统计量</strong>或 <strong>R-squared</strong>。R² 的取值范围从 0 到 1，用于衡量模型解释了数据变异的比例。它主要用于回归的解释性分析中，用来评估模型对数据的拟合程度。R² 的公式是： <span class="math display">\[R^2 = 1 - \frac{\sum_{i=1}^n (y_i-\hat{y}_i)^2}{\sum_{i=1}^n (y_i-\bar{y})^2}\]</span></p><p>分母与 <span class="math inline">\(Y\)</span> 的方差成比例。R 的输出中还报告了 <strong>调整后的 R²</strong>（adjusted R-squared），它会根据自由度进行调整，相当于对模型中增加的预测变量进行惩罚。在大数据集的多元回归中，调整后的 R² 与普通 R² 差别通常不大。</p><p>在估计系数的同时，R 和 statsmodels 还会报告系数的标准误差（SE）以及 <strong>t 统计量</strong>：</p><p><span class="math display">\[t_b=\frac{b}{SE(b)}\]</span></p><p><strong>t 统计量</strong>（以及其对应的 <strong>p 值</strong>）衡量系数在统计意义上是否显著——也就是它是否超出了预测变量和目标变量之间随机关系所能产生的范围。 t 统计量越高（p 值越低），该预测变量的重要性就越显著。由于“简约”是模型的重要特征，因此像 t 统计量这样的工具非常有用，可以用来指导是否在模型中保留某个预测变量（参见第156页“模型选择与逐步回归”）。</p><blockquote><p><strong>警告</strong></p><p>除了 t 统计量，R 和其他软件包通常还会报告 <strong>p 值</strong>（在 R 输出中为 Pr(&gt;|t|)）以及 <strong>F 统计量</strong>。数据科学家通常不会深入解释这些统计量或“统计显著性”问题，而是主要把 <strong>t 统计量</strong>作为一个有用的参考：</p><ul><li><strong>高 t 统计量（p 值接近 0）</strong> → 该预测变量应保留在模型中</li><li><strong>非常低的 t 统计量</strong> → 该预测变量可以考虑移除 关于 p 值的更多讨论，见第106页“p 值”。</li></ul></blockquote><h4 id="交叉验证">交叉验证</h4><p>Cross-Validation</p><p>经典的统计回归指标（R²、F 统计量和 p 值）都是“样本内”指标（in-sample metrics）——也就是应用于用于拟合模型的同一批数据。直觉上，你可以看出，留出部分原始数据，不用它来拟合模型，再将模型应用于这部分留出样本（holdout sample）来测试其表现，会更有意义。通常，你会用大部分数据来训练模型，用一小部分来测试模型。</p><p>这种“样本外”验证（out-of-sample validation）的思路并不新鲜，但直到大数据集更普遍出现后才真正流行起来；在小数据集下，分析人员通常希望用到全部数据以拟合出最优模型。</p><p>然而，使用单个留出样本会引入不确定性：如果你选择了不同的留出样本，评估结果会有多大差别？</p><p><strong>交叉验证</strong>将留出样本的思路扩展为多个连续的留出样本。基本 <strong>k 折交叉验证</strong>（k-fold cross-validation）的算法如下：</p><ol type="1"><li>把数据中的 <span class="math inline">\(1/k\)</span> 留作测试样本（holdout sample）。</li><li>用剩余的 <span class="math inline">\(k-1\)</span> 份数据训练模型。</li><li>用模型对这 <span class="math inline">\(1/k\)</span> 的留出样本进行预测，记录所需的模型评估指标。</li><li>把第一个 <span class="math inline">\(1/k\)</span> 的数据放回，换取下一个 <span class="math inline">\(1/k\)</span>（不包括已抽取过的记录）。</li><li>重复步骤 2 和 3。</li><li>持续重复，直到每条记录都曾被用作留出样本。</li><li>对所有折叠的评估指标进行平均或合并。</li></ol><p>这种把数据分成训练集（training sample）和留出集（training sample）的划分方式，也称为一个<strong>折叠（fold）</strong>。</p><h4 id="模型选择与逐步回归">模型选择与逐步回归</h4><p>Model Selection and Stepwise Regression</p><p>在某些问题中，回归模型可能有许多可用作预测变量的特征。例如，为了预测房屋价格，我们可以添加地下室面积、建造年份等额外变量。在 <strong>R</strong> 中，这些变量很容易添加到回归方程里：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">house_full <span class="operator">&lt;-</span> lm<span class="punctuation">(</span>AdjSalePrice <span class="operator">~</span> SqFtTotLiving <span class="operator">+</span> SqFtLot <span class="operator">+</span> Bathrooms <span class="operator">+</span></span><br><span class="line">Bedrooms <span class="operator">+</span> BldgGrade <span class="operator">+</span> PropertyType <span class="operator">+</span> NbrLivingUnits <span class="operator">+</span></span><br><span class="line">SqFtFinBasement <span class="operator">+</span> YrBuilt <span class="operator">+</span> YrRenovated <span class="operator">+</span></span><br><span class="line">NewConstruction<span class="punctuation">,</span></span><br><span class="line">data<span class="operator">=</span>house<span class="punctuation">,</span> na.action<span class="operator">=</span>na.omit<span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>在 <strong>Python</strong> 中，我们需要先把类别型和布尔型变量转换为数值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">predictors = [<span class="string">&#x27;SqFtTotLiving&#x27;</span>, <span class="string">&#x27;SqFtLot&#x27;</span>, <span class="string">&#x27;Bathrooms&#x27;</span>, <span class="string">&#x27;Bedrooms&#x27;</span>, <span class="string">&#x27;BldgGrade&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;PropertyType&#x27;</span>, <span class="string">&#x27;NbrLivingUnits&#x27;</span>, <span class="string">&#x27;SqFtFinBasement&#x27;</span>, <span class="string">&#x27;YrBuilt&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;YrRenovated&#x27;</span>, <span class="string">&#x27;NewConstruction&#x27;</span>]</span><br><span class="line">X = pd.get_dummies(house[predictors], drop_first=<span class="literal">True</span>)</span><br><span class="line">X[<span class="string">&#x27;NewConstruction&#x27;</span>] = [<span class="number">1</span> <span class="keyword">if</span> nc <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> nc <span class="keyword">in</span> X[<span class="string">&#x27;NewConstruction&#x27;</span>]]</span><br><span class="line">house_full = sm.OLS(house[outcome], X.assign(const=<span class="number">1</span>))</span><br><span class="line">results = house_full.fit()</span><br><span class="line">results.summary()</span><br></pre></td></tr></table></figure><p>然而，添加更多变量并不一定意味着我们得到了更好的模型。统计学家遵循奥卡姆剃刀原则（Occam’s razor）来指导模型选择：在其他条件相同的情况下，应该优先使用更简单的模型，而不是更复杂的模型。</p><p>在训练数据中，增加变量总是会降低 <strong>RMSE</strong> 并提高 <strong>R²</strong>，因此这些指标并不能很好地指导模型选择。一种把模型复杂度纳入考虑的方法是使用<strong>调整后的 R²</strong>：</p><p><span class="math display">\[R^2_{\text{adj}} = 1 - (1-R^2)\frac{n-1}{n-P-1}\]</span></p><p>其中 <span class="math inline">\(n\)</span> 是样本数，<span class="math inline">\(P\)</span> 是模型中的变量数。</p><blockquote><p><strong>警告</strong>:</p><p><strong>AIC、BIC 和 Mallows Cp</strong>：20世纪70年代，日本著名统计学家赤池弘次（Hirotugu Akaike）提出了一个指标 <strong>AIC</strong>（Akaike 信息准则，Akaike’s Information Criteria），它会对模型中新增的变量进行惩罚。对于回归模型，AIC 公式为： <span class="math display">\[AIC = 2P + n\log\left(\frac{RSS}{n}\right)\]</span></p><p>其中 <span class="math inline">\(P\)</span> 是变量数，<span class="math inline">\(n\)</span> 是样本数。目标是找到<strong>使 AIC 最小的模型</strong>；如果多加了 <span class="math inline">\(k\)</span> 个变量，模型会受到 <span class="math inline">\(2k\)</span> 的惩罚。</p><p>AIC 有一些常见变体：</p><ul><li><strong>AICc</strong>：AIC 在小样本情况下的修正版本。</li><li><strong>BIC</strong>（Bayesian information criteria，贝叶斯信息准则）：类似 AIC，但对新增变量的惩罚更强。</li><li><strong>Mallows Cp</strong>：Colin Mallows 提出的 AIC 变体。</li></ul><p>这些指标通常在训练集（样本内）上报告；如果数据科学家使用留出样本（holdout data）来评估模型，就不必过于担心这些指标之间的差异或其背后的理论。</p></blockquote><p>如何找到最小 AIC 或最大调整后 R² 的模型呢？一种方法是搜索所有可能的模型，称为<strong>全子集回归（all subset regression）</strong>。这种方法计算量很大，对于包含大量数据和变量的问题来说不可行。一个更有吸引力的替代方法是使用<strong>逐步回归（stepwise regression）</strong>。</p><ul><li>可以从完整模型开始，逐步去掉没有显著贡献的变量（称为<strong>后向消除 backward elimination</strong>）。</li><li>也可以从常数模型开始，逐步加入变量（<strong>前向选择 forward selection</strong>）。</li><li>还可以交替加入和移除预测变量，以找到能降低 AIC 或调整后 R² 的模型。</li></ul><p><strong>R</strong> 中 Venables 和 Ripley 提供的 <strong>MASS</strong> 包就有一个逐步回归函数 <strong>stepAIC</strong>：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">library<span class="punctuation">(</span>MASS<span class="punctuation">)</span></span><br><span class="line">step <span class="operator">&lt;-</span> stepAIC<span class="punctuation">(</span>house_full<span class="punctuation">,</span> direction<span class="operator">=</span><span class="string">&quot;both&quot;</span><span class="punctuation">)</span></span><br><span class="line">step</span><br></pre></td></tr></table></figure><p>示例输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Call:</span><br><span class="line">lm(formula = AdjSalePrice ~ SqFtTotLiving + Bathrooms + Bedrooms +</span><br><span class="line">BldgGrade + PropertyType + SqFtFinBasement + YrBuilt, data = house,</span><br><span class="line">na.action = na.omit)</span><br><span class="line"></span><br><span class="line">Coefficients:</span><br><span class="line">(Intercept)            SqFtTotLiving</span><br><span class="line">6.179e+06              1.993e+02</span><br><span class="line">Bathrooms              Bedrooms</span><br><span class="line">4.240e+04             -5.195e+04</span><br><span class="line">BldgGrade              PropertyTypeSingle Family</span><br><span class="line">1.372e+05              2.291e+04</span><br><span class="line">PropertyTypeTownhouse  SqFtFinBasement</span><br><span class="line">8.448e+04              7.047e+00</span><br><span class="line">YrBuilt</span><br><span class="line">-3.565e+03</span><br></pre></td></tr></table></figure><p>​</p><blockquote><p>个人注：以下是用gemini翻译 20250915 12:00</p></blockquote><p>scikit-learn 没有实现逐步回归的功能。我们在我们的 dmba 包中实现了 <code>stepwise_selection</code>、<code>forward_selection</code> 和 <code>backward_elimination</code> 函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">y = house[outcome]</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_model</span>(<span class="params">variables</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(variables) == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">    model = LinearRegression()</span><br><span class="line">    model.fit(X[variables], y)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">score_model</span>(<span class="params">model, variables</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(variables) == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> AIC_score(y, [y.mean()] * <span class="built_in">len</span>(y), model, df=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> AIC_score(y, model.predict(X[variables]), model)</span><br><span class="line">best_model, best_variables = stepwise_selection(X.columns, train_model, score_model, verbose=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Intercept: <span class="subst">&#123;best_model.intercept_:<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Coefficients:&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> name, coef <span class="keyword">in</span> <span class="built_in">zip</span>(best_variables, best_model.coef_):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27; <span class="subst">&#123;name&#125;</span>: <span class="subst">&#123;coef&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p>定义一个函数，该函数为一组给定的变量返回一个拟合好的模型。定义一个函数，该函数为给定的模型和一组变量返回一个得分。在本例中，我们使用了 dmba 包中实现的 <code>AIC_score</code>。该函数选择了一个模型，其中 <code>house_full</code> 中的几个变量被剔除：<code>SqFtLot</code>、<code>NbrLivingUnits</code>、<code>YrRenovated</code> 和 <code>NewConstruction</code>。</p><p>更简单的方法是<strong>前向选择</strong>和<strong>后向选择</strong>。在前向选择中，你从零个预测变量开始，然后逐一添加，每一步都添加对 <span class="math inline">\(R^2\)</span> 贡献最大的预测变量，当贡献不再具有统计显著性时停止。在后向选择，或者说后向消除中，你从完整模型开始，然后移除不具有统计显著性的预测变量，直到你剩下的模型中所有预测变量都具有统计显著性。</p><p><strong>惩罚回归</strong>在精神上与 <strong>AIC</strong> 相似。它不是通过显式地搜索离散的模型集合，而是在模型拟合方程中加入一个惩罚，对拥有太多变量（参数）的模型进行惩罚。惩罚回归不会像逐步、前向和后向选择那样完全消除预测变量，而是通过减少系数来应用惩罚，在某些情况下会使其接近于零。常见的惩罚回归方法是<strong>岭回归</strong>（ridge regression）和 <strong>Lasso 回归</strong>（lasso regression）。</p><p>逐步回归和所有子集回归都是<strong>样本内</strong>方法，用于评估和调整模型。这意味着模型选择可能存在<strong>过拟合</strong>（拟合数据中的噪声），并且在应用于新数据时性能可能不佳。避免这种情况的一种常见方法是使用<strong>交叉验证</strong>来验证模型。在线性回归中，过拟合通常不是一个主要问题，因为数据被强加了一个简单的（线性）全局结构。对于更复杂的模型类型，特别是那些对局部数据结构作出反应的迭代过程，交叉验证是一个非常重要的工具；详见第155页的“交叉验证”部分。</p><h4 id="加权回归">加权回归</h4><p>Weighted Regression</p><p>统计学家出于多种目的使用加权回归；尤其是在分析复杂调查数据时，它非常重要。数据科学家可能会在以下两种情况下发现加权回归很有用：</p><ol type="1"><li><strong>逆方差加权</strong>：当不同观测值的测量精度不同时，可以采用这种方法；方差较高的观测值权重较低。</li><li><strong>多案例数据分析</strong>：当数据中的每一行代表多个原始观测值时，权重变量编码了每行所代表的原始观测值数量。</li></ol><p>以住房数据为例，较早的销售数据通常不如最近的销售数据可靠。我们可以使用 <code>DocumentDate</code> 来确定销售年份，并计算一个 <code>Weight</code> 变量，该变量是自2005年（数据开始年份）以来的年数：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">library<span class="punctuation">(</span>lubridate<span class="punctuation">)</span></span><br><span class="line">house<span class="operator">$</span>Year <span class="operator">=</span> year<span class="punctuation">(</span>house<span class="operator">$</span>DocumentDate<span class="punctuation">)</span></span><br><span class="line">house<span class="operator">$</span>Weight <span class="operator">=</span> house<span class="operator">$</span>Year <span class="operator">-</span> <span class="number">2005</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">house[<span class="string">&#x27;Year&#x27;</span>] = [<span class="built_in">int</span>(date.split(<span class="string">&#x27;-&#x27;</span>)[<span class="number">0</span>]) <span class="keyword">for</span> date <span class="keyword">in</span> house.DocumentDate]</span><br><span class="line">house[<span class="string">&#x27;Weight&#x27;</span>] = house.Year - <span class="number">2005</span></span><br></pre></td></tr></table></figure><p>我们可以使用 <code>lm</code> 函数中的 <code>weight</code> 参数计算加权回归：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">house_wt <span class="operator">&lt;-</span> lm<span class="punctuation">(</span>AdjSalePrice <span class="operator">~</span> SqFtTotLiving <span class="operator">+</span> SqFtLot <span class="operator">+</span> Bathrooms <span class="operator">+</span></span><br><span class="line">Bedrooms <span class="operator">+</span> BldgGrade<span class="punctuation">,</span></span><br><span class="line">data<span class="operator">=</span>house<span class="punctuation">,</span> weight<span class="operator">=</span>Weight<span class="punctuation">)</span></span><br><span class="line"><span class="built_in">round</span><span class="punctuation">(</span>cbind<span class="punctuation">(</span>house_lm<span class="operator">=</span>house_lm<span class="operator">$</span>coefficients<span class="punctuation">,</span></span><br><span class="line">house_wt<span class="operator">=</span>house_wt<span class="operator">$</span>coefficients<span class="punctuation">)</span><span class="punctuation">,</span> digits<span class="operator">=</span><span class="number">3</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><table><thead><tr class="header"><th style="text-align: left;"></th><th style="text-align: left;">house_lm</th><th style="text-align: left;">house_wt</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;">(Intercept)</td><td style="text-align: left;">-521871.368</td><td style="text-align: left;">-584189.329</td></tr><tr class="even"><td style="text-align: left;">SqFtTotLiving</td><td style="text-align: left;">228.831</td><td style="text-align: left;">245.024</td></tr><tr class="odd"><td style="text-align: left;">SqFtLot</td><td style="text-align: left;">-0.060</td><td style="text-align: left;">-0.292</td></tr><tr class="even"><td style="text-align: left;">Bathrooms</td><td style="text-align: left;">-19442.840</td><td style="text-align: left;">-26085.970</td></tr><tr class="odd"><td style="text-align: left;">Bedrooms</td><td style="text-align: left;">-47769.955</td><td style="text-align: left;">-53608.876</td></tr><tr class="even"><td style="text-align: left;">BldgGrade</td><td style="text-align: left;">106106.963</td><td style="text-align: left;">115242.435</td></tr></tbody></table><p>加权回归中的系数与原始回归中的系数略有不同。</p><p>scikit-learn 中的大多数模型在其 <code>fit</code> 方法调用中都接受 <code>sample_weight</code> 作为关键字参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">predictors = [<span class="string">&#x27;SqFtTotLiving&#x27;</span>, <span class="string">&#x27;SqFtLot&#x27;</span>, <span class="string">&#x27;Bathrooms&#x27;</span>, <span class="string">&#x27;Bedrooms&#x27;</span>, <span class="string">&#x27;BldgGrade&#x27;</span>]</span><br><span class="line">outcome = <span class="string">&#x27;AdjSalePrice&#x27;</span></span><br><span class="line">house_wt = LinearRegression()</span><br><span class="line">house_wt.fit(house[predictors], house[outcome], sample_weight=house.Weight)</span><br></pre></td></tr></table></figure><p><strong>关键思想</strong></p><ul><li><strong>多元线性回归</strong>：对响应变量 <code>Y</code> 与多个预测变量 <code>X1, ..., Xp</code> 之间的关系进行建模。</li><li><strong>最重要的评估指标</strong>：评估模型的最重要指标是<strong>均方根误差 (RMSE)</strong> 和 <strong>R平方 (<span class="math inline">\(R^2\)</span>)</strong>。</li><li><strong>系数的标准误</strong>：可用于衡量变量对模型贡献的可靠性。</li><li><strong>逐步回归</strong>：一种自动确定哪些变量应包含在模型中的方法。</li><li><strong>加权回归</strong>：用于在方程拟合过程中给予某些记录更多或更少的权重。</li></ul><h3 id="使用回归进行预测">使用回归进行预测</h3><p>Prediction Using Regression</p><p>在数据科学中，回归的主要目的是<strong>预测</strong>。这一点值得我们牢记，因为回归作为一种古老且成熟的统计方法，附带了一些与其作为<strong>解释性建模</strong>工具的传统角色更相关的“包袱（baggage）”，而这些包袱与预测的目的关系不大。</p><p><strong>使用回归进行预测的关键术语</strong></p><ul><li><strong>预测区间（Prediction interval）</strong> 围绕单个预测值的不确定性区间。</li><li><strong>外推法（Extrapolation）</strong> 将模型扩展到用于拟合它的数据范围之外。</li></ul><h4 id="外推的危险性">外推的危险性</h4><p>The Dangers of Extrapolation</p><p>回归模型不应该被用来进行<strong>外推</strong>，<strong>即预测超出其训练数据范围之外的值</strong>（不包括将回归用于时间序列预测的情况）。模型仅对那些数据中存在足够值的预测变量值有效（即使有足够的数据，也可能存在其他问题，详见第176页的“回归诊断”）。</p><p>举一个极端的例子，假设我们使用 <code>model_lm</code> 来预测一块5000平方英尺空地的价值。在这种情况下，所有与建筑物相关的预测变量值都将为0，而回归方程将得出一个荒谬的预测值：-521,900 + 5,000 × -0.0605 = -$522,202。为什么会发生这种情况？因为我们的数据只包含带有建筑物的地块，没有任何对应空地的记录。因此，模型没有任何信息来告诉它如何预测空地的销售价格。</p><h4 id="置信区间与预测区间">置信区间与预测区间</h4><p>Confidence and Prediction Intervals</p><p>统计学的大部分内容都涉及理解和衡量<strong>变异性</strong>（不确定性）。回归输出中报告的 <code>t</code> 统计量和 <code>p</code> 值以一种正式的方式处理了这个问题，这有时对于变量选择是有用的（详见第153页的“评估模型”）。更有用的指标是<strong>置信区间</strong>，它是不确定性区间，<strong>围绕着回归系数和预测值</strong>。理解这一点的一个简单方法是通过<strong>自助法（bootstrap）</strong>（关于一般的自助法程序，详见第61页）。在软件输出中最常见的回归置信区间是针对回归参数（系数）的。</p><p>以下是为具有P个预测变量和n个记录（行）的数据集生成回归参数（系数）置信区间的自助法算法：</p><ol type="1"><li>将每一行（包括结果变量）视为一张单独的“票据”，并将所有n张票据放入一个盒子里。</li><li>随机抽取一张票据，记录下值，然后将其放回盒中。</li><li>重复步骤2共n次；你现在有了一个自助法重抽样样本。</li><li>对这个自助法样本进行回归拟合，并记录估计的系数。</li><li>重复步骤2到步骤4，例如1000次。</li><li>现在你拥有了每个系数的1000个自助法值；找到每个值的相应百分位数（例如，90%置信区间的第5和第95百分位数）。</li></ol><p>在 R 语言中，你可以使用 <code>Boot</code> 函数来生成实际的自助法置信区间，或者简单地使用 R 的常规输出中基于公式的区间。它们的<strong>概念含义和解释是相同的</strong>，并且对数据科学家来说不那么重要，因为它们关注的是回归系数。</p><p>对数据科学家来说，更重要的是围绕预测 <code>y</code> 值（<span class="math inline">\(Y_i\)</span>）的区间。围绕 <span class="math inline">\(Y_i\)</span> 的不确定性来自两个来源：</p><ul><li><strong>不确定性</strong>（Uncertainty）：关于相关的预测变量及其系数是什么（参考前面的自助法算法）。</li><li><strong>附加误差</strong>（Additional error）：个体数据点固有的附加误差。</li></ul><p>个体数据点误差可以这样理解：即使我们确切知道回归方程是什么（例如，如果我们有海量的记录来拟合它），对于一组给定的预测变量值，实际的结果值仍然会存在差异。例如，几栋房子——每栋都有8个房间、6500平方英尺的土地、3个浴室和一个地下室——它们的价值可能不同。我们可以用拟合值中的<strong>残差</strong>（residuals）来模拟这种个体误差（individual error）。</p><p>用于同时模拟回归模型误差和个体数据点误差的自助法算法如下所示：</p><ol type="1"><li>从数据中抽取一个自助法样本（前面已详细说明）。</li><li>拟合回归模型，并预测新值。</li><li>从原始回归拟合中随机抽取一个残差，将其加到预测值上，并记录结果。</li><li>重复步骤1到3，例如1000次。</li><li>找到结果的2.5%和97.5%百分位数。</li></ol><blockquote><p>个人注：第 3 步（随机抽取残差加到预测值上）的例子</p><p>1、 假设我们有这样一个回归模型</p><p>我们用房屋面积预测房价（单位：美元）：</p><table><thead><tr class="header"><th>面积 (X)</th><th>真实价格 (Y)</th></tr></thead><tbody><tr class="odd"><td>1000</td><td>200,000</td></tr><tr class="even"><td>1500</td><td>250,000</td></tr><tr class="odd"><td>2000</td><td>280,000</td></tr></tbody></table><p>我们拟合出一个简单回归：</p><p><span class="math display">\[\hat Y = 100000 + 90 X\]</span></p><p>这样预测值和残差分别是：</p><table><thead><tr class="header"><th>X</th><th>真实 Y</th><th>预测 <span class="math inline">\(\hat Y\)</span></th><th>残差 <span class="math inline">\(r=Y-\hat Y\)</span></th></tr></thead><tbody><tr class="odd"><td>1000</td><td>200000</td><td>190000</td><td>+10000</td></tr><tr class="even"><td>1500</td><td>250000</td><td>235000</td><td>+15000</td></tr><tr class="odd"><td>2000</td><td>280000</td><td>280000</td><td>0</td></tr></tbody></table><p>2、 我们要预测一个新房屋</p><p>面积 <strong>1800 平方英尺</strong>。模型预测值：</p><p><span class="math display">\[\hat Y_\text{new} = 100000 + 90*1800 = 262000\]</span></p><p>但是我们知道模型有误差，于是用 bootstrap 残差来模拟。</p><p>3、 第 3 步：随机抽取一个残差，加到预测值上</p><ul><li>我们有原始残差集合 <span class="math inline">\(\{+10000, +15000, 0\}\)</span></li><li>假设第一次随机抽到 <strong>+15000</strong></li><li>那么新的“模拟观测值”就是</li></ul><p><span class="math display">\[262000 + 15000 = 277000\]</span></p><p>下一次 bootstrap 可能抽到残差 <strong>0</strong>，那么就是</p><p><span class="math display">\[262000 + 0 = 262000\]</span></p><p>再下一次抽到残差 <strong>+10000</strong>，就是</p><p><span class="math display">\[262000 + 10000 = 272000\]</span></p><p>4、 重复多次</p><p>我们重复这个过程（步骤1–3）1000次，就能得到 <strong>262000、272000、277000…</strong> 这样一堆“预测+误差”的值。</p><p>把这些值排序，取第2.5百分位和第97.5百分位，就得到了这个新房屋价格预测的<strong>95%预测区间</strong>。</p><p>5、 <strong>总结</strong> 第3步的“随机取残差加回预测值”就是用模型拟合后的残差来“模拟”未来观测的随机误差，让你不仅有一个点预测值，还有一个预测区间。</p></blockquote><p><strong>关键思想</strong></p><ul><li><strong>外推</strong>：超出数据范围的预测可能导致错误。</li><li><strong>置信区间</strong>：量化围绕<strong>回归系数</strong>的不确定性。</li><li><strong>预测区间</strong>：量化<strong>个体预测值</strong>的不确定性。</li><li><strong>软件输出</strong>：包括 R 在内的大多数软件都会使用公式生成预测和置信区间，作为默认或指定的输出。</li><li><strong>自助法</strong>：也可以用于生成预测和置信区间；其解释和思想是相同的。</li></ul><blockquote><p><strong>警告：</strong></p><p>预测区间还是置信区间？预测区间<strong>（prediction interval）与</strong>单个值<strong>的不确定性有关，而</strong>置信区间<strong>（confidence interval）与从</strong>多个值<strong>计算得出的</strong>均值<strong>或其他统计量有关。因此，</strong>预测区间通常会比相同值的置信区间宽得多。我们在自助法模型中通过选择一个单独的残差并将其附加到预测值上来模拟这种个体值误差。你应该使用哪一个？这取决于分析的背景和目的，但总的来说，数据科学家对<strong>特定的个体预测</strong>更感兴趣，因此<strong>预测区间</strong>会更合适。当你应该使用预测区间时却使用了置信区间，将<strong>大大低估</strong>给定预测值中的不确定性。</p></blockquote><h3 id="回归中的因子变量">回归中的因子变量</h3><p>Factor Variables in Regression</p><p><strong>因子变量</strong>，也称为<strong>分类变量</strong>（categorical variables），取有限数量的离散值。例如，贷款用途可以是“债务整合”、“婚礼”、“汽车”等等。<strong>二元（binary）（是/否）变量</strong>，也叫<strong>指示变量</strong>（indicator variabl），是因子变量的一种特殊情况。</p><p>回归模型需要数值输入，因此因子变量需要重新编码才能在模型中使用。最常见的方法是将一个因子变量转换为一组<strong>二元虚拟变量</strong>（binary dummy variables）。</p><p><strong>因子变量的关键术语</strong></p><ul><li><p><strong>虚拟变量（Dummy variables）</strong> 由重新编码因子数据而来的二元 0-1 变量，用于回归和其他模型。</p></li><li><p><strong>参考编码（Reference coding）</strong> 统计学家最常用的编码类型，其中一个因子水平被用作参考，其他因子水平与该参考水平进行比较。 同义词：<strong>处理编码（treatment coding）</strong></p></li><li><p><strong>独热编码（One hot encoder）</strong> 机器学习社区常用的一种编码类型，其中所有因子水平都被保留。虽然对某些机器学习算法很有用，但这种方法不适用于多元线性回归。</p></li><li><p><strong>偏差编码（Deviation coding）</strong> 一种将每个水平与整体均值进行比较，而不是与参考水平进行比较的编码类型。 同义词：<strong>总和对比（sum contrasts）</strong></p></li></ul><h4 id="虚拟变量表示">虚拟变量表示</h4><p>Dummy Variables Representation</p><p>在金县住房数据中，有一个用于表示房产类型的因子变量；下面是其中六条记录的一个小样本：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">head<span class="punctuation">(</span>house<span class="punctuation">[</span><span class="punctuation">,</span> <span class="string">&#x27;PropertyType&#x27;</span><span class="punctuation">]</span><span class="punctuation">)</span></span><br><span class="line">Source<span class="operator">:</span> local data frame <span class="punctuation">[</span><span class="number">6</span> x <span class="number">1</span><span class="punctuation">]</span></span><br><span class="line">  PropertyType</span><br><span class="line">        <span class="punctuation">(</span>fctr<span class="punctuation">)</span></span><br><span class="line"><span class="number">1</span>  Multiplex</span><br><span class="line"><span class="number">2</span>  Single Family</span><br><span class="line"><span class="number">3</span>  Single Family</span><br><span class="line"><span class="number">4</span>  Single Family</span><br><span class="line"><span class="number">5</span>  Single Family</span><br><span class="line"><span class="number">6</span>  Townhouse</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">house.PropertyType.head()</span><br></pre></td></tr></table></figure><p>该变量有三个可能的值：<code>Multiplex</code>（多户住宅）、<code>Single Family</code>（独栋住宅）和 <code>Townhouse</code>（联排别墅）。为了使用这个因子变量，我们需要将其转换成一组二元变量。我们通过为因子变量的每个可能值创建一个二元变量来实现。在 R 语言中，我们使用 <code>model.matrix</code> 函数来完成此操作：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">prop_type_dummies <span class="operator">&lt;-</span> model.matrix<span class="punctuation">(</span><span class="operator">~</span>PropertyType <span class="operator">-</span><span class="number">1</span><span class="punctuation">,</span> data<span class="operator">=</span>house<span class="punctuation">)</span></span><br><span class="line">head<span class="punctuation">(</span>prop_type_dummies<span class="punctuation">)</span></span><br><span class="line">  PropertyTypeMultiplex PropertyTypeSingle Family PropertyTypeTownhouse</span><br><span class="line"><span class="number">1</span>                     <span class="number">1</span>                      <span class="number">0</span>                     <span class="number">0</span></span><br><span class="line"><span class="number">2</span>                     <span class="number">0</span>                      <span class="number">1</span>                     <span class="number">0</span></span><br><span class="line"><span class="number">3</span>                     <span class="number">0</span>                      <span class="number">1</span>                     <span class="number">0</span></span><br><span class="line"><span class="number">4</span>                     <span class="number">0</span>                      <span class="number">1</span>                     <span class="number">0</span></span><br><span class="line"><span class="number">5</span>                     <span class="number">0</span>                      <span class="number">1</span>                     <span class="number">0</span></span><br><span class="line"><span class="number">6</span>                     <span class="number">0</span>                      <span class="number">0</span>                     <span class="number">1</span></span><br></pre></td></tr></table></figure><p><code>model.matrix</code> 函数将数据框转换为适合线性模型的矩阵。具有三个不同水平的因子变量 <code>PropertyType</code> 被表示为一个三列的矩阵。在机器学习社区中，这种表示被称为<strong>独热编码</strong>（见第242页的“独热编码”）。</p><p>在 Python 中，我们可以使用 pandas 的 <code>get_dummies</code> 方法将分类变量转换为虚拟变量：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pd.get_dummies(house[<span class="string">&#x27;PropertyType&#x27;</span>]).head()</span><br><span class="line">pd.get_dummies(house[<span class="string">&#x27;PropertyType&#x27;</span>], drop_first=<span class="literal">True</span>).head()</span><br></pre></td></tr></table></figure><p>默认情况下，该函数返回分类变量的独热编码。关键字参数 <code>drop_first</code> 将返回 <strong>P-1</strong> 列。使用这个参数可以避免<strong>多重共线性</strong>问题。在某些机器学习算法中，例如<strong>最近邻算法</strong>和<strong>树模型</strong>，独热编码是表示因子变量的标准方法（例如，参见第249页的“树模型”）。</p><p>在回归设置中，一个具有 <span class="math inline">\(P\)</span> 个不同水平的因子变量通常只用一个具有 <span class="math inline">\(P-1\)</span> 列的矩阵来表示。这是因为回归模型通常包含一个<strong>截距项</strong>。有了截距项，一旦你定义了 <span class="math inline">\(P-1\)</span> 个二元变量的值，第 <span class="math inline">\(P\)</span> 个的值就已知了，可以被认为是多余的。添加第 <span class="math inline">\(P\)</span> 列会导致<strong>多重共线性</strong>错误（详见第172页的“多重共线性”）。</p><p>参考编码：R 中的默认表示方法是使用第一个因子水平作为<strong>参考水平</strong>，并解释其他水平相对于该水平的差异。</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lm<span class="punctuation">(</span>AdjSalePrice <span class="operator">~</span> SqFtTotLiving <span class="operator">+</span> SqFtLot <span class="operator">+</span> Bathrooms <span class="operator">+</span></span><br><span class="line">Bedrooms <span class="operator">+</span> BldgGrade <span class="operator">+</span> PropertyType<span class="punctuation">,</span> data<span class="operator">=</span>house<span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p><strong>输出结果：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Coefficients:</span><br><span class="line">(Intercept) SqFtTotLiving</span><br><span class="line">-4.468e+05 2.234e+02</span><br><span class="line">SqFtLot Bathrooms</span><br><span class="line">-7.037e-02 -1.598e+04</span><br><span class="line">Bedrooms BldgGrade</span><br><span class="line">-5.089e+04 1.094e+05</span><br><span class="line">PropertyTypeSingle Family PropertyTypeTownhouse</span><br><span class="line">-8.468e+04 -1.151e+05</span><br></pre></td></tr></table></figure><p><code>get_dummies</code> 方法接受一个可选的关键字参数 <code>drop_first</code>，用来排除第一个因子作为参考：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">predictors = [<span class="string">&#x27;SqFtTotLiving&#x27;</span>, <span class="string">&#x27;SqFtLot&#x27;</span>, <span class="string">&#x27;Bathrooms&#x27;</span>, <span class="string">&#x27;Bedrooms&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;BldgGrade&#x27;</span>, <span class="string">&#x27;PropertyType&#x27;</span>]</span><br><span class="line">X = pd.get_dummies(house[predictors], drop_first=<span class="literal">True</span>)</span><br><span class="line">house_lm_factor = LinearRegression()</span><br><span class="line">house_lm_factor.fit(X, house[outcome])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Intercept: <span class="subst">&#123;house_lm_factor.intercept_:<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Coefficients:&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> name, coef <span class="keyword">in</span> <span class="built_in">zip</span>(X.columns, house_lm_factor.coef_):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27; <span class="subst">&#123;name&#125;</span>: <span class="subst">&#123;coef&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p>R 回归的输出结果显示了两个与 <code>PropertyType</code> 对应的系数：<code>PropertyTypeSingle Family</code> 和 <code>PropertyTypeTownhouse</code>。由于当 <code>PropertyTypeSingle Family == 0</code> 且 <code>PropertyTypeTownhouse == 0</code> 时，<code>Multiplex</code> 的值被隐式定义，因此没有 <code>Multiplex</code> 的系数。这些系数被解释为相对于 <code>Multiplex</code> 的相对值，因此，一个独栋家庭住宅（Single Family）的价值比多户住宅（Multiplex）低近 $85,000，而一个联排别墅（Townhouse）的价值则低超过 $150,000。</p><blockquote><p><strong>警告</strong></p><p>不同的因子编码方式：<strong>有几种不同的编码因子变量的方法，统称为</strong>对比编码系统<strong>（contrast coding systems）。例如，</strong>偏差编码<strong>（deviation coding），也称为</strong>总和对比<strong>（sum contrasts），将每个水平与整体均值进行比较。另一种对比是</strong>多项式编码<strong>（polynomial coding），它适用于</strong>有序因子<strong>；详见第169页的“有序因子变量”部分。除了有序因子之外，数据科学家通常不会遇到除了</strong>参考编码<strong>或</strong>独热编码**之外的任何其他编码类型。</p></blockquote><h4 id="具有许多水平的因子变量">具有许多水平的因子变量</h4><p>Factor Variables with Many Levels</p><p>有些因子变量可能会产生大量的二元虚拟变量，例如邮政编码就是一个因子变量，而美国有43,000个邮政编码。在这种情况下，有必要探索数据以及预测变量和结果之间的关系，以确定这些类别中是否包含有用的信息。如果包含，你还必须决定是保留所有因子水平，还是应该进行合并。</p><p>在金县（King County）的住房数据中，有80个邮政编码有房屋销售记录：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">table<span class="punctuation">(</span>house<span class="operator">$</span>ZipCode<span class="punctuation">)</span></span><br></pre></td></tr></table></figure><table><thead><tr class="header"><th style="text-align: left;">98001</th><th style="text-align: left;">98002</th><th style="text-align: left;">...</th><th style="text-align: left;">98199</th><th style="text-align: left;">98224</th><th style="text-align: left;">98288</th><th style="text-align: left;">98354</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;">358</td><td style="text-align: left;">180</td><td style="text-align: left;">...</td><td style="text-align: left;">393</td><td style="text-align: left;">3</td><td style="text-align: left;">4</td><td style="text-align: left;">9</td></tr></tbody></table><p>pandas 数据框的 <code>value_counts</code> 方法返回相同的信息：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pd.DataFrame(house[<span class="string">&#x27;ZipCode&#x27;</span>].value_counts()).transpose()</span><br></pre></td></tr></table></figure><p>邮政编码是一个重要的变量，因为它代表了位置对房屋价值的影响。包含所有水平需要79个系数，对应79个自由度。原始模型 <code>house_lm</code> 只有5个自由度；参见第153页的“评估模型”。此外，有几个邮政编码只有一次销售记录。在某些问题中，你可以通过使用邮政编码的前两位或三位数字来合并它们，这对应于一个次级大都市地理区域。对于金县，几乎所有的销售都发生在 980xx 或 981xx，所以这种方法没有帮助。</p><p>另一种方法是根据另一个变量，如销售价格，对邮政编码进行分组。更好的方法是使用<strong>初始模型的残差</strong>来形成邮政编码组。以下 R 语言中的 <code>dplyr</code> 代码将80个邮政编码基于 <code>house_lm</code> 回归模型的残差中位数合并成五个组：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">zip_groups <span class="operator">&lt;-</span> house <span class="operator">%&gt;%</span></span><br><span class="line">mutate<span class="punctuation">(</span>resid <span class="operator">=</span> residuals<span class="punctuation">(</span>house_lm<span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">%&gt;%</span></span><br><span class="line">group_by<span class="punctuation">(</span>ZipCode<span class="punctuation">)</span> <span class="operator">%&gt;%</span></span><br><span class="line">summarize<span class="punctuation">(</span>med_resid <span class="operator">=</span> median<span class="punctuation">(</span>resid<span class="punctuation">)</span><span class="punctuation">,</span></span><br><span class="line">cnt <span class="operator">=</span> n<span class="punctuation">(</span><span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">%&gt;%</span></span><br><span class="line">arrange<span class="punctuation">(</span>med_resid<span class="punctuation">)</span> <span class="operator">%&gt;%</span></span><br><span class="line">mutate<span class="punctuation">(</span>cum_cnt <span class="operator">=</span> <span class="built_in">cumsum</span><span class="punctuation">(</span>cnt<span class="punctuation">)</span><span class="punctuation">,</span></span><br><span class="line">ZipGroup <span class="operator">=</span> ntile<span class="punctuation">(</span>cum_cnt<span class="punctuation">,</span> <span class="number">5</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">house <span class="operator">&lt;-</span> house <span class="operator">%&gt;%</span></span><br><span class="line">left_join<span class="punctuation">(</span>select<span class="punctuation">(</span>zip_groups<span class="punctuation">,</span> ZipCode<span class="punctuation">,</span> ZipGroup<span class="punctuation">)</span><span class="punctuation">,</span> by<span class="operator">=</span><span class="string">&#x27;ZipCode&#x27;</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>为每个邮政编码计算残差中位数，然后使用 <code>ntile</code> 函数将按中位数排序的邮政编码分成五个组。有关如何将此作为回归项来改进原始拟合的示例，请参阅第172页的“混杂变量”。</p><p>在 Python 中，我们可以按如下方式计算此信息：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">zip_groups = pd.DataFrame([</span><br><span class="line">    *pd.DataFrame(&#123;</span><br><span class="line">        <span class="string">&#x27;ZipCode&#x27;</span>: house[<span class="string">&#x27;ZipCode&#x27;</span>],</span><br><span class="line">        <span class="string">&#x27;residual&#x27;</span> : house[outcome] - house_lm.predict(house[predictors]),</span><br><span class="line">    &#125;)</span><br><span class="line">    .groupby([<span class="string">&#x27;ZipCode&#x27;</span>])</span><br><span class="line">    .apply(<span class="keyword">lambda</span> x: &#123;</span><br><span class="line">        <span class="string">&#x27;ZipCode&#x27;</span>: x.iloc[<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line">        <span class="string">&#x27;count&#x27;</span>: <span class="built_in">len</span>(x),</span><br><span class="line">        <span class="string">&#x27;median_residual&#x27;</span>: x.residual.median()</span><br><span class="line">    &#125;)</span><br><span class="line">]).sort_values(<span class="string">&#x27;median_residual&#x27;</span>)</span><br><span class="line">zip_groups[<span class="string">&#x27;cum_count&#x27;</span>] = np.cumsum(zip_groups[<span class="string">&#x27;count&#x27;</span>])</span><br><span class="line">zip_groups[<span class="string">&#x27;ZipGroup&#x27;</span>] = pd.qcut(zip_groups[<span class="string">&#x27;cum_count&#x27;</span>], <span class="number">5</span>, labels=<span class="literal">False</span>,</span><br><span class="line">retbins=<span class="literal">False</span>)</span><br><span class="line">to_join = zip_groups[[<span class="string">&#x27;ZipCode&#x27;</span>, <span class="string">&#x27;ZipGroup&#x27;</span>]].set_index(<span class="string">&#x27;ZipCode&#x27;</span>)</span><br><span class="line">house = house.join(to_join, on=<span class="string">&#x27;ZipCode&#x27;</span>)</span><br><span class="line">house[<span class="string">&#x27;ZipGroup&#x27;</span>] = house[<span class="string">&#x27;ZipGroup&#x27;</span>].astype(<span class="string">&#x27;category&#x27;</span>)</span><br></pre></td></tr></table></figure><p>利用残差来指导回归拟合的概念是建模过程中的一个基本步骤；详见第176页的“回归诊断”。</p><h4 id="有序因子变量">有序因子变量</h4><p>Ordered Factor Variables</p><p>有些因子变量反映了因子的水平高低；这些变量被称为<strong>有序因子变量</strong>或<strong>有序分类变量</strong>。例如，贷款评级可以是A、B、C等——每个评级都比前一个评级风险更高。通常，有序因子变量可以直接转换为数值并使用。例如，<code>BldgGrade</code>（建筑等级）就是一个有序因子变量。表4-1中展示了几种等级类型。虽然这些等级有特定的含义，但其数值从低到高排列，对应着更高等级的住宅。在第150页“多元线性回归”中拟合的回归模型 <code>house_lm</code> 中，<code>BldgGrade</code> 被视为一个数值变量。</p><p><img src="/img3/面向数据科学家的实用统计学/T4.1.png" alt="T4.1" style="zoom:50%;" /></p><p>将有序因子视为数值变量可以保留其包含的顺序信息，而如果将其转换为因子变量，这些信息就会丢失。</p><p><strong>关键思想</strong></p><ul><li><strong>因子变量</strong>：需要转换为数值变量才能在回归模型中使用。</li><li><strong>编码方法</strong>：对一个具有 <span class="math inline">\(P\)</span> 个不同值的因子变量进行编码最常用的方法是使用 <span class="math inline">\(P-1\)</span> 个<strong>虚拟变量</strong>来表示。</li><li><strong>水平过多的因子变量</strong>：即使在非常大的数据集中，具有许多水平的因子变量也可能需要合并为具有较少水平的变量。</li><li><strong>有序因子</strong>：某些因子的水平是有序的，可以表示为一个单独的数值变量。</li></ul><h3 id="解释回归方程">解释回归方程</h3><p>Interpreting the Regression Equation</p><p>在数据科学中，回归最主要的应用是预测某个<strong>因变量</strong>（结果变量）。然而，在某些情况下，从方程本身获得洞见以理解预测变量和结果之间的关系本质也很有价值。本节提供了关于如何检查和解释回归方程的指导。</p><p><strong>解释回归方程的关键术语</strong></p><ul><li><p><strong>相关变量（Correlated variables）</strong> 当预测变量高度相关时，很难解释单个系数。</p></li><li><p><strong>多重共线性（Multicollinearity）</strong> 当预测变量之间存在完全或接近完全的相关性时，回归可能不稳定或无法计算。 同义词：<strong>共线性（collinearity）</strong></p></li><li><p><strong>混杂变量（Confounding variables）</strong> 一个重要的预测变量，当被遗漏时，会导致回归方程中出现虚假关系。</p></li><li><p><strong>主效应（Main effects）</strong> 一个预测变量与结果变量之间的关系，独立于其他变量。</p></li><li><p><strong>交互作用（Interactions）</strong> 两个或多个预测变量与响应变量之间的相互依存关系。</p></li></ul><h4 id="相关预测变量">相关预测变量</h4><p>Correlated Predictors</p><p>在多元回归中，预测变量通常彼此之间存在相关性。举个例子，我们可以检查在第156页“模型选择与逐步回归”中拟合的模型 <code>step_lm</code> 的回归系数。</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">step_lm<span class="operator">$</span>coefficients</span><br><span class="line"><span class="punctuation">(</span>Intercept<span class="punctuation">)</span> SqFtTotLiving       Bathrooms</span><br><span class="line"> <span class="number">6.178645e+06</span>  <span class="number">1.992776e+02</span>    <span class="number">4.239616e+04</span></span><br><span class="line">     Bedrooms     BldgGrade PropertyTypeSingle Family</span><br><span class="line"> <span class="operator">-</span><span class="number">5.194738e+04</span>  <span class="number">1.371596e+05</span>    <span class="number">2.291206e+04</span></span><br><span class="line">PropertyTypeTownhouse SqFtFinBasement         YrBuilt</span><br><span class="line">  <span class="number">8.447916e+04</span>    <span class="number">7.046975e+00</span>   <span class="operator">-</span><span class="number">3.565425e+03</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Intercept: <span class="subst">&#123;best_model.intercept_:<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Coefficients:&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> name, coef <span class="keyword">in</span> <span class="built_in">zip</span>(best_variables, best_model.coef_):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27; <span class="subst">&#123;name&#125;</span>: <span class="subst">&#123;coef&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p><code>Bedrooms</code> 的系数是<strong>负数</strong>！这暗示着给一栋房子增加一间卧室会降低其价值。这怎么可能呢？这是因为<strong>预测变量之间是相关的</strong>：更大的房子往往有更多的卧室，而真正驱动房屋价值的是<strong>面积</strong>，而不是卧室的数量。你可以这样理解：考虑两栋面积完全相同的房子，我们有理由认为卧室更多但更小的那栋房子会不那么受欢迎。</p><p>拥有相关联的预测变量会使解释回归系数的符号和值变得困难（并且可能夸大估计值的标准误）。卧室数量、房屋面积和浴室数量这几个变量都是相互关联的。以下面的 R 语言示例为例，它拟合了一个新的回归模型，从方程中移除了 <code>SqFtTotLiving</code>、<code>SqFtFinBasement</code> 和 <code>Bathrooms</code> 变量。</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">update<span class="punctuation">(</span>step_lm<span class="punctuation">,</span> .</span><br><span class="line"><span class="operator">~</span></span><br><span class="line">.</span><br><span class="line"><span class="operator">-</span> SqFtTotLiving <span class="operator">-</span> SqFtFinBasement <span class="operator">-</span> Bathrooms<span class="punctuation">)</span></span><br><span class="line">Call<span class="operator">:</span></span><br><span class="line">lm<span class="punctuation">(</span>formula <span class="operator">=</span> AdjSalePrice <span class="operator">~</span> Bedrooms <span class="operator">+</span> BldgGrade <span class="operator">+</span> PropertyType <span class="operator">+</span></span><br><span class="line">YrBuilt<span class="punctuation">,</span> data <span class="operator">=</span> house<span class="punctuation">,</span> na.action <span class="operator">=</span> na.omit<span class="punctuation">)</span></span><br><span class="line">Coefficients<span class="operator">:</span></span><br><span class="line">      <span class="punctuation">(</span>Intercept<span class="punctuation">)</span>         Bedrooms</span><br><span class="line">          <span class="number">4913973</span>          <span class="number">27151</span></span><br><span class="line">        BldgGrade PropertyTypeSingle Family</span><br><span class="line">           <span class="number">248998</span>         <span class="operator">-</span><span class="number">19898</span></span><br><span class="line">PropertyTypeTownhouse         YrBuilt</span><br><span class="line">        <span class="operator">-</span><span class="number">47355</span>         <span class="operator">-</span><span class="number">3212</span></span><br></pre></td></tr></table></figure><p><code>update</code> 函数可用于从模型中添加或移除变量。现在，<code>Bedrooms</code> 的系数变为<strong>正数</strong>——这与我们的预期相符（尽管在移除了房屋面积等变量后，它实际上充当了房屋大小的替代变量）。</p><p>在 Python 中，没有与 R 的 <code>update</code> 函数等效的功能。我们需要使用修改后的预测变量列表重新拟合模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">predictors = [<span class="string">&#x27;Bedrooms&#x27;</span>, <span class="string">&#x27;BldgGrade&#x27;</span>, <span class="string">&#x27;PropertyType&#x27;</span>, <span class="string">&#x27;YrBuilt&#x27;</span>]</span><br><span class="line">outcome = <span class="string">&#x27;AdjSalePrice&#x27;</span></span><br><span class="line">X = pd.get_dummies(house[predictors], drop_first=<span class="literal">True</span>)</span><br><span class="line">reduced_lm = LinearRegression()</span><br><span class="line">reduced_lm.fit(X, house[outcome])</span><br></pre></td></tr></table></figure><p>相关变量只是解释回归系数的一个问题。在 <code>house_lm</code> 模型中，没有变量来解释房屋的位置，这个模型将非常不同的区域类型混在一起。<strong>位置</strong>可能是一个<strong>混杂变量</strong>；更多讨论请参见第172页的“混杂变量”。</p><h4 id="多重共线性">多重共线性</h4><p>Multicollinearity</p><p><strong>多重共线性</strong>是由<strong>相关变量</strong>所引发的极端情况——预测变量之间存在<strong>冗余</strong>。当一个预测变量可以表示为其他预测变量的线性组合时，就会出现<strong>完全多重共线性</strong>。</p><p>多重共线性通常发生在以下情况：</p><ul><li><strong>重复包含</strong>：一个变量被错误地包含了多次。</li><li><strong>不当的虚拟变量</strong>：由一个因子变量创建了 <span class="math inline">\(P\)</span> 个虚拟变量，而不是正确的 <span class="math inline">\(P-1\)</span> 个（参见第163页的“回归中的因子变量”）。</li><li><strong>高度相关</strong>：两个变量几乎是完全相关的。</li></ul><p>在回归分析中，<strong>必须处理多重共线性</strong>——应该移除变量直到多重共线性消失。在存在完全多重共线性的情况下，回归模型没有明确定义的解。许多软件，包括 R 和 Python，会自动处理某些类型的多重共线性。例如，如果在房屋数据回归中两次包含 <code>SqFtTotLiving</code> 变量，其结果与 <code>house_lm</code> 模型的结果相同。在<strong>非完全多重共线性</strong>的情况下，软件可能会得到一个解，但结果可能<strong>不稳定</strong>。</p><blockquote><p><strong>通用注解</strong></p><p>对于非线性回归方法，如树模型、聚类和最近邻算法，多重共线性不是一个大问题，在这些方法中保留全部 <span class="math inline">\(P\)</span> 个虚拟变量（而不是 <span class="math inline">\(P-1\)</span> 个）可能是可取的。即便如此，在这些方法中，预测变量的非冗余性仍然是一种优点。</p></blockquote><h4 id="混杂变量">混杂变量</h4><p>Confounding Variables</p><p><strong>相关变量</strong>的问题在于<strong>加入</strong>了太多相似的变量，而<strong>混杂变量</strong>的问题在于<strong>遗漏</strong>了重要的变量。如果对回归方程的系数进行天真的解释，可能会得出无效的结论。</p><p>以第151页“示例：金县住房数据”中的金县回归方程 <code>house_lm</code> 为例。<code>SqFtLot</code>、<code>Bathrooms</code> 和 <code>Bedrooms</code> 的回归系数都是负的。这个原始回归模型没有包含代表<strong>位置</strong>的变量——而位置是房屋价格的一个非常重要的预测因子。</p><p>为了对位置进行建模，我们引入一个ZipGroup变量，它将邮政编码分为五组，从最便宜（1）到最贵（5）：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">lm<span class="punctuation">(</span>formula <span class="operator">=</span> AdjSalePrice <span class="operator">~</span> SqFtTotLiving <span class="operator">+</span> SqFtLot <span class="operator">+</span> Bathrooms <span class="operator">+</span></span><br><span class="line">Bedrooms <span class="operator">+</span> BldgGrade <span class="operator">+</span> PropertyType <span class="operator">+</span> ZipGroup<span class="punctuation">,</span> data <span class="operator">=</span> house<span class="punctuation">,</span></span><br><span class="line">na.action <span class="operator">=</span> na.omit<span class="punctuation">)</span></span><br><span class="line">Coefficients<span class="operator">:</span></span><br><span class="line">      <span class="punctuation">(</span>Intercept<span class="punctuation">)</span>   SqFtTotLiving</span><br><span class="line">       <span class="operator">-</span><span class="number">6.666e+05</span>    <span class="number">2.106e+02</span></span><br><span class="line">        SqFtLot       Bathrooms</span><br><span class="line">        <span class="number">4.550e-01</span>    <span class="number">5.928e+03</span></span><br><span class="line">       Bedrooms       BldgGrade</span><br><span class="line">      <span class="operator">-</span><span class="number">4.168e+04</span>    <span class="number">9.854e+04</span></span><br><span class="line">PropertyTypeSingle Family PropertyTypeTownhouse</span><br><span class="line">       <span class="number">1.932e+04</span>    <span class="operator">-</span><span class="number">7.820e+04</span></span><br><span class="line">         ZipGroup2      ZipGroup3</span><br><span class="line">        <span class="number">5.332e+04</span>    <span class="number">1.163e+05</span></span><br><span class="line">         ZipGroup4      ZipGroup5</span><br><span class="line">        <span class="number">1.784e+05</span>    <span class="number">3.384e+05</span></span><br></pre></td></tr></table></figure><p>Python 中相同的模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">predictors = [<span class="string">&#x27;SqFtTotLiving&#x27;</span>, <span class="string">&#x27;SqFtLot&#x27;</span>, <span class="string">&#x27;Bathrooms&#x27;</span>, <span class="string">&#x27;Bedrooms&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;BldgGrade&#x27;</span>, <span class="string">&#x27;PropertyType&#x27;</span>, <span class="string">&#x27;ZipGroup&#x27;</span>]</span><br><span class="line">outcome = <span class="string">&#x27;AdjSalePrice&#x27;</span></span><br><span class="line">X = pd.get_dummies(house[predictors], drop_first=<span class="literal">True</span>)</span><br><span class="line">confounding_lm = LinearRegression()</span><br><span class="line">confounding_lm.fit(X, house[outcome])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Intercept: <span class="subst">&#123;confounding_lm.intercept_:<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Coefficients:&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> name, coef <span class="keyword">in</span> <span class="built_in">zip</span>(X.columns, confounding_lm.coef_):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27; <span class="subst">&#123;name&#125;</span>: <span class="subst">&#123;coef&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p><code>ZipGroup</code> 显然是一个重要的变量：位于最贵邮政编码组的房屋，其估计销售价格要高出近 $340,000。现在，<code>SqFtLot</code> 和 <code>Bathrooms</code> 的系数变为正数，增加一个浴室会使销售价格增加 $5,928。</p><p><code>Bedrooms</code> 的系数仍然是负数。虽然这看起来不合直觉，但这在房地产领域是一个众所周知的现象。对于居住面积和浴室数量相同的房屋，卧室数量更多（因此卧室更小）的房屋通常与较低的价值相关联。</p><h4 id="交互作用与主效应">交互作用与主效应</h4><p>Interactions and Main Effects</p><p>统计学家喜欢区分<strong>主效应</strong>（或称自变量 independent variables）和<strong>主效应之间的交互作用</strong>。主效应通常是指回归方程中的预测变量。当模型中只使用主效应时，一个隐含的假设是：预测变量与响应变量之间的关系是<strong>独立于</strong>其他预测变量的。然而，情况往往并非如此。</p><p>例如，在第172页“混杂变量”中拟合的金县住房数据模型包含了几个变量作为主效应，其中包括ZipCode。在房地产中，位置是决定一切的关键，因此很自然地会假设，房屋面积与销售价格之间的关系取决于位置。一个建在低租金区域的大房子，其价值不会与建在昂贵区域的大房子相同。在 R 中，你可以使用 <code>*</code> 运算符来包含变量之间的交互作用。对于金县数据，以下代码拟合了 <code>SqFtTotLiving</code> 和 <code>ZipGroup</code> 之间的交互作用：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">lm<span class="punctuation">(</span>formula <span class="operator">=</span> AdjSalePrice <span class="operator">~</span> SqFtTotLiving <span class="operator">*</span> ZipGroup <span class="operator">+</span> SqFtLot <span class="operator">+</span></span><br><span class="line">Bathrooms <span class="operator">+</span> Bedrooms <span class="operator">+</span> BldgGrade <span class="operator">+</span> PropertyType<span class="punctuation">,</span> data <span class="operator">=</span> house<span class="punctuation">,</span></span><br><span class="line">na.action <span class="operator">=</span> na.omit<span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p><strong>输出结果：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Coefficients:</span><br><span class="line">(Intercept) ... SqFtTotLiving:ZipGroup2 ...</span><br></pre></td></tr></table></figure><p>得到的模型有四个新项：<code>SqFtTotLiving:ZipGroup2</code>、<code>SqFtTotLiving:ZipGroup3</code> 等等。</p><p>在 Python 中，我们需要使用 <code>statsmodels</code> 包来训练包含交互作用的线性回归模型。这个包的设计类似于 R，并允许使用公式接口来定义模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model = smf.ols(formula=<span class="string">&#x27;AdjSalePrice ~ SqFtTotLiving*ZipGroup + SqFtLot + &#x27;</span> +</span><br><span class="line"><span class="string">&#x27;Bathrooms + Bedrooms + BldgGrade + PropertyType&#x27;</span>, data=house)</span><br><span class="line">results = model.fit()</span><br><span class="line">results.summary()</span><br></pre></td></tr></table></figure><p><code>statsmodels</code> 包会自动处理分类变量（例如 <code>ZipGroup[T.1]</code>、<code>PropertyType[T.Single Family]</code>）和交互项（例如 <code>SqFtTotLiving:ZipGroup[T.1]</code>）。</p><p>位置和房屋面积似乎存在强烈的交互作用。对于最低 <code>ZipGroup</code> 中的房屋，其斜率与主效应 <code>SqFtTotLiving</code> 的斜率相同，即每平方英尺118美元（这是因为 R 对因子变量使用参考编码；参见第163页的“回归中的因子变量”）。对于最高 <code>ZipGroup</code> 中的房屋，斜率是主效应加上 <code>SqFtTotLiving:ZipGroup5</code> 的总和，即115美元 + 227美元 = 每平方英尺342美元。换句话说，与增加一平方英尺的平均价值提升相比，在最昂贵的邮政编码组中增加一平方英尺，其预测销售价格的提升因子几乎是三倍。</p><blockquote><p><strong>知识点</strong></p><p>包含交互项的模型选择：在涉及许多变量的问题中，决定模型应包含哪些交互项可能具有挑战性。通常采取几种不同的方法：</p><ul><li>在某些问题中，<strong>先验知识和直觉</strong>可以指导选择要包含在模型中的交互项。</li><li>可以使用<strong>逐步选择</strong>（参见第156页的“模型选择与逐步回归”）来筛选各种模型。</li><li><strong>惩罚回归</strong>可以自动拟合一大组可能的交互项。</li><li>也许最常见的方法是使用<strong>树模型</strong>及其后代，如<strong>随机森林</strong>和<strong>梯度提升树</strong>。这类模型会自动搜索最佳的交互项；参见第249页的“树模型”。</li></ul></blockquote><p><strong>关键思想</strong></p><ul><li><strong>相关预测变量</strong>：由于预测变量之间的相关性，在解释多元线性回归的系数时必须谨慎。</li><li><strong>多重共线性</strong>：可能导致回归方程拟合时的数值不稳定。</li><li><strong>混杂变量</strong>：一个被模型遗漏的重要预测变量，可能导致回归方程中出现虚假关系。</li><li><strong>交互项</strong>：如果变量与响应变量之间的关系是相互依存的，则需要引入两个变量之间的交互项。</li></ul><h3 id="回归诊断">回归诊断</h3><p>Regression Diagnostics</p><p>在<strong>解释性建模</strong>（explanatory modeling）（即研究背景）中，除了前面提到的指标（参见第153页的“评估模型”）之外，还会采取各种步骤来评估模型对数据的拟合程度；这些步骤大多基于<strong>残差分析</strong>。它们不直接关乎预测准确性，但可以在预测场景中提供有用的洞见。</p><p><strong>回归诊断的关键术语</strong></p><ul><li><p><strong>标准化残差（Standardized residuals）</strong> 残差除以残差的标准误。</p></li><li><p><strong>离群值（Outliers）</strong> 与数据其余部分（或预测结果）相距甚远的记录（或结果值）。</p></li><li><p><strong>影响力值（Influential value）</strong> 其存在或缺失对回归方程产生重大影响的值或记录。</p></li><li><p><strong>杠杆值（Leverage）</strong> 单个记录对回归方程影响的程度。 同义词：<strong>帽子值（hat-value）</strong></p></li><li><p><strong>非正态残差（Non-normal residuals）</strong> 残差不呈正态分布会使回归的一些技术要求失效，但在数据科学中通常不是一个主要问题。</p></li><li><p><strong>异方差性（Heteroskedasticity）</strong> 当结果变量的某些范围内的残差具有更高的方差时（可能表明方程中缺少某个预测变量）。</p></li><li><p><strong>偏残差图（Partial residual plots）</strong> 一个诊断图，用于揭示结果变量与单个预测变量之间的关系。 同义词：<strong>增广变量图（added variables plot）</strong></p></li></ul><h4 id="离群值">离群值</h4><p>Outliers</p><p>一般来说，一个极端值，也称为<strong>离群值</strong>，是与大多数其他观测值相距甚远的值。正如在处理位置和变异性估计时需要处理离群值一样（参见第7页的“位置估计”和第13页的“变异性估计”），离群值也可能给回归模型带来问题。在回归中，离群值是指其实际 <span class="math inline">\(y\)</span> 值与预测值相距甚远的记录。你可以通过检查<strong>标准化残差</strong>来检测离群值，标准化残差是将残差除以残差的标准误。</p><p>没有统计学理论可以区分离群值和非离群值。相反，有一些（任意的）经验法则来界定一个观测值需要与主体数据相距多远才能被称为离群值。例如，对于箱线图，离群值是那些离箱体边界太远的数据点（参见第20页的“百分位数与箱线图”），其中“太远”是指“超过四分位距的1.5倍”。在回归中，<strong>标准化残差</strong>是通常用于判断一条记录是否被归类为离群值的指标。标准化残差可以被解释为“距离回归线有多少个标准误”。</p><p>让我们在 R 中对邮政编码98105的所有房屋销售数据拟合一个回归模型：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">house_98105 <span class="operator">&lt;-</span> house<span class="punctuation">[</span>house<span class="operator">$</span>ZipCode <span class="operator">==</span> <span class="number">98105</span><span class="punctuation">,</span><span class="punctuation">]</span></span><br><span class="line">lm_98105 <span class="operator">&lt;-</span> lm<span class="punctuation">(</span>AdjSalePrice <span class="operator">~</span> SqFtTotLiving <span class="operator">+</span> SqFtLot <span class="operator">+</span> Bathrooms <span class="operator">+</span></span><br><span class="line">Bedrooms <span class="operator">+</span> BldgGrade<span class="punctuation">,</span> data<span class="operator">=</span>house_98105<span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>在 Python 中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">house_98105 = house.loc[house[<span class="string">&#x27;ZipCode&#x27;</span>] == <span class="number">98105</span>, ]</span><br><span class="line">predictors = [<span class="string">&#x27;SqFtTotLiving&#x27;</span>, <span class="string">&#x27;SqFtLot&#x27;</span>, <span class="string">&#x27;Bathrooms&#x27;</span>, <span class="string">&#x27;Bedrooms&#x27;</span>, <span class="string">&#x27;BldgGrade&#x27;</span>]</span><br><span class="line">outcome = <span class="string">&#x27;AdjSalePrice&#x27;</span></span><br><span class="line">house_outlier = sm.OLS(house_98105[outcome],</span><br><span class="line">house_98105[predictors].assign(const=<span class="number">1</span>))</span><br><span class="line">result_98105 = house_outlier.fit()</span><br></pre></td></tr></table></figure><p>我们使用 R 中的 <code>rstandard</code> 函数提取标准化残差，并使用 <code>order</code> 函数获取最小残差的索引：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sresid <span class="operator">&lt;-</span> rstandard<span class="punctuation">(</span>lm_98105<span class="punctuation">)</span></span><br><span class="line">idx <span class="operator">&lt;-</span> order<span class="punctuation">(</span>sresid<span class="punctuation">)</span></span><br><span class="line">sresid<span class="punctuation">[</span>idx<span class="punctuation">[</span><span class="number">1</span><span class="punctuation">]</span><span class="punctuation">]</span></span><br><span class="line"><span class="number">20429</span></span><br><span class="line"><span class="operator">-</span><span class="number">4.326732</span></span><br></pre></td></tr></table></figure><p>在 <code>statsmodels</code> 中，使用 <code>OLSInfluence</code> 来分析残差：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">influence = OLSInfluence(result_98105)</span><br><span class="line">sresiduals = influence.resid_studentized_internal</span><br><span class="line">sresiduals.idxmin(), sresiduals.<span class="built_in">min</span>()</span><br></pre></td></tr></table></figure><p>模型中最大的高估值比回归线高出四个多标准误，这相当于高估了$757,754。与此离群值对应的原始数据记录在 R 中如下：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">house_98105<span class="punctuation">[</span>idx<span class="punctuation">[</span><span class="number">1</span><span class="punctuation">]</span><span class="punctuation">,</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;AdjSalePrice&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;SqFtTotLiving&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;SqFtLot&#x27;</span><span class="punctuation">,</span></span><br><span class="line"><span class="string">&#x27;Bathrooms&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;Bedrooms&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;BldgGrade&#x27;</span><span class="punctuation">)</span><span class="punctuation">]</span></span><br><span class="line">AdjSalePrice SqFtTotLiving SqFtLot Bathrooms Bedrooms BldgGrade</span><br><span class="line"><span class="punctuation">(</span>dbl<span class="punctuation">)</span> <span class="punctuation">(</span>int<span class="punctuation">)</span> <span class="punctuation">(</span>int<span class="punctuation">)</span> <span class="punctuation">(</span>dbl<span class="punctuation">)</span> <span class="punctuation">(</span>int<span class="punctuation">)</span> <span class="punctuation">(</span>int<span class="punctuation">)</span></span><br><span class="line"><span class="number">20429</span> <span class="number">119748</span> <span class="number">2900</span> <span class="number">7276</span> <span class="number">3</span> <span class="number">6</span> <span class="number">7</span></span><br></pre></td></tr></table></figure><p>在 Python 中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">outlier = house_98105.loc[sresiduals.idxmin(), :]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;AdjSalePrice&#x27;</span>, outlier[outcome])</span><br><span class="line"><span class="built_in">print</span>(outlier[predictors])</span><br></pre></td></tr></table></figure><p>在这种情况下，似乎这条记录出了问题：在这个邮政编码中，如此大小的房屋通常售价要远高于$119,748。图4-4展示了此次销售的法定契据摘录：很明显，此次销售仅涉及该房产的部分权益。在这种情况下，该离群值对应一次异常销售，不应被包含在回归分析中。离群值也可能是由其他问题造成的，例如“粗心”的数据录入或单位不匹配（例如，以千美元为单位报告销售额而不是以美元为单位）。</p><p><img src="/img3/面向数据科学家的实用统计学/F4.4.png" alt="F4.4" style="zoom:50%;" /></p><p>对于大数据问题，离群值通常不会对用于预测新数据的回归拟合造成问题。<strong>然而，在异常检测中，离群值是核心，因为找到离群值正是其全部目的。离群值也可能对应于欺诈或意外行为。无论如何，检测离群值可能是一个关键的商业需求。</strong></p><h4 id="影响力值">影响力值</h4><p>Influential Values</p><p>如果一个值在被移除后会显著改变回归方程，那么它就被称为<strong>有影响力的观测值</strong>。在回归中，这样的值不一定与大的残差相关联。例如，考虑图4-5中的两条回归线。实线对应于使用所有数据拟合的回归，而虚线对应于移除了右上角数据点后的回归。很明显，这个数据值对回归有巨大的影响，尽管它本身并不是一个大的离群值（从完整回归的角度来看）。这个数据值被认为对回归具有<strong>高杠杆</strong>。</p><p><img src="/img3/面向数据科学家的实用统计学/F4.5.png" alt="F4.5" style="zoom:33%;" /></p><p>除了标准化残差（参见第177页的“离群值”）之外，统计学家还开发了几个指标来确定单个记录对回归的影响。一个常见的杠杆度量是<strong>帽子值（hat-value）</strong>；当帽子值高于 <span class="math inline">\(2(P+1)/n\)</span> 时，表明该数据值具有高杠杆。</p><p>另一个指标是<strong>库克距离（Cook’s distance）</strong>，它将影响力定义为<strong>杠杆值</strong>和<strong>残差大小</strong>的组合。一个经验法则是，如果库克距离超过 <span class="math inline">\(4/(n-P-1)\)</span>，则该观测值具有高影响力。</p><p><img src="/img3/面向数据科学家的实用统计学/F4.6.png" alt="F4.6" style="zoom:50%;" /></p><p><strong>影响力图</strong>（或<strong>气泡图</strong>）(influence plot or bubble plot)将标准化残差、帽子值和库克距离组合在一个图表中。图4-6展示了金县房屋数据的影响力图，可以用以下 R 代码创建：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">std_resid <span class="operator">&lt;-</span> rstandard<span class="punctuation">(</span>lm_98105<span class="punctuation">)</span></span><br><span class="line">cooks_D <span class="operator">&lt;-</span> cooks.distance<span class="punctuation">(</span>lm_98105<span class="punctuation">)</span></span><br><span class="line">hat_values <span class="operator">&lt;-</span> hatvalues<span class="punctuation">(</span>lm_98105<span class="punctuation">)</span></span><br><span class="line">plot<span class="punctuation">(</span>subset<span class="punctuation">(</span>hat_values<span class="punctuation">,</span> cooks_D <span class="operator">&gt;</span> <span class="number">0.08</span><span class="punctuation">)</span><span class="punctuation">,</span> subset<span class="punctuation">(</span>std_resid<span class="punctuation">,</span> cooks_D <span class="operator">&gt;</span> <span class="number">0.08</span><span class="punctuation">)</span><span class="punctuation">,</span></span><br><span class="line">xlab<span class="operator">=</span><span class="string">&#x27;hat_values&#x27;</span><span class="punctuation">,</span> ylab<span class="operator">=</span><span class="string">&#x27;std_resid&#x27;</span><span class="punctuation">,</span></span><br><span class="line">cex<span class="operator">=</span><span class="number">10</span><span class="operator">*</span><span class="built_in">sqrt</span><span class="punctuation">(</span>subset<span class="punctuation">(</span>cooks_D<span class="punctuation">,</span> cooks_D <span class="operator">&gt;</span> <span class="number">0.08</span><span class="punctuation">)</span><span class="punctuation">)</span><span class="punctuation">,</span> pch<span class="operator">=</span><span class="number">16</span><span class="punctuation">,</span> col<span class="operator">=</span><span class="string">&#x27;lightgrey&#x27;</span><span class="punctuation">)</span></span><br><span class="line">points<span class="punctuation">(</span>hat_values<span class="punctuation">,</span> std_resid<span class="punctuation">,</span> cex<span class="operator">=</span><span class="number">10</span><span class="operator">*</span><span class="built_in">sqrt</span><span class="punctuation">(</span>cooks_D<span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">abline<span class="punctuation">(</span>h<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="operator">-</span><span class="number">2.5</span><span class="punctuation">,</span> <span class="number">2.5</span><span class="punctuation">)</span><span class="punctuation">,</span> lty<span class="operator">=</span><span class="number">2</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>以下是创建类似图表的 Python 代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">influence = OLSInfluence(result_98105)</span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line">ax.axhline(-<span class="number">2.5</span>, linestyle=<span class="string">&#x27;--&#x27;</span></span><br><span class="line">, color=<span class="string">&#x27;C1&#x27;</span>)</span><br><span class="line">ax.axhline(<span class="number">2.5</span>, linestyle=<span class="string">&#x27;--&#x27;</span></span><br><span class="line">, color=<span class="string">&#x27;C1&#x27;</span>)</span><br><span class="line">ax.scatter(influence.hat_matrix_diag, influence.resid_studentized_internal,</span><br><span class="line">s=<span class="number">1000</span> * np.sqrt(influence.cooks_distance[<span class="number">0</span>]),</span><br><span class="line">alpha=<span class="number">0.5</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">&#x27;hat values&#x27;</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&#x27;studentized residuals&#x27;</span>)</span><br></pre></td></tr></table></figure><p>图中有几个数据点明显对回归表现出较大的影响力。库克距离可以使用 <code>cooks.distance</code> 函数计算，你可以使用 <code>hatvalues</code> 来计算诊断值。帽子值绘制在 x 轴上，残差绘制在 y 轴上，点的大小与库克距离的值相关。</p><p><img src="/img3/面向数据科学家的实用统计学/T4.2.png" alt="T4.2" style="zoom:50%;" /></p><p>表4-2比较了使用完整数据集和移除了高影响力数据点（库克距离 &gt; 0.08）的回归结果。 <code>Bathrooms</code> 的回归系数变化相当大。</p><p>为了拟合一个能够可靠预测未来数据的回归模型，识别有影响力的观测值仅在较小的数据集中有用。对于包含许多记录的回归模型，单个观测值不太可能具有足够的权重来对拟合方程产生极端影响（尽管回归模型仍然可能存在大的离群值）。然而，对于<strong>异常检测</strong>而言，识别有影响力的观测值可能非常有用。</p><h4 id="异方差性非正态性与相关误差">异方差性、非正态性与相关误差</h4><p>Heteroskedasticity, Non-Normality, and Correlated Errors</p><p>统计学家们非常关注残差的分布。事实证明，在广泛的分布假设下，<strong>普通最小二乘法（OLS）</strong>（参见第148页的“最小二乘法”）是无偏的，并且在某些情况下是“最优”的估计量。这意味着在大多数问题中，数据科学家无需过度关注残差的分布。</p><p>残差的分布主要关系到<strong>正式统计推断</strong>（假设检验和 p 值）的有效性，而这对于主要关注<strong>预测准确性</strong>的数据科学家来说，其重要性是最小的。<strong>正态分布的误差</strong>是一个信号，表明模型是完整的；而<strong>非正态分布的误差</strong>则表明模型可能遗漏了某些东西。为了使正式推断完全有效，残差被假定为<strong>正态分布</strong>、具有<strong>相同的方差</strong>且<strong>相互独立</strong>。数据科学家可能需要关注这一点的一个领域是预测值的标准置信区间计算，它基于关于残差的这些假设（参见第161页的“置信区间与预测区间”）。</p><p>异方差性（Heteroskedasticity）是指在预测值的整个范围内缺乏恒定的残差方差。换句话说，在某些范围内的误差比其他范围更大。将数据可视化是分析残差的一种便捷方式。</p><p>以下 R 代码绘制了第177页“离群值”中拟合的 <code>lm_98105</code> 回归模型的绝对残差与预测值之间的关系图：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df <span class="operator">&lt;-</span> data.frame<span class="punctuation">(</span>resid <span class="operator">=</span> residuals<span class="punctuation">(</span>lm_98105<span class="punctuation">)</span><span class="punctuation">,</span> pred <span class="operator">=</span> predict<span class="punctuation">(</span>lm_98105<span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">ggplot<span class="punctuation">(</span>df<span class="punctuation">,</span> aes<span class="punctuation">(</span>pred<span class="punctuation">,</span> <span class="built_in">abs</span><span class="punctuation">(</span>resid<span class="punctuation">)</span><span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">+</span> geom_point<span class="punctuation">(</span><span class="punctuation">)</span> <span class="operator">+</span> geom_smooth<span class="punctuation">(</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>图4-7显示了生成的图。使用 <code>geom_smooth</code>，可以很容易地叠加一个绝对残差的平滑曲线。该函数调用 <code>loess</code> 方法（局部加权散点图平滑），以在散点图中生成 x 轴和 y 轴变量之间关系的平滑估计（参见第185页的“散点图平滑器”）。</p><p><img src="/img3/面向数据科学家的实用统计学/F4.7.png" alt="F4.7" style="zoom:50%;" /></p><p>在 Python 中，<code>seaborn</code> 包的 <code>regplot</code> 函数可以创建类似的图：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line">sns.regplot(result_98105.fittedvalues, np.<span class="built_in">abs</span>(result_98105.resid),</span><br><span class="line">scatter_kws=&#123;<span class="string">&#x27;alpha&#x27;</span>: <span class="number">0.25</span>&#125;, line_kws=&#123;<span class="string">&#x27;color&#x27;</span>: <span class="string">&#x27;C1&#x27;</span>&#125;,</span><br><span class="line">lowess=<span class="literal">True</span>, ax=ax)</span><br><span class="line">ax.set_xlabel(<span class="string">&#x27;predicted&#x27;</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&#x27;abs(residual)&#x27;</span>)</span><br></pre></td></tr></table></figure><p>很明显，残差的方差随着房屋价值的升高而增大，但对于低价值的房屋也很大。这个图表明 <code>lm_98105</code> 存在异方差性误差。</p><blockquote><p><strong>知识点</strong></p><p>为什么数据科学家要关心异方差性？**异方差性表明预测误差对于预测值的不同范围有所不同，这可能暗示模型不完整。例如，<code>lm_98105</code> 中的异方差性可能表明回归模型未能解释高价值和低价值房屋中遗漏的某些因素。</p></blockquote><p><img src="/img3/面向数据科学家的实用统计学/F4.8.png" alt="F4.8" style="zoom:33%;" /></p><p>图4-8是 <code>lm_98105</code> 回归模型的标准化残差直方图。该分布的尾部明显比正态分布长，并且对较大的残差表现出轻微的偏斜。</p><p>统计学家也可能检查误差是否独立的假设。对于随时间或空间收集的数据尤其如此。德宾-沃森统计量（Durbin-Watson statistic）可用于检测涉及时间序列数据的回归中是否存在显著的自相关。如果回归模型的误差是相关的，那么这些信息对于进行短期预测可能很有用，并且应该被纳入模型中。有关如何将自相关信息纳入时间序列数据回归模型的更多信息，请参阅 Galit Shmueli 和 Kenneth Lichtendahl（Axelrod Schnall，2018）撰写的《实用时间序列预测与 R，第2版》。如果目标是长期预测或解释性模型，微观层面上过度的自相关数据可能会分散注意力。在这种情况下，可能需要进行平滑处理，或者从一开始就以较低的粒度收集数据。</p><p>即使回归模型违反了某个分布假设，我们是否应该关心呢？在数据科学中，兴趣主要在于<strong>预测准确性</strong>，因此对异方差性进行一些检查可能是必要的。你可能会发现数据中存在模型尚未捕获的某种信号。然而，仅仅为了验证正式统计推断（p 值、F 统计量等）而满足分布假设，对数据科学家来说并没有那么重要。</p><blockquote><p><strong>通用注解</strong></p><p>散点图平滑器<strong>（Scatterplot Smoothers）：回归是关于建模响应变量和预测变量之间的关系。在评估回归模型时，使用</strong>散点图平滑器**以可视化方式突出两个变量之间的关系是很有用的。</p><p>例如，在图4-7中，绝对残差与预测值之间关系的平滑曲线显示，残差的方差取决于残差的值。在本例中，使用了 <code>loess</code> 函数；<code>loess</code> 通过重复拟合一系列局部回归到连续子集上来生成平滑曲线。虽然 <code>loess</code> 可能是最常用的平滑器，但 R 中还有其他散点图平滑器，例如 <code>super smooth (supsmu)</code> 和<strong>核平滑（kernel smoothing, ksmooth）</strong>。在 Python 中，我们可以在 <code>scipy</code> (<code>wiener</code> 或 <code>sav</code>) 和 <code>statsmodels</code> (<code>kernel_regression</code>) 中找到更多的平滑器。为了评估回归模型，通常无需担心这些散点图平滑器的具体细节。</p></blockquote><h4 id="偏残差图与非线性">偏残差图与非线性</h4><p>Partial Residual Plots and Nonlinearity</p><p><strong>偏残差图</strong>是一种可视化工具，用于评估估计的拟合效果，以及它如何解释一个预测变量与结果之间的关系。偏残差图的基本思想是<strong>隔离</strong>一个预测变量与响应变量之间的关系，同时<strong>考虑所有其他预测变量</strong>的影响。</p><p>偏残差可以被视为一个“合成结果”值，它结合了基于单个预测变量的预测值和完整回归方程的实际残差。预测变量 <span class="math inline">\(X_i\)</span> 的偏残差是<strong>普通残差加上与 <span class="math inline">\(X_i\)</span> 相关的回归项</strong>：</p><p><span class="math display">\[\text{偏残差} = \text{残差} + b_i X_i\]</span></p><p>其中，<span class="math inline">\(b_i\)</span> 是估计的回归系数。R 中的 <code>predict</code> 函数有一个选项可以返回单个回归项 <span class="math inline">\(b_i X_i\)</span>：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">terms <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>lm_98105<span class="punctuation">,</span> type<span class="operator">=</span><span class="string">&#x27;terms&#x27;</span><span class="punctuation">)</span></span><br><span class="line">partial_resid <span class="operator">&lt;-</span> resid<span class="punctuation">(</span>lm_98105<span class="punctuation">)</span> <span class="operator">+</span> terms</span><br></pre></td></tr></table></figure><p>偏残差图将 <span class="math inline">\(X_i\)</span> 预测变量显示在 x 轴上，偏残差显示在 y 轴上。使用 <code>ggplot2</code> 可以很容易地叠加偏残差的平滑曲线：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">df <span class="operator">&lt;-</span> data.frame<span class="punctuation">(</span>SqFtTotLiving <span class="operator">=</span> house_98105<span class="punctuation">[</span><span class="punctuation">,</span> <span class="string">&#x27;SqFtTotLiving&#x27;</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">Terms <span class="operator">=</span> terms<span class="punctuation">[</span><span class="punctuation">,</span> <span class="string">&#x27;SqFtTotLiving&#x27;</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">PartialResid <span class="operator">=</span> partial_resid<span class="punctuation">[</span><span class="punctuation">,</span> <span class="string">&#x27;SqFtTotLiving&#x27;</span><span class="punctuation">]</span><span class="punctuation">)</span></span><br><span class="line">ggplot<span class="punctuation">(</span>df<span class="punctuation">,</span> aes<span class="punctuation">(</span>SqFtTotLiving<span class="punctuation">,</span> PartialResid<span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">geom_point<span class="punctuation">(</span>shape<span class="operator">=</span><span class="number">1</span><span class="punctuation">)</span> <span class="operator">+</span> scale_shape<span class="punctuation">(</span>solid <span class="operator">=</span> <span class="literal">FALSE</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">geom_smooth<span class="punctuation">(</span>linetype<span class="operator">=</span><span class="number">2</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">geom_line<span class="punctuation">(</span>aes<span class="punctuation">(</span>SqFtTotLiving<span class="punctuation">,</span> Terms<span class="punctuation">)</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p><code>statsmodels</code> 包的 <code>sm.graphics.plot_ccpr</code> 方法可以创建类似的偏残差图：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sm.graphics.plot_ccpr(result_98105, <span class="string">&#x27;SqFtTotLiving&#x27;</span>)</span><br></pre></td></tr></table></figure><p>R 和 Python 的图表之间有一个常量偏移。在 R 中，添加了一个常量，使得项的均值为零。</p><p><img src="/img3/面向数据科学家的实用统计学/F4.9.png" alt="F4.9" style="zoom:50%;" /></p><p>由此生成的图表如图4-9所示。偏残差是对 <code>SqFtTotLiving</code> 对销售价格贡献的估计。<code>SqFtTotLiving</code> 与销售价格之间的关系显然是<strong>非线性的</strong>（虚线）。回归线（实线）低估了面积小于1000平方英尺的房屋价格，并高估了面积在2000到3000平方英尺之间的房屋价格。由于4000平方英尺以上的数据点太少，无法得出结论。</p><p>这种非线性关系在这种情况下是合理的：在一栋小房子中增加500英尺与在一栋大房子中增加500英尺所带来的价值差异是巨大的。这表明，我们应该考虑使用<strong>非线性项</strong>来替代简单的 <code>SqFtTotLiving</code> 线性项（参见第187页的“多项式和样条回归”）。</p><p><strong>关键思想</strong></p><ul><li><strong>离群值</strong>：虽然离群值可能会给小数据集带来问题，但其主要作用在于<strong>识别数据问题或定位异常情况</strong>。</li><li><strong>影响力</strong>：单个记录（包括回归离群值）在小数据集中可能对回归方程产生很大影响，但在大数据中这种影响会被稀释。</li><li><strong>残差分布</strong>：如果回归模型用于正式推断（p 值等），则应检查残差的分布假设。但总的来说，残差分布在数据科学中并不那么关键。</li><li><strong>偏残差图</strong>：可用于定性评估每个回归项的拟合效果，并可能导致模型的替代性设定。</li></ul><h3 id="多项式与样条回归">多项式与样条回归</h3><p>Polynomial and Spline Regression</p><p>响应变量与预测变量之间的关系不一定是线性的。例如，药物剂量与反应之间的关系通常是非线性的：将剂量加倍通常不会使反应也加倍。对产品的需求也不是营销投入的线性函数；在某个点之后，需求可能会饱和。有许多方法可以扩展回归模型以捕捉这些非线性效应。</p><p><strong>非线性回归的关键术语</strong></p><ul><li><strong>多项式回归（Polynomial regression）</strong> 在回归模型中添加多项式项（平方、立方等）。</li><li><strong>样条回归（Spline regression）</strong> 用一系列多项式分段来拟合一条平滑曲线。</li><li><strong>结点（Knots）</strong> 分隔样条分段的值。</li><li><strong>广义加性模型（Generalized additive models）</strong> 具有自动选择结点的样条模型。 同义词：<strong>GAM</strong></li></ul><blockquote><p><strong>知识点</strong></p><p>非线性回归（术语辨析）<strong>：当统计学家谈论</strong>非线性回归<strong>时，他们指的是</strong>不能<strong>使用最小二乘法进行拟合的模型。什么样的模型是非线性的？基本上，所有响应变量不能表示为预测变量或其某种变换的线性组合的模型都是非线性的。非线性回归模型更难拟合，计算也更密集，因为它需要</strong>数值优化**。因此，如果可能的话，通常首选使用线性模型。</p></blockquote><h4 id="多项式回归">多项式回归</h4><p>Polynomial</p><p><strong>多项式回归</strong>涉及在回归方程中包含多项式项。多项式回归的使用可以追溯到回归本身的发展时期，Gergonne 在1815年就发表了相关论文。例如，响应变量 <code>Y</code> 和预测变量 <code>X</code> 之间的二次回归方程形式如下： <span class="math display">\[Y = b_0 + b_1X + b_2X^2 + e\]</span></p><p>多项式回归可以在 R 中通过 <code>poly</code> 函数进行拟合。例如，以下代码为金县住房数据中的 <code>SqFtTotLiving</code> 拟合了一个二次多项式：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">lm<span class="punctuation">(</span>AdjSalePrice <span class="operator">~</span> poly<span class="punctuation">(</span>SqFtTotLiving<span class="punctuation">,</span> <span class="number">2</span><span class="punctuation">)</span> <span class="operator">+</span> SqFtLot <span class="operator">+</span></span><br><span class="line">BldgGrade <span class="operator">+</span> Bathrooms <span class="operator">+</span> Bedrooms<span class="punctuation">,</span></span><br><span class="line">data<span class="operator">=</span>house_98105<span class="punctuation">)</span></span><br><span class="line">Call<span class="operator">:</span></span><br><span class="line">lm<span class="punctuation">(</span>formula <span class="operator">=</span> AdjSalePrice <span class="operator">~</span> poly<span class="punctuation">(</span>SqFtTotLiving<span class="punctuation">,</span> <span class="number">2</span><span class="punctuation">)</span> <span class="operator">+</span> SqFtLot <span class="operator">+</span></span><br><span class="line">BldgGrade <span class="operator">+</span> Bathrooms <span class="operator">+</span> Bedrooms<span class="punctuation">,</span> data <span class="operator">=</span> house_98105<span class="punctuation">)</span></span><br><span class="line">Coefficients<span class="operator">:</span></span><br><span class="line"><span class="punctuation">(</span>Intercept<span class="punctuation">)</span> poly<span class="punctuation">(</span>SqFtTotLiving<span class="punctuation">,</span> <span class="number">2</span><span class="punctuation">)</span><span class="number">1</span> poly<span class="punctuation">(</span>SqFtTotLiving<span class="punctuation">,</span> <span class="number">2</span><span class="punctuation">)</span><span class="number">2</span></span><br><span class="line"><span class="operator">-</span><span class="number">402530.47</span> <span class="number">3271519.49</span> <span class="number">776934.02</span></span><br><span class="line">SqFtLot BldgGrade Bathrooms</span><br><span class="line"><span class="number">32.56</span> <span class="number">135717.06</span> <span class="operator">-</span><span class="number">1435.12</span></span><br><span class="line">Bedrooms</span><br><span class="line"><span class="operator">-</span><span class="number">9191.94</span></span><br></pre></td></tr></table></figure><p>在 <code>statsmodels</code> 中，我们使用 <code>I(SqFtTotLiving**2)</code> 将平方项添加到模型定义中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model_poly = smf.ols(formula=<span class="string">&#x27;AdjSalePrice ~ SqFtTotLiving + &#x27;</span> +</span><br><span class="line"><span class="string">&#x27;+ I(SqFtTotLiving**2) + &#x27;</span> +</span><br><span class="line"><span class="string">&#x27;SqFtLot + Bathrooms + Bedrooms + BldgGrade&#x27;</span>, data=house_98105)</span><br><span class="line">result_poly = model_poly.fit()</span><br><span class="line">result_poly.summary()</span><br></pre></td></tr></table></figure><p>截距和多项式系数与 R 的结果不同。这是由于不同的实现方式造成的。剩余的系数和预测结果是等效的。</p><p>现在，<code>SqFtTotLiving</code> 有两个相关的系数：一个用于线性项，一个用于二次项。</p><p>偏残差图（参见第185页的“偏残差图与非线性”）显示，与 <code>SqFtTotLiving</code> 相关的回归方程中存在一些<strong>曲率</strong>。与线性拟合相比，拟合线（实线）更紧密地匹配偏残差的平滑曲线（虚线，参见第189页的“样条”）（见图4-10）。</p><p><img src="/img3/面向数据科学家的实用统计学/F4.10.png" alt="F4.10" style="zoom:50%;" /></p><p><code>statsmodels</code> 的实现只适用于线性项。随附的源代码提供了也适用于多项式回归的实现。</p><h4 id="样条回归">样条回归</h4><p>Splines</p><p><strong>多项式回归</strong>只能捕捉非线性关系中的特定曲率。添加更高阶的项，例如三次或四次多项式，常常会导致回归方程出现不必要的“弯曲”。另一种（通常更优越）的非线性关系建模方法是使用<strong>样条回归</strong>。<strong>样条</strong>提供了一种在固定点之间平滑插值的方法。样条最初被制图员用来绘制平滑曲线，尤其是在造船和飞机制造领域。</p><p>样条曲线是通过用被称为“<strong>鸭子</strong>”（ducks）的重物来弯曲一块薄木片而创建的；参见图4-11。</p><p><img src="/img3/面向数据科学家的实用统计学/F4.11.png" alt="F4.11" style="zoom:50%;" /></p><p>样条的正式定义是<strong>一系列分段连续的多项式</strong>。它们最早由罗马尼亚数学家 I. J. Schoenberg 在二战期间于美国阿伯丁试验场开发。这些多项式分段在预测变量中的一系列（称为结点 knots）处平滑连接。样条的公式比多项式回归复杂得多；统计软件通常会处理拟合样条的细节。R 包 <code>splines</code> 包含了 <code>bs</code> 函数，用于在回归模型中创建 b 样条（基础样条）项。例如，以下代码为房屋回归模型添加了一个 b 样条项：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">library<span class="punctuation">(</span>splines<span class="punctuation">)</span></span><br><span class="line">knots <span class="operator">&lt;-</span> quantile<span class="punctuation">(</span>house_98105<span class="operator">$</span>SqFtTotLiving<span class="punctuation">,</span> p<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">.25</span><span class="punctuation">,</span> <span class="number">.5</span><span class="punctuation">,</span> <span class="number">.75</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">lm_spline <span class="operator">&lt;-</span> lm<span class="punctuation">(</span>AdjSalePrice <span class="operator">~</span> bs<span class="punctuation">(</span>SqFtTotLiving<span class="punctuation">,</span> knots<span class="operator">=</span>knots<span class="punctuation">,</span> degree<span class="operator">=</span><span class="number">3</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">SqFtLot <span class="operator">+</span> Bathrooms <span class="operator">+</span> Bedrooms <span class="operator">+</span> BldgGrade<span class="punctuation">,</span> data<span class="operator">=</span>house_98105<span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>需要指定两个参数：<strong>多项式的次数</strong>和<strong>结点的位置</strong>。在这个例子中，预测变量 <code>SqFtTotLiving</code> 使用<strong>三次样条</strong>（<code>degree=3</code>）被包含在模型中。默认情况下，<code>bs</code> 会在边界处设置结点；此外，结点也被设置在下四分位数、中位数和上四分位数处。</p><p><code>statsmodels</code> 的公式接口以类似于 R 的方式支持使用样条。在这里，我们使用 <code>df</code>（自由度）来指定 b 样条。这将创建 <code>df - degree = 6 - 3 = 3</code> 个内部结点，其位置的计算方式与上述 R 代码相同：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">formula = <span class="string">&#x27;AdjSalePrice ~ bs(SqFtTotLiving, df=6, degree=3) + &#x27;</span> +</span><br><span class="line"><span class="string">&#x27;SqFtLot + Bathrooms + Bedrooms + BldgGrade&#x27;</span></span><br><span class="line">model_spline = smf.ols(formula=formula, data=house_9105)</span><br><span class="line">result_spline = model_spline.fit()</span><br></pre></td></tr></table></figure><p>与线性项系数有直接意义不同，样条项的系数是<strong>不可解释的</strong>。相反，使用<strong>可视化展示</strong>来揭示样条拟合的性质更为有用。图4-12显示了回归模型的偏残差图。与多项式模型相比，样条模型更紧密地匹配平滑曲线，这展示了样条的更大灵活性。在这种情况下，拟合线更贴近数据。</p><p><img src="/img3/面向数据科学家的实用统计学/F4.12.png" alt="F4.12" style="zoom:50%;" /></p><p>这是否意味着样条回归是一个更好的模型？<strong>不一定</strong>。从经济学角度来看，面积非常小的房屋（小于1000平方英尺）的价值高于稍大一些的房屋是没有道理的。这可能是一个<strong>混杂变量</strong>造成的假象；参见第172页的“混杂变量”。</p><h4 id="广义加性模型">广义加性模型</h4><p>Generalized Additive Models</p><p>假设你通过先验知识或检查回归诊断，怀疑响应变量与某个预测变量之间存在非线性关系。多项式项可能不够灵活来捕捉这种关系，而样条项又需要指定结点。广义加性模型（Generalized Additive Models, GAM）是一种灵活的建模技术，可以用于自动拟合样条回归。R 中的 <code>mgcv</code> 包可用于将 GAM 模型拟合到住房数据上：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">library<span class="punctuation">(</span>mgcv<span class="punctuation">)</span></span><br><span class="line">lm_gam <span class="operator">&lt;-</span> gam<span class="punctuation">(</span>AdjSalePrice <span class="operator">~</span> s<span class="punctuation">(</span>SqFtTotLiving<span class="punctuation">)</span> <span class="operator">+</span> SqFtLot <span class="operator">+</span></span><br><span class="line">Bathrooms <span class="operator">+</span> Bedrooms <span class="operator">+</span> BldgGrade<span class="punctuation">,</span></span><br><span class="line">data<span class="operator">=</span>house_98105<span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p><code>s(SqFtTotLiving)</code> 项告诉 <code>gam</code> 函数为样条项寻找“最佳”结点。</p><p>在 Python 中，我们可以使用 <code>pyGAM</code> 包。它提供了回归和分类的方法。在这里，我们使用 <code>LinearGAM</code> 来创建一个回归模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">predictors = [<span class="string">&#x27;SqFtTotLiving&#x27;</span>, <span class="string">&#x27;SqFtLot&#x27;</span>, <span class="string">&#x27;Bathrooms&#x27;</span>, <span class="string">&#x27;Bedrooms&#x27;</span>, <span class="string">&#x27;BldgGrade&#x27;</span>]</span><br><span class="line">outcome = <span class="string">&#x27;AdjSalePrice&#x27;</span></span><br><span class="line">X = house_98105[predictors].values</span><br><span class="line">y = house_98105[outcome]</span><br><span class="line">gam = LinearGAM(s(<span class="number">0</span>, n_splines=<span class="number">12</span>) + l(<span class="number">1</span>) + l(<span class="number">2</span>) + l(<span class="number">3</span>) + l(<span class="number">4</span>))</span><br><span class="line">gam.gridsearch(X, y)</span><br></pre></td></tr></table></figure><p><code>n_splines</code> 的默认值是20。这对于较大的 <code>SqFtTotLiving</code> 值会导致过拟合。而12这个值可以得到一个更合理的拟合效果。</p><p><strong>关键思想</strong></p><ul><li><strong>离群值</strong>：回归中的离群值是具有大残差的记录。</li><li><strong>多重共线性</strong>：可能导致回归方程拟合时的数值不稳定。</li><li><strong>混杂变量</strong>：一个从模型中遗漏的重要预测变量，可能导致回归方程中出现虚假关系。</li><li><strong>交互项</strong>：如果一个变量的效果取决于另一个变量的水平或大小，则需要引入两个变量之间的交互项。</li><li><strong>多项式回归</strong>：可以拟合预测变量和结果变量之间的非线性关系。</li><li><strong>样条回归</strong>：是连接在结点上的一系列多项式分段。</li><li><strong>广义加性模型（GAM）</strong>：我们可以使用广义加性模型来自动化样条中指定结点的过程。</li></ul><h3 id="总结">总结</h3><p>在所有统计方法中，回归可能是多年来使用最广泛的一种方法。它旨在建立多个预测变量与一个结果变量之间的关系。其基本形式是<strong>线性</strong>的：每个预测变量都有一个系数来描述该预测变量与结果之间的线性关系。更高级的回归形式，如多项式和样条回归，允许这种关系是非线性的。在<strong>经典统计学</strong>中，重点在于找到对观察数据的一个良好拟合，以解释或描述某个现象，并且这种拟合的强度是通过传统的样本内指标来评估的。相比之下，在<strong>数据科学</strong>中，目标通常是预测新数据的值，因此使用基于样本外数据的预测准确性指标来进行模型评估。变量选择方法则用于降维和创建更紧凑的模型。</p>]]></content>
      
      
      <categories>
          
          <category> 翻译 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> AI </tag>
            
            <tag> 统计 </tag>
            
            <tag> 数据科学 </tag>
            
            <tag> R </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第3章 统计实验与显著性检验</title>
      <link href="/2025/09/25/%E7%AC%AC3%E7%AB%A0%20%E7%BB%9F%E8%AE%A1%E5%AE%9E%E9%AA%8C%E4%B8%8E%E6%98%BE%E8%91%97%E6%80%A7%E6%A3%80%E9%AA%8C/"/>
      <url>/2025/09/25/%E7%AC%AC3%E7%AB%A0%20%E7%BB%9F%E8%AE%A1%E5%AE%9E%E9%AA%8C%E4%B8%8E%E6%98%BE%E8%91%97%E6%80%A7%E6%A3%80%E9%AA%8C/</url>
      
        <content type="html"><![CDATA[<p><a href="/img3/面向数据科学家的实用统计学/Practical-Statistics-for-Data-Scientists.pdf">《Practical Statistics for Data Scientists》书籍英文版</a><br /><a href="/img3/面向数据科学家的实用统计学/面向数据科学家的实用统计学.pdf">《面向数据科学家的实用统计学》中文版书籍</a></p><h2 id="第-3-章-统计实验与显著性检验">第 3 章 统计实验与显著性检验</h2><p>实验设计是统计实践的基石，在几乎所有研究领域都有应用。其目标是<strong>设计实验以确认或拒绝某个假设</strong>。数据科学家往往需要持续进行实验，尤其是关于用户界面和产品营销方面的实验。本章回顾了传统实验设计，并讨论了数据科学中常见的一些挑战；还介绍了一些统计推断中经常被引用的概念，并解释了它们的含义及其与数据科学的相关性（或不相关性）。</p><p><img src="/img3/面向数据科学家的实用统计学/F3.1.png" alt="F3.1" style="zoom:50%;" /></p><p>当你看到统计显著性、t 检验或 p 值等术语时，通常是在经典统计推断“流水线”的上下文中（见图 3-1）。这个过程从一个假设开始（例如“药物 A 优于现有标准药物”或“价格 A 比现有价格 B 更有利可图”）。然后设计实验（可能是 A/B 测试）以检验这个假设——设计得尽可能能够得出结论性结果。接着收集并分析数据，然后得出结论。<strong>术语“推断”体现了这样一种意图：将涉及有限数据集的实验结果，应用到更大的过程或总体上。</strong></p><span id="more"></span><h3 id="ab-测试">A/B 测试</h3><p>A/B 测试是一种<strong>包含两个组的实验</strong>，用于确定两种处理、产品、程序等哪一种更优。通常两种处理中的一种是现有的标准处理，或是不做处理。如果使用标准处理（或不处理），则称其为<strong>对照组（control）</strong>。典型的假设是新处理优于对照组。</p><p><strong>A/B 测试关键术语</strong></p><ul><li><strong>处理（Treatment）</strong>：让受试对象接触的某种东西（药物、价格、网页标题等）。</li><li><strong>处理组（Treatment group）</strong>：接触特定处理的一组受试对象。</li><li><strong>对照组（Control group）</strong>：接触标准处理或不处理的一组受试对象。</li><li><strong>随机化（Randomization）</strong>：将受试对象随机分配到不同处理的过程。</li><li><strong>受试对象（Subjects）</strong>：暴露在处理下的个体（网页访问者、患者等）。</li><li><strong>检验统计量（Test statistic）</strong>：用于衡量处理效果的指标。</li></ul><p>A/B 测试在网页设计和营销中很常见，因为结果容易衡量。A/B 测试的一些例子包括：</p><ul><li>测试两种土壤处理方法，看哪种能让种子发芽更好</li><li>测试两种疗法，看哪种对抑制癌症更有效</li><li>测试两种定价，看哪种能带来更多净利润</li><li>测试两个网页标题，看哪一个能带来更多点击（图 3-2）</li><li>测试两个网页广告，看哪一个能带来更多转化</li></ul><p><img src="/img3/面向数据科学家的实用统计学/F3.2.png" alt="F3.2" style="zoom:33%;" /></p><p>一个<strong>合格的 A/B 测试</strong>要求受试对象可以被分配到某个处理或另一个处理。受试对象可以是人、植物种子、网页访问者；关键是受试对象接触到处理。理想情况下，受试对象要随机分配到处理组。这样你就知道，处理组之间的差异只可能来自两种原因：</p><ul><li>不同处理的效果</li><li>随机分配带来的偶然性（比如天生表现更好的受试对象恰好集中在 A 组或 B 组）</li></ul><p>你还需要注意用于比较 A 组和 B 组的检验统计量或指标。数据科学中最常见的指标是<strong>二元变量</strong>：点击或不点击、购买或不购买、欺诈或不欺诈等等。这些结果可以汇总在一个 2×2 表中。表 3-1 就是一个实际价格测试的 2×2 表（关于这些结果的进一步讨论见第 103 页“统计显著性与 p 值”）。</p><p><img src="/img3/面向数据科学家的实用统计学/T3.1.png" alt="T3.1" style="zoom:50%;" /></p><p>如果指标是<strong>连续变量</strong>（购买金额、利润等）或<strong>计数</strong>（例如住院天数、访问页面数），结果可能以不同方式展示。如果关注的不是转化率而是<strong>每次页面浏览的收入</strong>，那么表 3-1 中的价格测试结果在典型软件输出中可能看起来是这样的：</p><ul><li>价格 A 每次页面浏览收入：平均值 = 3.87，标准差 SD = 51.10</li><li>价格 B 每次页面浏览收入：平均值 = 4.11，标准差 SD = 62.98</li></ul><p>“SD”指的是各组内值的标准差。</p><blockquote><p><strong>警告：</strong></p><p>但要注意：统计软件（包括 R 和 Python）默认生成的输出，并不意味着所有输出都有用或相关。<strong>你可以看到，上述标准差并不太有用；表面上它们暗示很多值可能是负的，而负收入在现实中并不可行。这类数据由少量较高值（带转化的页面浏览）和大量 0 值（无转化的页面浏览）组成。用一个数字来概括这种数据的变异性是很困难的，虽然</strong>平均绝对偏差**（A 为 7.68，B 为 8.15）比标准差更合理一些。</p></blockquote><h4 id="为什么要有对照组">为什么要有对照组？</h4><p>为什么不跳过对照组，只对一个组施加感兴趣的处理，然后把结果和以往经验相比呢？</p><p>没有对照组，就不能保证“其他条件都相同”，也不能保证任何差异确实是由处理（或随机因素）引起的。当你有一个对照组时，除了研究的处理之外，它与处理组处在同样的条件下。如果只是与“基线”或以往经验比较，处理之外的其他因素也可能不同。</p><blockquote><p><strong>通用注解：</strong></p><p>研究中的盲法**：盲法（blind study）是指受试者不知道自己接受的是A处理还是B处理。知道自己接受某种处理可能会影响反应。双盲研究（double-blind study）是指研究者和执行者（如医疗研究中的医生和护士）也不知道哪些受试者接受了哪种处理。当处理本身显而易见时（例如计算机认知疗法与心理学家面对面的认知疗法），就无法进行盲法。</p></blockquote><p>A/B测试在数据科学中通常用于网页场景。处理可能是网页的设计、商品价格、标题的措辞或其他项目。需要一定的思考来保持随机化原则。通常实验中的“受试者”是网站访客，而我们感兴趣的结果是点击、购买、访问时长、访问页面数量、是否访问某一特定页面等在标准A/B实验中，你需要<strong>事先</strong>决定一个度量指标。可能会收集多个行为指标且都感兴趣，但如果实验预期要在处理A和处理B之间做出决策，那么必须在实验前确定一个单一指标或检验统计量。在实验结束后才选择检验统计量会引入研究者偏倚。</p><h4 id="为什么只是ab为什么不cd">为什么只是A/B？为什么不C、D……？</h4><p>A/B测试在市场营销和电商领域很流行，但并不是唯一的统计实验类型。可以纳入更多处理。受试者可能会被多次测量。在药物试验中，受试者稀缺、昂贵且需要长期招募，有时会设计多个中途停止实验并得出结论的机会。</p><p>传统的统计实验设计侧重于回答一个<strong>静态问题</strong>：</p><blockquote><p>“价格A和价格B之间的差异在统计上显著吗？”</p></blockquote><p>而数据科学家更关心的问题是：</p><blockquote><p>“在多个可能的价格中，哪个最好？”</p></blockquote><p>为此，使用了一种相对较新的实验设计：<strong>多臂老虎机算法</strong>（multi-arm bandit，见本书第131页）。</p><blockquote><p>警告：</p><p>获得许可：**在涉及人类受试者的科学和医学研究中，通常需要获得受试者的许可，并得到机构审查委员会（IRB）的批准。而在商业中作为日常运营一部分进行的实验几乎从不这么做。在大多数情况下（如定价实验、显示哪个标题或提供哪个优惠），这种做法被普遍接受。然而，Facebook在2014年因一次实验引发广泛批评。他们操纵用户新闻推送（newsfeed）中的情绪色彩：用情感分析将帖子分类为正面或负面，然后改变展示给用户的正/负比例。一些随机选择的用户看到更多正面帖子，另一些用户看到更多负面帖子。Facebook发现，看到更正面新闻推送的用户更可能自己发布正面内容，反之亦然。这种效应的幅度虽然很小，但Facebook因为在不告知用户的情况下进行实验而受到大量批评。一些用户甚至推测，若极度抑郁的用户看到负面版本的推送，可能会被“推向崩溃的边缘”。</p></blockquote><p><strong>关键要点</strong></p><ul><li>受试者被分配到两个（或更多）组，这些组除了研究的处理不同之外，其他条件完全相同。</li><li>理想情况下，受试者被<strong>随机分配</strong>到各组。</li></ul><h3 id="假设检验">假设检验</h3><p>假设检验（hypothesis tests），也叫显著性检验（significance tests），在传统的已发表研究的统计分析中无处不在。它们的目的是帮助你判断，随机因素是否可能是观测到的效果的原因。</p><p><strong>假设检验的关键术语</strong></p><ul><li><p><strong>原假设（Null hypothesis）</strong> 认为随机因素是原因的假设。</p></li><li><p><strong>备择假设（Alternative hypothesis）</strong> 原假设的对立面（你希望证明的东西）。</p></li><li><p><strong>单尾检验（One-way test）</strong> 只在一个方向上计算偶然结果的假设检验。</p></li><li><p><strong>双尾检验（Two-way test）</strong> 在两个方向上都计算偶然结果的假设检验。</p></li></ul><p>A/B 测试（见第88页“A/B Testing”）通常是带着假设来设计的。例如，假设可能是“价格B带来更高利润”。<strong>为什么我们需要一个假设？为什么不直接看实验的结果，选那个更好的处理就行？答案在于人类大脑倾向于低估自然随机行为的幅度。</strong>这种倾向的一种表现是无法预见极端事件，即所谓的“黑天鹅”（见第73页“长尾分布”）。另一种表现是倾向于将随机事件误解为具有某种有意义的模式。<strong>统计假设检验就是为了防止研究人员被随机性所欺骗而发明的。</strong></p><p><strong>误解随机性</strong>：</p><p>你可以通过这个小实验观察人类低估随机性的倾向：请几个朋友“编造”一系列 50 次掷硬币的结果，让他们写下一串随机的“H”和“T”。然后再让他们真的掷硬币 50 次，并把结果写下来。让他们把真正的掷硬币结果放在一堆，编造的结果放在另一堆。很容易看出哪些是真实结果：真实结果中“H”或“T”的连续串更长。在 50 次真实掷硬币中，连续出现 5~6 次“H”或“T”一点都不罕见。然而，当我们自己在编造随机掷硬币结果时，如果已经连续 3~4 次“H”，就会告诉自己“为了看起来随机”，最好换成“T”。从另一方面看，当我们在现实世界里看到“连续 6 次H”的等价情况（比如一个标题的点击率比另一个高出10%），我们往往倾向于把它归因于某种真实原因，而不仅仅是偶然。</p><p>在一个设计合理的 A/B 测试中，你以这样的方式收集A组和B组的数据，使得任何观测到的差异只能是以下两种原因之一：</p><ul><li>分配受试者时的随机因素</li><li>A和B之间的真实差异</li></ul><p>统计假设检验就是对A/B测试或任何随机实验的进一步分析，以评估“随机因素”是否是解释A组和B组差异的合理原因。</p><h4 id="原假设"><strong>原假设</strong></h4><p>The Null Hypothesis</p><p>假设检验的逻辑是这样的：“鉴于人类倾向于对不寻常但随机的行为作出反应，并把它解读为有意义、真实的东西，在我们的实验中，我们将要求证明，组间的差异比偶然因素合理产生的差异更为极端。”这包含一个基线假设：不同处理是等效的，组间的任何差异都源于随机因素。这个基线假设称为<strong>原假设</strong>（null hypothesis）。我们的希望是能够推翻原假设，证明 A 组与 B 组的结果比随机机会所能产生的差异更大。</p><p>一种方法是使用重抽样置换程序：把 A 组和 B 组的结果混合打乱，然后反复按相同的样本量重新分组，再观察得到与观测差异一样极端的情况出现的频率。A 组和 B 组混合后并抽样的结果体现了 A 组与 B 组等价且可互换的原假设，被称为<strong>原模型</strong>（null model）。更多细节参见第 96 页“重抽样”。</p><h4 id="备择假设"><strong>备择假设</strong></h4><p>Alternative Hypothesis</p><p>假设检验本质上不仅涉及原假设，还涉及与之相对的<strong>备择假设</strong>。例子如下：</p><ul><li>原假设：“A 组与 B 组均值无差异”；备择假设：“A 与 B 不同”（可能更大或更小）</li><li>原假设：“A ≤ B”；备择假设：“A &gt; B”</li><li>原假设：“B 不比 A 高 X%”；备择假设：“B 比 A 高 X%”</li></ul><p>原假设和备择假设加在一起必须涵盖所有可能性。原假设的性质决定了假设检验的结构。</p><h4 id="单尾检验与双尾检验"><strong>单尾检验与双尾检验</strong></h4><p>One-Way Versus Two-Way Hypothesis Tests</p><p>在 A/B 测试中，常见的情形是：你在测试一个新选项（如 B）与一个既定默认选项（A），并假定除非新选项能明确更好，否则你会继续沿用默认选项。在这种情况下，你希望假设检验保护你免于在有利于 B 的方向上被随机性误导。至于相反方向，你并不在意，因为除非 B 被证明确实更好，否则你会坚持 A。因此你需要一个<strong>有方向性的备择假设</strong>（B 优于 A）。这种情况下使用<strong>单尾（单向）假设检验</strong>，即只有一个方向上的极端随机结果计入 p 值。</p><p>如果你希望假设检验能保护你免于在任意方向上被随机性误导，那么备择假设是双向的（A 与 B 不同，可能更大或更小）。此时应使用<strong>双尾（双向）假设检验</strong>，即两个方向上的极端随机结果都计入 p 值。</p><p>单尾假设检验常常符合 A/B 决策的实际性质：通常一方被赋予“默认”地位，只有另一方更好时才替换。但软件（包括 R 和 Python 的 scipy）通常默认输出<strong>双尾检验</strong>结果，许多统计学家也倾向于使用更保守的双尾检验以避免争论。单尾与双尾的区别是一个容易混淆的话题，在数据科学中并不太重要，因为 p 值的精确性并不是特别关键。</p><p><strong>关键要点</strong></p><ul><li>原假设是一种逻辑构造，体现“什么特殊情况都没发生，观测到的效应完全是随机机会造成的”这一概念。</li><li>假设检验假定原假设成立，构建“原模型”（一个概率模型），并检验观测到的效应是否是该模型下的合理结果。</li></ul><h3 id="重抽样"><strong>重抽样</strong></h3><p>Resampling</p><p>在统计学中，重抽样指的是从已观测到的数据中反复抽取数值，其总体目标是评估某个统计量的随机变动性。它还可以用来评估和提高某些机器学习模型的准确度（例如，把多个自助抽样数据集上建立的决策树模型的预测结果取平均，这一过程称为“袋装法”——参见第 259 页“袋装法与随机森林”）。</p><p>重抽样程序主要有两种：<strong>自助法（bootstrap）</strong>和<strong>置换检验（permutation test）</strong>。自助法用来评估估计值的可靠性，在上一章已讨论过（见第 61 页“自助法”）。置换检验用于检验假设，通常涉及两个或多个组，本节将讨论这种方法。</p><p><strong>重抽样的关键术语</strong></p><p><strong>置换检验（Permutation test）</strong> 把两个或多个样本合并在一起，并随机（或穷尽地）重新分配观测值到新的重抽样中。</p><p><strong>同义词</strong> 随机化检验、随机置换检验、精确检验</p><p><strong>重抽样（Resampling）</strong> 从一个已观测到的数据集抽取附加样本（“重抽样”）。</p><p><strong>有放回或无放回（With or without replacement）</strong> 抽样时，每次抽出一个项目后，是否在下一次抽样前将其放回样本中。</p><h4 id="置换检验">置换检验</h4><p>Permutation Test</p><p>在置换程序中，涉及两个或更多的样本，通常是 A/B 测试或其他假设检验中的各组。<strong>Permute</strong> 的意思是改变一组数值的顺序。置换检验的第一步是将 A 组和 B 组（如果有 C、D……则包括它们）的结果合并。这体现了原假设的逻辑，即各组接受的处理并无差异。随后，我们通过从这个合并集里随机抽取各组，来检验这一假设并观察它们之间的差异。</p><p>置换程序如下：</p><ol type="1"><li>将不同组的结果合并为一个数据集。</li><li>将合并后的数据打乱，然后随机抽取（无放回）一个与 A 组同样大小的重抽样（显然其中会包含来自其他组的数据）。</li><li>从剩余数据中随机抽取（无放回）一个与 B 组同样大小的重抽样。</li><li>对 C、D 组等做同样操作。至此，你得到了一组与原始样本大小对应的重抽样。</li><li>对原始样本计算的统计量或估计值（如组间比例差异），在重抽样上重新计算并记录；这构成了一次置换迭代。</li><li>重复上述步骤 R 次，以得到检验统计量的置换分布。</li></ol><p>然后回到实际观测到的组间差异，把它与置换得到的差异集合进行比较。如果观测到的差异完全落在置换差异集合之内，我们并不能证明任何东西——观测差异在随机机会可能产生的范围内。但如果观测差异大多落在置换分布之外，则可以认为并非随机机会导致。从技术角度说，这个差异具有统计显著性（参见第 103 页“统计显著性与 p 值”）。</p><h4 id="示例网站黏性"><strong>示例：网站黏性</strong></h4><p>Web Stickiness</p><p>一家销售相对高价值服务的公司想要测试两种网页展示哪一种更能促进销售。由于所售服务价值高，销售频率低且周期长，如果直接看销售量，要花很长时间才能积累足够数据判断哪种展示更优。因此，公司决定用一个代理变量来衡量结果，具体是使用描述该服务的详细内部页面。</p><blockquote><p><strong>知识点：</strong></p><p>代理变量**（proxy variable）是用来替代真正感兴趣的变量的指标，这个真正的变量可能不可获得、成本太高或耗时太长。例如，在气候研究中，古代冰芯中的氧含量被用作温度的代理指标。最好至少有一些真正感兴趣变量的数据，以便评估它与代理变量之间的关联强度。</p></blockquote><p>对这家公司来说，一个潜在的代理变量是用户点击详细落地页的次数；一个更好的代理变量是用户在页面上停留的时间。合理的假设是，一个能更长时间吸引用户注意的网页展示更有可能带来更多销售。因此，我们的指标是平均会话时长，比较 A 页和 B 页。</p><p>由于这是一个内部的、特殊用途的页面，访客数并不多。还要注意，我们用 Google Analytics 来测量会话时长，但它无法测量用户访问的最后一个会话时长。Google Analytics 并不会将这类会话从数据中删除，而是记录为 0，因此数据需要额外处理以剔除这些会话。处理后，两种不同展示共 36 个会话，其中 A 页 21 个，B 页 15 个。</p><p><img src="/img3/面向数据科学家的实用统计学/F3.3.png" alt="F3.3" style="zoom:33%;" /></p><p>我们可以用 ggplot 通过并排箱线图直观比较会话时长：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ggplot<span class="punctuation">(</span>session_times<span class="punctuation">,</span> aes<span class="punctuation">(</span>x<span class="operator">=</span>Page<span class="punctuation">,</span> y<span class="operator">=</span>Time<span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">  geom_boxplot<span class="punctuation">(</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>pandas 的 boxplot 命令使用 <code>by</code> 参数来生成图形：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ax = session_times.boxplot(by=<span class="string">&#x27;Page&#x27;</span>, column=<span class="string">&#x27;Time&#x27;</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&#x27;Time (in seconds)&#x27;</span>)</span><br><span class="line">plt.suptitle(<span class="string">&#x27;&#x27;</span>)</span><br></pre></td></tr></table></figure><p>图 3-3 的箱线图表明，B 页的会话时长比 A 页更长。可以在 R 中这样计算各组均值：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mean_a <span class="operator">&lt;-</span> mean<span class="punctuation">(</span>session_times<span class="punctuation">[</span>session_times<span class="punctuation">[</span><span class="string">&#x27;Page&#x27;</span><span class="punctuation">]</span> <span class="operator">==</span> <span class="string">&#x27;Page A&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;Time&#x27;</span><span class="punctuation">]</span><span class="punctuation">)</span></span><br><span class="line">mean_b <span class="operator">&lt;-</span> mean<span class="punctuation">(</span>session_times<span class="punctuation">[</span>session_times<span class="punctuation">[</span><span class="string">&#x27;Page&#x27;</span><span class="punctuation">]</span> <span class="operator">==</span> <span class="string">&#x27;Page B&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;Time&#x27;</span><span class="punctuation">]</span><span class="punctuation">)</span></span><br><span class="line">mean_b <span class="operator">-</span> mean_a</span><br><span class="line"><span class="punctuation">[</span><span class="number">1</span><span class="punctuation">]</span> <span class="number">35.66667</span></span><br></pre></td></tr></table></figure><p>在 Python 中，我们先按页面筛选 pandas 数据框，再取 Time 列均值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mean_a = session_times[session_times.Page == <span class="string">&#x27;Page A&#x27;</span>].Time.mean()</span><br><span class="line">mean_b = session_times[session_times.Page == <span class="string">&#x27;Page B&#x27;</span>].Time.mean()</span><br><span class="line">mean_b - mean_a</span><br></pre></td></tr></table></figure><p>结果表明，B 页的平均会话时长比 A 页长 35.67 秒。问题在于，这个差异是否在随机因素可能产生的范围内，即是否具有统计显著性。一种方法是应用置换检验——把所有会话时长合并，然后反复打乱并按 21 个（A 页，nA=21）和 15 个（B 页，nB=15）分组。</p><p>为了应用置换检验，我们需要一个函数，把 36 个会话时长随机分配为一组 21 个（A 页）和一组 15 个（B 页）。R 版本的函数是：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">perm_fun <span class="operator">&lt;-</span> <span class="keyword">function</span><span class="punctuation">(</span>x<span class="punctuation">,</span> nA<span class="punctuation">,</span> nB<span class="punctuation">)</span></span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  n <span class="operator">&lt;-</span> nA <span class="operator">+</span> nB</span><br><span class="line">  idx_b <span class="operator">&lt;-</span> sample<span class="punctuation">(</span><span class="number">1</span><span class="operator">:</span>n<span class="punctuation">,</span> nB<span class="punctuation">)</span></span><br><span class="line">  idx_a <span class="operator">&lt;-</span> setdiff<span class="punctuation">(</span><span class="number">1</span><span class="operator">:</span>n<span class="punctuation">,</span> idx_b<span class="punctuation">)</span></span><br><span class="line">  mean_diff <span class="operator">&lt;-</span> mean<span class="punctuation">(</span>x<span class="punctuation">[</span>idx_b<span class="punctuation">]</span><span class="punctuation">)</span> <span class="operator">-</span> mean<span class="punctuation">(</span>x<span class="punctuation">[</span>idx_a<span class="punctuation">]</span><span class="punctuation">)</span></span><br><span class="line">  <span class="built_in">return</span><span class="punctuation">(</span>mean_diff<span class="punctuation">)</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>Python 版本的置换检验函数如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">perm_fun</span>(<span class="params">x, nA, nB</span>):</span><br><span class="line">    n = nA + nB</span><br><span class="line">    idx_B = <span class="built_in">set</span>(random.sample(<span class="built_in">range</span>(n), nB))</span><br><span class="line">    idx_A = <span class="built_in">set</span>(<span class="built_in">range</span>(n)) - idx_B</span><br><span class="line">    <span class="keyword">return</span> x.loc[idx_B].mean() - x.loc[idx_A].mean()</span><br></pre></td></tr></table></figure><p>这个函数的工作方式是：不放回地抽取 <span class="math inline">\(n_B\)</span> 个索引分配给 B 组；剩下的 <span class="math inline">\(n_A\)</span> 个索引分配给 A 组；<strong>返回两组均值之差</strong>。 将这个函数调用 R=1,000 次，并指定 <span class="math inline">\(n_A\)</span>=21、<span class="math inline">\(n_B\)</span>=15，就可以得到会话时长差异的分布，并画出直方图。 在 R 中用 hist 函数实现如下：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">perm_diffs <span class="operator">&lt;-</span> <span class="built_in">rep</span><span class="punctuation">(</span><span class="number">0</span><span class="punctuation">,</span> <span class="number">1000</span><span class="punctuation">)</span></span><br><span class="line"><span class="keyword">for</span> <span class="punctuation">(</span>i <span class="keyword">in</span> <span class="number">1</span><span class="operator">:</span><span class="number">1000</span><span class="punctuation">)</span> <span class="punctuation">&#123;</span></span><br><span class="line">  perm_diffs<span class="punctuation">[</span>i<span class="punctuation">]</span> <span class="operator">=</span> perm_fun<span class="punctuation">(</span>session_times<span class="punctuation">[</span><span class="punctuation">,</span> <span class="string">&#x27;Time&#x27;</span><span class="punctuation">]</span><span class="punctuation">,</span> <span class="number">21</span><span class="punctuation">,</span> <span class="number">15</span><span class="punctuation">)</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br><span class="line">hist<span class="punctuation">(</span>perm_diffs<span class="punctuation">,</span> xlab<span class="operator">=</span><span class="string">&#x27;Session time differences (in seconds)&#x27;</span><span class="punctuation">)</span></span><br><span class="line">abline<span class="punctuation">(</span>v<span class="operator">=</span>mean_b <span class="operator">-</span> mean_a<span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>在 Python 中，我们可以用 matplotlib 画类似的图：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">perm_diffs = [perm_fun(session_times.Time, nA, nB) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>)]</span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line">ax.hist(perm_diffs, bins=<span class="number">11</span>, rwidth=<span class="number">0.9</span>)</span><br><span class="line">ax.axvline(x = mean_b - mean_a, color=<span class="string">&#x27;black&#x27;</span>, lw=<span class="number">2</span>)</span><br><span class="line">ax.text(<span class="number">50</span>, <span class="number">190</span>, <span class="string">&#x27;Observed\ndifference&#x27;</span>, bbox=&#123;<span class="string">&#x27;facecolor&#x27;</span>:<span class="string">&#x27;white&#x27;</span>&#125;)</span><br><span class="line">ax.set_xlabel(<span class="string">&#x27;Session time differences (in seconds)&#x27;</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&#x27;Frequency&#x27;</span>)</span><br></pre></td></tr></table></figure><p>图 3-4 的直方图显示，随机置换的均值差异经常超过观测到的会话时长差异（竖线所示）。</p><p><img src="/img3/面向数据科学家的实用统计学/F3.4.png" alt="F3.4" style="zoom:33%;" /></p><p>在我们的结果中，这种情况发生在 12.6% 的案例里：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mean<span class="punctuation">(</span>perm_diffs <span class="operator">&gt;</span> <span class="punctuation">(</span>mean_b <span class="operator">-</span> mean_a<span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line"><span class="operator">-</span><span class="operator">-</span><span class="operator">-</span></span><br><span class="line"><span class="number">0.126</span></span><br></pre></td></tr></table></figure><p>由于模拟使用了随机数，这个百分比会有所不同。例如，在 Python 版本中，我们得到的是 12.1%：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">np.mean(perm_diffs &gt; mean_b - mean_a)</span><br><span class="line">---</span><br><span class="line"><span class="number">0.121</span></span><br></pre></td></tr></table></figure><p>这表明，A 页与 B 页之间观测到的会话时长差异完全在随机波动范围内，因此<strong>没有统计显著性</strong>。</p><h4 id="穷举置换检验与自助置换检验">穷举置换检验与自助置换检验</h4><p>Exhaustive and Bootstrap Permutation Tests</p><p>除了前面这种随机洗牌程序（又叫随机置换检验或随机化检验）外，置换检验还有两个变体：</p><ul><li><strong>穷举置换检验（exhaustive permutation test）</strong></li><li><strong>自助置换检验（bootstrap permutation test）</strong></li></ul><p>在穷举置换检验中，我们不是只随机打乱并分组，而是列举出所有可能的分组方式。这只在样本量较小时可行。随机置换检验在重复次数足够多时，其结果会逼近穷举置换检验，并在极限情况下收敛于它。 由于具有保证“原模型”不会以超过检验 α 水平被判定为“显著”的统计性质，穷举置换检验有时也被称为<strong>精确检验（exact test）</strong>（参见第 103 页“统计显著性与 p 值”）。</p><p>在自助置换检验中，随机置换检验步骤 2 和步骤 3 中的抽样改为<strong>有放回</strong>而非无放回。这样，重抽样程序不仅模拟处理分配给受试者的随机因素，还模拟从总体抽取受试者的随机因素。</p><p>这两种程序在统计学中都能遇到，它们之间的区别有些复杂，但在数据科学实践中并不重要。</p><h4 id="置换检验数据科学中的核心要点">置换检验：数据科学中的核心要点</h4><p>Permutation Tests: The Bottom Line for Data Science</p><p>置换检验是一种用来探索随机变异作用的有用启发式程序。它们相对容易编写代码、解释和说明，并且提供了一种有别于基于公式统计方法的“形式主义”和“虚假确定性”的有用路径，因为公式给出的“精确”答案往往暗示了不必要的确定性。与公式方法相比，重抽样的一个优点是，它更接近于一种“通用”推断方法。数据可以是数值型的或二值型的；样本容量可以相同也可以不同；不需要假设数据服从正态分布。</p><p><strong>关键要点</strong></p><ul><li>在置换检验中，先将多个样本合并再打乱顺序。</li><li>然后将打乱后的值分成重抽样样本，计算感兴趣的统计量。</li><li>重复这一过程，并记录重抽样统计量。</li><li>将观察到的统计量与重抽样分布进行比较，可以判断样本间观察到的差异是否可能由随机性产生。</li></ul><h3 id="统计显著性与-p-值">统计显著性与 p 值</h3><p>Statistical Significance and p-Values</p><p>统计显著性是统计学家衡量实验（或对现有数据的研究）所得结果是否比随机性产生的结果更极端的一种方法。如果结果超出了随机变异的范围，就称之为具有统计显著性。</p><p><strong>统计显著性与 p 值的关键术语</strong></p><ul><li><strong>p 值</strong>：在一个体现零假设的随机模型下，得到与观测结果一样不寻常或极端的结果的概率。</li><li><strong>α（显著性水平）</strong>：判断“异常”程度的概率阈值，实际结果超过该阈值就被视为具有统计显著性。</li><li><strong>第一类错误（Type 1 error）</strong>：错误地认为效应存在（其实是偶然造成的）。</li><li><strong>第二类错误（Type 2 error）</strong>：错误地认为效应是偶然的（其实是真实存在的）。</li></ul><p>请看前面网页测试示例的表 3-2：</p><p><img src="/img3/面向数据科学家的实用统计学/T3.2.png" alt="T3.2" style="zoom:67%;" /></p><p>价格 A 的转化率比价格 B 高将近 5%（0.8425% = 200/(23,539+200)*100，而 0.8057% = 182/(22,406+182)*100，差值为 0.0368 个百分点），在高流量业务中已足够有意义。这里我们有 45,000 多个数据点，容易认为这是“大数据”，不需要做统计显著性检验（主要用于处理小样本中的抽样变异）。然而，转化率不到 1%，实际有意义的数值（转化次数）只有几百个，真正决定所需样本量的是这些转化次数。我们可以用重抽样程序来检验价格 A 和 B 之间的转化率差异是否在随机变异的范围内。这里“随机变异”指的是一个体现零假设（即两种价格的转化率没有差别）的概率模型所产生的随机波动。</p><p>下述置换程序提出的问题是：“如果两种价格具有相同的转化率，随机波动能否产生像 5% 这样大的差异？”</p><ol type="1"><li>把标有 1 和 0 的卡片放入盒中：这代表假定的共享转化率，382 个 1 和 45,945 个 0 = 0.008246 = 0.8246%。</li><li>洗牌并抽取一个大小为 23,739（价格 A 的样本量）的重抽样，记录 1 的数量。</li><li>记录剩下的 22,588（价格 B 的样本量）中 1 的数量。</li><li>记录 1 的比例差异。</li><li>重复步骤 2–4。</li><li>统计差异 ≥ 0.0368 的次数。</li></ol><p>在 R 中可以重用第 98 页“示例：网页黏性”里定义的 <code>perm_fun</code> 函数来创建转化率随机置换差异的直方图：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">obs_pct_diff <span class="operator">&lt;-</span> 100 <span class="operator">*</span> <span class="punctuation">(</span><span class="number">200</span> <span class="operator">/</span> <span class="number">23739</span> <span class="operator">-</span> <span class="number">182</span> <span class="operator">/</span> <span class="number">22588</span><span class="punctuation">)</span></span><br><span class="line">conversion <span class="operator">&lt;-</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="built_in">rep</span><span class="punctuation">(</span><span class="number">0</span><span class="punctuation">,</span> <span class="number">45945</span><span class="punctuation">)</span><span class="punctuation">,</span> <span class="built_in">rep</span><span class="punctuation">(</span><span class="number">1</span><span class="punctuation">,</span> <span class="number">382</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">perm_diffs <span class="operator">&lt;-</span> <span class="built_in">rep</span><span class="punctuation">(</span><span class="number">0</span><span class="punctuation">,</span> <span class="number">1000</span><span class="punctuation">)</span></span><br><span class="line"><span class="keyword">for</span> <span class="punctuation">(</span>i <span class="keyword">in</span> <span class="number">1</span><span class="operator">:</span><span class="number">1000</span><span class="punctuation">)</span> <span class="punctuation">&#123;</span></span><br><span class="line">  perm_diffs<span class="punctuation">[</span>i<span class="punctuation">]</span> <span class="operator">=</span> <span class="number">100</span> <span class="operator">*</span> perm_fun<span class="punctuation">(</span>conversion<span class="punctuation">,</span> <span class="number">23739</span><span class="punctuation">,</span> <span class="number">22588</span><span class="punctuation">)</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br><span class="line">hist<span class="punctuation">(</span>perm_diffs<span class="punctuation">,</span> xlab<span class="operator">=</span><span class="string">&#x27;Conversion rate (percent)&#x27;</span><span class="punctuation">,</span> main<span class="operator">=</span><span class="string">&#x27;&#x27;</span><span class="punctuation">)</span></span><br><span class="line">abline<span class="punctuation">(</span>v<span class="operator">=</span>obs_pct_diff<span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>对应的 Python 代码为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">obs_pct_diff = <span class="number">100</span> * (<span class="number">200</span> / <span class="number">23739</span> - <span class="number">182</span> / <span class="number">22588</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Observed difference: <span class="subst">&#123;obs_pct_diff:<span class="number">.4</span>f&#125;</span>%&#x27;</span>)</span><br><span class="line">conversion = [<span class="number">0</span>] * <span class="number">45945</span></span><br><span class="line">conversion.extend([<span class="number">1</span>] * <span class="number">382</span>)</span><br><span class="line">conversion = pd.Series(conversion)</span><br><span class="line">perm_diffs = [<span class="number">100</span> * perm_fun(conversion, <span class="number">23739</span>, <span class="number">22588</span>)</span><br><span class="line">              <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>)]</span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line">ax.hist(perm_diffs, bins=<span class="number">11</span>, rwidth=<span class="number">0.9</span>)</span><br><span class="line">ax.axvline(x=obs_pct_diff, color=<span class="string">&#x27;black&#x27;</span>, lw=<span class="number">2</span>)</span><br><span class="line">ax.text(<span class="number">0.06</span>, <span class="number">200</span>, <span class="string">&#x27;Observed\ndifference&#x27;</span>, bbox=&#123;<span class="string">&#x27;facecolor&#x27;</span>:<span class="string">&#x27;white&#x27;</span>&#125;)</span><br><span class="line">ax.set_xlabel(<span class="string">&#x27;Conversion rate (percent)&#x27;</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&#x27;Frequency&#x27;</span>)</span><br></pre></td></tr></table></figure><p>在图 3-5 中展示了 1,000 个重抽样结果的直方图：就本例而言，观察到的 0.0368% 的差异完全处于随机波动范围内。</p><p><img src="/img3/面向数据科学家的实用统计学/F3.5.png" alt="F3.5" style="zoom:33%;" /></p><h4 id="p-值">p 值</h4><p>单纯看图并不是衡量统计显著性的精确方法，因此更有意义的是 p 值。p 值指的是“机会模型”（即零假设模型）产生比观测结果更极端结果的频率。我们可以用置换检验估计 p 值：计算置换检验产生的差异大于或等于观测差异的比例：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mean<span class="punctuation">(</span>perm_diffs <span class="operator">&gt;</span> obs_pct_diff<span class="punctuation">)</span></span><br><span class="line"><span class="punctuation">[</span><span class="number">1</span><span class="punctuation">]</span> <span class="number">0.308</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.mean([diff &gt; obs_pct_diff <span class="keyword">for</span> diff <span class="keyword">in</span> perm_diffs])</span><br></pre></td></tr></table></figure><p>这里，R 和 Python 都利用了“true 被当作 1、false 被当作 0”这一事实。p 值为 0.308，意味着在随机条件下，我们大约 30% 的时间会得到像这样极端或更极端的结果。</p><p>在这种情况下，其实不必用置换检验来得到 p 值。由于我们有二项分布，可以近似计算 p 值。R 代码如下：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">prop.test<span class="punctuation">(</span>x<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">200</span><span class="punctuation">,</span> <span class="number">182</span><span class="punctuation">)</span><span class="punctuation">,</span> n<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">23739</span><span class="punctuation">,</span> <span class="number">22588</span><span class="punctuation">)</span><span class="punctuation">,</span> alternative<span class="operator">=</span><span class="string">&#x27;greater&#x27;</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">2-sample test for equality of proportions with continuity correction</span><br><span class="line">data: c(200, 182) out of c(23739, 22588)</span><br><span class="line">X-squared = 0.14893, df = 1, p-value = 0.3498</span><br><span class="line">alternative hypothesis: greater</span><br><span class="line">95 percent confidence interval:</span><br><span class="line">-0.001057439 1.000000000</span><br><span class="line">sample estimates:</span><br><span class="line">prop 1 prop 2</span><br><span class="line">0.008424955 0.008057376</span><br></pre></td></tr></table></figure><p>参数 <code>x</code> 是每组成功次数，参数 <code>n</code> 是每组试验次数。</p><p><code>scipy.stats.chi2_contingency</code> 方法可接受表 3-2 中的数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">survivors = np.array([[<span class="number">200</span>, <span class="number">23739</span> - <span class="number">200</span>], [<span class="number">182</span>, <span class="number">22588</span> - <span class="number">182</span>]])</span><br><span class="line">chi2, p_value, df, _ = stats.chi2_contingency(survivors)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;p-value for single sided test: <span class="subst">&#123;p_value / <span class="number">2</span>:<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p>正态近似得出 p 值为 0.3498，与置换检验的 p 值接近。</p><h4 id="α显著性水平">α（显著性水平）</h4><p>统计学家不赞成把“结果是否太不寻常”交给研究者自由裁量，而是提前规定一个阈值，例如“比零假设下 5% 的结果更极端”。这个阈值称为 α。典型的 α 水平有 5% 和 1%。无论选择哪一个水平，本质上都是人为的决策，并不能保证 x% 的时候都作出正确判断。这是因为所回答的概率问题并不是“这个结果发生的概率是多少？”，而是“在机会模型成立的前提下，得到如此极端结果的概率是多少？”。然后我们再反推机会模型是否合适，但这种判断并不具有概率意义。这一点长期以来造成了许多混淆。</p><p><strong>p 值争议</strong>:p-value controversy</p><p>近年来，p 值的使用引发了大量争议。某心理学期刊甚至“禁止”投稿论文使用 p 值，理由是单纯根据 p 值决定是否发表，导致了劣质研究的出版。太多研究者对 p 值的真正含义只有模糊的认识，他们在数据里翻找、在不同假设之间挑选，直到找到一个显著的 p 值，于是就能发表论文。</p><p>真正的问题是，人们希望从 p 值中得到超出它本身的信息。我们希望 p 值代表的是：</p><blockquote><p>结果由随机性造成的概率</p></blockquote><p>我们希望这个值越低越好，这样就可以得出“我们证明了某件事”的结论。许多期刊编辑就是这样解读 p 值的。但实际上 p 值真正表示的是：</p><blockquote><p>在一个机会模型成立的条件下，得到像观测结果一样极端结果的概率</p></blockquote><p>两者区别微妙但真实。显著的 p 值并不能像它表面承诺的那样把你推向“证明”的地步。当理解了 p 值的真正含义时，“统计显著”这一结论的逻辑基础其实更弱。</p><p>2016 年 3 月，美国统计学会（ASA）经过内部长时间讨论，发布了关于 p 值使用的警示声明，揭示了人们对 p 值的误解。ASA 声明强调了供研究者和期刊编辑参考的六条原则：</p><ol type="1"><li>p 值可以反映数据与某个特定统计模型的不兼容程度。</li><li>p 值不能衡量所研究假设为真的概率，也不能衡量数据纯粹由随机性产生的概率。</li><li>科学结论和商业或政策决策不应仅仅基于 p 值是否超过某个阈值。</li><li>做出恰当推断需要全面报告和透明度。</li><li>p 值或统计显著性不能衡量效应大小或结果的重要性。</li><li>单靠 p 值无法提供关于模型或假设的有力证据。</li></ol><p><strong>实际显著性</strong>:Practical significance</p><p>即使一个结果在统计上显著，这也并不意味着它在实际中具有显著性。如果样本量足够大，即便是一个毫无实际意义的微小差异，也可能在统计上显著。大样本能确保即便是小且无意义的效应，也足以排除“随机”作为解释。然而，排除了随机性并不会神奇地使一个本质上无关紧要的结果变得重要。</p><h4 id="第一类错误与第二类错误">第一类错误与第二类错误</h4><p>在评估统计显著性时，可能发生两种错误：</p><ul><li><strong>第一类错误（Type 1 error）</strong>：当效应实际上只是随机造成的，却错误地得出效应真实存在的结论；</li><li><strong>第二类错误（Type 2 error）</strong>：当效应实际上是真实存在的，却错误地得出效应不存在（即归因于随机性）的结论。</li></ul><p>实际上，第二类错误与其说是错误，不如说是样本量太小，无法检测到效应。当 p 值未达到统计显著性（例如大于 5%）时，我们真正的意思是“效应未被证明”。可能换成更大的样本就会得到更小的 p 值。</p><p>显著性检验（也称假设检验）的基本功能是防止被随机性所欺骗，因此它们通常设计成最小化第一类错误。</p><h4 id="数据科学与-p-值">数据科学与 p 值</h4><p>数据科学家的工作通常不是为了在科学期刊上发表，因此围绕 p 值价值的争论更多是学术问题。对于数据科学家来说，p 值在想要知道某个看起来有趣且有用的模型结果是否处于正常随机波动范围内时，是一个有用的度量指标。作为实验中的决策工具，p 值不应被视为“决定性”的，而仅仅是为决策提供信息的一个参考点。例如，在某些统计或机器学习模型中，p 值有时作为中间输入使用——某个特征是否被包含在模型中，可能取决于它的 p 值。</p><p><strong>关键要点</strong></p><ul><li>显著性检验用于判断观测到的效应是否在零假设模型的随机波动范围内；</li><li>p 值是在零假设模型成立的前提下，得到像观测结果一样极端结果的概率；</li><li>α 值是零假设随机模型中“异常程度”的阈值；</li><li>显著性检验在正式研究报告中比在数据科学中更相关（但近年即便在研究中也逐渐淡化）。</li></ul><h3 id="t-检验">t 检验</h3><p>显著性检验有很多种，具体取决于数据是计数数据还是测量数据、样本有多少以及测量的是什么。非常常见的一种是 <strong>t 检验</strong>，它以 <strong>Student’s t 分布</strong>命名，最初由 W. S. Gosset 提出，用于近似单一样本均值的分布（参见第 75 页“Student’s t 分布”）。</p><p><strong>t 检验关键术语</strong></p><ul><li><strong>检验统计量（Test statistic）</strong>：衡量差异或感兴趣效应的指标；</li><li><strong>t 统计量（t-statistic）</strong>：均值等常见检验统计量的标准化版本；</li><li><strong>t 分布（t-distribution）</strong>：一个参考分布（此处来源于零假设），用来与观测到的 t 统计量进行比较。</li></ul><p>所有显著性检验都要求你指定一个检验统计量，用来衡量你感兴趣的效应，并帮助你判断观测到的效应是否处于正常随机波动的范围内。在重抽样检验中（见第 97 页“置换检验”的讨论），数据的量纲无关紧要。你从数据本身创建参考（零假设）分布，并直接使用该检验统计量。</p><p>在 20 世纪 20–30 年代，统计假设检验正在发展之时，随机打乱数据上千次以做重抽样检验在计算上并不可行。统计学家发现，一个对置换（打乱）分布的良好近似是 t 检验，它基于 Gosset 的 t 分布。它被用于非常常见的两样本比较——A/B 测试——当数据是数值型时尤为常见。但要使 t 分布在不考虑量纲的情况下使用，必须采用检验统计量的标准化形式。</p><p>一本经典的统计教材在此处会展示各种结合 Gosset 分布的公式，并演示如何将数据标准化以与标准 t 分布进行比较。这里没有列出这些公式，因为所有统计软件（包括 R 和 Python）都内置了实现这些公式的命令。在 R 中，对应的函数是 <code>t.test</code>：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">&gt;</span> t.test<span class="punctuation">(</span>Time <span class="operator">~</span> Page<span class="punctuation">,</span> data<span class="operator">=</span>session_times<span class="punctuation">,</span> alternative<span class="operator">=</span><span class="string">&#x27;less&#x27;</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line">Welch Two Sample t<span class="operator">-</span>test  </span><br><span class="line">data<span class="operator">:</span> Time by Page  </span><br><span class="line">t <span class="operator">=</span> <span class="operator">-</span><span class="number">1.0983</span><span class="punctuation">,</span> df <span class="operator">=</span> <span class="number">27.693</span><span class="punctuation">,</span> p<span class="operator">-</span>value <span class="operator">=</span> <span class="number">0.1408</span>  </span><br><span class="line">alternative hypothesis<span class="operator">:</span> true difference <span class="keyword">in</span> means is less than <span class="number">0</span>  </span><br><span class="line"><span class="number">95</span> percent confidence interval<span class="operator">:</span>  </span><br><span class="line"><span class="operator">-</span><span class="literal">Inf</span> <span class="number">19.59674</span>  </span><br><span class="line">sample estimates<span class="operator">:</span>  </span><br><span class="line">mean <span class="keyword">in</span> group Page A mean <span class="keyword">in</span> group Page B  </span><br><span class="line"><span class="number">126.3333</span> <span class="number">162.0000</span></span><br></pre></td></tr></table></figure><p>在 Python 中可以使用 <code>scipy.stats.ttest_ind</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">res = stats.ttest_ind(</span><br><span class="line">    session_times[session_times.Page == <span class="string">&#x27;Page A&#x27;</span>].Time,</span><br><span class="line">    session_times[session_times.Page == <span class="string">&#x27;Page B&#x27;</span>].Time,</span><br><span class="line">    equal_var=<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;p-value for single sided test: <span class="subst">&#123;res.pvalue / <span class="number">2</span>:<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p>备择假设是：Page A 的会话时间均值小于 Page B 的会话时间。p 值为 0.1408，与置换检验得到的 p 值 0.121 和 0.126（见第 98 页“示例：网页黏性”）相当接近。</p><p>在重抽样模式下，我们根据观测数据和待检验的假设来构造解，不必担心数据是数值型还是二元型，样本量是否均衡、样本方差、以及其他各种因素。而在公式的世界里，存在很多变体，可能令人眼花缭乱。统计学家需要在那个世界中导航并掌握它的地图，但数据科学家不必如此——他们通常无需像准备学术论文的研究者那样在假设检验和置信区间的细节上费尽心思。</p><p><strong>关键要点</strong></p><ul><li>在计算机出现之前，重抽样检验并不实际，统计学家使用标准参考分布。</li><li>然后可以将检验统计量标准化并与参考分布进行比较。</li><li>一个广泛使用的标准化统计量就是 <strong>t 统计量</strong>。</li></ul><h3 id="多重检验">多重检验</h3><p>Multiple Testing</p><p>正如我们之前提到过的，统计学里有句俗语：“折磨数据足够久，它终会招供。”这意味着，如果你从足够多的角度看数据、问足够多的问题，几乎总能找到某个“统计显著”的效应。</p><p>例如，如果你有 20 个预测变量和 1 个结果变量，且它们都是随机生成的，那么如果你按 α = 0.05 的显著性水平做 20 次显著性检验，至少有一个预测变量（错误地）显得统计显著的概率非常大。正如前文所述，这就是<strong>Ⅰ类错误</strong>。你可以这样计算这个概率：先求出所有检验都正确地不显著的概率。一个检验正确地不显著的概率是 0.95，那么 20 个都正确地不显著的概率就是0.95 × 0.95 × 0.95… = 0.95²⁰ = 0.36。至少一个预测变量（错误地）显著的概率就是这个概率的反面，即1 – （所有都不显著的概率）= 0.64。这被称为<strong>α 膨胀（alpha inflation）</strong>。</p><p>这一问题与数据挖掘中的过拟合有关，也就是“把模型拟合到噪声上”。你加入的变量越多、运行的模型越多，就越可能仅凭偶然性得出“显著”的结果。</p><p><strong>多重检验关键术语</strong></p><ul><li><p><strong>Ⅰ类错误（Type 1 error）</strong> 错误地认为某个效应具有统计显著性。</p></li><li><p><strong>假发现率（False discovery rate）</strong> 在多次检验中犯Ⅰ类错误的比例。</p></li><li><p><strong>α 膨胀（Alpha inflation）</strong> 随着进行更多检验，α（Ⅰ类错误概率）不断上升的多重检验现象。</p></li><li><p><strong>p 值调整（Adjustment of p-values）</strong> 针对同一数据做多次检验时进行的校正。</p></li><li><p><strong>过拟合（Overfitting）</strong> 拟合到了噪声。</p></li></ul><p>在监督学习任务中，使用一个模型未见过的数据集（保留集/验证集）来评估模型，可以降低这种风险。而在没有标注保留集的统计或机器学习任务中，基于统计噪声得出结论的风险依然存在。</p><p>在统计学中，有一些方法在特定情境下用来处理这个问题。例如，如果你在比较多个处理组的结果，你可能会问多个问题。对于 A–C 三个处理，你可能会问：</p><ul><li>A 是否不同于 B？</li><li>B 是否不同于 C？</li><li>A 是否不同于 C？</li></ul><p>或者，在临床试验中，你可能想在多个阶段观察某种疗法的结果。每一次提问，都会增加被偶然性“愚弄”的概率。统计学中的调整方法可以通过比单个假设检验更严格地设定显著性门槛来补偿这种风险。这些调整方法通常涉及根据检验的次数“分割 α”。这会导致每个检验的 α 更小（即显著性标准更严格）。</p><ul><li><strong>Bonferroni 调整</strong>：简单地把 α 除以比较的次数。</li><li><strong>Tukey 的“诚实显著性差异”（HSD）</strong>：用于比较多个组均值。这个检验针对组均值之间的最大差异，将其与一个基于 t 分布的基准进行比较（大致相当于把所有值混在一起，重新抽取与原来组大小相同的样本，然后找出这些重抽样组均值之间的最大差异）。</li></ul><p>然而，多重比较的问题超出了这些高度结构化的情形，并与反复“淘洗”数据的现象有关，这正是那句“折磨数据”的俗语的由来。换句话说，给定足够复杂的数据，如果你还没发现有趣的东西，那只是因为你还没看得足够久、问得足够多。如今可用的数据比以往任何时候都多，期刊文章的数量在 2002–2010 年间几乎翻了一番。这给了我们大量机会在数据中发现有趣的东西，同时也带来了多重性问题，例如：</p><ul><li>检查多个组间的成对差异</li><li>观察多个子组的结果（“整体治疗效果不显著，但我们在 30 岁以下未婚女性中发现了效果”）</li><li>尝试大量统计模型</li><li>在模型中包含大量变量</li><li>提出许多不同的问题（即不同的可能结果）</li></ul><blockquote><p><strong>通用注解</strong></p><p>假发现率（False Discovery Rate, FDR）**：“假发现率”这一术语最初用来描述一组假设检验中，错误地识别出显著效应的比例。随着基因组学研究的兴起，它变得尤其有用，因为在基因测序项目中可能要进行海量的统计检验。在这些情境中，这个术语适用于整个检验流程，而单个错误“发现”指的是某次假设检验的结果（例如比较两份样本）。研究人员希望设置检验过程的参数，以在指定水平上控制假发现率。该术语还被用在数据挖掘的分类问题中：它表示在类别 1 的预测中出现的误分类率。换句话说，它是“发现”是假的概率（把一条记录标为“1”时，它实际上是假的概率）。这种情形通常对应 0 类丰富、1 类稀少而有趣的情况（参见第 5 章“稀有类别问题”）。</p></blockquote><p>由于包括“多重性”在内的多种原因，更多的研究并不一定意味着更好的研究。例如，制药公司拜耳在 2011 年发现，当它试图重复 67 项科学研究时，只有 14 项能完全复现，近三分之二完全无法复现。</p><p>无论如何，高度结构化的统计检验的各种调整程序过于具体和僵化，不适合数据科学的普遍用途。数据科学家在多重性问题上的要点是：</p><ul><li>对于预测建模，通过交叉验证（见第 155 页“交叉验证”）和使用保留样本，可以减轻得到一个看似有效、但实际上主要源自随机性的虚幻模型的风险。</li><li>对于没有带标签的保留集来检查模型的其他过程，你必须依赖： — 意识到查询和操作数据越多，随机性起作用的可能性就越大； — 使用重采样和模拟的启发式方法提供随机基准，与观测结果进行比较。</li></ul><p><strong>关键要点</strong></p><ul><li>研究或数据挖掘项目中的多重性（多重比较、许多变量、许多模型等）增加了仅因偶然性而得出显著结论的风险。</li><li>在涉及多次统计比较（即多重显著性检验）的情形下，有统计学上的调整程序。</li><li>在数据挖掘情境中，使用带标签结果变量的保留样本可以帮助避免误导性结果。</li></ul><h3 id="自由度">自由度</h3><p>Degrees of Freedom</p><p>在许多统计检验和概率分布的文档及设置中，你会看到“自由度”一词。该概念应用于从样本数据计算得来的统计量，指可自由变化的数值个数。例如，如果你知道一个包含 10 个值的样本的均值，那么自由度就是 9（因为一旦你知道其中 9 个样本值，第 10 个值就可以计算出来，不再自由）。在许多概率分布中，自由度参数会影响分布的形状。</p><p>自由度的数量是许多统计检验的输入参数。例如，自由度就是在方差和标准差计算中分母 n – 1 的名称。为什么这很重要？当你用样本估计总体方差时，如果分母用 n，你会得到一个略微向下偏倚的估计；如果用 n – 1，估计就没有这种偏倚。</p><p><strong>自由度关键术语</strong></p><ul><li><strong>n 或样本量</strong>：数据中的观测数（也称行数或记录数）。</li><li><strong>d.f.（自由度）</strong></li></ul><p>传统统计课程或教材中很大一部分都在讲各种标准假设检验（t 检验、F 检验等）。当样本统计量被标准化以应用于传统统计公式时，自由度是标准化计算的一部分，以确保你的标准化数据与相应的参考分布（t 分布、F 分布等）相匹配。</p><p>对数据科学来说重要吗？其实不太重要，至少在显著性检验的语境中不是这样。一方面，数据科学中正式的统计检验使用得很有限；另一方面，数据规模通常足够大，因此对于数据科学家来说，分母是 n 还是 n – 1 几乎没有实质差别（当 n 变大时，用 n 作为分母所产生的偏倚会消失）。</p><p>不过，有一个情境与之相关：<strong>在回归（包括逻辑回归）中使用分解变量时。如果存在完全冗余的预测变量，一些回归算法会出错。这种情况最常见于把分类变量分解为二元指示变量（哑变量）。</strong>以“星期几”为例。虽然一周有七天，但“星期几”这个变量只有六个自由度。比如，一旦你知道“不是周一到周六”，就可以推知“必然是周日”。因此，如果你包含了周一到周六的指示变量，再加上周日，就会因为多重共线性导致回归失败。</p><p><strong>关键要点</strong></p><ul><li>自由度（d.f.）是标准化检验统计量以便与参考分布（t 分布、F 分布等）比较的计算的一部分。</li><li>自由度概念是把分类变量分解为 n – 1 个指示（哑）变量以进行回归的理论基础（避免多重共线性）。</li></ul><h3 id="anova方差分析">ANOVA（方差分析）</h3><p>假设我们不是进行 A/B 测试，而是要比较多个组，比如 A/B/C/D，每个组都有数值数据。用来检验各组之间是否存在统计学显著差异的统计程序称为<strong>方差分析</strong>（analysis of variance，简称 <strong>ANOVA</strong>）。</p><p><strong>ANOVA 的关键术语</strong></p><ul><li><p><strong>成对比较（Pairwise comparison）</strong> 在多个组中对任意两组之间（如均值）进行的假设检验。</p></li><li><p><strong>总体检验（Omnibus test）</strong> 对多个组均值总体方差进行的单一假设检验。</p></li><li><p><strong>方差分解（Decomposition of variance）</strong> 将对单个值有贡献的成分分离出来（例如相对于总体均值、相对于处理均值和相对于残差误差的成分）。</p></li><li><p><strong>F 统计量（F-statistic）</strong> 标准化的统计量，衡量<strong>组均值之间的差异</strong>是否超过随机模型下可能出现的差异程度。</p></li><li><p><strong>SS（平方和，Sum of Squares）</strong> 指的是相对于某个平均值的偏差（deviations）平方之和。</p></li></ul><p><img src="/img3/面向数据科学家的实用统计学/T3.3.png" alt="T3.3" style="zoom:50%;" /></p><p>表 3-3 显示了四个网页的“黏性”（stickiness），定义为访问者停留在页面上的秒数。四个页面是随机分配的，每位访客只会看到其中一个页面。每个页面有 5 位访客，表 3-3 中的每一列都是一组独立的数据。例如，第 1 页的第 1 位访客与第 2 页的第 1 位访客之间没有关联。注意在这种网页测试中，我们无法完全实现经典的随机抽样设计（即每位访客从某个庞大总体中随机选取），只能按访客到来的顺序分配。访客可能因一天中的时间、星期几、季节、网络条件、所用设备等而系统性地不同，在回顾实验结果时要把这些因素视为潜在偏差。</p><p><img src="/img3/面向数据科学家的实用统计学/F3.6.png" alt="F3.6" style="zoom:33%;" /></p><p>现在我们遇到了一个难题（见图 3-6）。当只比较两个组时，我们只需看两组均值的差异就行。但当有四个均值时，可能的成对比较有六种：</p><ul><li>第 1 页与第 2 页比较</li><li>第 1 页与第 3 页比较</li><li>第 1 页与第 4 页比较</li><li>第 2 页与第 3 页比较</li><li>第 2 页与第 4 页比较</li><li>第 3 页与第 4 页比较</li></ul><p>成对比较越多，就越有可能因随机性而得出错误的结论（见第 112 页“多重检验”）。与其担心页面间所有可能的比较，不如做一个<strong>总体检验</strong>，回答这样的问题：“所有页面的真实‘黏性’是否可能相同，观察到的差异是否仅仅是由于相同会话时间随机分配给四个页面造成的？”</p><p>用来检验这一点的程序就是 ANOVA。它的原理可以用下面的重抽样程序来说明（这里针对网页黏性的 A/B/C/D 测试）：</p><ol type="1"><li>将所有数据合并到一个整体中；</li><li>随机打乱后抽取四个包含 5 个值的重样本；</li><li>记录四个组的均值；</li><li>记录四个组均值之间的方差；</li><li>重复步骤 2–4 多次（比如 1,000 次）；</li></ol><p>重抽样后有多少次方差超过了观察到的方差？这就是 p 值。</p><p>这种置换检验比第 97 页“置换检验”所用的要复杂一些。幸运的是，R 中 <code>lmPerm</code> 包的 <code>aovp</code> 函数可以计算这种情况的置换检验：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">library<span class="punctuation">(</span>lmPerm<span class="punctuation">)</span></span><br><span class="line">summary<span class="punctuation">(</span>aovp<span class="punctuation">(</span>Time <span class="operator">~</span> Page<span class="punctuation">,</span> data<span class="operator">=</span>four_sessions<span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line"><span class="punctuation">[</span><span class="number">1</span><span class="punctuation">]</span> <span class="string">&quot;Settings: unique SS &quot;</span></span><br><span class="line">Component <span class="number">1</span> <span class="operator">:</span></span><br><span class="line">Df R Sum Sq R Mean Sq Iter Pr<span class="punctuation">(</span>Prob<span class="punctuation">)</span></span><br><span class="line">Page <span class="number">3</span> <span class="number">831.4</span> <span class="number">277.13</span> <span class="number">3104</span> <span class="number">0.09278</span> .</span><br><span class="line">Residuals <span class="number">16</span> <span class="number">1618.4</span> <span class="number">101.15</span></span><br><span class="line"><span class="operator">-</span><span class="operator">-</span><span class="operator">-</span></span><br><span class="line">Signif. codes<span class="operator">:</span> <span class="number">0</span> <span class="string">&#x27;***&#x27;</span> <span class="number">0.001</span> <span class="string">&#x27;**&#x27;</span> <span class="number">0.01</span> <span class="string">&#x27;*&#x27;</span> <span class="number">0.05</span> <span class="string">&#x27;.&#x27;</span> <span class="number">0.1</span> <span class="string">&#x27; &#x27;</span> <span class="number">1</span></span><br></pre></td></tr></table></figure><p>这里 <code>Pr(Prob)</code> 给出的 p 值是 0.09278。换句话说，假设底层黏性相同，9.3% 的时候四个页面的响应率可能会像实际观察到的那样不同，仅仅是因为随机性。这种程度的不可能性没有达到传统统计学的 5% 阈值，因此我们得出结论：四个页面之间的差异可能是由随机性造成的。</p><p>列 <code>Iter</code> 列出了置换检验的迭代次数。其他列对应于传统 ANOVA 表，下一节会详细介绍。</p><p>在 Python 中，我们可以用下面的代码计算这种置换检验：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">observed_variance = four_sessions.groupby(<span class="string">&#x27;Page&#x27;</span>).mean().var()[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Observed means:&#x27;</span>, four_sessions.groupby(<span class="string">&#x27;Page&#x27;</span>).mean().values.ravel())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Variance:&#x27;</span>, observed_variance)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">perm_test</span>(<span class="params">df</span>):</span><br><span class="line">    df = df.copy()</span><br><span class="line">    df[<span class="string">&#x27;Time&#x27;</span>] = np.random.permutation(df[<span class="string">&#x27;Time&#x27;</span>].values)</span><br><span class="line">    <span class="keyword">return</span> df.groupby(<span class="string">&#x27;Page&#x27;</span>).mean().var()[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">perm_variance = [perm_test(four_sessions) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3000</span>)]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Pr(Prob)&#x27;</span>, np.mean([var &gt; observed_variance <span class="keyword">for</span> var <span class="keyword">in</span> perm_variance]))</span><br></pre></td></tr></table></figure><h4 id="f-统计量"><strong>F 统计量</strong></h4><p>就像 <strong>t 检验可以用来替代置换检验以比较两个组的均值</strong>一样，<strong>基于 F 统计量也有一个用于方差分析（ANOVA）的统计检验。F 统计量是“组均值之间的方差（the variance across group means）”（即处理效应）与“残差误差的方差（the variance due to residual error）”的比值</strong>。这个比值越大，结果就越显著。如果数据服从正态分布，统计理论规定该统计量应当服从某种特定分布。基于此，我们就可以计算 p 值。</p><p>在 R 中，可以用 <code>aov</code> 函数计算 ANOVA 表：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">&gt;</span> summary<span class="punctuation">(</span>aov<span class="punctuation">(</span>Time <span class="operator">~</span> Page<span class="punctuation">,</span> data<span class="operator">=</span>four_sessions<span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">             Df Sum Sq Mean Sq <span class="built_in">F</span> value Pr<span class="punctuation">(</span><span class="operator">&gt;</span><span class="built_in">F</span><span class="punctuation">)</span>  </span><br><span class="line">Page          <span class="number">3</span>  <span class="number">831.4</span>  <span class="number">277.1</span>   <span class="number">2.74</span>   <span class="number">0.0776</span> .</span><br><span class="line">Residuals    <span class="number">16</span> <span class="number">1618.4</span>  <span class="number">101.2</span></span><br><span class="line"><span class="operator">-</span><span class="operator">-</span><span class="operator">-</span></span><br><span class="line">Signif. codes<span class="operator">:</span> <span class="number">0</span> ‘<span class="operator">*</span><span class="operator">*</span><span class="operator">*</span>’ <span class="number">0.001</span> ‘<span class="operator">*</span><span class="operator">*</span>’ <span class="number">0.01</span> ‘<span class="operator">*</span>’ <span class="number">0.05</span> ‘.’ <span class="number">0.1</span> ‘ ’ <span class="number">1</span></span><br></pre></td></tr></table></figure><p><code>statsmodels</code> 包在 Python 中也提供了 ANOVA 的实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = smf.ols(<span class="string">&#x27;Time ~ Page&#x27;</span>, data=four_sessions).fit()</span><br><span class="line">aov_table = sm.stats.anova_lm(model)</span><br><span class="line">aov_table</span><br></pre></td></tr></table></figure><p>Python 输出与 R 的输出几乎相同。</p><p>Df 表示“自由度（degrees of freedom）”，Sum Sq 表示“平方和（sum of squares）”，Mean Sq 表示“均方（mean squares，均方偏差）”，F value 表示 F 统计量。对于总平均值，平方和是总平均值偏离 0 的平方乘以 20（观测数）。总平均值的自由度按定义是 1。</p><p>对于处理均值，自由度是 3（当三个值确定后，再加上总平均值就固定了，另一个处理均值不能再变）。处理均值的平方和是各处理均值与总平均值之间偏差的平方和。</p><p>对于残差，自由度是 20（所有观测可以变动），平方和是各观测值与处理均值之间差异的平方和。均方（MS）就是平方和除以自由度。F 统计量是 MS(处理)/MS(误差)。F 值因此只依赖这个比率，并可以与标准 F 分布比较，以判断处理均值之间的差异是否超出了随机波动的期望范围。</p><blockquote><p><strong>通用注解</strong></p><p>方差分解**：数据集中每个观测值都可以视为不同组成部分的和。对于数据集中的任意观测值，我们可以将其分解为总平均值、处理效应和残差误差。这就是所谓的“方差分解”：</p><ol type="1"><li>从总平均值开始（网页粘性数据为 173.75）。</li><li>加上处理效应，可能是负的（自变量 = 网页）。</li><li>加上残差误差（residual error,），可能是负的。</li></ol><p>因此，A/B/C/D 测试表左上角值的方差分解如下：</p><ol type="1"><li>从总平均值开始：173.75。</li><li>加上处理（组）效应：–1.75（172 – 173.75）。</li><li>加上残差：–8（164 – 172）。</li><li>得到：164</li></ol></blockquote><h4 id="双因素-anova"><strong>双因素 ANOVA</strong></h4><p>Two-Way ANOVA</p><p>刚才描述的 A/B/C/D 测试是一种“单因素”ANOVA，即只有一个因子（组）在变化。如果有第二个因子——例如“周末 vs 工作日”——并且在每种组合（A 组周末、A 组工作日、B 组周末等）上收集数据，这就是“双因素 ANOVA”。处理方法与单因素 ANOVA 类似，但需要识别“交互效应”。在确定总平均效应和处理效应后，我们把每组的周末和工作日观测分开，计算这些子集平均值与处理平均值的差异。</p><p>可以看出，ANOVA 和双因素 ANOVA 是迈向完整统计建模（如回归和逻辑回归）的第一步，在完整模型中可以同时刻画多个因素及其效应（见第 4 章）。</p><p><strong>关键概念</strong></p><ul><li>ANOVA 是一种分析多个组实验结果的统计方法。</li><li>它是 A/B 测试等类似程序的推广，用于评估组间总体差异是否在随机波动范围内。</li><li>ANOVA 的一个有用结果是识别与组处理、交互效应和误差相关的方差组成部分。</li></ul><h3 id="卡方检验">卡方检验</h3><p>Chi-Square Test</p><p>在网络测试中，人们经常会超越简单的 A/B 测试，同时检验多种处理方案。卡方检验用于计数型数据，以检验其与某个期望分布的拟合程度。在统计实践中，卡方统计量最常见的用途是用于 <span class="math inline">\(r \times c\)</span> 列联表，以评估变量之间独立性的原假设是否合理（参见第 80 页“卡方分布”）。</p><p>卡方检验最早由 Karl Pearson 于 1900 年提出。“chi”这个词来源于 Pearson 在文章中使用的希腊字母 Χ。</p><p><strong>卡方检验关键术语</strong></p><ul><li><p><strong>卡方统计量（Chi-square statistic）</strong> 衡量一组观测数据偏离期望程度的指标。</p></li><li><p><strong>期望值（Expectation or expected）</strong> 在某个假设（通常是原假设）下，我们期望数据呈现的结果。</p></li></ul><blockquote><p><strong>通用注解</strong> <strong><span class="math inline">\(r \times c\)</span></strong> 表示“行 × 列”。例如，一个 <span class="math inline">\(2 \times 3\)</span> 的表格表示两行三列。</p></blockquote><h4 id="卡方检验一种重抽样方法">卡方检验：一种重抽样方法</h4><p>假设你在测试三条不同的标题——A、B 和 C——并分别在 1,000 位访客上运行测试，结果如表 3-4 所示。</p><p><img src="/img3/面向数据科学家的实用统计学/T3.4.png" alt="T3.4" style="zoom:50%;" /></p><p>标题之间显然存在差异。标题 A 的点击率几乎是 B 的两倍。但实际数字很小。我们可以通过重抽样方法来检验点击率的差异是否超出了偶然性造成的范围。对于这个检验，我们需要“期望”的点击分布。在原假设下，假定三条标题的点击率相同，总体点击率为 34/3,000。在这种假设下，我们的列联表如表 3-5 所示。</p><p><img src="/img3/面向数据科学家的实用统计学/T3.5.png" alt="T3.5" style="zoom:50%;" /></p><p><strong>Pearson 残差（Pearson residual）</strong> 定义为：</p><p><span class="math display">\[R = \frac{\text{Observed} - \text{Expected}}{ \sqrt{\text{Expected}}}\]</span></p><p><span class="math inline">\(R\)</span> 衡量实际计数与期望计数的差异程度（见表 3-6）。</p><p><img src="/img3/面向数据科学家的实用统计学/T3.6.png" alt="T3.6" style="zoom:50%;" /></p><p><strong>卡方统计量（Chi-square statistic）</strong> 定义为所有 Pearson 残差平方和：</p><p><span class="math display">\[\chi^2 = \sum_{i}^{r}\sum_{j}^{c}R_{ij}^2\]</span></p><p>其中 <span class="math inline">\(r\)</span> 和 <span class="math inline">\(c\)</span> 分别为行数和列数。本例中的卡方统计量为 1.666。这是否超过了偶然模型中合理出现的范围呢？</p><p>我们可以用以下重抽样算法进行检验：</p><ol type="1"><li>构建一个包含 34 个“1”（点击）和 2,966 个“0”（未点击）的总体。</li><li>打乱顺序，分别抽取三个 1,000 人样本，并统计每个样本的点击数。</li><li>计算抽样计数与期望计数之间的平方差并求和。</li><li>重复步骤 2 和 3，例如 1,000 次。</li><li>观察重抽样平方偏差和超过实际观测值的频率。这就是 p 值。</li></ol><p><code>chisq.test</code> 函数可用于在 R 中计算重抽样的卡方统计量。对于点击数据，卡方检验如下：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">&gt;</span> chisq.test<span class="punctuation">(</span>clicks<span class="punctuation">,</span> simulate.p.value<span class="operator">=</span><span class="literal">TRUE</span><span class="punctuation">)</span></span><br><span class="line">Pearson<span class="string">&#x27;s Chi-squared test with simulated p-value (based on 2000 replicates)</span></span><br><span class="line"><span class="string">data: clicks</span></span><br><span class="line"><span class="string">X-squared = 1.6659, df = NA, p-value = 0.4853</span></span><br></pre></td></tr></table></figure><p>该检验结果表明，这个观测值很可能只是随机性所致。</p><p>在 Python 中运行置换检验</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">box = [<span class="number">1</span>] * <span class="number">34</span></span><br><span class="line">box.extend([<span class="number">0</span>] * <span class="number">2966</span>)</span><br><span class="line">random.shuffle(box)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">chi2</span>(<span class="params">observed, expected</span>):</span><br><span class="line">    pearson_residuals = []</span><br><span class="line">    <span class="keyword">for</span> row, expect <span class="keyword">in</span> <span class="built_in">zip</span>(observed, expected):</span><br><span class="line">        pearson_residuals.append([(observe - expect) ** <span class="number">2</span> / expect</span><br><span class="line">                                 <span class="keyword">for</span> observe <span class="keyword">in</span> row])</span><br><span class="line">    <span class="comment"># 返回平方和</span></span><br><span class="line">    <span class="keyword">return</span> np.<span class="built_in">sum</span>(pearson_residuals)</span><br><span class="line"></span><br><span class="line">expected_clicks = <span class="number">34</span> / <span class="number">3</span></span><br><span class="line">expected_noclicks = <span class="number">1000</span> - expected_clicks</span><br><span class="line">expected = [<span class="number">34</span> / <span class="number">3</span>, <span class="number">1000</span> - <span class="number">34</span> / <span class="number">3</span>]</span><br><span class="line">chi2observed = chi2(clicks.values, expected)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">perm_fun</span>(<span class="params">box</span>):</span><br><span class="line">    sample_clicks = [<span class="built_in">sum</span>(random.sample(box, <span class="number">1000</span>)),</span><br><span class="line">                     <span class="built_in">sum</span>(random.sample(box, <span class="number">1000</span>)),</span><br><span class="line">                     <span class="built_in">sum</span>(random.sample(box, <span class="number">1000</span>))]</span><br><span class="line">    sample_noclicks = [<span class="number">1000</span> - n <span class="keyword">for</span> n <span class="keyword">in</span> sample_clicks]</span><br><span class="line">    <span class="keyword">return</span> chi2([sample_clicks, sample_noclicks], expected)</span><br><span class="line"></span><br><span class="line">perm_chi2 = [perm_fun(box) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2000</span>)]</span><br><span class="line"></span><br><span class="line">resampled_p_value = <span class="built_in">sum</span>(perm_chi2 &gt; chi2observed) / <span class="built_in">len</span>(perm_chi2)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Observed chi2: <span class="subst">&#123;chi2observed:<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Resampled p-value: <span class="subst">&#123;resampled_p_value:<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><h4 id="卡方检验统计理论">卡方检验：统计理论</h4><p>Chi-Square Test: Statistical Theory</p><p>渐近统计理论表明，卡方统计量的分布可以用卡方分布来近似（参见第 80 页“卡方分布”）。合适的标准卡方分布由<strong>自由度</strong>决定（参见第 116 页“自由度”）。对于一个列联表，自由度与行数 <span class="math inline">\(r\)</span> 和列数 <span class="math inline">\(c\)</span> 的关系如下：</p><p><span class="math display">\[\text{degrees of freedom} = (r-1)\times(c-1)\]</span></p><p>卡方分布通常是偏斜的，具有右长尾；见图 3-7（自由度分别为 1、2、5 和 20 的分布）。</p><p><img src="/img3/面向数据科学家的实用统计学/F3.7.png" alt="F3.7" style="zoom:50%;" /></p><p>观测统计量在卡方分布上的位置越靠右，p 值就越小。</p><p><code>chisq.test</code> 函数可使用卡方分布作为参考计算 p 值：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">&gt;</span> chisq.test<span class="punctuation">(</span>clicks<span class="punctuation">,</span> simulate.p.value<span class="operator">=</span><span class="literal">FALSE</span><span class="punctuation">)</span></span><br><span class="line">Pearson<span class="string">&#x27;s Chi-squared test</span></span><br><span class="line"><span class="string">data: clicks</span></span><br><span class="line"><span class="string">X-squared = 1.6659, df = 2, p-value = 0.4348</span></span><br></pre></td></tr></table></figure><p>在 Python 中，使用 <code>scipy.stats.chi2_contingency</code> 函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">chisq, pvalue, df, expected = stats.chi2_contingency(clicks)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Observed chi2: <span class="subst">&#123;chi2observed:<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;p-value: <span class="subst">&#123;pvalue:<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p>这个 p 值比重抽样的 p 值稍小；这是因为卡方分布只是对统计量实际分布的一种近似。</p><h4 id="费舍尔确切检验"><strong>费舍尔确切检验</strong></h4><p>Fisher’s Exact Test</p><p>卡方分布是前面描述的洗牌重抽样检验（shuffled resampling test）的一个很好的近似，但在计数值非常小（个位数，尤其是 5 或更少）时就不再适用。在这种情况下，重抽样程序会给出更精确的 p 值。事实上，大多数统计软件都有一个程序，能够枚举所有可能发生的重新排列（置换），统计它们的频率，并精确地判断观察到的结果有多极端。这种方法被称为 <strong>费舍尔确切检验</strong>，以纪念著名统计学家 R. A. Fisher。R 语言中调用费舍尔确切检验的代码非常简单：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">&gt;</span> fisher.test<span class="punctuation">(</span>clicks<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line">Fisher<span class="string">&#x27;s Exact Test for Count Data</span></span><br><span class="line"><span class="string">data: clicks</span></span><br><span class="line"><span class="string">p-value = 0.4824</span></span><br><span class="line"><span class="string">alternative hypothesis: two.sided</span></span><br></pre></td></tr></table></figure><p>得到的 p 值与使用重抽样方法得到的 0.4853 非常接近。</p><p>在某些情况下，如果部分计数非常小而其他计数很大（例如转化率中的分母很大），则可能需要执行 <strong>洗牌置换检验</strong>(a shuffled permutation test )，而不是完整的确切检验，因为计算所有可能的置换会非常困难。上述 R 函数提供了几个参数来控制是否使用近似方法（<code>simulate.p.value = TRUE 或 FALSE</code>）、迭代次数（<code>B = ...</code>），以及一个计算约束参数（<code>workspace = ...</code>），用于限制精确结果的计算规模。</p><p>Python 中并没有易用的费舍尔确切检验实现。</p><p><strong>科学造假的检测</strong></p><p>一个有趣的案例是塔夫茨大学（Tufts University）的研究员 <strong>Thereza Imanishi-Kari</strong>，她在 1991 年被指控在研究中捏造数据。美国国会议员 <strong>John Dingell</strong> 介入此事，最终导致她的同事、当时任洛克菲勒大学校长的 <strong>David Baltimore</strong> 辞职。</p><p>案件中的一个关键环节涉及其实验数据中数字的预期分布。她的每条观测数据包含多个数字，调查人员将注意力集中在<strong>中间的数字</strong>（忽略首位和末位数字）。按统计规律，这些中间数字应该遵循<strong>均匀随机分布</strong>，即每个数字出现的概率相等（首位数字可能偏向某个值，而末位数字可能受到四舍五入的影响）。表 3-7 给出了实际数据中各中间数字的频数。</p><p><img src="/img3/面向数据科学家的实用统计学/T3.7.png" alt="T3.7" style="zoom:50%;" /></p><p>图 3-8 显示了 315 个数字的分布，看起来明显不像是随机的。调查人员计算了与期望值的偏差（理论上严格均匀分布下每个数字应出现 31.5 次），并使用卡方检验（同样也可以使用重抽样方法）证明实际分布远远超出正常的随机波动范围，表明数据可能被捏造。</p><p>（注：Imanishi-Kari 最终在经历漫长调查后被澄清无罪。）</p><p><img src="/img3/面向数据科学家的实用统计学/F3.8.png" alt="F3.8" style="zoom:33%;" /></p><blockquote><p>个人注：<strong>辩护方的观点：</strong> 伊马尼西-卡里及其支持者（包括大卫·巴尔的摩）坚称，数据中的不一致性是由于实验过程中的疏忽、不规范的记录习惯，以及当时生物学实验固有的可变性所致。他们认为，指控方过于依赖统计学分析，而忽略了生物学实验的复杂性和现实情况。此外，他们还批评了调查程序缺乏正当性，对伊马尼西-卡里不公平</p></blockquote><h4 id="与数据科学的相关性"><strong>与数据科学的相关性</strong></h4><p>Relevance for Data Science</p><p>当你想知道某个效应是否真实存在，还是只是偶然产生时，就会用到 <strong>卡方检验（chi-square test）</strong> 或 <strong>费舍尔确切检验（Fisher’s exact test）</strong>。在大多数经典统计学的应用中，卡方检验的作用是建立<strong>统计显著性</strong>，通常这是研究或实验能够发表前所必须具备的条件。 但对于数据科学家来说，这一点并不那么重要。在大多数数据科学实验中（无论是 A/B 还是 A/B/C……），目标不仅仅是建立统计显著性，而是找到<strong>最佳处理方案</strong>。为此，<strong>多臂老虎机算法</strong>（见第 131 页 “Multi-Arm Bandit Algorithm”）提供了一个更完整的解决方案。</p><p>卡方检验（特别是费舍尔确切检验）在数据科学中的一个应用，是用来确定网络实验的<strong>适当样本量</strong>。这些实验往往点击率极低，即使有成千上万的曝光，计数率可能仍然太小，无法在实验中得出确定结论。在这种情况下，费舍尔确切检验、卡方检验以及其他检验可以作为<strong>功效（power）和样本量计算</strong>（见第 135 页 “Power and Sample Size”）的一部分发挥作用。</p><p><strong>研究人员广泛使用卡方检验，以寻找那个“难以捉摸的统计显著性 p 值”来支撑论文发表。而在数据科学应用中，卡方检验或类似的重抽样模拟更多地被用作筛选工具，用来判断某个效应或特征是否值得进一步研究，而不是作为形式化的显著性检验。</strong></p><p>例如，它们可以用在空间统计和制图中，以判断空间数据是否符合某个指定的零假设分布（比如：犯罪是否集中在某个区域，超出了随机机会所允许的程度？）。它们也可以用在机器学习中的自动化特征选择，评估不同特征上的类别分布，从而识别某些特征在某个类别上出现率异常高或异常低的情况，这种偏差无法用随机波动解释。</p><p><strong>关键要点（Key Ideas）</strong></p><ul><li>统计学中的一个常见做法是检验观测到的数据计数是否与<strong>独立性假设</strong>一致（例如，购买某种商品的倾向是否与性别无关）。</li><li><strong>卡方分布</strong>是一个参照分布（体现了独立性假设），用来与观测数据计算出的卡方统计量进行比较。</li></ul><h3 id="多臂老虎机算法"><strong>多臂老虎机算法</strong></h3><p>Multi-Arm Bandit Algorithm</p><p>多臂老虎机为测试（尤其是网络测试）提供了一种方法，它能比传统的实验设计与统计方法实现<strong>显式优化</strong>和<strong>更快速的决策</strong>。</p><p><strong>多臂老虎机关键术语</strong></p><ul><li><p><strong>多臂老虎机（Multi-arm bandit）</strong> 一个假想的老虎机，顾客可以选择多个拉杆，每个拉杆的收益不同，这里用作多处理（多方案）实验的类比。</p></li><li><p><strong>拉杆（Arm）</strong> 实验中的一种处理（例如：“网页测试中的标题 A”）。</p></li><li><p><strong>中奖（Win）</strong> 在实验中对应老虎机中奖的事件（例如：“顾客点击了链接”）。</p></li></ul><p>传统的 A/B 测试是在实验中按预先设定的设计收集数据，用来回答一个特定的问题，例如：“哪一个更好，处理 A 还是处理 B？” 这种方法默认一旦我们得到问题的答案，实验就结束，然后基于结果采取行动。</p><p>但这种方法存在几个问题：</p><ol type="1"><li><p><strong>结果可能不明确</strong>： “效果未被证明。” 换句话说，实验结果可能暗示有某种效应，但如果真的存在效应，我们的样本量可能不够大，无法按照传统统计标准“证明”它。那么我们该如何决策？</p></li><li><p><strong>想在实验结束前利用已有结果</strong>： 我们可能希望在实验完全结束前就根据已经获得的数据做出决策。</p></li><li><p><strong>想在实验后期调整或改变方案</strong>： 根据实验结束后新出现的数据，我们可能希望更换策略或做其他尝试。</p></li></ol><p>传统的实验与假设检验方法源自 20 世纪 20 年代，相当<strong>不灵活</strong>。而计算机算力和软件的发展使得更强大、更灵活的方法成为可能。此外，数据科学（以及商业领域）并不太在乎统计显著性，而更关注<strong>优化整体努力与结果</strong>。</p><p><strong>老虎机算法（Bandit algorithms）</strong>在网络测试中非常流行，它允许你<strong>同时测试多种处理方案</strong>，并比传统统计设计<strong>更快得出结论</strong>。它的名字来自赌博中的老虎机（slot machine，又称“单臂强盗”one-armed bandit，因为它们“稳定地”从赌徒那里赢钱）。如果你想象一台有多个拉杆的老虎机，每个拉杆的支付率不同，你就得到了一个“多臂老虎机”，也就是该算法的全称。</p><p>你的目标是<strong>赢取尽可能多的钱</strong>，更具体地说，<strong>尽早识别并选定获胜的拉杆</strong>。困难在于你并不知道每个拉杆的总体中奖率——你只知道每次拉动时的个别结果。假设每次中奖的奖金都是一样的（无论拉哪个杆），不同之处在于<strong>中奖的概率</strong>。再假设你最初每个拉杆都尝试 50 次，得到如下结果：</p><ul><li><strong>拉杆 A</strong>：50 次中赢了 10 次</li><li><strong>拉杆 B</strong>：50 次中赢了 2 次</li><li><strong>拉杆 C</strong>：50 次中赢了 4 次</li></ul><p><strong>一种极端做法</strong>是说：“看起来 A 拉杆是赢家——那我们就不再尝试其他拉杆，专注于 A 吧。”这样可以充分利用初始试验的信息。如果 A 真的更好，我们可以早早享受到这一优势。但另一方面，如果 B 或 C 实际上更好，我们就失去了发现它们的机会。</p><p><strong>另一种极端做法</strong>是说：“这些结果都可能是偶然的——那就继续平等地拉所有拉杆吧。”这样可以最大限度地给 A 之外的备选方案展示自己的机会。然而，在此过程中，我们也在持续投放看起来较差的处理。那我们要容忍多久？</p><p><strong>多臂老虎机算法采取折中方法</strong>： 我们开始更频繁地拉 A，以利用它的明显优势，但并不放弃 B 和 C，只是减少拉它们的次数。如果 A 持续优于其他，我们就继续把资源（拉动次数）从 B、C 转移到 A；如果相反，C 开始表现更好而 A 变差，我们可以把拉动次数从 A 转回 C。如果其中某个在初始试验中因偶然性被“埋没”的处理其实比 A 优秀，那么通过后续测试它就有机会显现出来。</p><p>现在把这个思路应用到<strong>网络测试</strong>： 不再是多个老虎机拉杆，而是多个优惠、标题、颜色等在网站上进行测试。顾客要么点击（商家“中奖”），要么不点击。最初，所有方案都是随机且均等展示。然而，如果某个方案开始优于其他方案，它就可以被更频繁地展示（“拉”）。但是，修改展示比例的算法参数应该怎么设定？应该改成什么比例？什么时候改？</p><p><strong>一个简单的算法：A/B 测试中的 ε-贪婪算法（epsilon-greedy）</strong></p><ol type="1"><li><p>生成一个 0 到 1 之间均匀分布的随机数。</p></li><li><p>如果这个数在 0 和 ε 之间（ε 是 0 到 1 之间的数，通常比较小），掷一枚公平的硬币（50/50 概率）：</p><ul><li>如果正面，展示方案 A；</li><li>如果反面，展示方案 B。</li></ul></li><li><p>如果这个数 ≥ ε，就展示目前响应率最高的方案。</p></li></ol><p>ε 是唯一控制这个算法的参数：</p><ul><li>如果 ε = 1，我们得到的是标准的简单 A/B 实验（每个用户在 A 和 B 间随机分配）。</li><li>如果 ε = 0，我们得到的是<strong>纯贪婪算法</strong>——始终选择当前表现最好的方案（局部最优），不再继续探索，仅把用户（网站访问者）分配到当前最佳处理。</li></ul><p><strong>更高级的算法：Thompson 采样（Thompson’s sampling）</strong></p><p>这个方法在每一步“抽样”（拉动拉杆），以最大化选择最佳拉杆的概率。当然你并不知道哪一个是最好的——这正是问题所在！—— 但随着每次抽样后你观察到收益，你获得越来越多的信息。Thompson 采样采用<strong>贝叶斯方法</strong>：最初假设奖励的先验分布（使用 Beta 分布，这是在贝叶斯问题中指定先验信息的常见机制）。随着每次抽样积累信息，这些信息被更新，从而使下一次抽样在选择正确拉杆时更优化。</p><p>多臂老虎机算法可以高效地处理 <strong>3 个以上处理方案</strong>，并趋向于最优的“最佳”选择。而在传统统计检验程序中，处理 3 个以上方案的决策复杂度远远超过传统 A/B 测试，这使得多臂老虎机算法的优势更大。</p><p><strong>关键要点</strong></p><ul><li>传统的 A/B 测试设想的是随机抽样过程，这可能导致对劣质处理的过多暴露。</li><li>多臂老虎机算法则相反，它在实验过程中利用获得的信息，动态调整抽样过程，减少对劣质处理的分配频率。</li><li>它们还能高效处理多于两种处理方案的情况。</li><li>有多种算法可以将抽样概率从较差处理方案转向（假定的）较优方案。</li></ul><h3 id="检验效能与样本量"><strong>检验效能与样本量</strong></h3><p>Power and Sample Size</p><p>如果你要运行一次网页测试，如何决定测试要运行多久（即每个处理需要多少展示次数）？尽管许多网页测试指南都有所提及，但实际上没有什么普遍适用的“标准答案”——主要取决于你所期望达成的目标出现的频率。</p><p><strong>检验效能与样本量的关键术语</strong></p><ul><li><p><strong>效应量（Effect size）</strong> 你希望在统计检验中能够检测到的最小效应，例如“点击率提高 20%”。</p></li><li><p><strong>检验效能（Power）</strong> 在给定样本量下检测到指定效应量的概率。</p></li><li><p><strong>显著性水平（Significance level）</strong> 检验所采用的统计显著性水平。</p></li></ul><p>在进行样本量的统计计算时，其中一步要问：“假设检验能否真正揭示处理 A 和 B 之间的差异？”假设检验的结果（即 p 值）取决于处理 A 和 B 之间真实的差异，也取决于抽样的运气——即实验中谁被分配到哪个组。但直觉上，A 和 B 之间真实差异越大，我们越有可能在实验中发现它；差异越小，就需要更多数据来检测它。比如，要区分棒球打击率 0.350 和 0.200 的打者，不需要太多打席就可以；而要区分 0.300 和 0.280 的打者，就需要更多打席。</p><p><strong>检验效能</strong>是指在给定样本特征（样本量与变异度）下，检测到指定效应量的概率。例如，我们可以（假设性地）说：在 25 次打席中区分打击率 0.330 和 0.200 的打者的概率是 0.75。这里的效应量是 0.130，而“检测到”意味着假设检验能够拒绝“无差异”的原假设，并得出“存在真实效应”的结论。所以这个包含两位打者、每人 25 次打席（n=25）、效应量 0.130 的实验，其（假设的）检验效能为 0.75，即 75%。</p><p>你可以看到这里有好几个变量，而且很容易在众多统计假设和公式中打结（需要指定样本变异、效应量、样本量、假设检验的 α 水平等，才能计算效能）。实际上，有专门的统计软件可以用来计算效能。大多数数据科学家并不需要完全按照发表论文那样严格地报告检验效能。不过，他们可能会遇到这样一种情况：要为一次 A/B 测试收集一些数据，而收集或处理数据是有成本的。这时，大致知道需要多少数据有助于避免“辛辛苦苦收集数据却得到无结论结果”的情况。下面是一种比较直观的替代方法：</p><ol type="1"><li>用一些假设数据作为最初的“最佳猜测”，代表预期的实验结果（或许基于以往数据）——例如，用一个包含 20 个“1”和 80 个“0”的盒子来代表打击率 0.200 的打者，或用一个盒子存放一些“在网站停留时间”的观测值。</li><li>通过在第一个样本的基础上加上期望的效应量来创建第二个样本——例如，第二个盒子中包含 33 个“1”和 67 个“0”，或者在每条最初“网站停留时间”上加 25 秒。</li><li>从每个盒子中各抽取一个大小为 n 的自助（bootstrap）样本。</li><li>对这两个自助样本进行一次置换检验（或基于公式的假设检验），并记录它们之间的差异是否具有统计显著性。</li><li>多次重复前两步，并计算差异显著的比例——这就是估计的检验效能。</li></ol><h4 id="样本量">样本量</h4><p>Sample Size</p><p>检验效能计算最常见的用途就是<strong>估算需要多大的样本量</strong>。</p><p>例如，假设你在观察点击率（点击次数占展示次数的百分比），并测试一个新广告与现有广告。那在研究中你需要积累多少点击数呢？ 如果你只对巨大的差异（比如 50% 的差异）感兴趣，相对较小的样本量可能就足够了；反之，如果你连很小的差异也想检测出来，那么就需要大得多的样本量。一种常见做法是先制定一个政策：新广告必须比现有广告好一定百分比，比如 10%；否则现有广告继续使用。<strong>这个目标——即“效应量”——就决定了所需的样本量。</strong></p><p>例如，假设当前点击率约为 1.1%，你希望获得 10% 的提升，即 1.21%。那么我们有两个“盒子”：盒子 A 含有 1.1% 的“1”（比如 110 个“1”和 9,890 个“0”），盒子 B 含有 1.21% 的“1”（比如 121 个“1”和 9,879 个“0”）。起步时，我们从每个盒子各抽取 300 次（就像每个广告 300 次展示）。 假设第一次抽取得到如下结果：</p><ul><li>盒子 A：3 个“1”</li><li>盒子 B：5 个“1”</li></ul><p>显然，任何假设检验都能看出这种差异（5 比 3）完全在随机波动范围之内。这个样本量（每组 n=300）和效应量（10% 差异）的组合太小，无法让任何假设检验可靠地显示差异。</p><p>所以我们可以尝试增加样本量（比如 2,000 次展示），并要求更大的提升（50% 而不是 10%）。 例如，假设当前点击率仍为 1.1%，但我们现在寻求 50% 的提升至 1.65%。于是两个盒子：盒子 A 仍为 1.1% 的“1”（比如 110 个“1”和 9,890 个“0”），盒子 B 为 1.65% 的“1”（比如 165 个“1”和 9,868 个“0”）。 我们从每个盒子各抽取 2,000 次。假设第一次抽取得到：</p><ul><li>盒子 A：19 个“1”</li><li>盒子 B：34 个“1”</li></ul><p>对这个差异（34–19）进行显著性检验，仍显示为“非显著”（但比之前的 5–3 更接近显著）。要计算检验效能，我们需要多次重复前面的步骤，或使用可以计算效能的统计软件，但我们初步抽取就已经提示：即便是检测 50% 的提升，也需要几千次广告展示。</p><p><strong>总结</strong>：在计算检验效能或所需样本量时，有四个“活动部件”：</p><ul><li>样本量</li><li>希望检测的效应量</li><li>检验的显著性水平（α）</li><li>检验效能（Power）</li></ul><p>指定其中三个，就可以计算出第四个。最常见的需求是计算<strong>样本量</strong>，因此必须指定其他三个条件。 在 R 和 Python 中，你还需要指定备择假设是“greater”还是“larger”以得到单尾检验；关于单尾与双尾检验的讨论见本书第 95 页“一尾与双尾假设检验”。</p><p>下面是一个 R 代码示例，计算两个比例的检验，其中两个样本大小相同（使用 pwr 包）：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">effect_size <span class="operator">=</span> ES.h<span class="punctuation">(</span>p1<span class="operator">=</span><span class="number">0.0121</span><span class="punctuation">,</span> p2<span class="operator">=</span><span class="number">0.011</span><span class="punctuation">)</span></span><br><span class="line">pwr.2p.test<span class="punctuation">(</span>h<span class="operator">=</span>effect_size<span class="punctuation">,</span> sig.level<span class="operator">=</span><span class="number">0.05</span><span class="punctuation">,</span> power<span class="operator">=</span><span class="number">0.8</span><span class="punctuation">,</span> alternative<span class="operator">=</span><span class="string">&#x27;greater&#x27;</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Difference of proportion power calculation for binomial distribution (arcsine transformation)</span><br><span class="line">h = 0.01029785</span><br><span class="line">n = 116601.7</span><br><span class="line">sig.level = 0.05</span><br><span class="line">alternative = greater</span><br><span class="line">NOTE: same sample sizes</span><br></pre></td></tr></table></figure><p>函数 <code>ES.h</code> 用来计算效应量。我们看到，如果我们要 80% 的检验效能，需要的样本量接近 120,000 次展示。如果我们寻求的是 50% 的提升（p1=0.0165），所需样本量则减少到 5,500 次展示。</p><p><code>statsmodels</code> 包中包含若干效能计算方法。这里用 <code>proportion_effectsize</code> 计算效应量，用 <code>TTestIndPower</code> 求样本量：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">effect_size = sm.stats.proportion_effectsize(<span class="number">0.0121</span>, <span class="number">0.011</span>)</span><br><span class="line">analysis = sm.stats.TTestIndPower()</span><br><span class="line">result = analysis.solve_power(effect_size=effect_size,</span><br><span class="line">                              alpha=<span class="number">0.05</span>, power=<span class="number">0.8</span>, alternative=<span class="string">&#x27;larger&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Sample Size: %.3f&#x27;</span> % result)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Sample Size: 116602.393</span><br></pre></td></tr></table></figure><p><strong>关键要点</strong></p><ul><li>确定所需样本量时，要提前考虑将要进行的统计检验。</li><li>必须指定你希望检测的最小效应量。</li><li>必须指定检测该效应量的概率（检验效能）。</li><li>最后，必须指定检验的显著性水平（α）。</li></ul><h3 id="总结">总结</h3><p>实验设计的原则——将受试者随机分配到接受不同处理的两个或多个组——使我们能够对处理效果得出有效的结论。最好包含一个“无变化”的对照处理。<strong>正式统计推断这一主题——假设检验、p 值、t 检验及更多类似内容——在传统统计课程或教材中占据大量时间和篇幅，但从数据科学的角度来看，这种形式化在多数情况下并非必需。然而，认识到随机变动在误导人类大脑方面可能发挥的作用仍然很重要。直观的重抽样程序（置换和自助法）让数据科学家能够评估随机变动在其数据分析中可能起到的作用程度。</strong></p>]]></content>
      
      
      <categories>
          
          <category> 翻译 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> AI </tag>
            
            <tag> 统计 </tag>
            
            <tag> 数据科学 </tag>
            
            <tag> R </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第2章 数据与抽样分布</title>
      <link href="/2025/09/25/%E7%AC%AC2%E7%AB%A0%20%E6%95%B0%E6%8D%AE%E4%B8%8E%E6%8A%BD%E6%A0%B7%E5%88%86%E5%B8%83/"/>
      <url>/2025/09/25/%E7%AC%AC2%E7%AB%A0%20%E6%95%B0%E6%8D%AE%E4%B8%8E%E6%8A%BD%E6%A0%B7%E5%88%86%E5%B8%83/</url>
      
        <content type="html"><![CDATA[<p><a href="/img3/面向数据科学家的实用统计学/Practical-Statistics-for-Data-Scientists.pdf">《Practical Statistics for Data Scientists》书籍英文版</a><br /><a href="/img3/面向数据科学家的实用统计学/面向数据科学家的实用统计学.pdf">《面向数据科学家的实用统计学》中文版书籍</a></p><h2 id="第-2-章-数据与抽样分布">第 2 章 数据与抽样分布</h2><p>人们常见的一个误解是，大数据时代意味着不再需要抽样。事实上，数据在质量和相关性上呈现爆炸式增长，反而强化了抽样作为高效处理各种数据并最小化偏差的工具的重要性。即使在大数据项目中，预测模型通常也是用样本开发和试运行的。样本还用于各种测试（例如，比较不同网页设计对点击率的影响）。</p><p><img src="/img3/面向数据科学家的实用统计学/F2.1.png" alt="F2.1" style="zoom:33%;" /></p><p>图 2-1 展示了支撑本章所讨论概念——数据与抽样分布——的示意图。左侧表示总体，在统计学中假定总体遵循某个潜在但未知的分布。我们唯一能获取的是右侧所示的样本数据及其经验分布。要从左侧到达右侧，需要一个抽样过程（由箭头表示）。<strong>传统统计学非常注重左侧，依赖于对总体作出强假设的理论。现代统计学则更多地转向右侧，不再需要这些假设。</strong></p><p><strong>总体而言，数据科学家不必担心左侧的理论性质，而应关注抽样过程和手头的数据。不过也有一些显著例外。有时数据源自可以建模的物理过程。最简单的例子是掷硬币：它服从二项分布。任何现实中的二项情境（购买或不购买、欺诈或非欺诈、点击或不点击）都可以有效地用一枚硬币来建模（当然，硬币正面出现的概率可调整）。在这些情况下，我们可以通过理解总体获得更多洞见。</strong></p><span id="more"></span><h3 id="随机抽样与样本偏差">随机抽样与样本偏差</h3><p>样本是来自更大数据集的一个子集；统计学家把这个更大的数据集称为总体。统计学中的总体并不等同于生物学中的“种群”——它是一个庞大、明确定义的（有时是理论的或假想的）数据集合。</p><p>随机抽样是一个过程，其中总体中每一个可供抽取的成员在每次抽取时都有相等的机会被选入样本。由此得到的样本称为<strong>简单随机样本</strong>。抽样可以<strong>有放回</strong>进行，即每次抽取后将观测值放回总体中以便未来可能再次被选中；也可以<strong>无放回</strong>进行，即一旦被抽取就不再返回总体，无法再次抽取。</p><p><strong>在基于样本进行估计或建模时，数据质量往往比数据数量更重要。</strong>数据科学中的数据质量包括完整性、格式一致性、整洁性和单个数据点的准确性。统计学还增加了“代表性”的概念。</p><p><strong>随机抽样关键术语</strong></p><ul><li><p><strong>样本（Sample）</strong> 从更大数据集中抽取的子集。</p></li><li><p><strong>总体（Population）</strong> 更大的数据集或数据集的概念。</p></li><li><p><strong>N（n）</strong> 总体（样本）的大小。</p></li><li><p><strong>随机抽样（Random sampling）</strong> 随机将总体元素抽取进样本。</p></li><li><p><strong>分层抽样（Stratified sampling）</strong> 将总体划分为若干层（strata），并从每一层随机抽样。</p></li><li><p><strong>层（Stratum，复数 strata）</strong> 总体中具有共同特征的同质子群。</p></li><li><p><strong>简单随机样本（Simple random sample）</strong> 直接从总体随机抽取、不分层而得到的样本。</p></li><li><p><strong>偏差（Bias）</strong> 系统性误差。</p></li><li><p><strong>样本偏差（Sample bias）</strong> 样本对总体的错误代表或失真。</p></li></ul><p>最经典的例子是 1936 年《文学文摘》（Literary Digest）的民意调查，该调查预测阿尔夫·兰登（Alf Landon）将战胜富兰克林·罗斯福（Franklin Roosevelt）。《文学文摘》当时是一份颇有影响力的期刊，它调查了其全部订阅者外加其他名单上的个人，总计超过 1000 万人，预测兰登将以压倒性优势获胜。而盖洛普民意调查（Gallup Poll）的创始人乔治·盖洛普（George Gallup）每两周仅对 2000 人进行调查，却准确预测了罗斯福的胜利。区别在于受访对象的选择方式。</p><p>《文学文摘》追求数量，却几乎不关注抽样方法。他们最终调查的对象多为社会经济地位相对较高的人群（他们自己的订阅者，加上那些因拥有电话和汽车等奢侈品而出现在营销名单上的人）。结果造成了<strong>样本偏差</strong>：样本在某些有意义且非随机的方面与它本应代表的总体不同。这里“非随机”一词很重要——几乎没有任何样本（包括随机样本）能完全代表总体。样本偏差发生在这种差异具有意义时，并且可以预期在用同样方法抽取的其他样本中继续存在。</p><blockquote><p><strong>通用注解：</strong></p><p>自我选择抽样偏差（Self-Selection Sampling Bias）：<strong>你在 Yelp 等社交媒体网站上看到的餐馆、酒店、咖啡馆等评论容易产生偏差，因为提交评论的人并不是随机选择的；相反，是他们自己主动撰写的。这就导致了</strong>自我选择偏差**：有动机写评论的人可能经历过糟糕的体验，可能与该商家有某种关系，或者本身与不写评论的人群在特质上不同。需要注意的是，尽管自我选择样本作为反映真实情况的指标可能不可靠，但在比较一家店与另一家类似店时可能更可靠——因为同样的自我选择偏差可能同时适用于二者。</p></blockquote><h4 id="偏差">偏差</h4><p>统计偏差是指由测量或抽样过程系统性地产生的测量或抽样误差。要区分随机机会造成的误差与偏差造成的误差。想象一个物理过程：一把枪射击靶子。它不会每次都击中靶心，甚至大多数时候都不会。一个无偏的过程也会产生误差，但这些误差是随机的，不会明显偏向某个方向（见图 2-2）。图 2-3 展示了一个有偏过程——在 x 和 y 方向仍存在随机误差，但还存在偏差：子弹往往落在右上象限。</p><p><img src="/img3/面向数据科学家的实用统计学/F2.2.png" alt="F2.2" style="zoom:50%;" /></p><p>偏差有不同的形式，可能是可观察的，也可能是不可见的。当结果显示出偏差迹象时（例如，通过与基准或实际值对比），通常意味着统计或机器学习模型被错误设定，或有重要变量被遗漏。</p><h4 id="随机选择">随机选择</h4><p>为了避免导致《文学文摘》预测兰登获胜的样本偏差，乔治·盖洛普（见图 2-4）采用了更科学的抽样方法，以获得代表美国选民的样本。如今已有多种方法实现代表性，但其核心都是<strong>随机抽样</strong>。</p><p><img src="/img3/面向数据科学家的实用统计学/F2.4.png" alt="F2.4" style="zoom:50%;" /></p><p>随机抽样并非总是容易。正确定义可接触的总体是关键。假设我们想生成客户的代表性画像并需要进行一次试点客户调查。调查需要具有代表性，但工作量很大。</p><p>首先，我们需要定义谁是客户。我们可能选择所有购买金额 &gt; 0 的客户记录。是否包含所有过去客户？是否包含退款？内部测试购买？经销商？既包括结算代理人又包括客户？</p><p>接下来，我们需要指定抽样程序。可能的做法是“随机选择 100 个客户”。当涉及从数据流中抽样（例如实时客户交易或网页访问者）时，时间因素可能很重要（例如，工作日早上 10 点的网页访问者可能与周末晚上 10 点的访问者不同）。</p><p>在<strong>分层抽样</strong>中，总体被划分为若干层，从每一层随机抽取样本。政治民调人员可能希望了解白人、黑人和西班牙裔的选举偏好。从总体中简单随机抽样得到的黑人和西班牙裔人数可能过少，因此可以在分层抽样中对这些层进行<strong>加权抽样</strong>，以获得等量的样本。</p><h4 id="规模与质量什么时候规模才重要">规模与质量：什么时候“规模”才重要？</h4><p>在大数据时代，有时候“小”反而更好。花时间和精力进行随机抽样不仅能减少偏差，还能让我们有更多精力关注数据探索和数据质量。例如，缺失数据和异常值可能包含有用信息。若要在几百万条记录中追踪缺失值或检查异常值，成本可能高得无法承受；但如果是在几千条记录的样本中，这样做或许可行。如果数据量过大，数据绘图和人工检查都会变得缓慢。</p><p>那么，什么时候需要海量数据？</p><p>经典的“大数据”价值场景，是数据不仅庞大，而且<strong>稀疏</strong>。想想谷歌收到的搜索查询：列是词语，行是单个搜索查询，单元格值为 0 或 1（取决于该查询是否包含某个词）。目标是为某个查询确定最佳预测的搜索结果。英语单词超过 15 万个，谷歌每年处理超过一万亿次查询，这形成了一张巨大的矩阵，其中绝大多数条目都是“0”。</p><p>这是一个真正的大数据问题——只有在积累了如此庞大的数据量后，才能为大多数查询返回有效搜索结果。而且数据越多，结果越好。对于热门搜索词，这不是大问题——对某些极受关注的热门话题，有效数据很快就能找到。现代搜索技术真正的价值在于：它能为<strong>海量多样</strong>的搜索查询返回详细而有用的结果，包括那些出现频率只有百万分之一的查询。</p><p>比如搜索短语 “Ricky Ricardo and Little Red Riding Hood”。在互联网早期，这个查询可能只会返回关于乐队领队 Ricky Ricardo、他出演的电视节目《我爱露西（I Love Lucy）》以及儿童故事《小红帽》的结果。前两个主题各自都有很多搜索，但这两个关键词的组合却几乎没人搜过。而如今，随着数万亿条搜索查询的积累，这个查询能直接返回《我爱露西》里 Ricky 以英语和西班牙语混合的滑稽方式给婴儿讲述《小红帽》的那一集。</p><p>要知道，真正相关的记录（即包含这个确切搜索查询或非常相似的查询，并附带用户最终点击了哪个链接的信息）可能只需几千条就足以产生有效结果。然而，要得到这些相关记录，需要采集数万亿个数据点（当然，随机抽样在这种场景下毫无帮助）。另见第 73 页“长尾分布”。</p><h4 id="样本均值与总体均值">样本均值与总体均值</h4><p>符号 <span class="math inline">\(\bar{x}\)</span>（读作“x-bar”）用于表示某总体样本的均值(the mean of a sample from a population)，而 <span class="math inline">\(\mu\)</span> 则用于表示总体的均值。为什么要区分？因为样本信息是<strong>观察到的</strong>，而关于庞大总体的信息往往是通过较小样本<strong>推断</strong>的。统计学家喜欢在符号上把两者分开。</p><p><strong>关键思想</strong></p><ul><li>即便在大数据时代，<strong>随机抽样</strong>仍然是数据科学家的重要武器之一。</li><li><strong>偏差</strong>是当测量或观察不具有总体代表性时所导致的系统性误差。</li><li><strong>数据质量</strong>往往比数据数量更重要；随机抽样可以减少偏差，并促进质量改进，否则这种改进的成本可能高得难以承担。</li></ul><h3 id="选择偏差">选择偏差</h3><p>用棒球运动员尤吉·贝拉（Yogi Berra）的话改写一下：如果你不知道自己在找什么，只要找得够久，总能找到点东西。</p><p><strong>选择偏差</strong>是指有意识或无意识地以某种方式选择数据，从而得出具有误导性或短暂性的结论。</p><p><strong>选择偏差相关术语</strong></p><ul><li><p><strong>选择偏差（Selection bias）</strong> 由于观测样本的选择方式所引入的偏差。</p></li><li><p><strong>数据探查（Data snooping）</strong> 在数据中大范围“狩猎”，寻找某些有趣现象的过程。</p></li><li><p><strong>大范围搜索效应（Vast search effect）</strong> 由于反复建立模型或在含大量自变量的数据上建模而导致的偏差或不可重复性。</p></li></ul><p>如果你事先提出假设并设计良好的实验来验证它，你就可以对结论有较高的信心。然而，现实中往往并非如此。通常，人们会先看现有的数据，再试图找出模式。但这些模式是真的？还是只是<strong>数据探查</strong>（在数据里反复“挖掘”，直到出现某些有趣的东西）的产物？<strong>统计学家有句玩笑话：“如果你足够折磨数据，数据迟早会招供。”</strong></p><p>实验验证的现象与在现有数据中发现的现象之间的区别，可以通过下面这个思想实验来说明：</p><p>设想有人告诉你，他能连续掷 10 次硬币都正面朝上。你当场挑战他（相当于做了一个实验），结果他真的连续 10 次正面朝上。显然你会认为他有某种特殊能力——因为连续 10 次掷硬币都是正面的概率仅为 1/1,000。</p><p>现在再设想一个体育场的播音员让在场的 20,000 名观众各自掷 10 次硬币，并在连续 10 次正面朝上的时候向引座员报告。某个人得到连续 10 次正面的概率极高（超过 99%，即 1 减去“没有任何人连续 10 次正面”的概率）。显然，<strong>事后</strong>选择体育场里那个（或那些）掷出 10 次正面的人，并不能说明他们有特殊能力——他们大概率只是运气好。</p><p>因为在数据科学中反复审视大型数据集本身就是重要价值之一，<strong>选择偏差</strong>是值得担心的问题。数据科学家尤其要注意一种特殊的选择偏差，即约翰·埃尔德（Elder Research 创始人，一家知名数据挖掘咨询公司）所说的“大范围搜索效应”。如果你在一个大型数据集上反复跑不同的模型、问不同的问题，你总能找到些“有趣的”东西。但这个结果是真正有意义的吗，还是偶然的离群值？</p><p>我们可以通过<strong>留出集（holdout set）</strong>来验证模型性能（有时甚至要使用多个留出集），以防止这种情况。埃尔德还提倡使用他称之为“<strong>目标打乱（target shuffling）</strong>”的方法（本质上是一种置换检验(a permutation test)），来测试数据挖掘模型所提示的预测关系是否有效。</p><p>在统计学中，除了“大范围搜索效应”之外，常见的选择偏差形式还包括：</p><ul><li>非随机抽样（参见第 48 页“随机抽样与样本偏差”）</li><li><strong>挑拣数据（cherry-picking）</strong></li><li>选择能够突出某种统计效应的时间区间</li><li>当结果看起来“有趣”时就提前停止实验</li></ul><h4 id="均值回归"><strong>均值回归</strong></h4><p>均值回归指的是对同一个变量进行连续测量时出现的一种现象：极端的观测值往往会被更接近中心的值所跟随。对极端值赋予特殊关注和意义可能会导致一种选择偏差。</p><p>体育迷们熟悉“年度最佳新秀，次年低潮”这一现象。在某一赛季开始职业生涯的运动员（新秀群体）中，总有一位表现优于所有其他人。通常，这位“年度最佳新秀”在第二年表现不如第一年。为什么？</p><p>几乎在所有主要的球类或冰球类运动中，总体表现都由两个因素共同作用：</p><ul><li>技能</li><li>运气</li></ul><p>均值回归是某种形式的选择偏差的结果。当我们选出表现最好的新秀时，技能和好运可能都在起作用。在他的下一赛季中，技能依然存在，但好运往往不再出现，所以他的表现会下降——即“回归”了。这个现象最早由弗朗西斯·高尔顿（Francis Galton）在 1886 年提出 [Galton-1886]，他在遗传倾向的研究中写到这一现象；例如，极高个男性的子女往往没有父亲那么高（见图 2-5）。</p><p><img src="/img3/面向数据科学家的实用统计学/F2.5.png" alt="F2.5" style="zoom:50%;" /></p><p>“均值回归”意为“回到（平均值）”，它与统计建模方法中的<strong>线性回归</strong>是不同的，后者指的是估计预测变量与结果变量之间的线性关系。</p><p><strong>关键思想</strong></p><ul><li>在制定假设后再按随机化和随机抽样原则收集数据，可以避免偏差。</li><li>所有其他形式的数据分析都可能因数据收集/分析过程而产生偏差（例如在数据挖掘中重复运行模型、研究中的数据探查、以及事后选择有趣事件）。</li></ul><h3 id="统计量的抽样分布"><strong>统计量的抽样分布</strong></h3><p><strong>“统计量的抽样分布”指的是从同一总体中抽取大量样本后，某个样本统计量的分布。传统统计学的许多内容都关注如何从（小）样本推断到（非常大）总体。</strong></p><p><strong>抽样分布的关键术语</strong></p><ul><li><p><strong>样本统计量（Sample statistic）</strong> 从更大总体中抽取的样本数据计算得出的度量。</p></li><li><p><strong>数据分布（Data distribution）</strong> 数据集中单个值的频率分布。</p></li><li><p><strong>抽样分布（Sampling distribution）</strong> 某个样本统计量在许多样本或重采样中的频率分布。</p></li><li><p><strong>中心极限定理（Central limit theorem）</strong> 随着样本量增加，<strong><em><u>抽样分布</u></em></strong>趋于正态分布的趋势。</p></li><li><p><strong>标准误（Standard error）</strong> 某个样本统计量在许多样本中的变异性（标准差），不要与“标准差”混淆，后者指的是单个数据值的变异性。</p></li></ul><p>通常，我们抽取样本的目的是测量某些东西（用样本统计量）或建立模型（用统计或机器学习模型）。由于我们的估计或模型是基于样本的，它可能存在误差；如果我们抽取不同的样本，结果可能会不同。因此，我们关心这种差异的大小——一个关键问题就是<strong>抽样变异性</strong>（sampling variability）。如果我们有大量数据，就可以抽取更多样本并直接观察某个样本统计量的分布。</p><p>通常情况下，如果有大量的数据，那么我们可以从中抽取更多的样本，进而直接观察样本统计量的分布情况。只要数据易于获取，那么我们一般会使用尽可能多的数据去计算估计量或拟合模型，而非总是使用从总体中抽取更多样本的方法。</p><blockquote><p><strong>警告：</strong></p><p>区分<strong>单个数据点的分布</strong>（称为数据分布 data distribution）和<strong>样本统计量的分布</strong>（称为抽样分布 sampling distribution）非常重要。</p></blockquote><p>通常，样本统计量（如均值等）的分布要比数据本身的分布更加规则，分布的形状更趋向于正态分布的钟形曲线。统计所基于的样本规模越大，上面的观点就愈发成立。此外，样本的规模越大，样本统计量的分布就越窄。</p><p>这一点可以通过一个例子说明：使用 LendingClub 贷款申请人的年度收入数据（有关数据说明见第 239 页“小例子：预测贷款违约”）。从这些数据中抽取三个样本：一个包含 1,000 个值的样本，一个包含 1,000 个由 5 个值求得均值的样本，以及一个包含 1,000 个由 20 个值求得均值的样本。然后绘制每个样本的直方图，就得到了图 2-6。</p><p><img src="/img3/面向数据科学家的实用统计学/F2.6.png" alt="F2.6" style="zoom:50%;" /></p><p>单个数据值的直方图分布得很宽，并且偏向较高的数值，这正是收入数据的常见特征。5 个值和 20 个值的均值的直方图则越来越集中，并且更呈现钟形分布。</p><p>下面是使用可视化包 <strong>ggplot2</strong> 生成这些直方图的 R 代码：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">library<span class="punctuation">(</span>ggplot2<span class="punctuation">)</span></span><br><span class="line"><span class="comment"># 抽取一个简单随机样本</span></span><br><span class="line">samp_data <span class="operator">&lt;-</span> data.frame<span class="punctuation">(</span>income<span class="operator">=</span>sample<span class="punctuation">(</span>loans_income<span class="punctuation">,</span> <span class="number">1000</span><span class="punctuation">)</span><span class="punctuation">,</span></span><br><span class="line">type<span class="operator">=</span><span class="string">&#x27;data_dist&#x27;</span><span class="punctuation">)</span></span><br><span class="line"><span class="comment"># 抽取5个值的均值样本</span></span><br><span class="line">samp_mean_05 <span class="operator">&lt;-</span> data.frame<span class="punctuation">(</span></span><br><span class="line">income <span class="operator">=</span> tapply<span class="punctuation">(</span>sample<span class="punctuation">(</span>loans_income<span class="punctuation">,</span> <span class="number">1000</span><span class="operator">*</span><span class="number">5</span><span class="punctuation">)</span><span class="punctuation">,</span></span><br><span class="line"><span class="built_in">rep</span><span class="punctuation">(</span><span class="number">1</span><span class="operator">:</span><span class="number">1000</span><span class="punctuation">,</span> <span class="built_in">rep</span><span class="punctuation">(</span><span class="number">5</span><span class="punctuation">,</span> <span class="number">1000</span><span class="punctuation">)</span><span class="punctuation">)</span><span class="punctuation">,</span> FUN<span class="operator">=</span>mean<span class="punctuation">)</span><span class="punctuation">,</span></span><br><span class="line">type <span class="operator">=</span> <span class="string">&#x27;mean_of_5&#x27;</span><span class="punctuation">)</span></span><br><span class="line"><span class="comment"># 抽取20个值的均值样本</span></span><br><span class="line">samp_mean_20 <span class="operator">&lt;-</span> data.frame<span class="punctuation">(</span></span><br><span class="line">income <span class="operator">=</span> tapply<span class="punctuation">(</span>sample<span class="punctuation">(</span>loans_income<span class="punctuation">,</span> <span class="number">1000</span><span class="operator">*</span><span class="number">20</span><span class="punctuation">)</span><span class="punctuation">,</span></span><br><span class="line"><span class="built_in">rep</span><span class="punctuation">(</span><span class="number">1</span><span class="operator">:</span><span class="number">1000</span><span class="punctuation">,</span> <span class="built_in">rep</span><span class="punctuation">(</span><span class="number">20</span><span class="punctuation">,</span> <span class="number">1000</span><span class="punctuation">)</span><span class="punctuation">)</span><span class="punctuation">,</span> FUN<span class="operator">=</span>mean<span class="punctuation">)</span><span class="punctuation">,</span></span><br><span class="line">type <span class="operator">=</span> <span class="string">&#x27;mean_of_20&#x27;</span><span class="punctuation">)</span></span><br><span class="line"><span class="comment"># 合并数据框并将type转换为因子</span></span><br><span class="line">income <span class="operator">&lt;-</span> rbind<span class="punctuation">(</span>samp_data<span class="punctuation">,</span> samp_mean_05<span class="punctuation">,</span> samp_mean_20<span class="punctuation">)</span></span><br><span class="line">income<span class="operator">$</span>type <span class="operator">=</span> factor<span class="punctuation">(</span>income<span class="operator">$</span>type<span class="punctuation">,</span></span><br><span class="line">levels<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;data_dist&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;mean_of_5&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;mean_of_20&#x27;</span><span class="punctuation">)</span><span class="punctuation">,</span></span><br><span class="line">labels<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;Data&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;Mean of 5&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;Mean of 20&#x27;</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line"><span class="comment"># 绘制直方图</span></span><br><span class="line">ggplot<span class="punctuation">(</span>income<span class="punctuation">,</span> aes<span class="punctuation">(</span>x<span class="operator">=</span>income<span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">geom_histogram<span class="punctuation">(</span>bins<span class="operator">=</span><span class="number">40</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">facet_grid<span class="punctuation">(</span>type <span class="operator">~</span> .<span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>Python 代码使用 seaborn 的 <strong>FacetGrid</strong> 显示三个直方图：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line">sample_data = pd.DataFrame(&#123;</span><br><span class="line"><span class="string">&#x27;income&#x27;</span>: loans_income.sample(<span class="number">1000</span>),</span><br><span class="line"><span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;Data&#x27;</span>,</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">sample_mean_05 = pd.DataFrame(&#123;</span><br><span class="line"><span class="string">&#x27;income&#x27;</span>: [loans_income.sample(<span class="number">5</span>).mean() <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>)],</span><br><span class="line"><span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;Mean of 5&#x27;</span>,</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">sample_mean_20 = pd.DataFrame(&#123;</span><br><span class="line"><span class="string">&#x27;income&#x27;</span>: [loans_income.sample(<span class="number">20</span>).mean() <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>)],</span><br><span class="line"><span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;Mean of 20&#x27;</span>,</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">results = pd.concat([sample_data, sample_mean_05, sample_mean_20])</span><br><span class="line">g = sns.FacetGrid(results, col=<span class="string">&#x27;type&#x27;</span>, col_wrap=<span class="number">1</span>, height=<span class="number">2</span>, aspect=<span class="number">2</span>)</span><br><span class="line">g.<span class="built_in">map</span>(plt.hist, <span class="string">&#x27;income&#x27;</span>, <span class="built_in">range</span>=[<span class="number">0</span>, <span class="number">200000</span>], bins=<span class="number">40</span>)</span><br><span class="line">g.set_axis_labels(<span class="string">&#x27;Income&#x27;</span>, <span class="string">&#x27;Count&#x27;</span>)</span><br><span class="line">g.set_titles(<span class="string">&#x27;&#123;col_name&#125;&#x27;</span>)</span><br></pre></td></tr></table></figure><h4 id="中心极限定理">中心极限定理</h4><p>我们刚刚描述的现象被称为<strong>中心极限定理</strong>。它指出，从多个样本抽取得到的均值会近似呈现熟悉的钟形<strong>正态曲线</strong>（参见第 69 页的“正态分布”），即使源总体并不服从正态分布，只要样本量足够大且数据偏离正态分布的程度不是太大。</p><p>中心极限定理使得像 t 分布这样的正态近似公式可以用于计算推断所需的抽样分布——也就是置信区间和假设检验。</p><p><strong>中心极限定理在传统统计教材中备受关注，因为它支撑着假设检验和置信区间这类工具的运作机制</strong>，而这些工具本身在这类教材中占据了大量篇幅。<strong>数据科学家应该意识到这一点；不过，由于形式化的假设检验和置信区间在数据科学中所占比重不大，并且无论如何都可以使用自助法（参见第 61 页的“自助法”），所以中心极限定理在数据科学实践中并没有那么“核心”。</strong></p><h4 id="标准误">标准误</h4><p>Standard Error</p><p><strong>标准误</strong>是一个汇总统计量，它概括了某个统计量的抽样分布中的变异程度。标准误可以用样本值的标准差 <span class="math inline">\(s\)</span> 和样本量 <span class="math inline">\(n\)</span> 来估计： <span class="math display">\[\text{标准误} = SE = \frac{s}{\sqrt{n}}\]</span></p><p>随着样本量增加，标准误减小，这对应于图 2-6 中的观察结果。标准误与样本量的关系有时被称为“平方根 n 法则”：若要把标准误降低一半，样本量必须增加四倍。</p><p>标准误公式的有效性源自中心极限定理。不过，其实你并不需要依赖中心极限定理也能理解标准误。可以考虑以下估计标准误的方法：</p><ol type="1"><li>从总体中收集若干个全新的样本；</li><li>对每个新样本计算某个统计量（如均值）；</li><li>计算步骤 2 中得到的所有统计量的标准差，把它作为标准误的估计值。</li></ol><p>在实际中，这种通过收集新样本来估计标准误的方法通常不可行（而且在统计上很浪费）。幸运的是，事实证明并不需要真的抽取全新的样本；你可以使用<strong>自助重抽样（bootstrap）</strong>。</p><p><strong>在现代统计学中，自助法已经成为估计标准误的标准方法。它几乎可以用于任何统计量，而且不依赖于中心极限定理或其他分布假设。</strong></p><blockquote><p><strong>警告</strong></p><p>标准差与标准误的区别：<strong>不要把</strong>标准差<strong>（衡量单个数据点的变异性）与</strong>标准误**（衡量样本统计量的变异性）混淆。</p><p>Standard Deviation Versus Standard Error：Do not confuse standard deviation (which measures the variability of individual data points) with standard error (which measures the variability of a sample metric).</p></blockquote><p><strong>关键要点</strong></p><ul><li>样本统计量的频率分布告诉我们这个指标在不同样本间可能有多大差异；</li><li><strong>这种抽样分布可以通过自助法估计，也可以通过依赖中心极限定理的公式估计；</strong></li><li>概括样本统计量变异性的一个关键指标就是它的<strong>标准误</strong>。</li></ul><h3 id="自助法">自助法</h3><p>Bootstrap</p><p>一种简单且有效的方法，可以用来估计某个统计量或模型参数的抽样分布，就是从原始样本本身中<strong>有放回地</strong>抽取额外样本，并对每个重抽样重新计算统计量或模型。这个过程叫作<strong>自助法（bootstrap）</strong>，它不一定需要假设数据或样本统计量服从正态分布。</p><p><strong>自助法的关键术语</strong></p><ul><li><strong>自助样本（Bootstrap sample）</strong>： 从观测数据集中有放回地抽取得到的样本。</li><li><strong>重抽样（Resampling）</strong>： 从观测数据中反复抽取样本的过程；包括自助法和置换（洗牌）等方法。 bootstrap and permutation (shuffling) procedures.</li></ul><p><strong>从概念上讲，你可以把自助法想象为把原始样本复制成几千甚至几百万份，从而得到一个“假想总体”，它包含了你原始样本中的所有信息（只是更大了）。然后你可以从这个假想总体中抽取样本，用来估计抽样分布</strong>（见图 2-7）。</p><p><img src="/img3/面向数据科学家的实用统计学/F2.7.png" alt="F2.7" style="zoom:50%;" /></p><p>在实践中，并不需要真的把样本复制成巨大的规模。我们只是每抽取一个观测值后再放回，也就是<strong>有放回抽样</strong>。这样就等效于创建了一个无限总体，其中每个元素在每次抽取时被选中的概率保持不变。</p><p><strong>对大小为 n 的样本做均值自助重抽样的算法如下：</strong></p><ol type="1"><li><p>抽取一个样本值，记录并放回；</p></li><li><p>重复上述过程 n 次；</p></li><li><p>记录这 n 个重抽样值的均值；</p></li><li><p>重复步骤 1–3 共 R 次；</p></li><li><p>使用这 R 次的结果来：</p><ul><li>计算它们的标准差（估计样本均值的标准误）；</li><li>绘制直方图或箱线图；</li><li>求出置信区间。</li></ul></li></ol><p>R（自助法迭代次数）是人为设定的。迭代次数越多，标准误或置信区间的估计就越精确。这个过程的结果是一组自助样本统计量或模型参数估计值，你可以据此观察它们的变异性。</p><p>R 包 <strong>boot</strong> 将这些步骤整合到一个函数中。例如，下面把自助法应用于贷款人收入的中位数：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">library<span class="punctuation">(</span>boot<span class="punctuation">)</span></span><br><span class="line">stat_fun <span class="operator">&lt;-</span> <span class="keyword">function</span><span class="punctuation">(</span>x<span class="punctuation">,</span> idx<span class="punctuation">)</span> median<span class="punctuation">(</span>x<span class="punctuation">[</span>idx<span class="punctuation">]</span><span class="punctuation">)</span></span><br><span class="line">boot_obj <span class="operator">&lt;-</span> boot<span class="punctuation">(</span>loans_income<span class="punctuation">,</span> R<span class="operator">=</span><span class="number">1000</span><span class="punctuation">,</span> statistic<span class="operator">=</span>stat_fun<span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>函数 <code>stat_fun</code> 会针对索引 <code>idx</code> 指定的样本计算中位数。结果如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Bootstrap Statistics :</span><br><span class="line">original bias std. error</span><br><span class="line">t1* 62000 -70.5595 209.1515</span><br></pre></td></tr></table></figure><p>中位数的原始估计值是 $62,000。自助分布显示该估计的偏差约为 –$70，标准误为 $209。不同运行之间结果会略有差异。</p><p>主要的 Python 包没有直接提供自助法实现。可以用 scikit-learn 的 <code>resample</code> 方法实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">results = []</span><br><span class="line"><span class="keyword">for</span> nrepeat <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    sample = resample(loans_income)</span><br><span class="line">    results.append(sample.median())</span><br><span class="line">results = pd.Series(results)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Bootstrap Statistics:&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;original: <span class="subst">&#123;loans_income.median()&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;bias: <span class="subst">&#123;results.mean() - loans_income.median()&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;std. error: <span class="subst">&#123;results.std()&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p>自助法可以用于<strong>多变量数据</strong>，其中按行作为单元进行抽样（见图 2-8）。然后可以在自助重抽样的数据上运行模型，例如：</p><ul><li>估计模型参数的稳定性（变异性）；</li><li>提升预测能力。</li></ul><p>对于<strong>分类与回归树（decision trees）</strong>，在自助样本上运行多棵树并对其预测进行平均（分类问题取多数投票）通常比只用一棵树效果更好。这个过程叫作<strong>Bagging</strong>（“Bootstrap Aggregating”的缩写；参见第 259 页“Bagging 和随机森林”）。</p><p><img src="/img3/面向数据科学家的实用统计学/F2.8.png" alt="F2.8" style="zoom:50%;" /></p><p>自助法的重复重采样在概念上非常简单，经济学家兼人口学家 Julian Simon 在其 1969 年的著作 <em>Basic Research Methods in Social Science</em>（《社会科学基础研究方法》，Random House）中就收录了包括自助法在内的重采样示例大全。然而，这种方法在计算上非常密集，在计算能力尚未普及之前并不可行。这个技术之所以得名并流行起来，是因为斯坦福大学统计学家 Bradley Efron 在 1970 年代末至 1980 年代初发表了多篇论文和一本书。它在需要用统计方法但并非统计学家的研究者中尤为受欢迎，尤其适用于那些缺乏数学近似方法的度量或模型。平均值的抽样分布自 1908 年以来已得到很好建立，但许多其他度量的抽样分布则没有。<strong>自助法还可用于样本量的确定；可以尝试不同的 n 值，观察抽样分布的变化情况。</strong></p><p><strong>自助法在首次提出时曾遭到相当多的怀疑，许多人觉得这就像是“把稻草变成黄金”。</strong>这种怀疑源于对自助法目的的误解。</p><blockquote><p><strong>警告：</strong> 自助法并不能弥补样本量小的问题；它不会创造新数据，也不会填补已有数据集的空白。它只是告诉我们：如果从类似原始样本的总体中抽取大量额外样本，结果会如何表现。</p></blockquote><h4 id="重采样与自助法的区别"><strong>重采样与自助法的区别</strong></h4><p>Resampling Versus Bootstrapping</p><p>有时“重采样”一词被用作“自助法”的同义词，如上所述。但更多时候，“重采样”还包括排列检验（见第 97 页“置换检验”），此时会将多个样本合并，并可能在不放回的情况下进行抽样。无论如何，“自助法”一词始终意味着从观测数据集中进行有放回的抽样。</p><p><strong>关键要点</strong></p><ul><li>自助法（从数据集中有放回抽样）是评估样本统计量变异性的有力工具。</li><li>自助法可以在各种情境下以类似方式应用，而无需对抽样分布进行大量数学近似研究。</li><li>它还可以让我们估计那些尚无数学近似公式的统计量的抽样分布。</li><li>当应用于预测模型时，对多个自助样本的预测结果进行汇总（即“袋装”）通常优于使用单一模型。</li></ul><h3 id="置信区间"><strong>置信区间</strong></h3><p>Confidence Intervals</p><p>频数表、直方图、箱线图和标准误差都是理解样本估计潜在误差的手段。置信区间是另一种方法。</p><p><strong>置信区间的关键术语</strong></p><ul><li><strong>置信水平（Confidence level）</strong>：按照同样方法从同一总体构造的置信区间中，预计包含目标统计量的比例（百分比）。</li><li><strong>区间端点（Interval endpoints）</strong>：置信区间的上下限。</li></ul><p>人类对不确定性有天然的厌恶感；人们（尤其是专家）很少说“我不知道”。分析师和管理者虽然承认不确定性，但当估计值以单一数字（点估计）呈现时，仍会对其抱有过高信任。将估计值呈现为一个区间而非单一数字，是对抗这种倾向的一种方式。<strong>置信区间基于统计抽样原理，正是这样做的。</strong></p><p>置信区间总是伴随一个覆盖水平，用（较高的）百分比表示，比如 90% 或 95%。一种理解 90% 置信区间的方式是：它是包含该样本统计量自助抽样分布中间 90% 的区间（见第 61 页“自助法”）。更一般地说，一个围绕样本估计的 x% 置信区间，在类似抽样程序下，平均应包含类似的样本估计 x% 的时间。</p><p><strong>基于自助法的置信区间算法（样本量 n，关注的统计量）：</strong></p><ol type="1"><li>从数据中有放回抽取 n 个观测值（重采样）。</li><li>记录重采样的统计量。</li><li>重复步骤 1–2 多次（R 次）。</li><li>对于 x% 的置信区间，从分布两端各裁掉 <span class="math inline">\((100 - x)/2\)</span>% 的 R 次重采样结果。</li><li>剩余区间的端点就是 x% 自助置信区间的端点。</li></ol><p>图 2-9 展示了一个贷款申请人年均收入的 90% 置信区间，该样本量为 20，均值为 62,231 美元。</p><p><img src="/img3/面向数据科学家的实用统计学/F2.9.png" alt="F2.9" style="zoom:33%;" /></p><p><strong>自助法是一种通用工具，可用于为大多数<em><u>统计量</u></em>或<em><u>模型参数</u></em>生成置信区间。</strong>传统的统计教材和软件由于其半个多世纪“无计算机”统计分析的历史，通常还会引用基于公式（尤其是 t 分布，见第 75 页“Student’s t 分布”）生成的置信区间。</p><blockquote><p><strong>通用注解：</strong></p><p>当然，当我们得到一个样本结果时，真正想问的问题往往是：“真实值落在某个区间内的概率是多少？”严格来说，这并不是置信区间回答的问题，但这却是大多数人理解置信区间答案的方式。</p><p>与置信区间相关的概率问题通常以这样的句子开头：“在给定一个抽样程序和总体的前提下，……的概率是多少？”如果要反向提问——“在给定一个样本结果的情况下，某件关于总体的事情为真的概率是多少？”——则需要更复杂的计算以及更深层次的不可知因素。</p></blockquote><p>与置信区间相关的百分比被称为<strong>置信水平</strong>。置信水平越高，区间越宽；样本量越小，区间也越宽（即不确定性越大）。两点都符合直觉：你越想要更高的置信度，或者你的数据越少，你就必须使置信区间更宽，才能有足够把握覆盖真实值。</p><blockquote><p><strong>通用注解：</strong></p><p>对于数据科学家而言，置信区间是一种帮助理解样本结果可能有多大变动的工具。数据科学家使用这类信息通常不是为了发表学术论文或向监管机构提交结果（如研究人员那样），而更可能是为了传达估计值潜在的误差，或评估是否需要更大的样本量。</p></blockquote><p><strong>关键要点</strong></p><ul><li>置信区间是把估计值呈现为区间范围的典型方法。</li><li>数据越多，样本估计值的变动越小。</li><li>你能容忍的置信水平越低，置信区间就越窄。</li><li>自助法是构建置信区间的有效方法。</li></ul><h3 id="正态分布"><strong>正态分布</strong></h3><p>钟形的正态分布在传统统计中具有标志性。<strong>样本统计量的分布往往呈正态形状</strong>，这一事实使得它成为开发逼近这些分布的数学公式的有力工具。</p><p><strong>正态分布关键术语</strong></p><ul><li><strong>误差（Error）</strong>：数据点与预测值或平均值之间的差异。</li><li><strong>标准化（Standardize）</strong>：减去均值并除以标准差。</li><li><strong>z 分数（z-score）</strong>：单个数据点标准化后的结果。</li><li><strong>标准正态（Standard normal）</strong>：均值 = 0、标准差 = 1 的正态分布。</li><li><strong>QQ 图（QQ-Plot）</strong>：用来可视化样本分布与指定分布（如正态分布）接近程度的图形。</li></ul><p>在正态分布中（图 2-10），68% 的数据位于均值 ±1 个标准差内，95% 的数据位于均值 ±2 个标准差内。</p><p><img src="/img3/面向数据科学家的实用统计学/F2.10.png" alt="F2.10" style="zoom:50%;" /></p><blockquote><p><strong>警告：</strong></p><p>一个常见的误解是，正态分布之所以叫“normal”，是因为大多数数据都服从正态分布——也就是说它是“正常的”。实际上，大多数典型数据科学项目中使用的变量——事实上绝大多数原始数据——并不服从正态分布（见第 73 页“长尾分布”）。<strong>正态分布的真正用途在于，许多统计量在其抽样分布中近似正态分布</strong>。即便如此，只有在经验概率分布或自助法分布不可用时，才会使用正态性假设作为最后手段。</p></blockquote><blockquote><p><strong>通用注解</strong></p><p>正态分布（Normal distribution）也被称为<strong>高斯分布（Gaussian distribution）</strong>，名字来源于 18 世纪末、19 世纪初的德国天才数学家卡尔·弗里德里希·高斯（Carl Friedrich Gauss）。 正态分布以前还被称为“误差分布（error distribution）”。在统计学中，误差指的是<strong>实际值与统计估计值（如样本均值）之间的差异</strong>。例如，标准差（standard deviation）（见第 13 页“变异性的估计”）就是基于数据与其均值之间的误差计算出来的。高斯对正态分布的研究最初源自他对天文观测误差的分析——这些误差被发现服从正态分布。</p></blockquote><h4 id="标准正态分布与-qq-图"><strong>标准正态分布与 QQ 图</strong></h4><p>标准正态分布是指<strong>横轴单位用“距均值的标准差个数”来表示</strong>的正态分布。要将数据与标准正态分布进行比较，需先<strong>减去均值，再除以标准差</strong>；这一过程也称为<strong>标准化（standardization）或正则化（normalization）</strong>（见第 243 页“标准化（Normalization，z 分数）”）。注意，这里所说的“标准化”与数据库记录的标准化（转换成统一格式）无关。转换后的值称为 <strong>z 分数（z-score）</strong>，而正态分布有时也称为 <strong>z 分布（z-distribution）</strong>。</p><p>QQ 图（QQ-Plot）用于可视化地判断一个样本是否接近某个指定分布（这里是正态分布）。QQ 图会先按从低到高排列 z 分数，并将每个值的 z 分数绘制在纵轴；横轴则是该值排名对应的正态分布分位数。由于数据已标准化，单位对应的是距均值的标准差个数。如果点大致落在对角线上，那么样本分布就可以认为接近正态分布。</p><p><img src="/img3/面向数据科学家的实用统计学/F2.11.png" alt="F2.11" style="zoom:33%;" /></p><p>图 2-11 显示了一个从正态分布随机生成的 100 个值的 QQ 图；如预期那样，点紧密跟随直线。 在 R 中可用以下代码生成：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">norm_samp <span class="operator">&lt;-</span> rnorm<span class="punctuation">(</span><span class="number">100</span><span class="punctuation">)</span></span><br><span class="line">qqnorm<span class="punctuation">(</span>norm_samp<span class="punctuation">)</span></span><br><span class="line">abline<span class="punctuation">(</span>a<span class="operator">=</span><span class="number">0</span><span class="punctuation">,</span> b<span class="operator">=</span><span class="number">1</span><span class="punctuation">,</span> col<span class="operator">=</span><span class="string">&#x27;grey&#x27;</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>在 Python 中，可以用 <code>scipy.stats.probplot</code> 方法创建 QQ 图：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">norm_sample = stats.norm.rvs(size=<span class="number">100</span>)</span><br><span class="line">stats.probplot(norm_sample, plot=ax)</span><br></pre></td></tr></table></figure><blockquote><p><strong>警告</strong></p><p>将数据转换为 z 分数（即标准化或正则化）并不会使数据本身服从正态分布**，它只是把数据放到与标准正态分布相同的尺度上，通常用于比较目的。</p></blockquote><p><strong>关键要点</strong></p><ul><li>正态分布在统计学发展史上至关重要，因为它允许用数学方法近似不确定性和变异性。</li><li>虽然原始数据通常不是正态分布的，但<strong>误差</strong>往往是正态分布的，大样本中的<strong>平均值和总和</strong>也常常是正态分布的。</li><li>将数据转换为 z 分数的方法：先减去数据的均值，再除以标准差；这样就能把数据与正态分布进行比较。</li></ul><h3 id="长尾分布">长尾分布</h3><p>Long-Tailed Distributions</p><p>尽管正态分布在统计学历史上具有重要地位，但与其名字所暗示的相反，数据通常并不是正态分布的。</p><p><strong>长尾分布关键术语</strong></p><p><strong>尾部（Tail）</strong> 频率分布中狭长的部分，其中相对极端的值以低频率出现。</p><p><strong>偏斜（Skew）</strong> 当分布的一条尾巴比另一条更长时，即为偏斜。</p><p>虽然<strong>正态分布常常适用于误差和样本统计量的分布，但它通常并不能刻画原始数据的分布</strong>。有时，分布高度偏斜（不对称），如收入数据；或者分布是离散的，如二项分布数据。对称和不对称分布都可能具有长尾。分布的尾部对应极端值（小值和大值）。在实际工作中，人们普遍认识到长尾现象及防范其风险。纳西姆·塔勒布提出的“黑天鹅”理论就预测，像股市崩盘这样的异常事件发生的概率远高于正态分布所预测的水平。</p><p><img src="/img3/面向数据科学家的实用统计学/F2.12.png" alt="F2.12" style="zoom:33%;" /></p><p>一个说明数据长尾特征的好例子是股票收益率。图 2-12 展示了 Netflix（NFLX）日度股票收益率的 QQ 图。这在 R 中可以这样生成：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">nflx <span class="operator">&lt;-</span> sp500_px<span class="punctuation">[</span><span class="punctuation">,</span><span class="string">&#x27;NFLX&#x27;</span><span class="punctuation">]</span></span><br><span class="line">nflx <span class="operator">&lt;-</span> diff<span class="punctuation">(</span><span class="built_in">log</span><span class="punctuation">(</span>nflx<span class="punctuation">[</span>nflx<span class="operator">&gt;</span><span class="number">0</span><span class="punctuation">]</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">qqnorm<span class="punctuation">(</span>nflx<span class="punctuation">)</span></span><br><span class="line">abline<span class="punctuation">(</span>a<span class="operator">=</span><span class="number">0</span><span class="punctuation">,</span> b<span class="operator">=</span><span class="number">1</span><span class="punctuation">,</span> col<span class="operator">=</span><span class="string">&#x27;grey&#x27;</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>对应的 Python 代码为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">nflx = sp500_px.NFLX</span><br><span class="line">nflx = np.diff(np.log(nflx[nflx&gt;<span class="number">0</span>]))</span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">stats.probplot(nflx, plot=ax)</span><br></pre></td></tr></table></figure><p>与图 2-11 相比，这些点在低值时远低于直线，在高值时远高于直线，表明数据并非正态分布。这意味着我们比正态分布所预测的更有可能观察到极端值。<strong>图 2-12 还展示了另一种常见现象：在均值一个标准差范围内的数据点接近直线。Tukey 将这一现象称为“中间正态”但尾部更长（参见 [Tukey-1987]）。</strong></p><p>关于将统计分布拟合到观测数据这一任务，有大量统计学文献。<strong>要警惕一种过于数据中心化的方法——这项工作既是科学也是艺术。数据具有变异性，并且表面上往往与不止一种形状和类型的分布一致。通常必须结合领域知识和统计知识，才能确定在某一情境下应使用何种分布进行建模。</strong>例如，我们可能有关于某台服务器在许多连续 5 秒时间段内的网络流量数据。此时，了解“每时间段事件数”最合适的分布是泊松分布（参见第 83 页“泊松分布”）就很有用。</p><p><strong>关键要点</strong></p><ul><li>大多数数据并非正态分布。</li><li>假设正态分布可能导致对极端事件（“黑天鹅”）的低估。</li></ul><h3 id="students-t-分布"><strong>Student’s t 分布</strong></h3><p>t 分布的形状与正态分布相似，只是尾部稍厚、稍长。它被广泛用于描述样本统计量的分布。样本均值的分布通常呈现出类似 t 分布的形状，并且 t 分布有一个“族”，它们的形状随样本量大小而不同：样本越大，t 分布越接近正态分布。</p><p><strong>学生 t 分布的关键术语</strong></p><p><strong><span class="math inline">\(n\)</span></strong> 样本量。</p><p><strong>自由度</strong> 一个参数，使 t 分布能够根据不同的样本量、统计量和组数进行调整。</p><p>t 分布常被称为 <strong>Student’s t 分布</strong>，因为它最早由 W. S. Gosset 以“Student”笔名在 1908 年的《Biometrika》上发表。Gosset 当时在健力士（Guinness）啤酒厂工作，雇主不希望竞争对手知道他们正在使用统计方法，因此要求 Gosset 不要用真名发表文章。</p><p>Gosset 想回答的问题是：“从总体中抽取一个样本后，样本均值的抽样分布是什么样的？” 他最初通过重抽样实验来探索这一问题：他从包含 3,000 个罪犯身高和左中指长度的数据中，随机抽取大小为 4 的样本。（在当时的优生学背景下，人们对犯罪数据和犯罪倾向与身体或心理特征之间的相关性十分感兴趣。）Gosset 将标准化后的结果（即 z 分数）画在 x 轴上，将频率画在 y 轴上。同时，他推导出一个现在被称为 <strong>Student’s t</strong> 的函数，并将该函数与样本结果拟合并进行比较（见图 2-13）。</p><p><img src="/img3/面向数据科学家的实用统计学/F2.13.png" alt="F2.13" style="zoom:33%;" /></p><p>许多不同的统计量在标准化后都可以与 t 分布进行比较，从而在考虑抽样变异的情况下估计置信区间。设样本量为 <span class="math inline">\(n\)</span>，样本均值为 <span class="math inline">\(\bar{x}\)</span>，样本标准差为 <span class="math inline">\(s\)</span>。则样本均值的 90% 置信区间为：</p><p><span class="math display">\[\bar{x} \pm t_{n-1}(0.05)\frac{s}{\sqrt{n}}\]</span></p><p>其中 <span class="math inline">\(t_{n-1}(0.05)\)</span> 是具有 <span class="math inline">\(n-1\)</span> 个自由度（见第 116 页“自由度”部分）且在两端各“切掉”5% 的 t 统计量值。<strong>t 分布</strong>可作为参考来描述样本均值、两个样本均值之差、<strong>回归参数</strong>以及其他统计量的分布。</p><p>如果 1908 年就已经普及强大的计算能力，那么统计学无疑会从一开始就更多地依赖计算量密集的重抽样方法。由于当时没有计算机，统计学家们转向数学与诸如 t 分布这样的函数来近似抽样分布。到了 1980 年代，计算能力使得实际的重抽样实验成为可能，但那时 t 分布等方法已经在教材和软件中根深蒂固。</p><p><strong>t 分布能够准确描述样本统计量的前提，是该统计量在样本中的分布近似正态分布。事实证明，即使总体数据本身并不服从正态分布，样本统计量往往仍近似正态分布（这一事实促使 t 分布被广泛应用）。这又回到了一个现象——中心极限定理（见第 60 页“中心极限定理”）。</strong></p><blockquote><p><strong>通用注解：</strong> <strong>数据科学家需要了解 t 分布和中心极限定理的哪些内容？</strong>其实不用太多。t 分布用于经典统计推断，但对数据科学的核心目的来说并非那么重要。理解和量化不确定性与变异性对数据科学家来说很重要，但经验性自助（bootstrap）抽样可以回答大多数关于抽样误差的问题。不过，<strong>数据科学家在统计软件和 R 的统计过程输出中会经常遇到 t 统计量——例如在 A/B 测试和回归分析中</strong>——因此了解它的用途还是有帮助的。</p></blockquote><p><strong>关键要点</strong></p><ul><li>t 分布实际上是一族分布，它们看起来像正态分布，但尾部更厚。</li><li>t 分布被广泛用作样本均值分布、两个样本均值之差、回归参数等分布的参考基础。</li></ul><h3 id="二项分布"><strong>二项分布</strong></h3><p>Binomial Distribution</p><p>是/否（二项）结果处于分析的核心，因为它们通常是决策或其他过程的最终结果：购买/不购买、点击/不点击、生存/死亡等等。理解二项分布的核心是“试验集合”的概念：每次试验有两个可能的结果，并且每个结果的概率是确定的。</p><p>例如，掷硬币 10 次就是一个包含 10 次试验的二项试验，每次试验有两个可能的结果（正面或反面）；见图 2-14。此类是/否或 0/1 的结果称为二元结果，它们不一定是 50/50 的概率。只要概率之和为 1.0，就都是可能的。在统计学中，习惯上称“1”这个结果为“成功”结果；也常将“1”分配给较少见的结果。“成功”一词并不意味着结果是可取的或有益的，而只是指感兴趣的结果。例如，贷款违约或欺诈交易是我们可能感兴趣预测的相对少见事件，因此它们被称为“1”或“成功”。</p><p><img src="/img3/面向数据科学家的实用统计学/F2.14.png" alt="F2.14" style="zoom:50%;" /></p><p><strong>二项分布的关键术语</strong></p><ul><li><p><strong>试验（Trial）</strong> 一个具有离散结果的事件（例如抛硬币）。</p></li><li><p><strong>成功（Success）</strong> 试验中关注的结果。 <strong>同义词</strong>： “1”（相对于“0”）</p></li><li><p><strong>二项（Binomial）</strong> 具有两个结果。 <strong>同义词</strong>： 是/否（yes/no）、0/1、二元（binary）</p></li><li><p><strong>二项试验（Binomial trial）</strong> 具有两种结果的试验。 <strong>同义词</strong>：伯努利试验（Bernoulli trial）</p></li><li><p><strong>二项分布（Binomial distribution）</strong> <strong>在 <span class="math inline">\(x\)</span> 次试验中成功次数的分布。</strong> <strong>同义词</strong>：伯努利分布（Bernoulli distribution）</p></li></ul><p><strong>二项分布就是在给定试验次数 <span class="math inline">\(n\)</span> 和每次试验成功概率 <span class="math inline">\(p\)</span> 的条件下，成功次数 <span class="math inline">\(x\)</span> 的频率分布。</strong>根据 <span class="math inline">\(n\)</span> 和 <span class="math inline">\(p\)</span> 的不同，有一族二项分布。二项分布能回答这样的问题：</p><blockquote><p>如果点击转化为购买的概率是 0.02，那么在 200 次点击中观察到 0 次购买的概率是多少？</p></blockquote><p>R 函数 <code>dbinom</code> 用来计算二项概率。例如：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dbinom<span class="punctuation">(</span>x<span class="operator">=</span><span class="number">2</span><span class="punctuation">,</span> size<span class="operator">=</span><span class="number">5</span><span class="punctuation">,</span> p<span class="operator">=</span><span class="number">0.1</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>会返回 0.0729，即在 5 次试验中，每次试验成功概率 <span class="math inline">\(p=0.1\)</span> 时，恰好观察到 <span class="math inline">\(x=2\)</span> 次成功的概率。对于上面的例子，我们用 <span class="math inline">\(x=0\)</span>、<code>size=200</code> 和 <span class="math inline">\(p=0.02\)</span>。用这些参数，<code>dbinom</code> 返回的概率是 0.0176。</p><p>我们经常还想知道在 <span class="math inline">\(n\)</span> 次试验中，<span class="math inline">\(x\)</span> 次或更少成功的概率。这时使用函数 <code>pbinom</code>：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pbinom<span class="punctuation">(</span><span class="number">2</span><span class="punctuation">,</span> <span class="number">5</span><span class="punctuation">,</span> <span class="number">0.1</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>它会返回 0.9914，即在 5 次试验中，每次试验成功概率 0.1 时，观察到 2 次或更少成功的概率。</p><p><code>scipy.stats</code> 模块实现了大量统计分布。对于二项分布，可以使用函数 <code>stats.binom.pmf</code> 和 <code>stats.binom.cdf</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">stats.binom.pmf(<span class="number">2</span>, n=<span class="number">5</span>, p=<span class="number">0.1</span>)</span><br><span class="line">stats.binom.cdf(<span class="number">2</span>, n=<span class="number">5</span>, p=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure><p>二项分布的均值是 <span class="math inline">\(n \times p\)</span>；也可以将其看作 <span class="math inline">\(n\)</span> 次试验中预期的成功次数（成功概率 = <span class="math inline">\(p\)</span>）。 方差是 <span class="math inline">\(n \times p (1-p)\)</span>。当试验次数足够大（尤其是当 <span class="math inline">\(p\)</span> 接近 0.50 时），二项分布几乎与正态分布无法区分。实际上，在大样本情况下计算二项概率计算量很大，因此大多数统计过程会用正态分布（用均值和方差）进行近似。</p><p><strong>关键要点</strong></p><ul><li>建模二项结果非常重要，因为它们代表了许多基本决策（如购买或不购买、点击或不点击、生存或死亡等）。</li><li>二项试验是一个有两种可能结果的实验：一种的概率为 <span class="math inline">\(p\)</span>，另一种的概率为 <span class="math inline">\(1 - p\)</span>。</li><li><strong>当 <span class="math inline">\(n\)</span> 很大且 <span class="math inline">\(p\)</span> 不太接近 0 或 1 时，二项分布可以用正态分布近似。</strong></li></ul><h3 id="卡方分布">卡方分布</h3><p>Chi-Square Distribution</p><p>在统计学中，一个重要的思想是“偏离期望”，尤其是针对类别计数而言。这里“期望”可粗略地定义为“数据中没有什么异常或值得注意的现象”（例如变量之间没有相关性或可预测的模式）。这也被称为“原假设”或“零模型”（参见第94页“原假设”）。</p><p>例如，你可能想要检验一个变量（比如表示性别的行变量）是否独立于另一个变量（比如表示“是否在工作中获得晋升”的列变量），并且你手头有每个数据表单元格中的人数计数。衡量结果偏离独立性原假设期望程度的统计量就是卡方统计量。它的计算方法是：观察值与期望值之差，除以期望值的平方根，再平方，然后对所有类别求和。这个过程对统计量进行了标准化，使其可以与参考分布进行比较。更一般地说，卡方统计量衡量的是一组观测值与某一指定分布“拟合”的程度（即“拟合优度”检验）。它对于确定多种处理（“A/B/C…测试”）的效果是否彼此不同非常有用。</p><p>卡方分布就是在从零模型中反复抽样时，这个统计量的分布——详细算法见第124页“卡方检验”及数据表的卡方公式。一组计数的卡方值低，表示它们与期望分布非常接近；卡方值高，则表示它们与期望差异明显。与不同自由度（例如观测数——见第116页“自由度”）相关的卡方分布有多种。</p><p><strong>关键要点</strong></p><ul><li>卡方分布通常关注的是落入不同类别中的个体或项目的计数。</li><li>卡方统计量衡量的是数据相对于零模型期望的偏离程度。</li></ul><h3 id="f-分布"><strong>F 分布</strong></h3><p>F-Distribution</p><p>在科学实验中，一个常见的做法是跨不同组测试多种处理——例如，在田地不同区块上使用不同的肥料。这与卡方分布中提到的 A/B/C 测试（见第 80 页“卡方分布”）类似，但我们<strong>这里处理的是连续测量值，而不是计数值</strong>。在这种情况下，我们关注的是组间均值差异是否大于在正常随机波动下可能预期的程度。F 统计量衡量的就是这一点，它是组间均值变异性与组内变异性（也称为残差变异性）的比值。这种比较称为<strong>方差分析(analysis of variance)</strong>（见第 118 页“ANOVA”）。F 统计量的分布是对在所有组均值相等（即零假设模型）的数据进行随机置换所产生的所有可能值的频率分布。不同自由度（例如组数——见第 116 页“自由度”）会对应不同的 F 分布。F 的计算在 ANOVA 一节中有详细说明。在<strong>线性回归</strong>中，F 统计量还用于比较回归模型解释的变异与数据总体变异。R 和 Python 在回归和 ANOVA 的例程中会自动生成 F 统计量。</p><p><strong>关键概念</strong></p><ul><li><strong>F 分布</strong>用于涉及测量数据的实验和线性模型。</li><li><strong>F 统计量</strong>比较感兴趣因素造成的变异与总体变异的大小。</li></ul><h3 id="泊松分布及相关分布">泊松分布及相关分布</h3><p>Poisson and Related Distributions</p><p>许多过程在某个总体速率下随机产生事件——例如，网站访客到达（随时间分布的事件）、收费站汽车到达（随时间分布的事件）；每平方米布料中的瑕疵、每 100 行代码中的错别字（随空间分布的事件）。</p><p><strong>泊松及相关分布的关键术语</strong></p><ul><li><p><strong>λ（Lambda）</strong> 事件在单位时间或单位空间发生的速率。</p></li><li><p><strong>泊松分布（Poisson distribution）</strong> 抽样单位时间或空间内事件数量的频率分布。</p></li><li><p><strong>指数分布（Exponential distribution）</strong> 从一个事件到下一个事件的时间或距离的频率分布。</p></li><li><p><strong>威布尔分布（Weibull distribution）</strong> 指数分布的广义版本，事件发生速率允许随时间变化。</p></li></ul><h4 id="泊松分布">泊松分布</h4><p>从先前的总体数据（例如每年的流感感染数）可以估计单位时间或单位空间内事件的平均数（例如每天或每个人口普查单元的感染数）。我们也可能想知道从一个时间/空间单位到另一个时间/空间单位，这个数可能有多大差异。泊松分布告诉我们，当抽取多个这样的单位时，每个单位时间或空间内事件数的分布情况。它在排队问题中很有用，比如：“我们需要多大容量才能在任意五秒内有 95% 的把握完全处理服务器上到达的网络流量？”</p><p>泊松分布的关键参数是 λ（lambda）。这是在指定时间或空间区间内发生事件的平均数。泊松分布的方差也是 λ。</p><p>一个常见的技巧是在排队模拟中生成来自泊松分布的随机数。R 中的 <code>rpois</code> 函数就是这样做的，它只需要两个参数——随机数数量和 λ：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rpois<span class="punctuation">(</span><span class="number">100</span><span class="punctuation">,</span> lambda<span class="operator">=</span><span class="number">2</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>对应的 Python 函数是 <code>stats.poisson.rvs</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stats.poisson.rvs(<span class="number">2</span>, size=<span class="number">100</span>)</span><br></pre></td></tr></table></figure><p>这段代码会从 λ = 2 的泊松分布中生成 100 个随机数。例如，如果平均每分钟有两个客服电话进来，这段代码就模拟了 100 分钟，每分钟返回呼叫数。</p><h4 id="指数分布">指数分布</h4><p>使用和泊松分布相同的参数 λ，我们也可以建模事件间隔的分布：如两次访问网站之间的时间，或两辆车到达收费站之间的时间。指数分布还用于工程领域建模失效时间，在流程管理中建模每个服务调用所需时间等。</p><p>R 中生成指数分布随机数的代码需要两个参数：</p><ul><li><code>n</code>（生成的随机数个数）</li><li><code>rate</code>（每个时间段的事件数）</li></ul><p>例如：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rexp<span class="punctuation">(</span>n<span class="operator">=</span><span class="number">100</span><span class="punctuation">,</span> rate<span class="operator">=</span><span class="number">0.2</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>在 Python 的 <code>stats.expon.rvs</code> 中，参数顺序相反：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stats.expon.rvs(<span class="number">0.2</span>, size=<span class="number">100</span>)</span><br></pre></td></tr></table></figure><p>这段代码会从平均每时间段 0.2 次事件的指数分布中生成 100 个随机数。你可以用它来模拟 100 个服务电话之间的间隔（单位：分钟），其中平均呼叫速率为每分钟 0.2 次。</p><p>对于泊松分布或指数分布的任何模拟研究，一个关键假设是速率 λ 在所考虑的期间内保持不变。总体上这一假设很少完全成立，例如，道路或数据网络的流量会因时段和星期几而变化。然而，通常可以把时间段或空间区域划分为足够同质的小段，以便在这些小段内进行分析或模拟是合理的。</p><h4 id="估计失效率">估计失效率</h4><p>在许多应用中，事件速率 λ 是已知的，或者可以从先前数据中估计。然而，对于<strong>罕见事件</strong>，情况就不一定如此。例如，飞机发动机失效（幸好）非常罕见，对于某种特定型号的发动机，可能几乎没有数据来估计故障间隔时间。<strong>在完全没有数据的情况下，几乎没有依据去估计事件速率</strong>。</p><p>不过，你可以做一些推测：如果在 20 小时后还没有出现任何事件，你几乎可以肯定速率不是“每小时 1 次”。通过模拟或直接计算概率，你可以评估不同的假设事件速率，并估算速率极不可能低于的阈值。如果有一些数据，但不足以提供精确、可靠的速率估计，就可以对不同速率应用<strong>拟合优度检验</strong>（参见第 124 页的“卡方检验”），以判断这些速率与观测数据的拟合程度。</p><h4 id="威布尔分布">威布尔分布</h4><p>Weibull Distribution</p><p>在许多情况下，事件速率<strong>并不随时间保持不变</strong>。如果速率变化的周期远大于典型事件间隔，就没有问题；只要把分析划分为速率相对恒定的时间段即可（前面提到过）。但是，如果在事件间隔期间速率就发生变化，那么指数分布（或泊松分布）就不再适用。这在机械故障中很常见——随着时间推移，失效风险增加。</p><p>威布尔分布是指数分布的推广版本，它允许事件速率随时间变化，并通过<strong>形状参数 β</strong>来指定：</p><ul><li>如果 β &gt; 1，事件发生的概率随时间<strong>增加</strong>；</li><li>如果 β &lt; 1，事件发生的概率随时间<strong>减少</strong>。</li></ul><p>由于威布尔分布用于“失效时间分析”而不是事件速率分析，它的第二个参数不是每时间段的事件速率，而是用<strong>特征寿命</strong>（characteristic life）来表示。符号为 η（希腊字母 eta），也叫<strong>尺度参数（scale parameter）</strong>。</p><p>对于威布尔分布，估计任务现在包括估计<strong>两个参数 β 和 η</strong>。通常通过软件对数据建模，得到最合适的威布尔分布估计。</p><p>R 语言生成威布尔分布随机数的函数 <code>rweibull</code> 需要三个参数：</p><ul><li><code>n</code>：生成随机数的个数</li><li><code>shape</code>：形状参数</li><li><code>scale</code>：尺度参数（特征寿命）</li></ul><p>例如，以下代码从形状参数 1.5、特征寿命 5,000 的威布尔分布中生成 100 个随机数（寿命）：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rweibull<span class="punctuation">(</span><span class="number">100</span><span class="punctuation">,</span> <span class="number">1.5</span><span class="punctuation">,</span> <span class="number">5000</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>Python 中可用 <code>stats.weibull_min.rvs</code> 实现同样的功能：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stats.weibull_min.rvs(<span class="number">1.5</span>, scale=<span class="number">5000</span>, size=<span class="number">100</span>)</span><br></pre></td></tr></table></figure><p><strong>关键要点</strong></p><ul><li>对于以恒定速率发生的事件，单位时间或单位空间内事件数可以建模为<strong>泊松分布</strong>。</li><li>你也可以用<strong>指数分布</strong>来建模从一个事件到下一个事件的时间或距离。</li><li>对于随时间变化的事件速率（如设备失效概率随时间增加），可以用<strong>威布尔分布</strong>建模。</li></ul><h3 id="总结">总结</h3><p>在大数据时代，当需要精确估计时，<strong>随机抽样的原则依然重要</strong>。随机选择数据可以减少偏差，比单纯使用方便取得的数据集得到更高质量的数据。掌握不同抽样和数据生成分布的知识，可以帮助我们量化估计中由于随机变化可能产生的误差。</p><p><strong>同时，自助法（bootstrap）——即从观测数据集中有放回抽样——是一种非常有吸引力的“通用”方法，用于评估样本估计的可能误差。</strong></p>]]></content>
      
      
      <categories>
          
          <category> 翻译 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> AI </tag>
            
            <tag> 统计 </tag>
            
            <tag> 数据科学 </tag>
            
            <tag> R </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第1章 探索性数据分析</title>
      <link href="/2025/09/25/%E7%AC%AC1%E7%AB%A0%20%E6%8E%A2%E7%B4%A2%E6%80%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
      <url>/2025/09/25/%E7%AC%AC1%E7%AB%A0%20%E6%8E%A2%E7%B4%A2%E6%80%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<p><a href="/img3/面向数据科学家的实用统计学/Practical-Statistics-for-Data-Scientists.pdf">《Practical Statistics for Data Scientists》书籍英文版</a><br /><a href="/img3/面向数据科学家的实用统计学/面向数据科学家的实用统计学.pdf">《面向数据科学家的实用统计学》中文版书籍</a></p><h2 id="前言"><strong>前言</strong></h2><p>本书旨在帮助对 <strong>R 和/或 Python 编程语言</strong>有一定熟悉度、并对统计学有过一些前期（可能零星或短暂的）接触的<strong>数据科学家</strong>。两位作者从统计学领域进入数据科学世界，对统计学能为数据科学这门艺术所做的贡献心怀感激。与此同时，我们深知传统统计学教学的局限性：统计学作为一门学科已有一个半世纪的历史，大多数统计学教科书和课程都承载着巨轮般的动量和惯性。本书中的所有方法都与统计学这门学科有着某种联系——无论是历史上的还是方法论上的。那些主要从计算机科学演变而来的方法，比如神经网络，则不包括在内。</p><p>本书的两个目标是：</p><ul><li>以易于消化、导航和参考的形式，列出与数据科学相关的<strong>关键统计概念</strong>。</li><li>从数据科学的角度解释<strong>哪些概念是重要且有用的，哪些则不那么重要，以及原因何在</strong>。</li></ul><span id="more"></span><h3 id="本书使用的约定"><strong>本书使用的约定</strong></h3><p>本书使用以下排版约定：</p><ul><li><strong>斜体（Italic）</strong> 表示新术语、URL、电子邮件地址、文件名和文件扩展名。</li><li><strong>等宽字体（Constant width）</strong> 用于程序列表，以及在段落中引用程序元素，如变量或函数名、数据库、数据类型、环境变量、语句和关键字。</li><li><strong>粗体等宽字体（Constant width bold）</strong> 显示应由用户按字面键入的命令或其他文本。</li></ul><p><strong>关键术语</strong></p><p>数据科学是多个学科的融合，包括<strong>统计学、计算机科学、信息技术和特定领域</strong>。因此，可以使用几个不同的术语来指代同一个概念。关键术语及其同义词将在本书中通过如下所示的侧边栏进行突出显示。</p><ul><li>这个元素表示提示或建议。</li><li>这个元素表示一般性注释。</li><li>这个元素表示警告或注意事项。</li></ul><h3 id="使用代码示例"><strong>使用代码示例</strong></h3><p>在所有情况下，本书都<strong>先给出 R 语言的代码示例，然后是 Python</strong>。为了避免不必要的重复，我们通常只展示由 R 代码产生的输出和图表。我们还省略了加载所需包和数据集的代码。您可以在 <a href="https://github.com/gedeck/practical-statistics-for-data-scientists">https://github.com/gedeck/practical-statistics-for-data-scientists</a> 下载完整的代码和数据集。</p><p>本书旨在帮助您完成工作。通常，如果本书提供了示例代码，您可以在您的程序和文档中使用它。除非您要复制很大一部分代码，否则无需联系我们获得许可。例如，编写一个使用本书中几个代码片段的程序不需要许可。但销售或分发来自 O'Reilly 书籍的示例需要许可。通过引用本书并引用示例代码来回答问题不需要许可。将本书中的大量示例代码整合到您的产品文档中则需要许可。</p><p>我们感激您的署名，但并非强制要求。署名通常包括书名、作者、出版商和 ISBN。例如：“《Practical Statistics for Data Scientists》由 Peter Bruce、Andrew Bruce 和 Peter Gedeck（O'Reilly）著。版权所有 2020 Peter Bruce、Andrew Bruce 和 Peter Gedeck，978-1-492-07294-2。”</p><p>如果您觉得您对代码示例的使用超出了合理使用或上述许可范围，请随时通过 permissions@oreilly.com 联系我们。</p><h3 id="oreilly-在线学习"><strong>O’Reilly 在线学习</strong></h3><p>40多年来，O’Reilly Media 一直提供技术和商业培训、知识和见解，以帮助公司取得成功。</p><p>我们独特的专家和创新者网络通过书籍、文章和我们的在线学习平台分享他们的知识和专长。O’Reilly 的在线学习平台为您提供按需访问的直播培训课程、深入的学习路径、交互式编码环境以及来自 O’Reilly 和200多家其他出版商的大量文本和视频。欲了解更多信息，请访问 <a href="http://oreilly.com">http://oreilly.com</a>。</p><h3 id="如何联系我们"><strong>如何联系我们</strong></h3><p>有关本书的意见和问题，请联系出版商：</p><p>O’Reilly Media, Inc. 1005 Gravenstein Highway North Sebastopol, CA 95472 800-998-9938（美国或加拿大境内） 707-829-0515（国际或本地） 707-829-0104（传真）</p><p>我们为本书设立了一个网页，其中列出了勘误表、示例和任何其他信息。您可以在 <a href="https://oreil.ly/practicalStats_dataSci_2e">https://oreil.ly/practicalStats_dataSci_2e</a> 访问该页面。</p><p>发送电子邮件至 bookquestions@oreilly.com，对本书发表评论或提出技术问题。</p><p>如需了解有关我们书籍和课程的新闻及更多信息，请访问我们的网站 <a href="http://oreilly.com">http://oreilly.com</a>。</p><p>在 Facebook 上找到我们：<a href="http://facebook.com/oreilly">http://facebook.com/oreilly</a> 在 Twitter 上关注我们：<a href="http://twitter.com/oreillymedia">http://twitter.com/oreillymedia</a> 在 YouTube 上观看我们：<a href="http://www.youtube.com/oreillymedia">http://www.youtube.com/oreillymedia</a></p><h3 id="致谢"><strong>致谢</strong></h3><p>作者们感谢许多帮助本书成为现实的人。</p><p>数据挖掘公司 Elder Research 的首席执行官 Gerhard Pilcher 审阅了本书的早期草稿，并给予了我们详细而有益的修正和评论。同样，SAS 的统计学家 Anya McGuirk 和 Wei Xiao，以及 O’Reilly 的作者同事 Jay Hilfiger，也对本书的初稿提供了有益的反馈。将第一版翻译成日文的 Toshiaki Kurokawa 在此过程中进行了全面的审阅和纠正。Aaron Schumacher 和 Walter Paczkowski 全面审阅了本书的第二版，并提供了许多有益和宝贵的建议，我们对此表示由衷的感谢。毋庸置疑，任何遗留的错误都由我们独自承担。</p><p>在 O’Reilly，Shannon Cutt 以愉悦的心情和恰当的催促引导我们完成了出版过程，而 Kristen Brown 则顺利地将我们的书带入了制作阶段。Rachel Monaghan 和 Eliahu Sussman 小心翼翼、耐心细致地纠正和改进了我们的写作，而 Ellen Troutman-Zaig 则负责编制了索引。Nicole Tache 负责了第二版的编辑工作，她不仅有效地指导了整个过程，还提供了许多优秀的编辑建议，以提高本书对广大读者的可读性。我们还要感谢 Marie Beaugureau，她在 O’Reilly 发起了我们的项目，以及 Ben Bengfort，O’Reilly 的作者和 Statistics.com 的讲师，是他将我们介绍给了 O’Reilly。</p><p>我们和本书也受益于 Peter 多年来与 Galit Shmueli（另一本图书项目的合著者）进行的多次交谈。</p><p>最后，我们特别感谢 Elizabeth Bruce 和 Deborah Donnell，她们的耐心和支持使这项工作成为可能。</p><h2 id="第1章-探索性数据分析"><strong>第1章 探索性数据分析</strong></h2><p>Exploratory Data Analysis</p><p>本章重点介绍任何数据科学项目的第一步：<strong>探索数据</strong>。</p><p>经典的统计学几乎完全专注于<strong>推论（inference）</strong>，这是一套有时很复杂的程序，用于基于小样本对大总体得出结论。1962年，John W. Tukey（图1-1）在他那篇开创性论文《数据分析的未来》[Tukey-1962]中呼吁对统计学进行改革。他提出了一门名为<strong>数据分析</strong>的新科学学科，其中<strong>统计推论</strong>仅作为其一个组成部分。Tukey 与工程和计算机科学界建立了联系（他创造了<strong>bit</strong>，即<strong>binary digit</strong>的缩写，以及 <strong>software</strong> 等术语），他的原始原则出人意料地持久，并构成了数据科学的基础之一。1977年，Tukey 凭借其如今已成为经典的著作<strong>《探索性数据分析》</strong>[Tukey-1977]奠定了<strong>探索性数据分析</strong>这一领域。Tukey 提出了简单的图表（例如，箱线图、散点图），这些图表与汇总统计量（均值、中位数、分位数等）一起，有助于描绘数据集的画面。</p><figure><img src="/img3/面向数据科学家的实用统计学/F1.1.png" alt="F1.1" /><figcaption aria-hidden="true">F1.1</figcaption></figure><p>随着计算能力和富有表现力的数据分析软件的普及，探索性数据分析已经远远超出了其最初的范围。推动这一学科发展的关键因素是<strong>新技术的快速发展</strong>、<strong>获取更多和更大数据的能力</strong>以及在各种学科中<strong>更广泛地使用定量分析</strong>。斯坦福大学统计学教授、Tukey 的前本科生 David Donoho，根据他在新泽西州普林斯顿举行的 Tukey 百年纪念研讨会上的演讲撰写了一篇出色的文章 [Donoho-2015]。Donoho 将数据科学的起源追溯到 Tukey 在数据分析方面的开创性工作。</p><h3 id="结构化数据的元素"><strong>结构化数据的元素</strong></h3><p>Elements of Structured Data</p><p>数据来自许多来源：传感器测量、事件、文本、图像和视频。物联网（IoT）正在喷涌出信息流。这些数据中的大部分是<strong>非结构化</strong>的：图像是像素的集合，每个像素都包含 RGB（红、绿、蓝）颜色信息。文本是单词和非单词字符的序列，通常按章节、小节等组织。点击流是用户与应用程序或网页交互时的一系列动作。事实上，数据科学的一个主要挑战就是将这种原始数据的洪流转化为<strong>可操作的信息</strong>。为了应用本书涵盖的统计概念，必须对非结构化的原始数据进行处理和操作，使其成为<strong>结构化形式</strong>。结构化数据最常见的形式之一是<strong>带有行和列的表格</strong>——就像可能从关系数据库中出现或为研究而收集的数据一样。</p><p>结构化数据有两种基本类型：<strong>数值型</strong>和<strong>类别型</strong>。<strong>数值型数据</strong>有两种形式：<strong>连续型</strong>，例如风速或持续时间；<strong>离散型</strong>，例如事件发生的次数。<strong>类别型数据</strong>只取一个固定的值集，例如电视屏幕的类型（等离子、LCD、LED等）或州名（阿拉巴马州、阿拉斯加州等）。<strong>二元数据</strong>是类别型数据的一个重要特例，它只取两个值中的一个，例如0/1、是/否或真/假。另一种有用的类别型数据是<strong>有序数据（ordinal data）</strong>，其中的类别是有序的；一个例子是数值评级（1、2、3、4或5）。</p><p>我们为什么要费心对数据类型进行分类呢？事实证明，出于<strong>数据分析</strong>和<strong>预测建模</strong>的目的，数据类型对于帮助确定<strong>视觉显示</strong>、<strong>数据分析</strong>或<strong>统计模型</strong>的类型非常重要。事实上，R 和 Python 等数据科学软件利用这些数据类型来提高计算性能。更重要的是，变量的数据类型决定了软件将如何处理该变量的计算。</p><p><strong>数据类型的关键术语</strong></p><ul><li><strong>数值型（Numeric）</strong> 在数值尺度上表达的数据。</li><li><strong>连续型（Continuous）</strong> 可以在一个区间内取任意值的数据。（同义词：<code>interval</code>（区间）、<code>float</code>（浮点））</li><li><strong>离散型（Discrete）</strong> 只能取整数值的数据，例如计数。（同义词：<code>integer</code>（整数）、<code>count</code>（计数））</li><li><strong>类别型（Categorical）</strong> 只能取一组代表可能类别的特定值的数据。（同义词：<code>enums</code>（枚举）、<code>enumerated</code>（枚举）、<code>factors</code>（因子）、<code>nominal</code>（名义））</li><li><strong>二元型（Binary）</strong> 类别型数据的一个特例，只有两个类别的值，例如 0/1、真/假。（同义词：<code>dichotomous</code>（二分）、<code>logical</code>（逻辑）、<code>indicator</code>（指示符）、<code>boolean</code>（布尔））</li><li><strong>有序型（Ordinal）</strong> 具有明确排序的类别型数据。（同义词：<code>ordered factor</code>（有序因子））</li></ul><p>软件工程师和数据库程序员可能会想，我们为什么还需要为分析引入<strong>类别型</strong>和<strong>有序型</strong>数据的概念。毕竟，类别仅仅是文本（或数值）值的集合，底层数据库会自动处理其内部表示。然而，将数据明确标识为类别型（与文本区分开来）确实提供了一些优势：</p><ul><li>知道数据是类别型可以作为一个信号，告诉软件如何执行统计过程，例如生成图表或拟合模型。特别是，有序数据在 R 中可以表示为 <code>ordered.factor</code>，在图表、表格和模型中保留用户指定的顺序。在 Python 中，<code>scikit-learn</code> 通过 <code>sklearn.preprocessing.OrdinalEncoder</code> 支持有序数据。</li><li>存储和索引可以得到优化（如在关系数据库中）。</li><li>给定类别变量可能取的值被强制在软件中（类似于枚举 <code>enum</code>）。</li></ul><p>第三个“好处”可能会导致意外或意料之外的行为：R 中数据导入函数（例如 <code>read.csv</code>）的默认行为是自动将文本列转换为因子 <code>factor</code>。之后对该列的操作将假定该列唯一允许的值是最初导入的值，并且分配一个新的文本值将引发警告并产生一个 <code>NA</code>（缺失值）。Python 中的 <code>pandas</code> 包不会自动进行这种转换。但是，您可以在 <code>read_csv</code> 函数中显式指定一个列为类别型。</p><p><strong>关键思想</strong></p><ul><li>在软件中，数据通常按<strong>类型</strong>进行分类。</li><li>数据类型包括<strong>数值型</strong>（连续型、离散型）和<strong>类别型</strong>（二元型、有序型）。</li><li>软件中的数据类型作为<strong>一个信号</strong>，告诉软件如何处理数据。</li></ul><h3 id="矩形数据"><strong>矩形数据</strong></h3><p>Rectangular Data</p><p>在数据科学中，典型的分析参考框架是一个<strong>矩形数据</strong>对象，就像电子表格或数据库表一样。</p><p><strong>矩形数据</strong>是一个通用术语，指代一个二维矩阵，其中<strong>行</strong>表示<strong>记录</strong>（案例），<strong>列</strong>表示<strong>特征</strong>（变量）；<strong>数据框（data frame）</strong>是 R 和 Python 中特有的格式。数据并非总是以这种形式开始：非结构化数据（例如文本）必须经过处理和操作，才能以一组特征的形式在矩形数据中表示（参见第2页的“结构化数据的元素”）。关系数据库中的数据必须被提取并放入单个表中，以进行大多数数据分析和建模任务。</p><p><strong>矩形数据的关键术语</strong></p><ul><li><strong>数据框（Data frame）</strong> 矩形数据（如电子表格）是统计和机器学习模型的基本数据结构。</li><li><strong>特征（Feature）</strong> 表中的一列通常被称为特征。 <strong>同义词</strong>：<code>attribute</code>（属性）、<code>input</code>（输入）、<code>predictor</code>（预测变量）、<code>variable</code>（变量）</li><li><strong>结果（Outcome）</strong> 许多数据科学项目涉及预测一个结果——通常是<strong>是/否</strong>的结果（在表1-1中，它是“拍卖是否具有竞争力”）。在实验或研究中，特征有时被用来预测结果。 <strong>同义词</strong>：<code>dependent variable</code>（因变量）、<code>response</code>（响应）、<code>target</code>（目标）、<code>output</code>（输出）</li><li><strong>记录（Records）</strong> 表中的一行通常被称为记录。 <strong>同义词</strong>：<code>case</code>（案例）、<code>example</code>（例子）、<code>instance</code>（实例）、<code>observation</code>（观察）、<code>pattern</code>（模式）、<code>sample</code>（样本）</li></ul><p><img src="/img3/面向数据科学家的实用统计学//T1.1.png" alt="T1.1" style="zoom:70%;" /></p><p>在表1-1中，混合了测量或计数的数据（例如持续时间和价格）和类别数据（例如类别和货币）。如前所述，类别变量的一种特殊形式是<strong>二元（是/否或0/1）变量</strong>，如表1-1最右边的列所示——一个指示变量，显示拍卖是否具有竞争力（有多个竞标者）。当场景是预测拍卖是否具有竞争力时，这个指示变量也恰好是一个<strong>结果变量</strong>。</p><h4 id="数据框和索引"><strong>数据框和索引</strong></h4><p>Data Frames and Indexes</p><p>传统数据库表有一个或多个列被指定为<strong>索引</strong>，本质上是一个行号。这可以极大地提高某些数据库查询的效率。在 Python 中，使用 <strong>pandas</strong> 库，基本的矩形数据结构是 <strong>DataFrame</strong> 对象。默认情况下，DataFrame 会根据行的顺序创建一个自动整数索引。在 pandas 中，也可以设置多级/分层索引以提高某些操作的效率。</p><p>在 R 中，基本的矩形数据结构是 <code>data.frame</code> 对象。<code>data.frame</code> 也有一个基于行顺序的隐式整数索引。原生的 R <code>data.frame</code> 不支持用户指定的或多级索引，但可以通过 <code>row.names</code> 属性创建自定义键。为了克服这一不足，两个新的包正在被广泛使用：<code>data.table</code> 和 <code>dplyr</code>。两者都支持多级索引，并在处理 <code>data.frame</code> 时提供了显著的速度提升。</p><blockquote><p>警告：</p><p>术语差异 <strong>Terminology Differences</strong></p><p>矩形数据的术语可能令人困惑。统计学家和数据科学家对同一事物使用不同的术语。对于统计学家来说，<code>predictor variables</code>（预测变量）用于模型中以预测 <code>response</code> 或 <code>dependent variable</code>（响应或因变量）。对于数据科学家来说，<code>features</code>（特征）用于预测 <code>target</code>（目标）。有一个同义词尤其令人困惑：计算机科学家将 <code>sample</code>（样本）这个术语用于单行；而对于统计学家来说，<code>sample</code> 意味着多行的集合。</p></blockquote><h4 id="非矩形数据结构"><strong>非矩形数据结构</strong></h4><p>Nonrectangular Data Structures</p><p>除了矩形数据之外，还有其他数据结构。</p><p><strong>时间序列数据</strong>记录同一变量的连续测量值。它是统计预测方法的原始材料，也是由设备（物联网）产生的数据的关键组成部分。</p><p><strong>空间数据结构</strong>用于地图和位置分析，比矩形数据结构更复杂、种类更多。在<strong>对象表示</strong>中，数据的焦点是一个<strong>对象</strong>（例如，一栋房子）及其空间坐标。相比之下，<strong>字段视图</strong>则侧重于<strong>小的空间单元</strong>和相关度量的值（例如，像素亮度）。</p><p><strong>图（或网络）数据结构</strong>用于表示物理、社交和抽象关系。例如，像 Facebook 或 LinkedIn 这样的社交网络图可以表示网络中人与人之间的联系。由道路连接的分销枢纽是物理网络的一个例子。图结构对于某些类型的问题很有用，例如网络优化和推荐系统。</p><p>每种数据类型在数据科学中都有其专门的方法。本书的重点是<strong>矩形数据</strong>，它是<strong>预测建模的基本构建块</strong>。</p><blockquote><p>警告：</p><p><strong>统计学中的“图”</strong> Graphs in Statistics在计算机科学和信息技术中，“图（graph）”这个术语通常指实体之间连接的描绘，以及底层的数据结构。而在统计学中，“图”则用于指各种<strong>图表和可视化</strong>，不仅仅是实体之间的连接，并且该术语仅适用于<strong>可视化</strong>，而不适用于数据结构。</p></blockquote><p><strong>关键思想</strong></p><ul><li>数据科学中的基本数据结构是一个<strong>矩形矩阵</strong>，其中行是记录，列是变量（特征）。</li><li>术语可能令人困惑；由于对数据科学做出贡献的不同学科（统计学、计算机科学和信息技术），存在各种同义词。</li></ul><h3 id="位置估计"><strong>位置估计</strong></h3><p>Estimates of Location</p><p>测量或计数的数据变量可能具有数千个不同的值。探索数据的一个基本步骤是获取每个<strong>特征</strong>（变量）的“<strong>典型值</strong>”：对大多数数据所在位置的估计（即其<strong>中心趋势</strong>）。</p><p><strong>位置估计的关键术语</strong></p><ul><li><strong>均值（Mean）</strong> 所有值的总和除以值的数量。 <strong>同义词</strong>：<code>average</code>（平均值）</li><li><strong>加权平均值（Weighted mean）</strong> 所有值乘以一个权重，再除以所有权重的总和。 <strong>同义词</strong>：<code>weighted average</code>（加权平均值）</li><li><strong>中位数（Median）</strong> 数据中有一半值在其之上，一半值在其之下的那个值。 <strong>同义词</strong>：<code>50th percentile</code>（第50百分位数）</li><li><strong>百分位数（Percentile）</strong> 数据中有 <span class="math inline">\(P\)</span> 百分比的值在其之下的那个值。 <strong>同义词</strong>：<code>quantile</code>（分位数）</li><li><strong>加权中位数（Weighted median）</strong> 排序数据中，权重的总和有一半在其之上，一半在其之下的那个值。</li><li><strong>截尾均值（Trimmed mean）</strong> 在去除固定数量的极端值后，所有值的平均值。 <strong>同义词</strong>：<code>truncated mean</code>（截断均值）</li><li><strong>稳健（Robust）</strong> 对极端值不敏感。 <strong>同义词</strong>：<code>resistant</code>（有抵抗力）</li><li><strong>异常值（Outlier）</strong> 与大部分数据非常不同的数据值。 <strong>同义词</strong>：<code>extreme value</code>（极端值）</li></ul><p>乍一看，汇总数据似乎相当简单：只需计算数据的均值。然而，尽管均值易于计算且方便使用，但它可能并非总是衡量中心值的最佳方法。因此，统计学家开发并推广了几个替代均值的估计量。</p><blockquote><p>通用注解：<strong>度量与估计</strong> Metrics and Estimates 统计学家经常使用“<strong>估计</strong>（estimate）”一词来指代从现有数据中计算出的值，以区分我们从数据中看到的内容与理论上的真实或精确状态。数据科学家和业务分析师更倾向于将此类值称为“<strong>度量</strong>（metric）”。这种差异反映了统计学与数据科学各自的方法：<strong>考虑不确定性是统计学学科的核心</strong>，而<strong>具体的业务或组织目标是数据科学的焦点</strong>。因此，<strong>统计学家进行估计，而数据科学家进行度量</strong>。</p></blockquote><h4 id="均值"><strong>均值</strong></h4><p>Mean</p><p>最基本的位置估计量是<strong>均值（mean）</strong>，或称平均值。均值是<strong>所有值的总和除以值的数量</strong>。考虑以下这组数字：<span class="math inline">\(\{3, 5, 1, 2\}\)</span>。均值为 <span class="math inline">\((3 + 5 + 1 + 2) / 4 = 11 / 4 = 2.75\)</span>。您会看到符号 <span class="math inline">\(\bar{x}\)</span>（读作“x-bar”）用于表示从总体中抽取的样本的均值。计算一组 <span class="math inline">\(n\)</span> 个值 <span class="math inline">\(x_1, x_2, \dots, x_n\)</span> 的均值的公式是：</p><p><span class="math display">\[\text{Mean} = \bar{x} = \frac{\sum_{i=1}^{n} x_i}{n}\]</span></p><blockquote><p>通用注解：</p><p><span class="math inline">\(N(或 n)\)</span>指代记录或观察的总数。在统计学中，如果指的是总体，则大写；如果指的是从总体中抽取的样本，则小写。在数据科学中，这种区别并不重要，因此您可能会看到这两种用法。</p></blockquote><p>均值的一个变体是<strong>截尾均值（trimmed mean）</strong>，其计算方法是<strong>先剔除排序后两端固定数量的值</strong>，然后对剩余的值求平均。用 <span class="math inline">\(x_{(1)}, x_{(2)}, \dots, x_{(n)}\)</span> 表示排序后的值，其中 <span class="math inline">\(x_{(1)}\)</span> 是最小值，<span class="math inline">\(x_{(n)}\)</span> 是最大值，计算剔除 <span class="math inline">\(p\)</span> 个最小和最大值的截尾均值的公式是：</p><p><span class="math display">\[\text{Trimmed mean} = \bar{x}_{t} = \frac{\sum_{i=p+1}^{n-p} x_{(i)}}{n-2p}\]</span></p><p>截尾均值<strong>消除了极端值的影响</strong>。例如，在国际跳水比赛中，会去掉五名裁判的最高分和最低分，最终得分是剩余三名裁判的平均分。这使得单个裁判难以操纵分数，可能为了偏袒他们本国的选手。截尾均值被广泛使用，在许多情况下优于使用普通均值——有关进一步讨论，请参见第10页的“中位数和稳健估计”。</p><p>另一种类型的均值是<strong>加权平均值（weighted mean）</strong>，其计算方法是<strong>将每个数据值 <span class="math inline">\(x_i\)</span> 乘以用户指定的权重 <span class="math inline">\(w_i\)</span>，然后将它们的总和除以权重的总和</strong>。加权平均值的公式是：</p><p><span class="math display">\[\text{Weighted mean} = \bar{x}_w = \frac{\sum_{i=1}^{n} w_i x_i}{\sum_{i=1}^{n} w_i}\]</span></p><p>使用加权平均值有两个主要动机：</p><ul><li><strong>一些值的内在变异性比其他值更大</strong>，变异性大的观测值被赋予较低的权重。例如，如果我们从多个传感器取平均值，其中一个传感器的精度较低，那么我们可能会降低该传感器数据的权重。</li><li><strong>收集到的数据不能平等地代表我们感兴趣的不同群体</strong>。例如，由于在线实验的进行方式，我们可能没有一组数据能够准确反映用户群中的所有群体。为了纠正这一点，我们可以给那些代表性不足的群体的值赋予更高的权重。</li></ul><h4 id="中位数和稳健估计"><strong>中位数和稳健估计</strong></h4><p>Median and Robust Estimates</p><p><strong>中位数</strong>是<strong>排序数据列表中的中间数</strong>。如果数据值的数量是偶数，中间值并不是数据集中实际存在的某个值，而是将排序数据分为上半部分和下半部分的<strong>两个值的平均值</strong>。与使用所有观察值的均值相比，中位数仅依赖于排序数据中心的值。虽然这看似是一个缺点，因为均值对数据更为敏感，但在许多情况下，中位数是衡量位置的更好指标。比如说，我们想看看西雅图华盛顿湖周围社区的典型家庭收入。在比较 Medina 社区和 Windermere 社区时，如果使用均值会产生非常不同的结果，因为比尔·盖茨住在 Medina。如果我们使用中位数，比尔·盖茨有多富有都无关紧要——<strong>中间观察值的位置将保持不变</strong>。</p><p>出于使用加权均值的原因，也可以计算<strong>加权中位数</strong>。与中位数一样，我们首先对数据进行排序，尽管每个数据值都有一个相关的权重。加权中位数不是中间的数字，而是<strong>一个值，使得排序列表中较低和较高部分权重的总和相等</strong>。像中位数一样，加权中位数对异常值是稳健的。</p><p><strong>异常值</strong></p><p>Outliers</p><p>中位数被称为<strong>位置的稳健估计</strong>，因为它不受可能使结果扭曲的<strong>异常值（极端情况）</strong>的影响。<strong>异常值</strong>是数据集中与大多数其他值非常不同的任何值。异常值的确切定义有些主观，尽管在各种数据摘要和图表中使用了某些惯例（参见第20页的“百分位数和箱线图”）。作为异常值本身并不意味着数据值无效或有误（如比尔·盖茨的例子）。尽管如此，异常值通常是由于数据错误造成的，例如混合了不同单位的数据（千米与米）或传感器读数不良。当异常值是糟糕数据的结果时，均值将导致一个糟糕的位置估计，而<strong>中位数仍然是有效的</strong>。无论如何，异常值都应该被识别出来，并且通常值得进一步调查。</p><blockquote><p>通用注解：<strong>异常检测</strong>Anomaly Detection</p><p>与典型的数据分析不同，在典型数据分析中，异常值有时具有信息性，有时是麻烦，而在<strong>异常检测</strong>中，感兴趣的点就是<strong>异常值</strong>，而大部分数据主要用于定义<strong>“正常”</strong>，以此来衡量异常。</p></blockquote><p>中位数不是唯一的位置稳健估计。事实上，<strong>截尾均值</strong>被广泛用于避免异常值的影响。例如，截去数据底部和顶部的10%（一个常见的选择）将为除了最小的数据集之外的所有数据集提供对抗异常值的保护。截尾均值可以被认为是中位数和均值之间的一种折衷：它对数据中的极端值是稳健的，但使用更多的数据来计算位置估计。</p><blockquote><p>知识点：</p><p><strong>其他位置的稳健度量</strong> Other Robust Metrics for Location</p><p>统计学家已经开发了大量其他用于位置的估计量，其主要目标是开发比均值更稳健、也更有效（即，更能辨别数据集之间微小位置差异）的估计量。虽然这些方法对于小型数据集可能有用，但对于大型或甚至中等大小的数据集，它们不太可能提供额外的好处。</p></blockquote><h4 id="示例"><strong>示例</strong></h4><p>人口和谋杀率的位置估计</p><p>Example: Location Estimates of Population and Murder Rates</p><p><img src="/img3/面向数据科学家的实用统计学//T1.2.png" alt="T1.2" style="zoom:67%;" /></p><p>表1-2显示了数据集中前几行，其中包含美国各州的人口和谋杀率（单位为每年每10万人中的谋杀案数）（2010年人口普查）。</p><p>使用 R 计算人口的均值、截尾均值和中位数：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">&gt;</span> state <span class="operator">&lt;-</span> read.csv<span class="punctuation">(</span><span class="string">&#x27;state.csv&#x27;</span><span class="punctuation">)</span></span><br><span class="line"><span class="operator">&gt;</span> mean<span class="punctuation">(</span>state<span class="punctuation">[[</span><span class="string">&#x27;Population&#x27;</span><span class="punctuation">]</span><span class="punctuation">]</span><span class="punctuation">)</span></span><br><span class="line"><span class="punctuation">[</span><span class="number">1</span><span class="punctuation">]</span> <span class="number">6162876</span></span><br><span class="line"><span class="operator">&gt;</span> mean<span class="punctuation">(</span>state<span class="punctuation">[[</span><span class="string">&#x27;Population&#x27;</span><span class="punctuation">]</span><span class="punctuation">]</span><span class="punctuation">,</span> trim<span class="operator">=</span><span class="number">0.1</span><span class="punctuation">)</span></span><br><span class="line"><span class="punctuation">[</span><span class="number">1</span><span class="punctuation">]</span> <span class="number">4783697</span></span><br><span class="line"><span class="operator">&gt;</span> median<span class="punctuation">(</span>state<span class="punctuation">[[</span><span class="string">&#x27;Population&#x27;</span><span class="punctuation">]</span><span class="punctuation">]</span><span class="punctuation">)</span></span><br><span class="line"><span class="punctuation">[</span><span class="number">1</span><span class="punctuation">]</span> <span class="number">4436370</span></span><br></pre></td></tr></table></figure><p>要在 Python 中计算均值和中位数，我们可以使用数据框的 pandas 方法。截尾均值需要 <code>scipy.stats</code> 中的 <code>trim_mean</code> 函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">state = pd.read_csv(<span class="string">&#x27;state.csv&#x27;</span>)</span><br><span class="line">state[<span class="string">&#x27;Population&#x27;</span>].mean()</span><br><span class="line">trim_mean(state[<span class="string">&#x27;Population&#x27;</span>], <span class="number">0.1</span>)</span><br><span class="line">state[<span class="string">&#x27;Population&#x27;</span>].median()</span><br></pre></td></tr></table></figure><p>均值<strong>大于</strong>截尾均值，截尾均值<strong>大于</strong>中位数。这是因为截尾均值<strong>排除了最大和最小的五个州</strong>（<code>trim=0.1</code> 从每一端剔除10%）。如果我们想计算<strong>全国的平均谋杀率</strong>，我们需要使用<strong>加权均值或中位数</strong>来考虑各州的不同人口。由于基础 R 没有加权中位数的函数，我们需要安装一个像 <code>matrixStats</code> 这样的包：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">&gt;</span> weighted.mean<span class="punctuation">(</span>state<span class="punctuation">[[</span><span class="string">&#x27;Murder.Rate&#x27;</span><span class="punctuation">]</span><span class="punctuation">]</span><span class="punctuation">,</span> w<span class="operator">=</span>state<span class="punctuation">[[</span><span class="string">&#x27;Population&#x27;</span><span class="punctuation">]</span><span class="punctuation">]</span><span class="punctuation">)</span></span><br><span class="line"><span class="punctuation">[</span><span class="number">1</span><span class="punctuation">]</span> <span class="number">4.445834</span></span><br><span class="line"><span class="operator">&gt;</span> library<span class="punctuation">(</span><span class="string">&#x27;matrixStats&#x27;</span><span class="punctuation">)</span></span><br><span class="line"><span class="operator">&gt;</span> weightedMedian<span class="punctuation">(</span>state<span class="punctuation">[[</span><span class="string">&#x27;Murder.Rate&#x27;</span><span class="punctuation">]</span><span class="punctuation">]</span><span class="punctuation">,</span> w<span class="operator">=</span>state<span class="punctuation">[[</span><span class="string">&#x27;Population&#x27;</span><span class="punctuation">]</span><span class="punctuation">]</span><span class="punctuation">)</span></span><br><span class="line"><span class="punctuation">[</span><span class="number">1</span><span class="punctuation">]</span> <span class="number">4.4</span></span><br></pre></td></tr></table></figure><p>加权均值可以通过 NumPy 获得。对于加权中位数，我们可以使用专门的包 <code>wquantiles</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">np.average(state[<span class="string">&#x27;Murder.Rate&#x27;</span>], weights=state[<span class="string">&#x27;Population&#x27;</span>])</span><br><span class="line">wquantiles.median(state[<span class="string">&#x27;Murder.Rate&#x27;</span>], weights=state[<span class="string">&#x27;Population&#x27;</span>])</span><br></pre></td></tr></table></figure><p>在这种情况下，加权均值和加权中位数<strong>大致相同</strong>。</p><p><strong>关键思想</strong></p><ul><li>位置的基本度量是<strong>均值</strong>，但它可能对<strong>极端值（异常值）敏感</strong>。</li><li>其他度量（<strong>中位数、截尾均值</strong>）对异常值和不寻常的分布<strong>不太敏感</strong>，因此<strong>更具稳健性</strong>。</li></ul><h3 id="变异性估计"><strong>变异性估计</strong></h3><p>Estimates of Variability</p><p><strong>位置</strong>只是概括一个特征的一个维度。第二个维度，<strong>变异性</strong>（也称为<strong>离散度</strong>），衡量数据值是<strong>紧密聚集</strong>还是<strong>分散开来</strong>。统计学的核心在于<strong>变异性</strong>：测量它、减少它、区分随机变异与真实变异、识别真实变异的各种来源，并在变异存在的情况下做出决策。</p><p><strong>变异性度量的关键术语</strong></p><ul><li><strong>偏差（Deviations）</strong> 观察值与位置估计值之间的差。 <strong>同义词</strong>：<code>errors</code>（误差）、<code>residuals</code>（残差）</li><li><strong>方差（Variance）</strong> 观察值与均值之间偏差的平方和，除以 <span class="math inline">\(n-1\)</span>，其中 <span class="math inline">\(n\)</span> 是数据值的数量。 <strong>同义词</strong>：<code>mean-squared-error</code>（均方误差）</li><li><strong>标准差（Standard deviation）</strong> 方差的平方根。</li><li><strong>平均绝对偏差（Mean absolute deviation）</strong> 观察值与均值之间偏差的绝对值的均值。 <strong>同义词</strong>：<code>l1-norm</code>（L1范数）、<code>Manhattan norm</code>（曼哈顿范数）</li><li><strong>中位数绝对偏差（Median absolute deviation from the median）</strong> 观察值与中位数之间偏差的绝对值的中位数。</li><li><strong>极差（Range）</strong> 数据集中最大值和最小值之间的差。</li><li><strong>顺序统计量（Order statistics）</strong> 基于从最小到最大排序的数据值计算的度量。 <strong>同义词</strong>：<code>ranks</code>（秩）</li><li><strong>百分位数（Percentile）</strong> 数据中 <span class="math inline">\(P\)</span> 百分比的值取这个值或更小，<span class="math inline">\((100-P)\)</span> 百分比的值取这个值或更大的那个值。 <strong>同义词</strong>：<code>quantile</code>（分位数）</li><li><strong>四分位距（Interquartile range）</strong> 第75百分位数和第25百分位数之间的差。 <strong>同义词</strong>：<code>IQR</code></li></ul><p>正如衡量位置有不同的方法（均值、中位数等），衡量变异性也有不同的方法。</p><h4 id="标准差和相关估计"><strong>标准差和相关估计</strong></h4><p>Standard Deviation and Related Estimates</p><p>最广泛使用的变异性估计是基于<strong>位置估计</strong>与<strong>观测数据</strong>之间的<strong>差异</strong>或<strong>偏差</strong>。对于一组数据 <span class="math inline">\(\{1, 4, 4\}\)</span>，均值为3，中位数为4。与均值的偏差是：<span class="math inline">\(1 - 3 = -2\)</span>，<span class="math inline">\(4 - 3 = 1\)</span>，<span class="math inline">\(4 - 3 = 1\)</span>。这些偏差告诉我们数据围绕中心值的离散程度。</p><p>衡量变异性的一种方法是<strong>估计这些偏差的典型值</strong>。对偏差本身求平均值不会告诉我们太多——负偏差会抵消正偏差。事实上，与均值的偏差总和正好为零。相反，一个简单的方法是取<strong>与均值偏差的绝对值的平均值</strong>。在前面的例子中，偏差的绝对值是 <span class="math inline">\(\{2, 1, 1\}\)</span>，它们的平均值是 <span class="math inline">\((2 + 1 + 1) / 3 = 1.33\)</span>。这被称为<strong>平均绝对偏差</strong>，其计算公式为：</p><p><span class="math display">\[\text{平均绝对偏差} = \frac{\sum_{i=1}^{n} |x_i - \bar{x}|}{n}\]</span></p><p>其中 <span class="math inline">\(\bar{x}\)</span> 是样本均值。</p><p>最著名的变异性估计是<strong>方差</strong>和<strong>标准差</strong>，它们基于<strong>平方偏差</strong>。方差是平方偏差的平均值，标准差是方差的平方根：</p><p><span class="math display">\[\text{方差} = s^2 = \frac{\sum_{i=1}^{n} (x_i - \bar{x})^2}{n-1}\]</span> <span class="math display">\[\text{标准差} = s = \sqrt{\text{方差}}\]</span></p><p>标准差比方差更容易解释，因为它与原始数据处于<strong>相同的尺度</strong>。然而，由于其更复杂、更不直观的公式，统计学中对标准差的偏爱胜过平均绝对偏差，这可能看起来很奇特。它之所以享有盛誉，归功于<strong>统计学理论</strong>：在数学上，使用平方值比使用绝对值方便得多，尤其是对于统计模型。</p><p><strong>自由度，以及 <span class="math inline">\(n\)</span> 还是 <span class="math inline">\(n-1\)</span>？</strong></p><p>在统计学书籍中，总会讨论为什么方差公式的分母是 <span class="math inline">\(n-1\)</span> 而不是 <span class="math inline">\(n\)</span>，这引出了<strong>自由度</strong>的概念。这种区别并不重要，因为 <span class="math inline">\(n\)</span> 通常足够大，以至于除以 <span class="math inline">\(n\)</span> 或 <span class="math inline">\(n-1\)</span> 都没有太大区别。但如果您感兴趣，这里是原因。它基于一个前提：您想根据<strong>样本</strong>对<strong>总体</strong>进行估计。</p><p>如果您在方差公式中使用直观的分母 <span class="math inline">\(n\)</span>，您将<strong>低估</strong>总体中方差和标准差的真实值。这被称为<strong>有偏估计（biased estimate）</strong>。然而，如果您用 <span class="math inline">\(n-1\)</span> 而不是 <span class="math inline">\(n\)</span> 来除，方差就变成了<strong>无偏估计（unbiased estimate）</strong>。</p><p>要完全解释为什么使用 <span class="math inline">\(n\)</span> 会导致有偏估计，需要<strong>自由度</strong>的概念，它考虑了计算估计量时的约束数量。在这种情况下，有 <span class="math inline">\(n-1\)</span> 个自由度，因为存在一个约束：<strong>标准差的计算依赖于样本均值</strong>。对于大多数问题，数据科学家<strong>不需要担心自由度</strong>。</p><p>无论是方差、标准差还是平均绝对偏差，它们都<strong>不稳健</strong>，对异常值和极端值敏感（有关位置的稳健估计的讨论，请参见第10页的“中位数和稳健估计”）。方差和标准差由于基于<strong>平方偏差</strong>，对异常值尤其敏感。</p><p><strong>变异性</strong>的一个<strong>稳健估计</strong>是<strong>中位数绝对偏差（MAD）</strong>：</p><p><span class="math display">\[\text{中位数绝对偏差} = \text{Median}\{|x_1 - m|, |x_2 - m|, \dots, |x_N - m|\}\]</span></p><p>其中 <span class="math inline">\(m\)</span> 是中位数。像中位数一样，MAD<strong>不受极端值的影响</strong>。也可以计算类似于截尾均值的<strong>截尾标准差</strong>（参见第9页的“均值”）。</p><blockquote><p>通用注解：</p><p>方差、标准差、平均绝对偏差和中位数绝对偏差即使在数据来自正态分布的情况下，也<strong>不是等价的估计量</strong>。事实上，标准差总是大于平均绝对偏差，而平均绝对偏差本身又大于中位数绝对偏差。有时，中位数绝对偏差会乘以一个常数<strong>比例因子</strong>，以便在正态分布的情况下，将 MAD 放在与标准差相同的尺度上。常用的因子是1.4826，这意味着50%的正态分布落在 ±MAD 的范围内（例如，参见 <a href="https://oreil.ly/SfDk2">https://oreil.ly/SfDk2</a>）。</p></blockquote><h4 id="基于百分位数的估计"><strong>基于百分位数的估计</strong></h4><p>Estimates Based on Percentiles</p><p>另一种估计<strong>离散度</strong>的方法是查看<strong>排序数据</strong>的<strong>扩展（spread）</strong>。基于排序（排名）数据的统计量被称为<strong>顺序统计量（order statistics）</strong>。最基本的度量是<strong>极差（range）</strong>：最大值和最小值之间的差。最小值和最大值本身很有用，有助于识别异常值，但<strong>极差</strong>对异常值<strong>极其敏感</strong>，作为数据离散度的通用度量<strong>用处不大</strong>。</p><p>为了避免对异常值的敏感性，我们可以查看<strong>剔除两端值后</strong>数据的极差。形式上，这些类型的估计基于<strong>百分位数之间的差</strong>。在一个数据集中，<strong>第 <span class="math inline">\(P\)</span> 个百分位数</strong>是一个值，<strong>至少有 <span class="math inline">\(P\)</span> 百分比的值取该值或更小，并且至少有 <span class="math inline">\((100-P)\)</span> 百分比的值取该值或更大</strong>。例如，要找到第80个百分位数，请对数据进行排序。然后，从最小值开始，前进到<strong>距离最大值80%的位置</strong>。请注意，<strong>中位数</strong>与<strong>第50个百分位数</strong>是同一回事。百分位数本质上与<strong>分位数（quantile）</strong>相同，分位数用小数索引（因此 .8 分位数与第80百分位数相同）。</p><p>一个常见的变异性度量是<strong>第25百分位数和第75百分位数之间的差</strong>，称为<strong>四分位距（interquartile range，IQR）</strong>。这里有一个简单的例子：<span class="math inline">\(\{3,1,5,3,6,7,2,9\}\)</span>。我们对其进行排序得到 <span class="math inline">\(\{1,2,3,3,5,6,7,9\}\)</span>。第25百分位数在2.5处，第75百分位数在6.5处，因此四分位距为 <span class="math inline">\(6.5 - 2.5 = 4\)</span>。软件可能有略微不同的方法，会产生不同的答案（参见下面的提示）；通常，这些差异很小。</p><p>对于<strong>非常大的数据集</strong>，计算<strong>精确的百分位数</strong>在计算上可能非常昂贵，因为它需要对所有数据值进行排序。机器学习和统计软件使用特殊算法，例如 [Zhang-Wang-2007]，来获得可以非常快速计算并保证具有一定准确度的<strong>近似百分位数</strong>。</p><blockquote><p>知识点：</p><p><strong>百分位数：精确定义</strong> Percentile: Precise Definition</p><p>如果我们的数据数量是偶数（<span class="math inline">\(n\)</span> 是偶数），那么根据前面的定义，百分位数是<strong>模糊的</strong>。事实上，在满足以下条件的 <span class="math inline">\(j\)</span> 的情况下，我们可以取<strong>任何介于顺序统计量 <span class="math inline">\(x_{(j)}\)</span> 和 <span class="math inline">\(x_{(j+1)}\)</span> 之间的值</strong>： <span class="math display">\[ \frac{100 \cdot j}{n} \le P &lt; \frac{100 \cdot (j+1)}{n} \]</span></p><p>正式地，百分位数是<strong>加权平均值</strong>：</p><p><span class="math display">\[ \text{百分位数 } P = (1 - w)x_{(j)} + wx_{(j+1)} \]</span></p><p>其中 <span class="math inline">\(w\)</span> 是介于0和1之间的一些权重。统计软件在选择 <span class="math inline">\(w\)</span> 的方法上略有不同。事实上，R 函数 <code>quantile</code> 提供了<strong>九种</strong>不同的计算分位数的方法。除了小型数据集，您通常<strong>不需要担心百分位数的确切计算方式</strong>。在撰写本文时，Python 的 <code>numpy.quantile</code> <strong>仅支持</strong>一种方法，即<strong>线性插值</strong>。</p></blockquote><h4 id="示例-1"><strong>示例</strong></h4><p>州人口的变异性估计</p><p>Example: Variability Estimates of State Population</p><p><img src="/img3/面向数据科学家的实用统计学//T1.3.png" alt="T1.3" style="zoom:67%;" /></p><p>表1-3（为方便起见重复了表1-2）显示了数据集中包含每个州的人口和谋杀率的前几行。</p><p>使用 R 的内置函数来计算标准差（<code>sd</code>）、四分位距（<code>IQR</code>）和中位数绝对偏差（<code>mad</code>），我们可以计算州人口数据的变异性估计：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">&gt;</span> sd<span class="punctuation">(</span>state<span class="punctuation">[[</span><span class="string">&#x27;Population&#x27;</span><span class="punctuation">]</span><span class="punctuation">]</span><span class="punctuation">)</span></span><br><span class="line"><span class="punctuation">[</span><span class="number">1</span><span class="punctuation">]</span> <span class="number">6848235</span></span><br><span class="line"><span class="operator">&gt;</span> IQR<span class="punctuation">(</span>state<span class="punctuation">[[</span><span class="string">&#x27;Population&#x27;</span><span class="punctuation">]</span><span class="punctuation">]</span><span class="punctuation">)</span></span><br><span class="line"><span class="punctuation">[</span><span class="number">1</span><span class="punctuation">]</span> <span class="number">4847308</span></span><br><span class="line"><span class="operator">&gt;</span> mad<span class="punctuation">(</span>state<span class="punctuation">[[</span><span class="string">&#x27;Population&#x27;</span><span class="punctuation">]</span><span class="punctuation">]</span><span class="punctuation">)</span></span><br><span class="line"><span class="punctuation">[</span><span class="number">1</span><span class="punctuation">]</span> <span class="number">3849870</span></span><br></pre></td></tr></table></figure><p>pandas 数据框提供了计算标准差和分位数的方法。使用分位数，我们可以轻松确定 IQR。对于稳健的 MAD，我们使用 <code>statsmodels</code> 包中的 <code>robust.scale.mad</code> 函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">state[<span class="string">&#x27;Population&#x27;</span>].std()</span><br><span class="line">state[<span class="string">&#x27;Population&#x27;</span>].quantile(<span class="number">0.75</span>) - state[<span class="string">&#x27;Population&#x27;</span>].quantile(<span class="number">0.25</span>)</span><br><span class="line">robust.scale.mad(state[<span class="string">&#x27;Population&#x27;</span>])</span><br></pre></td></tr></table></figure><p>标准差几乎是 MAD 的两倍（在 R 中，默认情况下，MAD 的尺度被调整为与均值在同一尺度上）。这并不奇怪，因为<strong>标准差对异常值很敏感</strong>。</p><p><strong>关键思想</strong></p><ul><li><strong>方差</strong>和<strong>标准差</strong>是<strong>最广泛使用</strong>和<strong>常规报告</strong>的变异性统计量。</li><li>两者都<strong>对异常值敏感</strong>。</li><li>更<strong>稳健</strong>的度量包括<strong>平均绝对偏差</strong>、<strong>中位数绝对偏差</strong>和<strong>百分位数（分位数）</strong>。</li></ul><h3 id="探索数据分布"><strong>探索数据分布</strong></h3><p>Exploring the Data Distribution</p><p>我们所涵盖的每一个估计量都将数据汇总为一个单一数字，来描述数据的位置或变异性。探索数据的<strong>整体分布</strong>也很有用。</p><p><strong>探索分布的关键术语</strong></p><ul><li><strong>箱线图（Boxplot）</strong> Tukey 提出的一种图，作为快速可视化数据分布的方法。 <strong>同义词</strong>：<code>box and whiskers plot</code>（盒须图）</li><li><strong>频率表（Frequency table）</strong> 落入一组区间（<strong>箱</strong>）的数值数据的计数。</li><li><strong>直方图（Histogram）</strong> 频率表的图，其中<strong>箱</strong>在 x 轴上，<strong>计数</strong>（或比例）在 y 轴上。虽然在视觉上相似，但<strong>条形图不应与直方图混淆</strong>。有关差异的讨论，请参见第27页的“探索二元和类别数据”。</li><li><strong>密度图（Density plot）</strong> 直方图的平滑版本，通常基于<strong>核密度估计</strong>。</li></ul><h4 id="百分位数和箱线图"><strong>百分位数和箱线图</strong></h4><p>Percentiles and Boxplots</p><p>在第16页的“基于百分位数的估计”中，我们探讨了百分位数如何用于衡量数据的<strong>离散度</strong>。百分位数对于<strong>汇总整个分布</strong>也很有价值。通常会报告<strong>四分位数</strong>（第25、50和75百分位数）和<strong>十分位数</strong>（第10、20、...、90百分位数）。百分位数对于<strong>汇总分布的尾部</strong>（外部范围）特别有价值。流行文化创造了“百分之一的人”（one-percenters）这个术语来指代那些财富处于第99百分位数的人。</p><p><img src="/img3/面向数据科学家的实用统计学//T1.4.png" alt="T1.4" style="zoom:70%;" /></p><p>表1-4显示了按州划分的谋杀率的一些百分位数。在 R 中，这将由 <code>quantile</code> 函数生成：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">quantile<span class="punctuation">(</span>state<span class="punctuation">[[</span><span class="string">&#x27;Murder.Rate&#x27;</span><span class="punctuation">]</span><span class="punctuation">]</span><span class="punctuation">,</span> p<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">.05</span><span class="punctuation">,</span> <span class="number">.25</span><span class="punctuation">,</span> <span class="number">.5</span><span class="punctuation">,</span> <span class="number">.75</span><span class="punctuation">,</span> <span class="number">.95</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">   5%   25%   50%   75%   95% </span><br><span class="line">1.600 2.425 4.000 5.550 6.510 </span><br></pre></td></tr></table></figure><p>pandas 数据框方法 <code>quantile</code> 在 Python 中提供此功能：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">state[<span class="string">&#x27;Murder.Rate&#x27;</span>].quantile([<span class="number">0.05</span>, <span class="number">0.25</span>, <span class="number">0.5</span>, <span class="number">0.75</span>, <span class="number">0.95</span>])</span><br></pre></td></tr></table></figure><p>中位数是每10万人中有4起谋杀案，尽管存在相当大的变异性：第5百分位数仅为1.6，而第95百分位数是6.51。</p><p><img src="/img3/面向数据科学家的实用统计学//F1.2.png" alt="F1.2" style="zoom:50%;" /></p><p>由 Tukey [Tukey-1977] 引入的<strong>箱线图</strong>基于百分位数，提供了一种快速可视化数据分布的方法。图1-2显示了由 R 生成的按州划分的人口箱线图：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">boxplot<span class="punctuation">(</span>state<span class="punctuation">[[</span><span class="string">&#x27;Population&#x27;</span><span class="punctuation">]</span><span class="punctuation">]</span><span class="operator">/</span><span class="number">1000000</span><span class="punctuation">,</span> ylab<span class="operator">=</span><span class="string">&#x27;Population (millions)&#x27;</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>pandas 为数据框提供了许多基本的探索性绘图；其中之一就是箱线图：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ax = (state[<span class="string">&#x27;Population&#x27;</span>]/<span class="number">1_000_000</span>).plot.box()</span><br><span class="line">ax.set_ylabel(<span class="string">&#x27;Population (millions)&#x27;</span>)</span><br></pre></td></tr></table></figure><p>从这个箱线图我们可以立即看到，州人口中位数约为500万，一半的州人口介于约200万到约700万之间，并且存在一些人口<strong>异常值</strong>。箱子的顶部和底部分别是<strong>第75和第25百分位数</strong>。<strong>中位数</strong>由箱子中的水平线显示。虚线，被称为<strong>须（whiskers）</strong>，从箱子的顶部和底部延伸，表示大部分数据的范围。箱线图有许多变体；例如，参见 R 函数 <code>boxplot</code> 的文档 [R-base-2015]。默认情况下，R 函数将须延伸到箱子以外的最远点，但不会超过 <strong>1.5倍的 IQR</strong>。Matplotlib 使用相同的实现；其他软件可能使用不同的规则。</p><p>须线之外的任何数据都作为<strong>单个点或圆圈</strong>绘制（通常被认为是<strong>异常值</strong>）。</p><h4 id="频率表和直方图"><strong>频率表和直方图</strong></h4><p>Frequency Tables and Histograms</p><p><img src="/img3/面向数据科学家的实用统计学//T1.5.png" alt="T1.5" style="zoom:50%;" /></p><p>变量的<strong>频率表</strong>将变量范围划分为<strong>等距的片段</strong>，并告诉我们有多少个值落入每个片段中。表1-5显示了在 R 中计算的按州划分的人口频率表：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">breaks <span class="operator">&lt;-</span> seq<span class="punctuation">(</span>from<span class="operator">=</span><span class="built_in">min</span><span class="punctuation">(</span>state<span class="punctuation">[[</span><span class="string">&#x27;Population&#x27;</span><span class="punctuation">]</span><span class="punctuation">]</span><span class="punctuation">)</span><span class="punctuation">,</span></span><br><span class="line">to<span class="operator">=</span><span class="built_in">max</span><span class="punctuation">(</span>state<span class="punctuation">[[</span><span class="string">&#x27;Population&#x27;</span><span class="punctuation">]</span><span class="punctuation">]</span><span class="punctuation">)</span><span class="punctuation">,</span> <span class="built_in">length</span><span class="operator">=</span><span class="number">11</span><span class="punctuation">)</span></span><br><span class="line">pop_freq <span class="operator">&lt;-</span> cut<span class="punctuation">(</span>state<span class="punctuation">[[</span><span class="string">&#x27;Population&#x27;</span><span class="punctuation">]</span><span class="punctuation">]</span><span class="punctuation">,</span> breaks<span class="operator">=</span>breaks<span class="punctuation">,</span></span><br><span class="line">right<span class="operator">=</span><span class="literal">TRUE</span><span class="punctuation">,</span> include.lowest<span class="operator">=</span><span class="literal">TRUE</span><span class="punctuation">)</span></span><br><span class="line">table<span class="punctuation">(</span>pop_freq<span class="punctuation">)</span></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">(563626,4232659] (4232659,7901692] (7901692,11570725] </span><br><span class="line">               24                13                 6 </span><br><span class="line">(11570725,15239758] (15239758,18908791] (18908791,22577824] </span><br><span class="line">                 2                  1                  2 </span><br><span class="line">(22577824,26246857] (26246857,29915890] (29915890,33584923] </span><br><span class="line">                 1                  1                  0 </span><br><span class="line">(33584923,37253956] </span><br><span class="line">                 1 </span><br></pre></td></tr></table></figure><p><code>pandas.cut</code> 函数创建一个系列，将值映射到这些片段。使用 <code>value_counts</code> 方法，我们得到频率表：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">binnedPopulation = pd.cut(state[<span class="string">&#x27;Population&#x27;</span>], <span class="number">10</span>)</span><br><span class="line">binnedPopulation.value_counts()</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">(526935.67, 4232659.0]     24</span><br><span class="line">(4232659.0, 7901692.0]     13</span><br><span class="line">(7901692.0, 11570725.0]     6</span><br><span class="line">(15239758.0, 18908791.0]     2</span><br><span class="line">(18908791.0, 22577824.0]     2</span><br><span class="line">(11570725.0, 15239758.0]     1</span><br><span class="line">(33584923.0, 37253956.0]     1</span><br><span class="line">(22577824.0, 26246857.0]     1</span><br><span class="line">(26246857.0, 29915890.0]     1</span><br><span class="line">(29915890.0, 33584923.0]     0</span><br><span class="line">Name: Population, dtype: int64</span><br></pre></td></tr></table></figure><p>人口最少的州是怀俄明州，有563,626人，人口最多的州是加利福尼亚州，有37,253,956人。这给了我们一个 <span class="math inline">\(37,253,956 - 563,626 = 36,690,330\)</span> 的范围，我们必须将其划分为<strong>大小相等的箱</strong>——假设有10个箱。对于10个大小相等的箱，每个箱的宽度将是 <span class="math inline">\(3,669,033\)</span>，因此第一个箱的范围将是从563,626到4,232,658。相比之下，最顶部的箱，从33,584,923到37,253,956，只有一个州：加利福尼亚。加利福尼亚下面的两个箱是空的，直到我们到达德克萨斯州。重要的是要<strong>包括空箱</strong>；这些箱中没有值的事实是有用的信息。用不同大小的箱进行实验也很有用。如果箱太大，分布的重要特征可能会被掩盖。如果箱太小，结果会过于精细，从而失去看到大局的能力。</p><blockquote><p>通用注解：</p><p>频率表和百分位数都通过创建<strong>箱</strong>来概括数据。一般来说，四分位数和十分位数的每个箱中将有<strong>相同的计数</strong>（等计数箱），但<strong>箱的大小会不同</strong>。相反，频率表的箱中将有<strong>不同的计数</strong>（等大小箱），而<strong>箱的大小将相同</strong>。</p></blockquote><p><strong>直方图</strong>是一种可视化频率表的方法，其中<strong>箱</strong>在 x 轴上，<strong>数据计数</strong>在 y 轴上。例如，在图1-3中，以1000万（1e+07）为中心的箱的范围大约是从800万到1200万，并且该箱中有<strong>六个州</strong>。</p><p>为了在 R 中创建与表1-5对应的直方图，使用 <code>hist</code> 函数并带 <code>breaks</code> 参数：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hist<span class="punctuation">(</span>state<span class="punctuation">[[</span><span class="string">&#x27;Population&#x27;</span><span class="punctuation">]</span><span class="punctuation">]</span><span class="punctuation">,</span> breaks<span class="operator">=</span>breaks<span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>pandas 使用 <code>DataFrame.plot.hist</code> 方法支持数据框的直方图。使用关键字参数 <code>bins</code> 来定义箱的数量。各种绘图方法返回一个 <code>axis</code> 对象，可以使用 Matplotlib 对可视化进行进一步的微调：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ax = (state[<span class="string">&#x27;Population&#x27;</span>] / <span class="number">1_000_000</span>).plot.hist(figsize=(<span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">ax.set_xlabel(<span class="string">&#x27;Population (millions)&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="/img3/面向数据科学家的实用统计学//F1.3.png" alt="F1.3" style="zoom:50%;" /></p><p>直方图如图1-3所示。通常，直方图的绘制方式使得：</p><ul><li>图中包含<strong>空箱</strong>。</li><li>箱子的<strong>宽度相等</strong>。</li><li>箱子的数量（或者等价地，箱子的大小）由<strong>用户决定</strong>。</li><li>条形是<strong>连续的</strong>——条形之间没有空隙，除非有空箱。</li></ul><blockquote><p>知识点：</p><p><strong>统计矩</strong> Statistical Moments在统计理论中，位置和变异性分别被称为分布的<strong>第一和第二矩</strong>。第三和第四矩分别被称为<strong>偏度（skewness）和峰度（kurtosis）</strong>。偏度指的是数据是否偏向于更大或更小的值，而峰度则表示数据具有极端值的倾向。通常，不使用度量来测量偏度和峰度；相反，这些是通过视觉显示来发现的，例如图1-2和图1-3。</p></blockquote><h4 id="密度图和密度估计"><strong>密度图和密度估计</strong></h4><p>Density Plots and Estimates</p><p><img src="/img3/面向数据科学家的实用统计学//F1.4.png" alt="F1.4" style="zoom:50%;" /></p><p>与直方图相关的是<strong>密度图</strong>，它以<strong>连续线</strong>的形式显示数据值的分布。密度图可以被认为是<strong>平滑的直方图</strong>，尽管它通常通过<strong>核密度估计</strong>直接从数据计算得到（有关简短教程，请参见 [Duong-2001]）。图1-4展示了一个叠加在直方图上的密度估计。在 R 中，您可以使用 <code>density</code> 函数计算密度估计：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hist<span class="punctuation">(</span>state<span class="punctuation">[[</span><span class="string">&#x27;Murder.Rate&#x27;</span><span class="punctuation">]</span><span class="punctuation">]</span><span class="punctuation">,</span> freq<span class="operator">=</span><span class="literal">FALSE</span><span class="punctuation">)</span></span><br><span class="line">lines<span class="punctuation">(</span>density<span class="punctuation">(</span>state<span class="punctuation">[[</span><span class="string">&#x27;Murder.Rate&#x27;</span><span class="punctuation">]</span><span class="punctuation">]</span><span class="punctuation">)</span><span class="punctuation">,</span> lwd<span class="operator">=</span><span class="number">3</span><span class="punctuation">,</span> col<span class="operator">=</span><span class="string">&#x27;blue&#x27;</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p><code>pandas</code> 提供了 <code>density</code> 方法来创建密度图。使用参数 <code>bw_method</code> 来控制密度曲线的平滑度：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ax = state[<span class="string">&#x27;Murder.Rate&#x27;</span>].plot.hist(density=<span class="literal">True</span>, xlim=[<span class="number">0</span>,<span class="number">12</span>], bins=<span class="built_in">range</span>(<span class="number">1</span>,<span class="number">12</span>))</span><br><span class="line">state[<span class="string">&#x27;Murder.Rate&#x27;</span>].plot.density(ax=ax)</span><br><span class="line">ax.set_xlabel(<span class="string">&#x27;Murder Rate (per 100,000)&#x27;</span>)</span><br></pre></td></tr></table></figure><blockquote><p>绘图函数通常接受一个可选的轴（<code>ax</code>）参数，这会使绘图被添加到同一张图上。</p></blockquote><p>与图1-3中绘制的直方图的一个关键区别是<strong>y轴的刻度</strong>：密度图对应于将直方图绘制为<strong>比例</strong>而不是<strong>计数</strong>（在 R 中，您可以使用参数 <code>freq=FALSE</code> 指定这一点）。请注意，<strong>密度曲线下的总面积=1</strong>，并且您计算的是曲线下<strong>任意两点之间的面积</strong>，而不是箱中的计数，这对应于分布中介于这两点之间的<strong>比例</strong>。</p><blockquote><p>知识点：</p><p><strong>密度估计</strong> Density Estimation</p><p>密度估计是一个丰富的课题，在统计文献中有悠久的历史。事实上，已经有超过20个 R 包发布，提供了用于密度估计的函数。[Deng-Wickham-2011] 对 R 包进行了全面综述，特别推荐了 <code>ASH</code> 或 <code>KernSmooth</code>。<code>pandas</code> 和 <code>scikit-learn</code> 中的密度估计方法也提供了很好的实现。对于许多数据科学问题，**没有必要担心各种类型的密度估计；使用基本函数就足够了。</p></blockquote><p><strong>关键思想</strong></p><ul><li><strong>频率直方图</strong>在 y 轴上绘制频率计数，在 x 轴上绘制变量值；它让人一目了然地了解数据的分布。</li><li><strong>频率表</strong>是直方图中频率计数的表格版本。</li><li><strong>箱线图</strong>——箱子的顶部和底部分别是第75和第25百分位数——也让人对数据分布有一个快速的了解；它经常用于并排显示以比较分布。</li><li><strong>密度图</strong>是直方图的平滑版本；它需要一个函数来基于数据估计一个图（当然，可能有多种估计方法）。</li></ul><h3 id="探索二元和类别数据"><strong>探索二元和类别数据</strong></h3><p>Exploring Binary and Categorical Data</p><p>对于<strong>类别数据</strong>，简单的<strong>比例或百分比</strong>就能说明数据的情况。</p><p><strong>探索类别数据的关键术语</strong></p><ul><li><strong>众数（Mode）</strong> 数据集中最常出现的类别或值。</li><li><strong>期望值（Expected value）</strong> 当类别可以与数值关联时，这表示<strong>基于类别发生概率的平均值</strong>。</li><li><strong>条形图（Bar charts）</strong> 将每个类别的频率或比例绘制成条形。</li><li><strong>饼图（Pie charts）</strong> 将每个类别的频率或比例绘制成饼状图中的扇形。</li></ul><p><img src="/img3/面向数据科学家的实用统计学//T1.6.png" alt="T1.6" style="zoom:67%;" /></p><p>获取一个二元变量或只有几个类别的类别变量的摘要是相当容易的：我们只需计算1的比例，或重要类别的比例。例如，表1-6显示了自2010年以来达拉斯/沃斯堡机场（Dallas/Fort Worth Airport）航班延误的百分比，按延误原因划分。延误被归类为由航空公司控制的因素、空中交通管制（ATC）系统延误、天气、安保或晚到的入港飞机。</p><p><img src="/img3/面向数据科学家的实用统计学//F1.5.png" alt="F1.5" style="zoom:33%;" /></p><p><strong>条形图</strong>在流行媒体中经常出现，是显示单个类别变量的常见可视化工具。类别列在x轴上，频率或比例列在y轴上。图1-5显示了达拉斯/沃斯堡（DFW）机场每年的延误情况，按原因划分，由R函数<code>barplot</code>生成：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">barplot<span class="punctuation">(</span>as.matrix<span class="punctuation">(</span>dfw<span class="punctuation">)</span> <span class="operator">/</span> <span class="number">6</span><span class="punctuation">,</span> cex.axis<span class="operator">=</span><span class="number">0.8</span><span class="punctuation">,</span> cex.names<span class="operator">=</span><span class="number">0.7</span><span class="punctuation">,</span></span><br><span class="line">xlab<span class="operator">=</span><span class="string">&#x27;Cause of delay&#x27;</span><span class="punctuation">,</span> ylab<span class="operator">=</span><span class="string">&#x27;Count&#x27;</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>pandas也支持数据框的条形图：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ax = dfw.transpose().plot.bar(figsize=(<span class="number">4</span>, <span class="number">4</span>), legend=<span class="literal">False</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">&#x27;Cause of delay&#x27;</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&#x27;Count&#x27;</span>)</span><br></pre></td></tr></table></figure><p>请注意，<strong>条形图与直方图相似</strong>；在<strong>条形图</strong>中，x轴表示因子变量的不同<strong>类别</strong>，而在<strong>直方图</strong>中，x轴表示单个变量在<strong>数值尺度</strong>上的值。在<strong>直方图</strong>中，条形通常彼此<strong>接触</strong>，间隙表示数据中未出现的值。在<strong>条形图</strong>中，条形彼此<strong>分离</strong>。</p><p><strong>饼图</strong>是条形图的替代品，尽管统计学家和数据可视化专家通常<strong>避用饼图</strong>，因为它们在视觉上提供的信息较少（参见 [Few-2007]）。</p><blockquote><p>通用注解：</p><p><strong>将数值数据视为类别数据</strong></p><p>在第22页的“频率表和直方图”中，我们研究了基于<strong>分箱</strong>数据的频率表。这隐式地将数值数据转换为<strong>有序因子</strong>。从这个意义上讲，直方图和条形图是相似的，只是条形图x轴上的类别<strong>没有顺序</strong>。将数值数据转换为类别数据是数据分析中一个重要且广泛使用的步骤，因为它<strong>降低了数据的复杂性（和大小）</strong>。这有助于在分析的初始阶段发现特征之间的关系。</p></blockquote><h4 id="众数"><strong>众数</strong></h4><p>Mode</p><p><strong>众数</strong>是数据中出现<strong>最频繁的值</strong>，如果出现平局则为多个值。例如，达拉斯/沃斯堡机场航班延误原因的众数是“<strong>入港</strong>”。另一个例子是，在美国大部分地区，宗教信仰的众数将是<strong>基督教</strong>。众数是<strong>类别数据</strong>的一种简单汇总统计量，通常不用于<strong>数值数据</strong>。</p><h4 id="期望值"><strong>期望值</strong></h4><p>Expected Value</p><p>一种特殊类型的<strong>类别数据</strong>是<strong>其类别代表或可以映射到相同尺度上离散值的数据</strong>。例如，一家新云技术的营销人员提供两种服务级别，一种定价为每月300美元，另一种为每月50美元。营销人员提供免费网络研讨会以产生潜在客户，公司估计5%的参与者会注册300美元的服务，15%会注册50美元的服务，80%不会注册任何服务。出于财务目的，这些数据可以用一个单一的“<strong>期望值</strong>”来汇总，这是一种<strong>加权平均值</strong>的形式，其中<strong>权重是概率</strong>。</p><p>期望值的计算方法如下： 1. 将每个结果乘以其发生的概率。 2. 将这些值相加。</p><p>在云服务示例中，一个网络研讨会参与者的期望值是每月22.50美元，计算如下：</p><p><span class="math display">\[EV = 0.05 \times 300 + 0.15 \times 50 + 0.80 \times 0 = 22.5\]</span> 期望值实际上是<strong>加权平均值的一种形式</strong>：它加入了<strong>未来预期</strong>和<strong>概率权重</strong>的概念，这些概率权重通常基于主观判断。期望值是<strong>商业估值和资本预算</strong>中的一个基本概念——例如，新收购公司五年利润的期望值，或者诊所新患者管理软件的预期成本节约。</p><h4 id="概率"><strong>概率</strong></h4><p>Probability</p><p>我们上面提到了一个值发生的<strong>概率</strong>。大多数人对概率有直观的理解，经常在天气预报（下雨的可能性）或体育分析（获胜的概率）中遇到这个概念。体育和比赛更多地以<strong>赔率</strong>来表达，赔率可以很容易地转换为概率（如果一支球队获胜的赔率是2比1，其获胜的概率是 <span class="math inline">\(2/(2+1) = 2/3\)</span>）。然而，令人惊讶的是，当涉及到定义它时，概率的概念可能是深刻哲学讨论的来源。幸运的是，我们在这里不需要一个正式的数学或哲学定义。就我们的目的而言，一个事件发生的概率是<strong>如果情况可以重复无数次，它发生的次数所占的比例</strong>。这通常是一个想象中的构造，但它足以作为对概率的操作性理解。</p><p><strong>关键思想</strong></p><ul><li>类别数据通常用<strong>比例</strong>来汇总，并可以在<strong>条形图</strong>中可视化。</li><li>类别可能代表<strong>不同的事物</strong>（苹果和橙子、男性和女性）、<strong>因子变量的级别</strong>（低、中、高），或已被<strong>分箱的数值数据</strong>。</li><li><strong>期望值</strong>是<strong>值乘以其发生概率的总和</strong>，常用于汇总因子变量的级别。</li></ul><h3 id="相关性"><strong>相关性</strong></h3><p>Correlation</p><p>在许多建模项目（无论是数据科学还是研究）中，探索性数据分析都涉及<strong>检查预测变量之间</strong>以及<strong>预测变量与目标变量之间</strong>的<strong>相关性</strong>。如果变量 X 和 Y（每个都有测量数据）的高值伴随着高值，低值伴随着低值，则称它们<strong>正相关</strong>。如果变量 X 的高值伴随着 Y 的低值，反之亦然，则变量<strong>负相关</strong>。</p><p><strong>相关性的关键术语</strong></p><ul><li><strong>相关系数（Correlation coefficient）</strong> 衡量数值变量之间关联程度的度量（范围从 -1 到 +1）。</li><li><strong>相关矩阵（Correlation matrix）</strong> 一个表格，其中变量同时显示在行和列上，单元格中的值是变量之间的相关性。</li><li><strong>散点图（Scatterplot）</strong> 一个图，其中 x 轴是某个变量的值，y 轴是另一个变量的值。</li></ul><p>考虑这两个变量，它们在从低到高的意义上是<strong>完全相关</strong>的：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">v1: &#123;1, 2, 3&#125;</span><br><span class="line">v2: &#123;4, 5, 6&#125;</span><br></pre></td></tr></table></figure><p>它们的向量积之和是 <span class="math inline">\(1 \cdot 4 + 2 \cdot 5 + 3 \cdot 6 = 32\)</span>。现在尝试打乱其中一个并重新计算——向量积之和永远不会高于32。所以这个积之和可以用作一个度量；也就是说，观察到的总和32可以与大量随机打乱的结果进行比较（事实上，这个想法与基于重采样（resampling-based）的估计有关；参见第97页的“置换检验”）。然而，这个度量产生的值本身并没有太大意义，除非参考重采样分布。</p><p>更有用的是一个<strong>标准化变体</strong>：<strong>相关系数（correlation coefficient）</strong>，它提供了<strong>两个变量之间相关性的估计</strong>，并且始终在<strong>相同的尺度上</strong>。要计算<strong>皮尔逊相关系数（Pearson's correlation coefficient）</strong>，我们将变量1与均值的偏差乘以变量2与均值的偏差，然后除以标准差的乘积：</p><p><span class="math display">\[r = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{(n-1)s_x s_y}\]</span> 请注意，我们除以 <span class="math inline">\(n-1\)</span> 而不是 <span class="math inline">\(n\)</span>；有关更多详细信息，请参见第15页的“自由度，以及 n 还是 n-1？”。相关系数始终介于 +1（完全正相关）和 -1（完全负相关）之间；<strong>0表示没有相关性</strong>。</p><p>变量之间可能存在<strong>非线性</strong>关联，在这种情况下，相关系数可能不是一个有用的度量。税率和税收之间的关系就是一个例子：当税率从零增加时，税收也会增加。然而，一旦税率达到高水平并接近100%，逃税行为会增加，税收实际上会下降。</p><p><img src="/img3/面向数据科学家的实用统计学//T1.7.png" alt="T1.7" style="zoom:50%;" /></p><p>表1-7，被称为<strong>相关矩阵</strong>，显示了2012年7月至2015年6月期间电信股票日收益之间的相关性。从表中可以看出，威瑞森（VZ）和美国电话电报公司（T）的相关性最高。作为一家基础设施公司的 Level 3（LVLT），与其它公司的相关性最低。请注意对角线上的值都是1（一只股票与自身的相关性为1），以及对角线上方和下方的信息是冗余的。</p><p><img src="/img3/面向数据科学家的实用统计学//F1.6.png" alt="F1.6" style="zoom:50%;" /></p><p>像表1-7这样的相关表通常被绘制出来，以<strong>视觉化地展示多个变量之间的关系</strong>。图1-6显示了主要<strong>交易所交易基金（ETF）</strong>日收益之间的相关性。在R中，我们可以使用 <code>corrplot</code> 包轻松创建：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">etfs <span class="operator">&lt;-</span> sp500_px<span class="punctuation">[</span>row.names<span class="punctuation">(</span>sp500_px<span class="punctuation">)</span> <span class="operator">&gt;</span> <span class="string">&#x27;2012-07-01&#x27;</span><span class="punctuation">,</span></span><br><span class="line">sp500_sym<span class="punctuation">[</span>sp500_sym<span class="operator">$</span>sector <span class="operator">==</span> <span class="string">&#x27;etf&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;symbol&#x27;</span><span class="punctuation">]</span><span class="punctuation">]</span></span><br><span class="line">library<span class="punctuation">(</span>corrplot<span class="punctuation">)</span></span><br><span class="line">corrplot<span class="punctuation">(</span>cor<span class="punctuation">(</span>etfs<span class="punctuation">)</span><span class="punctuation">,</span> method<span class="operator">=</span><span class="string">&#x27;ellipse&#x27;</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>在Python中也可以创建相同的图，但常用包中没有现成的实现。然而，大多数都支持使用<strong>热图</strong>来可视化相关矩阵。以下代码演示了使用 <code>seaborn.heat map</code> 包的方法。在随附的源代码库中，我们包含了用于生成更全面的可视化的Python代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">etfs = sp500_px.loc[sp500_px.index &gt; <span class="string">&#x27;2012-07-01&#x27;</span>,</span><br><span class="line">sp500_sym[sp500_sym[<span class="string">&#x27;sector&#x27;</span>] == <span class="string">&#x27;etf&#x27;</span>][<span class="string">&#x27;symbol&#x27;</span>]]</span><br><span class="line">sns.heatmap(etfs.corr(), vmin=-<span class="number">1</span>, vmax=<span class="number">1</span>,</span><br><span class="line">cmap=sns.diverging_palette(<span class="number">20</span>, <span class="number">220</span>, as_cmap=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure><p>标准普尔500指数ETF（SPY）和道琼斯指数ETF（DIA）具有较高的相关性。同样，主要由科技公司组成的QQQ和XLK也呈正相关。防御性ETF，例如那些追踪黄金价格（GLD）、石油价格（USO）或市场波动率（VXX）的ETF，倾向于与其它ETF<strong>弱相关或负相关</strong>。椭圆的<strong>方向</strong>表明两个变量是<strong>正相关</strong>（椭圆指向右上）还是<strong>负相关</strong>（椭圆指向左上）。椭圆的<strong>阴影和宽度</strong>表示关联的<strong>强度</strong>：越薄、越深的椭圆对应着越强的关系。</p><p>与均值和标准差一样，<strong>相关系数对数据中的异常值敏感</strong>。软件包提供了<strong>经典相关系数的稳健替代品</strong>。例如，R包 <code>robust</code> 使用 <code>covRob</code> 函数来计算相关性的稳健估计。<code>scikit-learn</code> 模块 <code>sklearn.covariance</code> 中的方法实现了多种方法。</p><blockquote><p>知识点：</p><p><strong>其他相关性估计</strong></p><p>统计学家很久以前就提出了其他类型的相关系数，例如<strong>斯皮尔曼's <span class="math inline">\(\rho\)</span></strong>（rho）或<strong>肯德尔's <span class="math inline">\(\tau\)</span></strong>（tau）。这些是基于数据<strong>秩</strong>的相关系数。由于它们使用<strong>秩</strong>而不是<strong>值</strong>，这些估计量对<strong>异常值是稳健的</strong>，并且可以处理某些类型的<strong>非线性</strong>。然而，数据科学家通常可以坚持使用<strong>皮尔逊相关系数</strong>及其<strong>稳健替代品</strong>来进行探索性分析。基于秩的估计的吸引力主要在于<strong>较小的数据集</strong>和<strong>特定的假设检验</strong>。</p></blockquote><h4 id="散点图"><strong>散点图</strong></h4><p>Scatterplots</p><p>可视化两个测量数据变量之间关系的<strong>标准方法</strong>是使用<strong>散点图</strong>。x轴代表一个变量，y轴代表另一个变量，图上的每个点都是一个<strong>记录</strong>。图1-7显示了美国电话电报公司（ATT）和威瑞森（Verizon）日收益之间相关性的图。这在 R 中使用以下命令生成：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot<span class="punctuation">(</span>telecom<span class="operator">$</span><span class="built_in">T</span><span class="punctuation">,</span> telecom<span class="operator">$</span>VZ<span class="punctuation">,</span> xlab<span class="operator">=</span><span class="string">&#x27;ATT (T)&#x27;</span><span class="punctuation">,</span> ylab<span class="operator">=</span><span class="string">&#x27;Verizon (VZ)&#x27;</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>使用 pandas 的 <code>scatter</code> 方法可以在 Python 中生成相同的图：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ax = telecom.plot.scatter(x=<span class="string">&#x27;T&#x27;</span>, y=<span class="string">&#x27;VZ&#x27;</span>, figsize=(<span class="number">4</span>, <span class="number">4</span>), marker=<span class="string">&#x27;$\u25EF$&#x27;</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">&#x27;ATT (T)&#x27;</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&#x27;Verizon (VZ)&#x27;</span>)</span><br><span class="line">ax.axhline(<span class="number">0</span>, color=<span class="string">&#x27;grey&#x27;</span>, lw=<span class="number">1</span>)</span><br><span class="line">ax.axvline(<span class="number">0</span>, color=<span class="string">&#x27;grey&#x27;</span>, lw=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>收益之间存在<strong>正向关系</strong>：虽然它们聚集在零点附近，但在大多数日子里，两只股票同步上涨或下跌（右上和左下象限）。很少有一天是一只股票显著下跌而另一只股票上涨，反之亦然（右下和左上象限）。</p><p><img src="/img3/面向数据科学家的实用统计学//F1.7.png" alt="F1.7" style="zoom:50%;" /></p><p>虽然图1-7只显示了754个数据点，但很明显，要识别图中中心的细节有多么困难。我们将在后面看到，<strong>如何通过增加点的透明度，或使用六边形分箱和密度图来帮助发现数据中的额外结构</strong>。</p><p><strong>关键思想</strong></p><ul><li><strong>相关系数</strong>衡量两个成对变量（例如，个人的身高和体重）彼此关联的程度。</li><li>当 v1 的高值伴随着 v2 的高值时，v1 和 v2 <strong>正相关</strong>。</li><li>当 v1 的高值伴随着 v2 的低值时，v1 和 v2 <strong>负相关</strong>。</li><li><strong>相关系数是一个标准化度量</strong>，因此它始终在 <strong>-1</strong>（完全负相关）到 <strong>+1</strong>（完全正相关）之间。</li><li><strong>相关系数为零</strong>表示<strong>没有相关性</strong>，但要注意，数据的随机排列仅凭偶然性就会产生正值和负值的相关系数。</li></ul><h3 id="探索两个或多个变量"><strong>探索两个或多个变量</strong></h3><p>Exploring Two or More Variables</p><p>我们所熟悉的估计量，如均值和方差，每次都只考察一个变量（<strong>单变量分析</strong>）。<strong>相关性分析</strong>（参见第30页的“相关性”）是一种重要的比较<strong>两个变量</strong>（<strong>双变量分析</strong>）的方法。在本节中，我们将探讨额外的估计量和图表，以及超过两个变量的情况（<strong>多变量分析</strong>）。</p><p><strong>探索两个或多个变量的关键术语</strong></p><ul><li><strong>列联表（Contingency table）</strong> 两个或多个类别变量之间的计数。</li><li><strong>六边形分箱（Hexagonal binning）</strong> 一种图，将两个数值变量的记录分箱到六边形中。</li><li><strong>等高线图（Contour plot）</strong> 一种像地形图一样显示两个数值变量密度的图。</li><li><strong>小提琴图（Violin plot）</strong> 类似于箱线图，但显示的是密度估计。</li></ul><p>与单变量分析一样，双变量分析既包括计算汇总统计量，也包括生成可视化显示。适当的双变量或多变量分析类型取决于数据的性质：数值型还是类别型。</p><h4 id="六边形分箱和等高线图"><strong>六边形分箱和等高线图</strong></h4><p><strong>（绘制数值型 vs. 数值型数据）</strong></p><p>Hexagonal Binning and Contours</p><p>(Plotting Numeric Versus Numeric Data)</p><p>当数据值相对较少时，<strong>散点图</strong>很好用。图1-7中的股票收益图只涉及大约750个点。对于拥有数十万或数百万条记录的数据集，散点图会<strong>过于密集</strong>，因此我们需要一种不同的方式来可视化这种关系。为了说明这一点，考虑 <code>kc_tax</code> 数据集，其中包含华盛顿州金县住宅的税收评估价值。为了专注于数据的主要部分，我们使用 <code>subset</code> 函数去除了非常昂贵、非常小或非常大的住宅：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kc_tax0 <span class="operator">&lt;-</span> subset<span class="punctuation">(</span>kc_tax<span class="punctuation">,</span> TaxAssessedValue <span class="operator">&lt;</span> <span class="number">750000</span> <span class="operator">&amp;</span></span><br><span class="line">SqFtTotLiving <span class="operator">&gt;</span> <span class="number">100</span> <span class="operator">&amp;</span></span><br><span class="line">SqFtTotLiving <span class="operator">&lt;</span> <span class="number">3500</span><span class="punctuation">)</span></span><br><span class="line">nrow<span class="punctuation">(</span>kc_tax0<span class="punctuation">)</span></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[1] 432693</span><br></pre></td></tr></table></figure><p>在pandas中，我们按以下方式筛选数据集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kc_tax0 = kc_tax.loc[(kc_tax.TaxAssessedValue &lt; <span class="number">750000</span>) &amp;</span><br><span class="line">(kc_tax.SqFtTotLiving &gt; <span class="number">100</span>) &amp;</span><br><span class="line">(kc_tax.SqFtTotLiving &lt; <span class="number">3500</span>), :]</span><br><span class="line">kc_tax0.shape</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(432693, 3)</span><br></pre></td></tr></table></figure><p><img src="/img3/面向数据科学家的实用统计学//F1.8.png" alt="F1.8" style="zoom:70%;" /></p><p>图1-8是金县住宅的<strong>已完成居住面积</strong>和<strong>税收评估价值</strong>之间关系的<strong>六边形分箱图</strong>。我们没有绘制点（这会像一个整体的黑云），而是将记录分组到<strong>六边形箱</strong>中，并用颜色绘制这些六边形，<strong>颜色表示该箱中的记录数量</strong>。在这张图中，平方英尺和税收评估价值之间的正向关系是清晰的。一个有趣的特征是，在底部的主（最暗）带上方隐约可见额外的条带，这表明这些住宅与主带中的住宅具有相同的居住面积，但税收评估价值更高。</p><p>图1-8由 Hadley Wickham 开发的强大 R 包 <code>ggplot2</code> 生成 [ggplot2]。<code>ggplot2</code> 是几个用于数据高级探索性视觉分析的新软件库之一；请参见第43页的“可视化多个变量”：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ggplot<span class="punctuation">(</span>kc_tax0<span class="punctuation">,</span> <span class="punctuation">(</span>aes<span class="punctuation">(</span>x<span class="operator">=</span>SqFtTotLiving<span class="punctuation">,</span> y<span class="operator">=</span>TaxAssessedValue<span class="punctuation">)</span><span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">stat_binhex<span class="punctuation">(</span>color<span class="operator">=</span><span class="string">&#x27;white&#x27;</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">theme_bw<span class="punctuation">(</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">scale_fill_gradient<span class="punctuation">(</span>low<span class="operator">=</span><span class="string">&#x27;white&#x27;</span><span class="punctuation">,</span> high<span class="operator">=</span><span class="string">&#x27;black&#x27;</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">labs<span class="punctuation">(</span>x<span class="operator">=</span><span class="string">&#x27;Finished Square Feet&#x27;</span><span class="punctuation">,</span> y<span class="operator">=</span><span class="string">&#x27;Tax-Assessed Value&#x27;</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>在 Python 中，使用 pandas 数据框方法 <code>hexbin</code> 可以轻松获得六边形分箱图：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ax = kc_tax0.plot.hexbin(x=<span class="string">&#x27;SqFtTotLiving&#x27;</span>, y=<span class="string">&#x27;TaxAssessedValue&#x27;</span>,</span><br><span class="line">gridsize=<span class="number">30</span>, sharex=<span class="literal">False</span>, figsize=(<span class="number">5</span>, <span class="number">4</span>))</span><br><span class="line">ax.set_xlabel(<span class="string">&#x27;Finished Square Feet&#x27;</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&#x27;Tax-Assessed Value&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="/img3/面向数据科学家的实用统计学//F1.9.png" alt="F1.9" style="zoom:50%;" /></p><p>图1-9使用<strong>等高线</strong>叠加在散点图上，以可视化两个数值变量之间的关系。这些等高线本质上是两个变量的<strong>地形图</strong>；每个等高线带代表一个特定的<strong>点密度</strong>，当靠近“峰值”时密度会增加。这张图显示了与图1-8类似的情况：在主峰值的“北方”有一个次要峰值。这张图也是使用 <code>ggplot2</code> 和内置的 <code>geom_density2d</code> 函数创建的：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ggplot<span class="punctuation">(</span>kc_tax0<span class="punctuation">,</span> aes<span class="punctuation">(</span>SqFtTotLiving<span class="punctuation">,</span> TaxAssessedValue<span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">theme_bw<span class="punctuation">(</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">geom_point<span class="punctuation">(</span>alpha<span class="operator">=</span><span class="number">0.1</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">geom_density2d<span class="punctuation">(</span>color<span class="operator">=</span><span class="string">&#x27;white&#x27;</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">labs<span class="punctuation">(</span>x<span class="operator">=</span><span class="string">&#x27;Finished Square Feet&#x27;</span><span class="punctuation">,</span> y<span class="operator">=</span><span class="string">&#x27;Tax-Assessed Value&#x27;</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>Python中的 <code>seaborn</code> 包 <code>kdeplot</code> 函数创建等高线图：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ax = sns.kdeplot(kc_tax0.SqFtTotLiving, kc_tax0.TaxAssessedValue, ax=ax)</span><br><span class="line">ax.set_xlabel(<span class="string">&#x27;Finished Square Feet&#x27;</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&#x27;Tax-Assessed Value&#x27;</span>)</span><br></pre></td></tr></table></figure><p>还有其他类型的图表用于显示两个数值变量之间的关系，包括<strong>热图</strong>。热图、六边形分箱和等高线图都提供了<strong>二维密度</strong>的视觉表示。从这个意义上说，它们是<strong>直方图和密度图的自然类比</strong>。</p><h4 id="两个类别变量"><strong>两个类别变量</strong></h4><p>Two Categorical Variables</p><p><img src="/img3/面向数据科学家的实用统计学//T1.8.png" alt="T1.8" style="zoom:50%;" /></p><p>汇总两个类别变量的一种有用方法是<strong>列联表</strong>（contingency table）——一个按类别划分的<strong>计数表</strong>。表1-8显示了个人贷款的<strong>评级</strong>和该贷款的<strong>结果</strong>之间的列联表。这取自点对点借贷行业的领导者 Lending Club 提供的数据。评级从A（高）到G（低）。结果要么是<strong>全额偿付（fully paid）</strong>、<strong>当前（current）</strong>、<strong>逾期（late）</strong>，要么是<strong>核销（charged off）</strong>（不期望收回贷款余额）。该表显示了<strong>计数</strong>和<strong>行百分比</strong>。与低评级贷款相比，高评级贷款的逾期/核销百分比非常低。</p><p>列联表可以只看<strong>计数</strong>，也可以包括<strong>列和总百分比</strong>。Excel中的<strong>数据透视表</strong>可能是用于创建列联表的最常用工具。在 R 中，<code>descr</code> 包中的 <code>CrossTable</code> 函数可以生成列联表，以下代码用于创建表1-8：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">library<span class="punctuation">(</span>descr<span class="punctuation">)</span></span><br><span class="line">x_tab <span class="operator">&lt;-</span> CrossTable<span class="punctuation">(</span>lc_loans<span class="operator">$</span>grade<span class="punctuation">,</span> lc_loans<span class="operator">$</span>status<span class="punctuation">,</span></span><br><span class="line">prop.c<span class="operator">=</span><span class="literal">FALSE</span><span class="punctuation">,</span> prop.chisq<span class="operator">=</span><span class="literal">FALSE</span><span class="punctuation">,</span> prop.t<span class="operator">=</span><span class="literal">FALSE</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p><code>pivot_table</code> 方法在 Python 中创建数据透视表。<code>aggfunc</code> 参数允许我们获得计数。计算百分比稍微复杂一些：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">crosstab = lc_loans.pivot_table(index=<span class="string">&#x27;grade&#x27;</span>, columns=<span class="string">&#x27;status&#x27;</span>,</span><br><span class="line">aggfunc=<span class="keyword">lambda</span> x: <span class="built_in">len</span>(x), margins=<span class="literal">True</span>)</span><br><span class="line">df = crosstab.loc[<span class="string">&#x27;A&#x27;</span>:<span class="string">&#x27;G&#x27;</span>,:].copy()</span><br><span class="line">df.loc[:,<span class="string">&#x27;Charged Off&#x27;</span>:<span class="string">&#x27;Late&#x27;</span>] = df.loc[:,<span class="string">&#x27;Charged Off&#x27;</span>:<span class="string">&#x27;Late&#x27;</span>].div(df[<span class="string">&#x27;All&#x27;</span>],</span><br><span class="line">axis=<span class="number">0</span>)</span><br><span class="line">df[<span class="string">&#x27;All&#x27;</span>] = df[<span class="string">&#x27;All&#x27;</span>] / <span class="built_in">sum</span>(df[<span class="string">&#x27;All&#x27;</span>])</span><br><span class="line">perc_crosstab = df</span><br></pre></td></tr></table></figure><p><code>margins</code> 关键字参数将添加列和行总和。 我们创建数据透视表的副本，忽略列总和。 我们用行总和除以行。 我们将“All”列除以它的总和。</p><h4 id="类别数据和数值数据"><strong>类别数据和数值数据</strong></h4><p>Categorical and Numeric Data</p><p><img src="/img3/面向数据科学家的实用统计学//F1.10.png" alt="F1.10" style="zoom:50%;" /></p><p><strong>箱线图</strong>（参见第20页的“百分位数和箱线图”）是一种<strong>简单的方法</strong>，用于<strong>直观地比较</strong>根据<strong>类别变量</strong>分组的<strong>数值变量</strong>的分布。例如，我们可能想比较航班延误的百分比在不同航空公司之间有何差异。图1-10显示了一个月内，由于航空公司自身可控因素导致的航班延误百分比：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">boxplot<span class="punctuation">(</span>pct_carrier_delay <span class="operator">~</span> airline<span class="punctuation">,</span> data<span class="operator">=</span>airline_stats<span class="punctuation">,</span> ylim<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">0</span><span class="punctuation">,</span> <span class="number">50</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>pandas 的 <code>boxplot</code> 方法接受 <code>by</code> 参数，该参数将数据集分成组并创建单独的箱线图：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ax = airline_stats.boxplot(by=<span class="string">&#x27;airline&#x27;</span>, column=<span class="string">&#x27;pct_carrier_delay&#x27;</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&#x27;Daily % of Delayed Flights&#x27;</span>)</span><br><span class="line">plt.suptitle(<span class="string">&#x27;&#x27;</span>)</span><br></pre></td></tr></table></figure><p>阿拉斯加航空以<strong>延误最少</strong>而脱颖而出，而美国航空<strong>延误最多</strong>：美国航空的下四分位数<strong>高于</strong>阿拉斯加航空的上四分位数。</p><p>由 [Hintze-Nelson-1998] 引入的<strong>小提琴图</strong>是对箱线图的增强，它将<strong>密度估计</strong>与 y 轴上的密度一起绘制。密度被<strong>镜像并翻转</strong>，填充形成的形状，创造出类似<strong>小提琴</strong>的图像。小提琴图的优点是它可以显示箱线图中<strong>不明显</strong>的分布细微差别。另一方面，箱线图更清晰地显示了数据中的<strong>异常值</strong>。在 <code>ggplot2</code> 中，<code>geom_violin</code> 函数可用于创建小提琴图，如下所示：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ggplot<span class="punctuation">(</span>data<span class="operator">=</span>airline_stats<span class="punctuation">,</span> aes<span class="punctuation">(</span>airline<span class="punctuation">,</span> pct_carrier_delay<span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">ylim<span class="punctuation">(</span><span class="number">0</span><span class="punctuation">,</span> <span class="number">50</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">geom_violin<span class="punctuation">(</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">labs<span class="punctuation">(</span>x<span class="operator">=</span><span class="string">&#x27;&#x27;</span><span class="punctuation">,</span> y<span class="operator">=</span><span class="string">&#x27;Daily % of Delayed Flights&#x27;</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>小提琴图可以使用 <code>seaborn</code> 包的 <code>violinplot</code> 方法获得：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ax = sns.violinplot(airline_stats.airline, airline_stats.pct_carrier_delay,</span><br><span class="line">inner=<span class="string">&#x27;quartile&#x27;</span>, color=<span class="string">&#x27;white&#x27;</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&#x27;Daily % of Delayed Flights&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="/img3/面向数据科学家的实用统计学//F1.11.png" alt="F1.11" style="zoom:50%;" /></p><p>相应的图如图1-11所示。小提琴图显示，阿拉斯加航空以及程度较轻的达美航空，其分布<strong>集中在零点附近</strong>。这种现象在箱线图中<strong>不那么明显</strong>。您可以通过在绘图中添加 <code>geom_boxplot</code> 将小提琴图与箱线图结合起来（尽管这在使用颜色时效果最佳）。</p><h4 id="可视化多个变量"><strong>可视化多个变量</strong></h4><p>Visualizing Multiple Variables</p><p>用于比较两个变量的图表类型——散点图、六边形分箱和箱线图——可以很容易地通过<strong>条件化</strong>（conditioning）的概念扩展到更多的变量。作为一个例子，回顾图1-8，它展示了房屋的已完成居住面积和其税收评估价值之间的关系。我们观察到，似乎有一群房屋每平方英尺的税收评估价值更高。深入挖掘，图1-12通过绘制一组邮政编码的数据来解释<strong>位置的影响</strong>。现在情况清晰得多：一些邮政编码（98105, 98126）的税收评估价值比其他邮政编码（98108, 98188）<strong>高得多</strong>。这种差异导致了在图1-8中观察到的<strong>聚类</strong>。</p><p><img src="/img3/面向数据科学家的实用统计学//F1.12.png" alt="F1.12" style="zoom:50%;" /></p><p>我们使用 <code>ggplot2</code> 和<strong>分面</strong>（facets）或<strong>条件化变量</strong>（conditioning variable）（在本例中为邮政编码）的概念创建了图1-12：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ggplot<span class="punctuation">(</span>subset<span class="punctuation">(</span>kc_tax0<span class="punctuation">,</span> ZipCode <span class="operator">%in%</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="number">98188</span><span class="punctuation">,</span> <span class="number">98105</span><span class="punctuation">,</span> <span class="number">98108</span><span class="punctuation">,</span> <span class="number">98126</span><span class="punctuation">)</span><span class="punctuation">)</span><span class="punctuation">,</span></span><br><span class="line">aes<span class="punctuation">(</span>x<span class="operator">=</span>SqFtTotLiving<span class="punctuation">,</span> y<span class="operator">=</span>TaxAssessedValue<span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">stat_binhex<span class="punctuation">(</span>color<span class="operator">=</span><span class="string">&#x27;white&#x27;</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">theme_bw<span class="punctuation">(</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">scale_fill_gradient<span class="punctuation">(</span>low<span class="operator">=</span><span class="string">&#x27;white&#x27;</span><span class="punctuation">,</span> high<span class="operator">=</span><span class="string">&#x27;blue&#x27;</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">labs<span class="punctuation">(</span>x<span class="operator">=</span><span class="string">&#x27;Finished Square Feet&#x27;</span><span class="punctuation">,</span> y<span class="operator">=</span><span class="string">&#x27;Tax-Assessed Value&#x27;</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">facet_wrap<span class="punctuation">(</span><span class="string">&#x27;ZipCode&#x27;</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>使用 <code>ggplot</code> 函数 <code>facet_wrap</code> 和 <code>facet_grid</code> 来指定条件化变量。</p><p>大多数 Python 包的<strong>可视化都基于 Matplotlib</strong>。虽然原则上可以使用 Matplotlib 创建分面图，但代码可能会变得很复杂。幸运的是，<code>seaborn</code> 有一种相对简单的方法来创建这些图：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">zip_codes = [<span class="number">98188</span>, <span class="number">98105</span>, <span class="number">98108</span>, <span class="number">98126</span>]</span><br><span class="line">kc_tax_zip = kc_tax0.loc[kc_tax0.ZipCode.isin(zip_codes),:]</span><br><span class="line">kc_tax_zip</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">hexbin</span>(<span class="params">x, y, color,</span></span><br><span class="line"><span class="params">**kwargs</span>):</span><br><span class="line">cmap = sns.light_palette(color, as_cmap=<span class="literal">True</span>)</span><br><span class="line">plt.hexbin(x, y, gridsize=<span class="number">25</span>, cmap=cmap,</span><br><span class="line">**kwargs)</span><br><span class="line">g = sns.FacetGrid(kc_tax_zip, col=<span class="string">&#x27;ZipCode&#x27;</span>, col_wrap=<span class="number">2</span>)</span><br><span class="line">g.<span class="built_in">map</span>(hexbin, <span class="string">&#x27;SqFtTotLiving&#x27;</span>, <span class="string">&#x27;TaxAssessedValue&#x27;</span>,</span><br><span class="line">extent=[<span class="number">0</span>, <span class="number">3500</span>, <span class="number">0</span>, <span class="number">700000</span>])</span><br><span class="line">g.set_axis_labels(<span class="string">&#x27;Finished Square Feet&#x27;</span>, <span class="string">&#x27;Tax-Assessed Value&#x27;</span>)</span><br><span class="line">g.set_titles(<span class="string">&#x27;Zip code &#123;col_name:.0f&#125;&#x27;</span>)</span><br></pre></td></tr></table></figure><p>使用参数 <code>col</code> 和 <code>row</code> 来指定条件化变量。对于单个条件化变量，将 <code>col</code> 与 <code>col_wrap</code> 一起使用，以将分面图包装成多行。<code>map</code> 方法使用原始数据集中不同邮政编码的子集来调用 <code>hexbin</code> 函数。<code>extent</code> 定义了 x 和 y 轴的<strong>范围</strong>。</p><p>图形系统中的<strong>条件化变量</strong>概念由 Rick Becker、Bill Cleveland 等人在贝尔实验室开发的 <strong>Trellis graphics</strong> [Trellis-Graphics] 首创。这个想法已经传播到各种现代图形系统，例如 R 中的 <code>lattice</code> [lattice] 和 <code>ggplot2</code> 包，以及 Python 中的 <code>seaborn</code> [seaborn] 和 <code>Bokeh</code> [bokeh] 模块。条件化变量也是 Tableau 和 Spotfire 等商业智能平台不可或缺的一部分。随着巨大计算能力的出现，现代可视化平台已经远远超出了探索性数据分析的简陋开端。然而，半个世纪前发展起来的关键概念和工具（例如，简单的箱线图）仍然构成了这些系统的基础。</p><p><strong>关键思想</strong></p><ul><li><strong>六边形分箱</strong>和<strong>等高线图</strong>是<strong>有用的工具</strong>，可以同时对两个数值变量进行<strong>图形化检查</strong>，而不会被海量数据所淹没。</li><li><strong>列联表</strong>是查看两个<strong>类别变量</strong>计数的<strong>标准工具</strong>。</li><li><strong>箱线图</strong>和<strong>小提琴图</strong>允许您将一个<strong>数值变量</strong>与一个<strong>类别变量</strong>进行绘图。</li></ul><h3 id="总结"><strong>总结</strong></h3><p>由<strong>约翰·图基（John Tukey）</strong>首创的<strong>探索性数据分析（Exploratory data analysis）（EDA）</strong>为数据科学领域奠定了基础。EDA 的<strong>核心思想</strong>是：任何基于数据的项目，其<strong>第一步也是最重要的一步就是查看数据</strong>。通过<strong>汇总</strong>和<strong>可视化</strong>数据，您可以获得对项目的宝贵直觉和理解。</p><p>本章回顾了从简单度量（如位置和变异性的估计）到丰富的视觉显示（探索多个变量之间的关系，如图1-12所示）等各种概念。开源社区正在开发的各种工具和技术，结合 R 和 Python 语言的表现力，创造了大量探索和分析数据的方法。<strong>探索性分析应该成为任何数据科学项目的基石</strong>。</p>]]></content>
      
      
      <categories>
          
          <category> 翻译 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> AI </tag>
            
            <tag> 统计 </tag>
            
            <tag> 数据科学 </tag>
            
            <tag> R </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《机器学习的数学基础》第12章&quot;支持向量机分类&quot;</title>
      <link href="/2025/09/09/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E7%AC%AC12%E7%AB%A0%22%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E5%88%86%E7%B1%BB%22/"/>
      <url>/2025/09/09/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E7%AC%AC12%E7%AB%A0%22%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E5%88%86%E7%B1%BB%22/</url>
      
        <content type="html"><![CDATA[<h2 id="第12章-支持向量机分类">第12章 支持向量机分类</h2><p>在许多情况下，我们希望机器学习算法能够预测若干（离散）结果中的一个。例如，电子邮件客户端会将邮件分为个人邮件和垃圾邮件，这就有两个结果。另一个例子是望远镜识别夜空中的天体是星系、恒星还是行星。通常结果的数量较少，更重要的是，这些结果之间往往没有额外的结构。 在本章中，我们考虑输出二元值的预测器，也就是说，只有两种可能的结果。这种机器学习任务称为<strong>二分类（binary classification）</strong>。这与第 9 章形成对比，当时我们讨论的是连续值输出的预测问题。对于二分类，标签/输出可以取的值是二元的，本章中我们用 {+1,−1} 表示。换句话说，我们考虑的预测器形式为 <span class="math display">\[f:\mathbb{R}^D \to \{+1,-1\}. \tag{12.1}\]</span></p><p>回忆第 8 章，我们用 <span class="math inline">\(D\)</span> 个实数的特征向量表示每个样本（数据点）<span class="math inline">\(x_n\)</span>。标签通常分别称为正类（positive class）和负类（negative class）。但应当注意，不要根据“正”或“负”字面意义推断+1类的直观属性。例如，在癌症检测任务中，有癌症的患者往往被标记为+1。原则上可以使用任何两个不同的值，例如{True,False}、{0,1}或{red,blue}。二分类问题研究得比较充分，其他方法的综述我们放到 12.6 节再介绍。我们将介绍一种称为<strong>支持向量机（Support Vector Machine, SVM）</strong>的方法，它用于解决二分类任务。与回归类似，这是一个<strong>监督学习</strong>任务：我们有一组样本 <span class="math inline">\(x_n \in \mathbb{R}^D\)</span>，以及对应的（二元）标签 <span class="math inline">\(y_n \in \{+1,-1\}\)</span>。给定一个包含样本–标签对 <span class="math inline">\(\{(x_1,y_1),\dots,(x_N,y_N)\}\)</span> 的训练数据集，我们希望估计模型参数，使分类错误率最小。类似第 9 章，<strong>我们考虑线性模型，并把非线性隐藏在对样本的一个变换 <span class="math inline">\(\phi\)</span> 中（见式(9.13)）</strong>。我们将在 12.4 节重新讨论 <span class="math inline">\(\phi\)</span>。</p><p>支持向量机（SVM）在许多应用中都能提供最先进的结果，并且具有坚实的理论保证（Steinwart 和 Christmann, 2008）。我们之所以选择用 SVM 来说明二分类问题，主要有两个原因。</p><p>首先，SVM 提供了一种<strong>几何方式</strong>来思考监督学习问题。 在第 9 章中，我们是从<strong>概率模型</strong>的角度来看待机器学习问题，并用极大似然估计和贝叶斯推断来解决。而在这里，我们考虑一种替代的方法，即从几何角度来推理机器学习任务。这种方法高度依赖于我们在第 3 章讨论过的<strong>内积</strong>和<strong>投影</strong>等概念。</p><p>第二个原因是，与第 9 章不同，SVM 的优化问题<strong>没有解析解</strong>，因此需要借助第 7 章介绍的各种优化工具。 SVM 对机器学习的理解与第 9 章的极大似然方法存在细微的差异：极大似然方法是基于数据分布的概率观点提出一个模型，再由此推导出一个优化问题；而 SVM 的方法则是从几何直觉出发，先设计一个需要在训练过程中被优化的函数。我们已经在第 10 章看到过类似的情况，当时我们从几何原理出发推导了 PCA。在 SVM 的例子中，我们通过设计一个损失函数来度量训练数据上的误差，并遵循<strong>经验风险最小化原则</strong>（第 8.2 节），在训练中加以最小化。</p><span id="more"></span><p>接下来，我们推导 SVM 的训练优化问题。 直观上，我们可以想象一个二分类数据，它们能够被一个超平面分开，如图 12.1 所示。</p><p><img src="/img3/机器学习的数学基础Part2/F12.1.png" alt="F12.1" style="zoom:50%;" /></p><p>在这里，每个样本 <span class="math inline">\(x_n\)</span>（一个二维向量）是一个二维位置（<span class="math inline">\(x^{(1)}_n, x^{(2)}_n\)</span>），对应的二元标签 <span class="math inline">\(y_n\)</span> 则是两种不同的符号（橙色叉号或蓝色圆点）。“超平面”（hyperplane）是机器学习中常用的术语，我们在第 2.8 节已经遇到过。超平面是一个仿射子空间，维度为 <span class="math inline">\(D-1\)</span>（如果对应的向量空间维度是 <span class="math inline">\(D\)</span>）。在二分类中，两个类别的样本（对应两种可能的标签）在特征空间中分布得刚好可以用一条直线（在二维情况下）来分开。</p><blockquote><p>个人注：注意：<strong>在 <span class="math inline">\(n\)</span> 维空间 (<span class="math inline">\(\mathbb{R}^n\)</span>) 中，超平面（hyperplane）按定义就是一个 <span class="math inline">\((n-1)\)</span> 维的仿射子空间</strong>。</p></blockquote><blockquote><p>个人注：引用前文2.8.1节的例子。</p><p>例 2.26（仿射子空间）</p><ul><li>一维仿射子空间称为<strong>直线</strong></li></ul><p>可写为 <span class="math display">\[y = x_0 + \lambda b_1,\]</span></p><p>其中 <span class="math inline">\(\lambda \in \mathbb{R}\)</span>，且 <span class="math inline">\(U=\text{span}[b_1]\subseteq \mathbb{R}^n\)</span> 是 <span class="math inline">\(\mathbb{R}^n\)</span> 的一维子空间。 这意味着一条直线由一个<strong>支撑点</strong>（support point）<span class="math inline">\(x_0\)</span> 和一个定义方向的向量 <span class="math inline">\(b_1\)</span> 决定。图 2.13 给出了示意图。</p><ul><li><span class="math inline">\(\mathbb{R}^n\)</span> 中的二维仿射子空间称为<strong>平面</strong></li></ul><p>其参数方程为</p><p><span class="math display">\[y = x_0 + \lambda_1 b_1 + \lambda_2 b_2,\]</span></p><p>其中 <span class="math inline">\(\lambda_1,\lambda_2 \in \mathbb{R}\)</span>，且 <span class="math inline">\(U=\text{span}[b_1,b_2]\subseteq \mathbb{R}^n\)</span>。 这意味着一个平面由一个支撑点 <span class="math inline">\(x_0\)</span> 和<strong>两个线性无关的向量 <span class="math inline">\(b_1,b_2\)</span> 确定</strong>，这两个向量张成方向空间。</p><ul><li>在 <span class="math inline">\(\mathbb{R}^n\)</span> 中，<span class="math inline">\((n-1)\)</span> 维的仿射子空间称为<strong>超平面</strong>（hyperplane）</li></ul><p>其对应的参数方程为</p><p><span class="math display">\[y = x_0 + \sum_{i=1}^{n-1} \lambda_i b_i,\]</span></p><p>其中 <span class="math inline">\(b_1,\dots,b_{n-1}\)</span> 组成 <span class="math inline">\(\mathbb{R}^n\)</span> 中一个 <span class="math inline">\((n-1)\)</span> 维子空间 <span class="math inline">\(U\)</span> 的一组基。 这意味着一个超平面由一个支撑点 <span class="math inline">\(x_0\)</span> 和 <span class="math inline">\((n-1)\)</span> 个线性无关的向量 <span class="math inline">\(b_1,\dots,b_{n-1}\)</span> 确定，这些向量张成方向空间。</p><p><strong>在 <span class="math inline">\(\mathbb{R}^2\)</span> 中，一条直线也是一个超平面；在 <span class="math inline">\(\mathbb{R}^3\)</span> 中，一个平面也是一个超平面。</strong></p><p><img src="/img3/机器学习的数学基础Part2/F2.13.png" alt="F2.13" style="zoom:50%;" /></p></blockquote><p>在下面的推导中，我们将形式化“寻找一个线性分隔器”的思想。我们会引入<strong>间隔（margin）</strong>的概念，并进一步推广线性分隔器，使其允许某些样本落在“错误”的一侧，从而引入分类误差。我们将介绍两种等价的 SVM 形式化方式：</p><ul><li><strong>几何视角</strong>（第 12.2.4 节），</li><li><strong>损失函数视角</strong>（第 12.2.5 节）。</li></ul><p>随后，我们利用 <strong>拉格朗日乘子法</strong>（第 7.2 节）推导 SVM 的<strong>对偶形式</strong>。对偶 SVM 使我们能够看到第三种形式化 SVM 的方式：即从每个类别样本的<strong>凸包</strong>的角度来理解（第 12.3.2 节）。最后，我们会简要介绍<strong>核方法</strong>，以及如何数值化地求解<strong>非线性核 SVM</strong>的优化问题。</p><h3 id="分离超平面">12.1 分离超平面</h3><p>给定两个用向量 <span class="math inline">\(x_i\)</span> 和 <span class="math inline">\(x_j\)</span> 表示的样本，衡量它们相似度的一种方法是使用<strong>内积</strong> <span class="math inline">\(\langle x_i, x_j\rangle\)</span>。回忆第 3.2 节，内积与两个向量之间的<strong>夹角</strong>密切相关。两个向量内积的值不仅取决于它们之间的夹角，还取决于每个向量的<strong>长度（范数）</strong>。此外，内积还允许我们严格地定义<strong>几何概念</strong>，比如正交性和投影。</p><p>许多分类算法背后的主要思想是：把数据表示在 <span class="math inline">\(\mathbb{R}^D\)</span> 空间中，然后对这个空间进行划分，理想情况下，同一标签的样本（且不包含其他样本）会位于同一个分区中。对于二分类，空间会被划分成两部分，分别对应正类和负类。我们考虑一种特别方便的划分方式：用一个<strong>超平面</strong>将空间（线性地）分成两半。设样本 <span class="math inline">\(x\in \mathbb{R}^D\)</span> 是数据空间中的一个元素，考虑一个函数</p><p><span class="math display">\[f:\mathbb{R}^D \to \mathbb{R} \tag{12.2a}\]</span></p><p><span class="math display">\[x \mapsto f(x):=\langle w,x\rangle+b \tag{12.2b}\]</span></p><p>其中参数 <span class="math inline">\(w\in\mathbb{R}^D\)</span>，<span class="math inline">\(b\in\mathbb{R}\)</span>。回忆第 2.8 节，超平面是一个<strong>仿射子空间</strong>。因此，我们可以将二分类问题中分隔两类的超平面定义为：</p><p><span class="math display">\[\{ x\in\mathbb{R}^D: f(x)=0. \} \tag{12.3}\]</span></p><p>图 12.2 给出了这个超平面的示意图，其中向量 <span class="math inline">\(w\)</span> 是超平面的<strong>法向量</strong>，<span class="math inline">\(b\)</span> 是<strong>截距</strong>。</p><p><img src="/img3/机器学习的数学基础Part2/F12.2.png" alt="F12.2" style="zoom:50%;" /></p><p>我们可以证明，<span class="math inline">\(w\)</span> 确实是 (12.3) 中超平面的法向量。方法是：任选两个在超平面上的样本 <span class="math inline">\(x_a\)</span> 和 <span class="math inline">\(x_b\)</span>，证明它们的差向量与 <span class="math inline">\(w\)</span> <strong>正交</strong>。</p><p>写成方程形式：</p><p><span class="math display">\[\begin{align}f(x_a)-f(x_b) &amp;= \langle w,x_a\rangle+b - \big(\langle w,x_b\rangle+b\big) \tag{12.4a}\\&amp;=\langle w,x_a-x_b\rangle, \tag{12.4b}\end{align}\]</span></p><p>第二行利用了内积的<strong>线性性</strong>（第 3.2 节）。由于我们选择了 <span class="math inline">\(x_a\)</span> 和 <span class="math inline">\(x_b\)</span> 在超平面上，这意味着 <span class="math inline">\(f(x_a)=0\)</span> 且 <span class="math inline">\(f(x_b)=0\)</span>，因此 <span class="math inline">\(\langle w,x_a-x_b\rangle=0\)</span>。回忆：当两个向量的内积为 0 时，它们正交。因此，我们得到 <span class="math inline">\(w\)</span> 与超平面上任意向量都正交，也就是说 <span class="math inline">\(w\)</span> 是超平面的法向量。</p><p><strong>注释</strong>:回忆第 2 章，我们可以从不同角度理解向量。在本章中，我们把参数向量 <span class="math inline">\(w\)</span> 看作一个表示方向的箭头，也就是说，我们把 <span class="math inline">\(w\)</span> 当作几何向量来理解。相反，我们把样本向量 <span class="math inline">\(x\)</span>看作一个数据点（由其坐标表示），也就是说，我们把 <span class="math inline">\(x\)</span> 当作相对于标准基的一个向量坐标来理解。</p><p>在给定一个测试样本时，我们根据它位于超平面的哪一侧将其分类为正例或负例。注意，式 (12.3) 不仅定义了一个超平面，还定义了一个方向。换句话说，它规定了超平面的正侧和负侧。因此，为了分类测试样本 <span class="math inline">\(x_{test}\)</span>，我们计算函数 <span class="math inline">\(f(x_{test})\)</span> 的值，如果 <span class="math inline">\(f(x_{test}) ≥ 0\)</span> 则判为 +1，否则判为 −1。从几何角度看，正例位于超平面“上方”，负例位于“下方”。</p><p>在训练分类器时，我们希望带有正标签的样本位于超平面的正侧，即</p><p><span class="math display">\[\langle w, x_n \rangle + b \ge 0 \quad \text{当} \; y_n = +1 \tag{12.5}\]</span></p><p>并且希望带有负标签的样本位于超平面的负侧，即</p><p><span class="math display">\[\langle w, x_n \rangle + b &lt; 0 \quad \text{当} \; y_n = -1 \tag{12.6}\]</span></p><p>参见图 12.2 来获得关于正负样本的几何直观。这两个条件常常可以写成一个式子：</p><p><span class="math display">\[y_n \,(\langle w, x_n \rangle + b) \ge 0 \tag{12.7}\]</span></p><p>式 (12.7) 与 (12.5) 和 (12.6) 是等价的，因为我们分别把 (12.5) 和 (12.6) 两边乘上 <span class="math inline">\(y_n = +1\)</span> 和 <span class="math inline">\(y_n = -1\)</span>。</p><h3 id="原始支持向量机">12.2 原始支持向量机</h3><p>基于点到超平面的距离概念，我们现在可以讨论支持向量机了。对于一个线性可分的数据集<span class="math inline">\(\{(x_1, y_1), \ldots, (x_N, y_N)\}\)</span>，我们有无穷多候选超平面（见图 12.3），也就有无穷多候选分类器，可以在没有（训练）错误的情况下解决分类问题。为了找到唯一解，一个思路是选择能够<strong>最大化正例和负例之间间隔（margin）</strong>的分离超平面。换句话说，我们希望正负样本被一个“间隔很大”的超平面分开（第 12.2.1 节）。下面我们将计算样本与超平面的距离，从而推导出 margin 的定义。回忆给定点（样本 <span class="math inline">\(x_n\)</span>）到超平面最近的点是通过<strong>正交投影</strong>获得的（第 3.8 节）。</p><p><img src="/img3/机器学习的数学基础Part2/F12.3.png" alt="F12.3" style="zoom:50%;" /></p><h4 id="间隔的概念">12.2.1 间隔的概念</h4><p>Margin</p><p>间隔的概念直观上非常简单：它是分离超平面到数据集中<strong>最近样本</strong>的距离（假设数据集线性可分）。然而，在形式化这个距离时有一个技术小问题，可能会让人困惑。这个小问题是我们需要定义一个<strong>测量距离的尺度</strong>。一个潜在的尺度是数据本身的尺度，即 <span class="math inline">\(x_n\)</span> 的原始值。但这样做有问题：我们可以改变 <span class="math inline">\(x_n\)</span> 的度量单位而改变 <span class="math inline">\(x_n\)</span> 的值，从而改变到超平面的距离。正如我们很快将看到的，我们用超平面方程 (12.3) 本身来定义尺度。考虑一个超平面 <span class="math inline">\(\langle w,x\rangle + b\)</span>，以及一个样本 <span class="math inline">\(x_a\)</span>，如图 12.4 所示。不失一般性，我们可以假设样本 <span class="math inline">\(x_a\)</span> 在超平面的正侧，即 <span class="math inline">\(\langle w, x_a\rangle + b &gt; 0\)</span>。我们希望计算 <span class="math inline">\(x_a\)</span> 到超平面的距离 <span class="math inline">\(r&gt;0\)</span>。我们通过考虑 <span class="math inline">\(x_a\)</span> 到超平面的<strong>正交投影</strong>（第 3.8 节）来实现，投影点记为 <span class="math inline">\(x_a&#39;\)</span>。</p><p><img src="/img3/机器学习的数学基础Part2/F12.4.png" alt="F12.4" style="zoom:50%;" /></p><p>由于 <span class="math inline">\(w\)</span> 垂直于超平面，我们知道距离 <span class="math inline">\(r\)</span> 只是向量 <span class="math inline">\(w\)</span> 的一个缩放。如果 <span class="math inline">\(w\)</span> 的长度已知，我们就可以用这个缩放因子 <span class="math inline">\(r\)</span> 求出 <span class="math inline">\(x_a\)</span> 和 <span class="math inline">\(x_a&#39;\)</span> 之间的绝对距离。为了方便，我们选择使用一个单位长度的向量（范数为 1），通过将 <span class="math inline">\(w\)</span> 除以它的范数得到：<span class="math inline">\(\frac{w}{\|w\|}\)</span>。使用向量加法（第 2.4 节），我们得到：</p><p><span class="math display">\[x_a = x_a&#39; + r \,\frac{w}{\|w\|} \tag{12.8}\]</span></p><p>另一种理解 <span class="math inline">\(r\)</span> 的方式是：它是 <span class="math inline">\(x_a\)</span> 在由 <span class="math inline">\(\frac{w}{\|w\|}\)</span> 张成的子空间中的坐标。现在我们把 <span class="math inline">\(x_a\)</span> 到超平面的距离表达为 <span class="math inline">\(r\)</span>。如果我们选择 <span class="math inline">\(x_a\)</span> 是距离超平面最近的点，那么这个距离 <span class="math inline">\(r\)</span> 就是 <strong>margin（间隔）</strong>。回忆我们希望正例在超平面正方向上距离至少为 <span class="math inline">\(r\)</span>，负例在超平面负方向上距离至少为 <span class="math inline">\(r\)</span>。与把 (12.5) 和 (12.6) 合并为 (12.7) 类似，我们将这个目标写为：</p><p><span class="math display">\[y_n\bigl(\langle w,x_n\rangle + b\bigr) \;\geq\; r \tag{12.9}\]</span></p><p>换句话说，我们把样本在正负方向上都至少距离超平面 <span class="math inline">\(r\)</span> 的要求合并到了一条不等式里。</p><p>由于我们只关心方向，我们在模型中加一个假设：参数向量 <span class="math inline">\(w\)</span> 是单位长度的，即 <span class="math inline">\(\|w\|=1\)</span>，其中 <span class="math inline">\(\|w\|=\sqrt{w^\top w}\)</span> 是欧几里得范数（第 3.1 节）。这个假设也使得 (12.8) 中距离 <span class="math inline">\(r\)</span> 有了更直观的解释，因为它是一个长度为 1 的向量的缩放因子。</p><p><strong>注</strong>：熟悉其他 margin 定义的读者会注意到，我们这里取 <span class="math inline">\(\|w\|=1\)</span> 的方式与 Schölkopf 和 Smola (2002) 等书中的标准表述不同。在第 12.2.3 节我们会展示两种方法的等价性。</p><p>将三个要求收集到一个带约束的优化问题中，我们得到目标函数：</p><p><span class="math display">\[\max_{w,b,r} \; r  \tag{12.10}\]</span></p><p>满足约束：</p><ul><li><span class="math inline">\(y_n(\langle w,x_n\rangle + b)\;\geq\;r\)</span> （数据满足约束），</li><li><span class="math inline">\(\|w\|=1\)</span> （归一化），</li><li><span class="math inline">\(r&gt;0\)</span>，</li></ul><p>这表示我们要在保证所有数据在超平面正确一侧的前提下，最大化间隔 <span class="math inline">\(r\)</span>。</p><p><strong>注</strong>：margin 的概念在机器学习中无处不在。Vladimir Vapnik 和 Alexey Chervonenkis 利用这个概念证明了当 margin 很大时，函数类的“复杂度”较低，因此学习是可行的（Vapnik, 2000）。事实证明，这个概念在许多不同的理论分析泛化误差的方法中都非常有用（Steinwart 和 Christmann, 2008；Shalev-Shwartz 和 Ben-David, 2014）。</p><h4 id="边界的传统推导">12.2.2 边界的传统推导</h4><p>Margin</p><p>在前一节中，我们通过观察只关心 <span class="math inline">\(w\)</span> 的方向而不关心其长度，得到了假设 <span class="math inline">\(\|w\|=1\)</span>，并由此推导出式 (12.10)。在本节中，我们将通过另一种假设来推导边界最大化问题。我们不再选择参数向量归一化，而是选择对数据进行缩放。我们通过这样的缩放，使得预测器 <span class="math inline">\(\langle w,x\rangle + b\)</span> 在距离超平面最近的样本处等于 1。我们将数据集中距离超平面最近的样本记作 <span class="math inline">\(x_a\)</span>。</p><p><img src="/img3/机器学习的数学基础Part2/F12.5.png" alt="F12.5" style="zoom:50%;" /></p><p>图 12.5 与图 12.4 相同，只是现在我们重新缩放了坐标轴，使得样本 <span class="math inline">\(x_a\)</span> 正好位于边界上，即</p><p><span class="math display">\[\langle w,x_a\rangle + b = 1.\]</span></p><p>由于 <span class="math inline">\(x&#39;_a\)</span> 是 <span class="math inline">\(x_a\)</span> 在超平面上的正交投影，按定义它必定位于超平面上，即</p><p><span class="math display">\[\langle w,x&#39;_a\rangle + b = 0. \tag{12.11}\]</span></p><p>将 (12.8) 代入 (12.11)，我们得到</p><p><span class="math display">\[\langle w,\;x_a - \frac{r}{\|w\|} w \rangle + b = 0. \tag{12.12}\]</span></p><p>利用内积的双线性性质（见第 3.2 节），我们得到</p><p><span class="math display">\[\langle w,x_a\rangle + b - r \frac{\langle w,w\rangle}{\|w\|} = 0. \tag{12.13}\]</span></p><p>注意，根据我们的缩放假设，第一项为 1，即 <span class="math inline">\(\langle w,x_a\rangle + b=1\)</span>。根据第 3.1 节的式 (3.16)，<span class="math inline">\(\langle w,w\rangle = \|w\|^2\)</span>。因此，第二项可简化为 <span class="math inline">\(r\|w\|\)</span>。利用这些简化，我们得到</p><p><span class="math display">\[\frac{1}{r} = \|w\|. \tag{12.14}\]</span></p><p>这意味着我们将距离 <span class="math inline">\(r\)</span> 表示为了超平面的法向量 <span class="math inline">\(w\)</span> 的形式。乍一看，这个等式似乎不直观，因为我们好像是用向量 <span class="math inline">\(w\)</span> 的长度来表示超平面的距离，但我们还不知道这个向量。一种理解方式是，将距离 <span class="math inline">\(r\)</span> 看作一个临时变量，仅用于推导。因此，在本节余下部分，我们用 <span class="math inline">\(\frac{1}{\|w\|}\)</span> 表示到超平面的距离。在第 12.2.3 节中，我们将看到“边界等于 1”这一选择与第 12.2.1 节中假设 <span class="math inline">\(\|w\|=1\)</span> 是等价的。类似于推导式 (12.9) 的思路，我们希望正例和负例都至少距离超平面 1 个单位，因此得到条件</p><blockquote><p>我们还可以把这个距离理解为当 <span class="math inline">\(x_a\)</span> 投影到超平面上时产生的投影误差。</p></blockquote><p><span class="math display">\[y_n(\langle w,x_n\rangle + b)\ge 1. \tag{12.15}\]</span></p><p>将边界（margin）最大化与“样本需要位于超平面正确一侧（依据其标签）”这一事实结合起来，就得到</p><p><span class="math display">\[\max_{w,b}\;\frac{1}{\|w\|} \tag{12.16}\]</span></p><p>满足约束条件</p><p><span class="math display">\[y_n(\langle w,x_n\rangle + b)\ge 1,\quad \text{对于所有 }n=1,\dots,N. \tag{12.17}\]</span></p><p>与其像式 (12.16) 那样最大化范数的倒数，我们通常最小化范数的平方。<strong>同时我们还经常加入一个常数 <span class="math inline">\(\tfrac{1}{2}\)</span>，它不会影响最优的 <span class="math inline">\(w,b\)</span>，但在计算梯度时能使形式更简洁。</strong>于是我们的目标变为</p><p><span class="math display">\[\min_{w,b}\;\frac{1}{2}\|w\|^2 \tag{12.18}\]</span></p><p>满足约束条件</p><p><span class="math display">\[y_n(\langle w,x_n\rangle + b)\ge 1,\quad \text{对于所有 }n=1,\dots,N. \tag{12.19}\]</span></p><p>式 (12.18) 被称为<strong>硬间隔支持向量机（hard margin SVM）</strong>。“硬（hard）”这一说法是因为这种形式不允许任何违反边界条件的情况。 我们将在第 12.2.4 节看到，如果数据不是线性可分的，这个“硬”条件可以放宽，以容纳一些违反边界条件的样本。</p><h4 id="为什么我们可以把间隔设为-1">12.2.3 为什么我们可以把间隔设为 1</h4><p>在第 12.2.1 节，我们提出我们希望最大化某个值 <span class="math inline">\(r\)</span>，它表示距离超平面最近的样本点到超平面的距离。在第 12.2.2 节，我们通过缩放数据，使得最近的样本点到超平面的距离正好为 1。在本节，我们把这两种推导联系起来，并展示它们是等价的。</p><p><strong>定理 12.1</strong>:在 (12.10) 中考虑归一化权重的情况下，最大化间隔 <span class="math inline">\(r\)</span>： <span class="math display">\[\max_{w,b,r} \; r \quad\text{（margin）} \]</span></p><p>满足</p><p><span class="math display">\[y_n(\langle w,x_n\rangle + b) \ge r\quad\text{（data fitting）},\qquad\|w\|=1\quad\text{（normalization）},\qquadr&gt;0,\tag{12.20}\]</span></p><p>等价于对数据进行缩放，使得间隔为 1：</p><p><span class="math display">\[\min_{w,b} \;\frac{1}{2}\|w\|^2 \quad\text{（margin）}\]</span></p><p>满足</p><p><span class="math display">\[y_n(\langle w,x_n\rangle + b) \ge 1 \quad\text{（data fitting）}.\tag{12.21}\]</span></p><p><strong>证明</strong> 考虑 (12.20)。因为平方对于非负自变量是严格单调的，所以在目标函数中把 <span class="math inline">\(r\)</span> 换成 <span class="math inline">\(r^2\)</span> 最大值保持不变。由于 <span class="math inline">\(\|w\|=1\)</span>，我们可以通过引入一个未归一化的新权重向量 <span class="math inline">\(w&#39;\)</span> 重新参数化：显式写成 <span class="math inline">\(w=w&#39;/\|w&#39;\|\)</span>。于是得到： <span class="math display">\[\max_{w&#39;,b,r} r^2 \quad \text{s.t.}\quad y_n\Bigl( \frac{w&#39;}{\|w&#39;\|},x_n\Bigr)+b \ge r,\quad r&gt;0.\tag{12.22}\]</span></p><p>式 (12.22) 明确表明距离 <span class="math inline">\(r\)</span> 是正的。因此，我们可以将第一个约束除以 <span class="math inline">\(r\)</span>，得到：</p><p><span class="math display">\[\max_{w&#39;,b,r} r^2 \quad \text{s.t.}\quad y_n\!\left(\frac{w&#39;}{\|w&#39;\|r},x_n+\frac{b}{r}\right)\ge 1,\quad r&gt;0,\tag{12.23}\]</span></p><p>然后把参数重命名为 <span class="math inline">\(w&#39;&#39;\)</span> 和 <span class="math inline">\(b&#39;&#39;\)</span>。因为 <span class="math inline">\(w&#39;&#39;=\frac{w&#39;}{\|w&#39;\|r}\)</span> 得到<span class="math inline">\(\|w&#39;&#39;\|=\frac{1}{r}\cdot r\)</span>。将这一结果代入 (12.23)，我们得到：</p><p><span class="math display">\[\max_{w&#39;&#39;,b&#39;&#39;} \frac{1}{\|w&#39;&#39;\|^2}\quad \text{s.t.}\quad y_n(\langle w&#39;&#39;,x_n\rangle + b&#39;&#39;)\ge 1.\tag{12.24}\]</span></p><p>最后一步是注意到，最大化 <span class="math inline">\(\frac{1}{\|w&#39;&#39;\|^2}\)</span> 与最小化 <span class="math inline">\(\frac{1}{2}\|w&#39;&#39;\|^2\)</span> 得到的解是相同的，这就完成了定理 12.1 的证明。</p><p><span class="math display">\[\begin{equation}\begin{aligned}&amp; \max_{\boldsymbol{w}&#39;&#39;, b&#39;&#39;} \frac{1}{\|\boldsymbol{w}&#39;&#39;\|^2} \\&amp; \text{subject to} \quad y_n \left( \langle \boldsymbol{w}&#39;&#39;, \boldsymbol{x}_n \rangle + b&#39;&#39; \right) \geq 1.\end{aligned}\tag{12.25}\end{equation}\]</span></p><h4 id="软间隔-svm几何视角">12.2.4 软间隔 SVM：几何视角</h4><p>当数据不是线性可分时，我们可能希望允许某些样本落在间隔区域内，甚至位于超平面的错误一侧，如图 12.6 所示。允许一定分类错误的模型称为<strong>软间隔 SVM（Soft Margin SVM）</strong>。在本节中，我们用几何方法推导其优化问题。在第 12.2.5 节，我们将用损失函数的思想推导一个等价的优化问题。在第 12.3 节中，我们将利用拉格朗日乘子（第 7.2 节）推导 SVM 的对偶优化问题。</p><p><img src="/img3/机器学习的数学基础Part2/F12.6.png" alt="F12.6" style="zoom:50%;" /></p><p><img src="/img3/机器学习的数学基础Part2/F12.7.png" alt="F12.7" style="zoom:50%;" /></p><p>这个对偶优化问题使我们能够从第三种角度理解 SVM：即作为一个超平面，它平分了正样本和负样本对应的凸包之间的线段（第 12.3.2 节）。关键的几何思想是：为每个样本—标签对 <span class="math inline">\((x_n,y_n)\)</span> 引入一个松弛变量 <span class="math inline">\(\xi_n\)</span>，允许某个样本位于间隔之内，甚至在超平面的错误一侧（参见图 12.7）。我们从间隔中减去 <span class="math inline">\(\xi_n\)</span> 的值，并约束 <span class="math inline">\(\xi_n\)</span> 为非负。为了鼓励样本的正确分类，我们在目标函数中加入 <span class="math inline">\(\xi_n\)</span>： <span class="math display">\[\min_{w,b,\xi} \frac{1}{2}\|w\|^2 + C\sum_{n=1}^N \xi_n \tag{12.26a}\]</span></p><p>约束条件为</p><p><span class="math display">\[y_n(\langle w,x_n\rangle+b)\ge 1-\xi_n \tag{12.26b}\]</span></p><p><span class="math display">\[\xi_n \ge 0 \tag{12.26c}\]</span></p><p>其中 <span class="math inline">\(n=1,\dots,N\)</span>。与硬间隔 SVM 的优化问题 (12.18) 相比，这里称为软间隔 SVM。参数 <span class="math inline">\(C&gt;0\)</span> 在间隔大小与总松弛量之间进行权衡。这个参数称为<strong>正则化参数</strong>，因为（如我们在下一节将看到的那样）目标函数中的间隔项 (12.26a) 实际上是一个正则化项。其中的 <span class="math inline">\(\|w\|^2\)</span> 被称为<strong>正则器</strong>，在许多数值优化的书籍中，正则化参数会乘在这一项上（第 8.2.3 节）。这与我们在本节的表述不同。<strong>在这里，较大的 <span class="math inline">\(C\)</span> 值意味着较低的正则化，因为我们给松弛变量更大的权重，从而更优先考虑那些没有落在正确间隔一侧的样本。</strong></p><p><strong>备注：</strong>在软间隔 SVM (12.26a) 的表述中，<span class="math inline">\(w\)</span> 被正则化，而 <span class="math inline">\(b\)</span> 没有被正则化。我们可以通过观察正则化项不包含 <span class="math inline">\(b\)</span> 来看出这一点。未正则化的 <span class="math inline">\(b\)</span> 会使理论分析更加复杂（Steinwart 和 Christmann, 2008，第 1 章），并降低计算效率（Fan 等人, 2008）。</p><blockquote><p>个人注：</p><p><strong>软间隔 SVM 中参数 <span class="math inline">\(C\)</span> 的几何意义</strong>，并和前面章节的正则化写法对比。</p><ol type="1"><li><strong>软间隔 SVM 的目标函数</strong></li></ol><p>在书的 12.2.4 里，软间隔 SVM 写成：</p><p><span class="math display">\[\min_{w,b,\xi} \frac{1}{2}\|w\|^2 + C\sum_{n=1}^N \xi_n\]</span></p><p>其中</p><ul><li><span class="math inline">\(\frac{1}{2}\|w\|^2\)</span> 是正则化项（控制间隔的宽度）；</li><li><span class="math inline">\(\xi_n\)</span> 是松弛变量（表示第 <span class="math inline">\(n\)</span> 个样本违反间隔或分错的程度）；</li><li><span class="math inline">\(C\)</span> 是惩罚系数。</li></ul><ol start="2" type="1"><li><strong>两种等价的写法</strong></li></ol><p>有的书（或有的章节）把目标函数写成：</p><p><span class="math display">\[\min_{w,b,\xi} \lambda\frac{1}{2}\|w\|^2 + \sum_{n=1}^N \xi_n\]</span></p><p>这时 <span class="math inline">\(\lambda\)</span> 越大，正则化越强。</p><p>而在《Mathematics for Machine Learning》这一节，写法是：</p><p><span class="math display">\[\frac{1}{2}\|w\|^2 + C\sum \xi_n\]</span></p><p>它把 <span class="math inline">\(C\)</span> 放在松弛项前面，而不是正则项前面。于是</p><ul><li>这里 <span class="math inline">\(C\)</span> 大 <strong>不表示正则化强</strong>，反而表示对松弛变量的惩罚更大；</li><li>也就是模型会“尽量减少”违反间隔的点，宁可让 <span class="math inline">\(\|w\|\)</span> 增大（间隔变窄）。</li></ul><p>因此 <strong>正则化的程度与 <span class="math inline">\(C\)</span> 反比</strong>：</p><ul><li><p><span class="math inline">\(C\)</span> <strong>大</strong>：更重视分类正确（少违例），放松正则化（间隔窄一些）；</p></li><li><p><span class="math inline">\(C\)</span> <strong>小</strong>：更重视间隔宽（强正则化），允许更多点在间隔内或分错。</p><ol start="3" type="1"><li><strong>书中那句话的意思</strong></li></ol></li></ul><blockquote><p>Here a large value of C implies low regularization, as we give the slack variables larger weight, hence giving more priority to examples that do not lie on the correct side of the margin.</p></blockquote><p>翻译＋解释：</p><ul><li>“在这一节的写法里，<span class="math inline">\(C\)</span> 越大意味着正则化越弱，因为我们给松弛变量更大的权重”；</li><li>“这样模型就更关注那些不在正确间隔一侧的样本（尽量减少违例），而不是保持大间隔”。</li></ul><p>这和前面章节把 <span class="math inline">\(\lambda\)</span> 乘在 <span class="math inline">\(\|w\|^2\)</span> 上的写法相反，所以要特别注意。</p><ol start="4" type="1"><li><strong>几何直观</strong></li></ol><ul><li><span class="math inline">\(C\)</span> 大： 惩罚违例强 → 尽量让所有点都分对 → 容忍小间隔（<span class="math inline">\(\|w\|\)</span> 大） → 正则化弱。</li><li><span class="math inline">\(C\)</span> 小： 惩罚违例弱 → 可以容忍点进入间隔或分错 → 追求大间隔（<span class="math inline">\(\|w\|\)</span> 小） → 正则化强。</li></ul><p>所以几何上看：<span class="math inline">\(C\)</span> 控制了“宽间隔”与“少违例”之间的折中。</p></blockquote><h4 id="软间隔-svm从损失函数的角度">12.2.5 软间隔 SVM：从损失函数的角度</h4><p>让我们考虑一种推导 SVM 的不同方法，遵循经验风险最小化的原则（第 8.2 节）。对于 SVM，我们选择超平面作为假设类，也就是说：</p><p><span class="math display">\[f(x)=\langle w,x\rangle+b. \tag{12.27}\]</span></p><p>我们将在本节看到，间隔对应于正则化项。剩下的问题是：损失函数是什么？与第 9 章不同，在第 9 章我们考虑的是回归问题（预测器的输出是一个实数），而在本章我们考虑的是二分类问题（预测器的输出是两个标签 <span class="math inline">\(\{+1,-1\}\)</span> 之一）。因此，每一个样本–标签对的误差/损失函数需要适合二分类。例如，回归所用的平方损失（公式 9.10b）并不适合二分类。</p><p><strong>注：</strong><u>二元标签之间的理想损失函数是统计预测与真实标签不一致的次数。</u>这意味着，对于某个预测器 <span class="math inline">\(f\)</span> 作用在样本 <span class="math inline">\(x_n\)</span> 上，我们将输出 <span class="math inline">\(f(x_n)\)</span> 与标签 <span class="math inline">\(y_n\)</span> 进行比较。如果二者一致，则定义损失为 0；不一致，则定义为 1。记为 <span class="math inline">\(1(f(x_n)\neq y_n)\)</span>，称为“0-1 损失”。不幸的是，0-1 损失会导致求最优参数 <span class="math inline">\(w,b\)</span> 时出现组合优化问题。组合优化问题（与第 7 章讨论的连续优化问题不同）一般更难求解。</p><p>那么，SVM 对应的损失函数是什么？考虑预测器 <span class="math inline">\(f(x_n)\)</span> 的输出与标签 <span class="math inline">\(y_n\)</span> 之间的误差。损失描述了在训练数据上产生的错误。推导 (12.26a) 的等价方法是使用<strong>铰链损失（hinge loss）</strong></p><p><span class="math display">\[\ell(t)=\max\{0,1-t\},\quad \text{其中}\ t=yf(x)=y(\langle w,x\rangle+b). \tag{12.28}\]</span></p><p>如果 <span class="math inline">\(f(x)\)</span> 落在超平面对应标签 <span class="math inline">\(y\)</span> 的正确一侧，且距离大于 1，也就是说 <span class="math inline">\(t\ge 1\)</span>，则铰链损失返回 0。如果 <span class="math inline">\(f(x)\)</span> 在正确一侧但离超平面太近（<span class="math inline">\(0&lt;t&lt;1\)</span>），样本 <span class="math inline">\(x\)</span> 处于间隔区内，铰链损失返回一个正值。当样本在超平面的错误一侧（<span class="math inline">\(t&lt;0\)</span>）时，铰链损失返回更大的值，且值线性增加。换句话说，一旦我们距离超平面小于间隔，即使预测正确也要支付代价，而且代价随距离线性增加。</p><p>另一种表达铰链损失的方法是把它看成两个线性分段：</p><p><span class="math display">\[\ell(t)=\begin{cases}0 &amp; \text{如果 } t\ge 1\\[4pt]1-t &amp; \text{如果 } t&lt;1\end{cases}\tag{12.29}\]</span></p><p>如图 12.8 所示。</p><p><img src="/img3/机器学习的数学基础Part2/F12.8.png" alt="F12.8" style="zoom:50%;" /></p><p>对应硬间隔 SVM (公式 12.18) 的损失函数定义为：</p><p><span class="math display">\[\ell(t)=\begin{cases}0 &amp; \text{如果 } t\ge 1\\[4pt]\infty &amp; \text{如果 } t&lt;1\end{cases}\tag{12.30}\]</span></p><p>这个损失可以理解为：<strong>绝不允许任何样本进入间隔之内</strong>。</p><p>对于一个给定的训练集 <span class="math inline">\(\{(x_1,y_1),\ldots,(x_N,y_N)\}\)</span>，我们希望在正则化目标函数的同时最小化总损失，其中正则化采用 <span class="math inline">\(\ell_2\)</span>-正则化（参见第 8.2.3 节）。使用铰链损失 (12.28) 可以得到如下<strong>无约束优化问题</strong>：</p><p><span class="math display">\[\min_{w,b} \;\;\frac{1}{2}\|w\|^2 \quad \text{（正则化项）}+ C\sum_{n=1}^N \max\{0,1-y_n(\langle w,x_n\rangle+b)\}\quad \text{（误差项）}.\tag{12.31}\]</span></p><p>(12.31) 中的第一项称为<strong>正则化项</strong>（或正则器，见第 8.2.3 节），第二项称为<strong>损失项</strong>或<strong>误差项</strong>。回忆第 12.2.4 节，项 <span class="math inline">\(\frac{1}{2}\|w\|^2\)</span> 直接来自于间隔。换句话说，<strong>最大化间隔可以解释为正则化</strong>。</p><p>原则上，(12.31) 中的无约束优化问题可以直接用（子）梯度下降方法求解（参见第 7.1 节）。要看出 (12.31) 和 (12.26a) 等价，只需注意铰链损失 (12.28) 本质上由两段线性部分组成，如 (12.29) 所示。考虑单个样本–标签对的铰链损失 (12.28)，我们可以把对 <span class="math inline">\(t\)</span> 的铰链损失最小化，等价地替换成对一个松弛变量 <span class="math inline">\(\xi\)</span> 的最小化，并加上两个约束。公式形式为：</p><p><span class="math display">\[\min_t \max\{0,1-t\} \tag{12.32}\]</span></p><p>等价于</p><p><span class="math display">\[\min_{\xi,t}\;\xi \quad \text{subject to } \xi\ge 0,\;\xi\ge 1-t. \tag{12.33}\]</span></p><p>把这个表达式代入 (12.31)，并重排其中一个约束，我们就能得到<strong>软间隔 SVM</strong> (12.26a) 的形式。</p><p><strong>注：</strong>让我们对比一下本节选择的损失函数和第 9 章线性回归中的损失函数。回忆第 9.2.1 节，对于求最大似然估计，我们通常最小化负对数似然。此外，因为带高斯噪声的线性回归的似然项是高斯分布，所以每个样本的负对数似然就是一个平方误差函数。<strong>平方误差函数正是我们在寻找最大似然解时最小化的损失函数。</strong></p><h3 id="对偶支持向量机">12.3 对偶支持向量机</h3><p>前面几节中，我们用变量 <span class="math inline">\(w\)</span> 和 <span class="math inline">\(b\)</span> 来描述 SVM，这种形式被称为<strong>原始（primal）SVM</strong>。回忆我们考虑的输入 <span class="math inline">\(x\in \mathbb{R}^D\)</span> 有 <span class="math inline">\(D\)</span> 个特征。由于 <span class="math inline">\(w\)</span> 与 <span class="math inline">\(x\)</span> 维度相同，这意味着在优化问题中参数的数量（即 <span class="math inline">\(w\)</span> 的维度）会随着特征数量线性增长。接下来，我们考虑一个<strong>等价的优化问题</strong>（所谓的<strong>对偶形式</strong>），它与特征数无关。相反，参数的数量随着训练集中的样本数增长。我们在第 10 章中已经看到过类似的思想，当时我们将学习问题表达为一种不随特征数扩展的形式。这对于特征数远多于训练样本数的问题尤其有用。对偶 SVM 还有一个额外的优点，就是它很容易引入核函数（kernel），我们将在本章末尾看到这一点。“对偶”这个词在数学文献中经常出现，在本例中它指的是<strong>凸对偶性</strong>。接下来的各小节本质上是第 7.2 节中讨论过的凸对偶理论的一个应用。</p><h4 id="通过拉格朗日乘子看凸对偶">12.3.1 通过拉格朗日乘子看凸对偶</h4><p>回忆软间隔 SVM 的原始形式 (12.26a)。我们称与原始 SVM 对应的变量 <span class="math inline">\(w\)</span>、<span class="math inline">\(b\)</span> 和 <span class="math inline">\(\xi\)</span> 为<strong>原始变量（primal variables）</strong>。我们使用 <span class="math inline">\(\alpha_n \ge 0\)</span> 作为拉格朗日乘子，对应于约束 (12.26b) 中“样本被正确分类”的要求；使用 <span class="math inline">\(\gamma_n \ge 0\)</span> 作为拉格朗日乘子，对应于松弛变量非负性的约束 (12.26c)。拉格朗日函数为：</p><p><span class="math display">\[\begin{aligned}L(w,b,\xi,\alpha,\gamma) &amp;= \frac{1}{2}\|w\|^2 + C \sum_{n=1}^N \xi_n  \\&amp;\quad - \sum_{n=1}^N \alpha_n \bigl( y_n (\langle w,x_n\rangle + b) - 1 + \xi_n \bigr) \\&amp;\quad - \sum_{n=1}^N \gamma_n \xi_n\end{aligned} \tag{12.34}\]</span></p><p>其中</p><ul><li>第一行是原目标，</li><li>第二行对应约束 (12.26b)，</li><li>第三行对应约束 (12.26c)。</li></ul><p>分别对三个原始变量 <span class="math inline">\(w\)</span>、<span class="math inline">\(b\)</span> 和 <span class="math inline">\(\xi\)</span> 求偏导，我们得到：</p><p><span class="math display">\[\frac{\partial L}{\partial w} = w^\top - \sum_{n=1}^N \alpha_n y_n x_n^\top \tag{12.35}\]</span></p><p><span class="math display">\[\frac{\partial L}{\partial b} = - \sum_{n=1}^N \alpha_n y_n \tag{12.36}\]</span></p><p><span class="math display">\[\frac{\partial L}{\partial \xi_n} = C - \alpha_n - \gamma_n \tag{12.37}\]</span></p><p>通过令这些偏导数分别为零来求解拉格朗日函数的极值。将 (12.35) 置零，可得：</p><p><span class="math display">\[w = \sum_{n=1}^N \alpha_n y_n x_n \tag{12.38}\]</span></p><p>这是“表示定理（representer theorem）”的一个具体实例（Kimeldorf 和 Wahba, 1970）。式 (12.38) 表明，在原始形式下最优的权重向量是样本 <span class="math inline">\(x_n\)</span> 的线性组合。回忆 2.6.1 节的内容，这意味着优化问题的解位于训练数据的线性张成空间内。此外，由 (12.36) 置零得到的约束意味着最优权重向量是样本的一个仿射组合。</p><p>表示定理在非常一般的“正则化经验风险最小化”设定下都成立（Hofmann 等, 2008; Argyriou 和 Dinuzzo, 2014）。该定理还有更一般的版本（Schölkopf 等, 2001），其存在的充要条件可见 Yu 等 (2013)。</p><p><strong>注释：</strong> 表示定理（式 12.38）还解释了“支持向量机”这一名称的来源。对于那些对应参数 <span class="math inline">\(\alpha_n = 0\)</span> 的样本 <span class="math inline">\(x_n\)</span>，它们对解 <span class="math inline">\(w\)</span> 完全没有贡献。而其他满足 <span class="math inline">\(\alpha_n &gt; 0\)</span> 的样本则被称为“支持向量”，因为它们“支撑”了超平面。</p><p>将 <span class="math inline">\(w\)</span> 的表达式代入拉格朗日函数（12.34）后，我们得到对偶式</p><p><span class="math display">\[\begin{align}D(\xi,\alpha,\gamma)&amp; =\frac{1}{2}\, y_i y_j \alpha_i \alpha_j \langle x_i, x_j\rangle- \sum_{i=1}^N y_i\alpha_i + C\sum_{i=1}^N \xi_i+\sum_{i=1}^N \alpha_i \\&amp; -\sum_{i=1}^N y_i \alpha_i-\sum_{i=1}^N\sum_{j=1}^N \alpha_i \xi_i-\sum_{i=1}^N y_j\alpha_j \langle x_j,x_i\rangle- \sum_{i=1}^N \gamma_i \xi_i.\tag{12.39}\end{align}\]</span></p><p>注意，现在不再有任何涉及原始变量 <span class="math inline">\(w\)</span> 的项。通过将式（12.36）设为零，我们得到 <span class="math inline">\(\sum_{n=1}^N y_n \alpha_n = 0\)</span>。因此，涉及 <span class="math inline">\(b\)</span> 的项也消失了。回忆内积是对称且双线性的（见 3.2 节），所以（12.39）中前两项（蓝色部分）是针对同样对象的，可以进行简化，于是我们得到拉格朗日函数</p><p><span class="math display">\[D(\xi,\alpha,\gamma)=-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^Ny_i y_j \alpha_i \alpha_j \langle x_i, x_j\rangle+\sum_{i=1}^N \alpha_i+\sum_{i=1}^N (C-\alpha_i-\gamma_i)\xi_i.\tag{12.40}\]</span></p><p>这个式子的最后一项包含了所有带有松弛变量 <span class="math inline">\(\xi_i\)</span> 的项。通过将（12.37）设为零，我们看到（12.40）中最后一项也为零。此外，利用同一个方程并回忆拉格朗日乘子 <span class="math inline">\(\gamma_i\)</span> 非负，我们可以推出 <span class="math inline">\(\alpha_i \leq C\)</span>。于是我们得到支持向量机的<strong>对偶优化问题</strong>，它仅用拉格朗日乘子 <span class="math inline">\(\alpha_i\)</span> 表示。根据拉格朗日对偶性（定义 7.1），我们要最大化对偶问题。这等价于最小化负的对偶问题，从而得到<strong>对偶 SVM</strong>：</p><p><span class="math display">\[\begin{align}&amp; \min_{\alpha}\quad\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^Ny_i y_j \alpha_i \alpha_j \langle x_i, x_j\rangle-\sum_{i=1}^N \alpha_i\\[6pt] &amp; \text{subject to}\quad\sum_{i=1}^N y_i \alpha_i = 0,\\[4pt]&amp; 0\leq \alpha_i \leq C,\quad i=1,\dots,N.\end{align}\tag{12.41}\]</span></p><p>其中，等式约束 <span class="math inline">\(\sum_{i=1}^N y_i \alpha_i = 0\)</span> 来自将（12.36）设为零；不等式约束 <span class="math inline">\(\alpha_i \ge 0\)</span> 是对不等式约束的拉格朗日乘子的条件（见 7.2 节）；不等式约束 <span class="math inline">\(\alpha_i \le C\)</span> 则是上一段中讨论的结果。</p><p>在 SVM 中的不等式约束集合被称为“盒约束（box constraints）”，因为它们限制拉格朗日乘子向量</p><p><span class="math display">\[\boldsymbol{\alpha} = [\alpha_1,\dots,\alpha_N]^\top \in \mathbb{R}^N\]</span></p><p>在每个坐标轴上都位于由 0 和 <span class="math inline">\(C\)</span> 定义的盒子内。这样的轴对齐盒子在数值求解器中实现起来特别高效（Dostál，2009，第 5 章）。一旦我们得到了对偶参数 <span class="math inline">\(\alpha\)</span>，就可以通过表示定理（12.38）恢复原始参数 <span class="math inline">\(w\)</span>。我们把最优的原始参数称为 <span class="math inline">\(w^*\)</span>。不过，仍然存在如何得到参数 <span class="math inline">\(b^*\)</span> 的问题。考虑一个恰好位于间隔边界上的样本 <span class="math inline">\(x_n\)</span>，即</p><p><span class="math display">\[\langle w^*,x_n\rangle + b = y_n.\]</span></p><p>回忆 <span class="math inline">\(y_n\)</span> 只能取 +1 或 −1。因此唯一未知的是 <span class="math inline">\(b\)</span>，它可以通过下式计算：</p><p><span class="math display">\[b^* = y_n - \langle w^*,x_n\rangle. \tag{12.42}\]</span></p><p><strong>注释。</strong> 原则上，可能没有样本恰好落在间隔上。在这种情况下，我们应该对所有支持向量计算<span class="math inline">\(|y_n - \langle w^*,x_n\rangle|\)</span>，然后取这个绝对值差的<strong>中位数</strong>作为 <span class="math inline">\(b^*\)</span> 的值。推导可以在<a href="http://fouryears.eu/2012/06/07/the-svm-bias-term-conspiracy/">http://fouryears.eu/2012/06/07/the-svm-bias-term-conspiracy/</a> 找到。</p><h4 id="对偶-svm凸包视角">12.3.2 对偶 SVM：凸包视角</h4><p>获得对偶 SVM 的另一种方法是考虑一个替代性的几何论证。考虑所有标签相同的样本 <span class="math inline">\(x_n\)</span> 的集合。我们希望构建一个包含所有样本的<strong>凸集</strong>，并且这个集合尽可能小。这就是所谓的“凸包（convex hull）”，如图 12.9 所示。</p><p><img src="/img3/机器学习的数学基础Part2/F12.9.png" alt="F12.9" style="zoom:50%;" /></p><p>我们先直观理解一下点的凸组合。考虑两个点 <span class="math inline">\(x_1\)</span> 和 <span class="math inline">\(x_2\)</span>，以及对应的非负权重 <span class="math inline">\(\alpha_1,\alpha_2 \ge 0\)</span>，并且 <span class="math inline">\(\alpha_1+\alpha_2=1\)</span>。方程 <span class="math inline">\(\alpha_1x_1+\alpha_2x_2\)</span> 描述了 <span class="math inline">\(x_1\)</span> 和 <span class="math inline">\(x_2\)</span> 之间的每一个点。接下来考虑加入第三个点 <span class="math inline">\(x_3\)</span> 以及权重 <span class="math inline">\(\alpha_3\ge 0\)</span>，满足<span class="math inline">\(\sum_{n=1}^{3}\alpha_n=1\)</span>。这三个点 <span class="math inline">\(x_1,x_2,x_3\)</span> 的凸组合张成了一个二维区域。这个区域的凸包就是由每一对点连成的三角形边界。随着我们加入更多的点，并且点的数量大于维度时，其中一些点将会位于凸包内部，如图 12.9(a) 所示。一般来说，构造一个凸包可以通过为每个样本 <span class="math inline">\(x_n\)</span> 引入非负权重 <span class="math inline">\(\alpha_n \ge 0\)</span> 来完成。这样，凸包可以表示为集合</p><p><span class="math display">\[\text{conv}(X)= \sum_{n=1}^N \alpha_n x_n \quad \text{其中} \quad \sum_{n=1}^N \alpha_n=1,\;\alpha_n\ge 0, \tag{12.43}\]</span></p><p>对于所有 <span class="math inline">\(n=1,\dots,N\)</span> 都成立。如果正类和负类对应的两组点云是分开的，那么它们的凸包就不会重叠。给定训练数据 <span class="math inline">\((x_1,y_1),\dots,(x_N,y_N)\)</span>，我们分别构造正类和负类的两个凸包。</p><blockquote><p>个人注：直观理解</p><ul><li><strong>两个点 <span class="math inline">\(x_1,x_2\)</span></strong>： 取 <span class="math inline">\(\alpha_1=t,\alpha_2=1-t,t\in[0,1]\)</span>，得到 <span class="math inline">\(\alpha_1x_1+\alpha_2x_2\)</span> 是这两个点连线上的所有点；这就是它们的凸包。</li><li><strong>三个点 <span class="math inline">\(x_1,x_2,x_3\)</span></strong>： 同理，权重非负且和 1，就得到三角形内部所有点；这就是三个点的凸包。</li><li><strong>更多点</strong>： 就是把所有点“包”起来的多面体。</li></ul></blockquote><p>我们在正类样本凸包中选取一个点 <span class="math inline">\(c\)</span>，它距离负类分布最近；同样，我们在负类样本凸包中选取一个点 <span class="math inline">\(d\)</span>，它距离正类分布最近；见图 12.9(b)。我们将 <span class="math inline">\(d\)</span> 与 <span class="math inline">\(c\)</span> 的差定义为 <span class="math display">\[w := c - d. \tag{12.44}\]</span></p><p>像上面那样选取 <span class="math inline">\(c\)</span> 和 <span class="math inline">\(d\)</span>，并要求它们彼此尽可能接近，这等价于最小化 <span class="math inline">\(w\)</span> 的长度/范数，从而得到相应的优化问题</p><p><span class="math display">\[\arg\min_w \|w\| = \arg\min_w \frac{1}{2}\|w\|^2. \tag{12.45}\]</span></p><p>由于 <span class="math inline">\(c\)</span> 必须在正类凸包中，它可以表示为正类样本的凸组合，即对于非负系数 <span class="math inline">\(\alpha_n^+\)</span>：</p><p><span class="math display">\[c = \sum_{n:\,y_n=+1}\alpha_n^+ x_n. \tag{12.46}\]</span></p><p>在 (12.46) 中，我们使用记号 <span class="math inline">\(n: y_n=+1\)</span> 来表示标签 <span class="math inline">\(y_n=+1\)</span> 的索引集合。类似地，对于负类样本我们有</p><p><span class="math display">\[d = \sum_{n:\,y_n=-1}\alpha_n^- x_n. \tag{12.47}\]</span></p><p>将 (12.44)、(12.46) 和 (12.47) 代入 (12.45)，我们得到目标函数</p><p><span class="math display">\[\min_{\alpha} \frac{1}{2}\left\|\sum_{n:\,y_n=+1}\alpha_n^+x_n-\sum_{n:\,y_n=-1}\alpha_n^-x_n\right\|^2. \tag{12.48}\]</span></p><p>设 <span class="math inline">\(\alpha\)</span> 是所有系数的集合，即 <span class="math inline">\(\alpha^+\)</span> 和 <span class="math inline">\(\alpha^-\)</span> 的拼接。回忆我们要求每个凸包内的系数之和为 1：</p><p><span class="math display">\[\sum_{n:\,y_n=+1}\alpha_n^+=1,\quad \sum_{n:\,y_n=-1}\alpha_n^-=1. \tag{12.49}\]</span></p><p>这意味着约束条件</p><p><span class="math display">\[\sum_{n=1}^N y_n\alpha_n=0. \tag{12.50}\]</span></p><p>通过将每一类分别展开可以看到这一点：</p><p><span class="math display">\[\sum_{n=1}^N y_n\alpha_n=\sum_{n:\,y_n=+1}(+1)\alpha_n^++\sum_{n:\,y_n=-1}(-1)\alpha_n^- \tag{12.51a}\]</span></p><p><span class="math display">\[=\sum_{n:\,y_n=+1}\alpha_n^+-\sum_{n:\,y_n=-1}\alpha_n^-=1-1=0. \tag{12.51b}\]</span></p><p>目标函数 (12.48) 与约束 (12.50)，再加上 <span class="math inline">\(\alpha \ge 0\)</span> 的假设，给我们一个受约束的（凸）优化问题。可以证明这个优化问题与对偶硬间隔 SVM 的优化问题是相同的（Bennett 和 Bredensteiner，2000a）。</p><p><strong>注释。</strong> 要得到软间隔对偶问题，我们考虑<strong>缩减凸包（reduced hull）</strong>。缩减凸包与凸包类似，但对系数 <span class="math inline">\(\alpha\)</span> 的大小有一个上界。 <span class="math inline">\(\alpha\)</span> 的元素最大可能值限制了凸包所能取的大小。换句话说，对 <span class="math inline">\(\alpha\)</span> 的上界将凸包缩小到一个更小的体积（Bennett 和 Bredensteiner，2000b）。</p><h3 id="核函数">12.4 核函数</h3><p>Kernels</p><p>考虑对偶 SVM（式 12.41）的形式。注意到，目标函数中的内积只出现在样本 <span class="math inline">\(x_i\)</span> 和 <span class="math inline">\(x_j\)</span> 之间；并没有样本与参数之间的内积。因此，如果我们用一组特征 <span class="math inline">\(\phi(x_i)\)</span> 来表示 <span class="math inline">\(x_i\)</span>，在对偶 SVM 中唯一的变化就是把内积替换掉。这种<strong>模块化的形式，使得分类方法（SVM 的选择）与特征表示 <span class="math inline">\(\phi(x)\)</span> 的选择可以分开考虑，从而为我们独立探索这两个问题提供了灵活性。</strong>本节我们讨论 <span class="math inline">\(\phi(x)\)</span> 的表示，并简要介绍核函数的概念，但不深入技术细节。</p><p>由于 <span class="math inline">\(\phi(x)\)</span> 可以是一个非线性函数，我们就可以使用假设线性分类器的 SVM 来构造在样本 <span class="math inline">\(x_n\)</span> 上<strong>非线性</strong>的分类器。这为处理非线性可分的数据集提供了第二条途径（第一条是软间隔）。事实证明，有许多算法和统计方法也具有我们在对偶 SVM 中看到的这种性质：内积只出现在样本之间。我们无需显式定义非线性特征映射 <span class="math inline">\(\phi(\cdot)\)</span> 并计算样本 <span class="math inline">\(x_i\)</span> 与 <span class="math inline">\(x_j\)</span> 之间的内积，而是定义样本 <span class="math inline">\(x_i\)</span> 与 <span class="math inline">\(x_j\)</span> 之间的相似度函数 <span class="math inline">\(k(x_i,x_j)\)</span>。对于某一类<strong>相似度函数（称为核函数）</strong>，相似度函数<strong>隐式地</strong>定义了一个非线性特征映射 <span class="math inline">\(\phi(\cdot)\)</span>。</p><blockquote><p>个人注：</p><ul><li><strong>软间隔 SVM + 无核函数</strong> → 仍然是线性分割（超平面）。</li><li><strong>软间隔 SVM + 核函数</strong> → 在原空间里是非线性分割（在特征空间里仍然是线性超平面）。</li></ul></blockquote><p>核函数按定义是函数 <span class="math inline">\(k: \mathcal{X}\times\mathcal{X}\to\mathbb{R}\)</span>，满足存在一个希尔伯特空间 <span class="math inline">\(\mathcal{H}\)</span> 以及一个特征映射 <span class="math inline">\(\phi: \mathcal{X}\to\mathcal{H}\)</span>，使得</p><p><span class="math display">\[k(x_i,x_j)=\langle\phi(x_i),\phi(x_j)\rangle_{\mathcal{H}}. \tag{12.52}\]</span></p><p>每个核函数 <span class="math inline">\(k\)</span> 都有一个唯一的再生核希尔伯特空间（RKHS）与之对应（Aronszajn, 1950；Berlinet 和 Thomas-Agnan, 2004）。在这种唯一对应下，<span class="math inline">\(\phi(x)=k(\cdot,x)\)</span> 被称为<strong>规范特征映射</strong>（canonical feature map）。从<strong>内积推广到核函数（式 12.52）被称为核技巧（kernel trick），它把显式的非线性特征映射隐藏了起来，</strong>（Schölkopf 和 Smola, 2002；Shawe-Taylor 和 Cristianini, 2004）。</p><blockquote><p>个人注：注意<strong>核函数（kernel function）</strong>和<strong>核密度函数（kernel density function）</strong>的区别。详见《<strong>核函数</strong>和<strong>核密度函数</strong>的区别.md》,特别是高斯核（RBF核）<span class="math inline">\(k(x,x&#39;)=\exp(-\|x-x&#39;\|^2/2\sigma^2)\)</span> 和 高斯核密度函数 <span class="math inline">\(K(u)=\frac{1}{\sqrt{2\pi}}e^{-u^2/2}\)</span></p><p><strong>例子</strong>：</p><ul><li>线性核 <span class="math inline">\(k(x,x&#39;)=x^\top x&#39;\)</span></li><li>多项式核 <span class="math inline">\(k(x,x&#39;)=(x^\top x&#39;+c)^d\)</span></li><li>高斯核（RBF核）<span class="math inline">\(k(x,x&#39;)=\exp(-\|x-x&#39;\|^2/2\sigma^2)\)</span></li></ul><p>注意：这里的“核”只是指“内积核（Mercer核）”，与概率密度没有关系。</p></blockquote><p>矩阵 <span class="math inline">\(K \in \mathbb{R}^{N \times N}\)</span>，由内积或将 <span class="math inline">\(k(\cdot,\cdot)\)</span> 应用于一个数据集而得到，被称为<strong>Gram 矩阵</strong>，通常也直接称为<strong>核矩阵</strong>。核函数必须是对称且半正定的函数，这样每一个核矩阵 <span class="math inline">\(K\)</span> 都是对称且半正定的（见 3.2.3 节）：</p><p><span class="math display">\[\forall z\in\mathbb{R}^N: \quad z^\top K z \ge 0. \tag{12.53}\]</span></p><p>对于多元实值数据 <span class="math inline">\(x_i\in\mathbb{R}^D\)</span>，一些常见的核函数包括多项式核、Gaussian 径向基函数核（RBF 核），以及有理二次核（Schölkopf 和 Smola, 2002；Rasmussen 和 Williams, 2006）。图 12.10 展示了不同核函数对分离超平面的影响。<strong>请注意，我们仍然在求解超平面，也就是说，假设空间里的函数依然是线性的；那些非线性分割面是由核函数造成的。</strong>（个人注：正如本节开头说的模块化！）</p><p><img src="/img3/机器学习的数学基础Part2/F12.10.png" alt="F12.10" style="zoom:50%;" /></p><p><strong>注：</strong> <strong>对于刚接触机器学习的人来说，不幸的是，“核”这个词有多种含义。在本章中，“核”一词来自再生核希尔伯特空间（RKHS）的概念（Aronszajn, 1950；Saitoh, 1988）。我们已经在 2.7.3 节线性代数中讨论过“kernel”的另一种含义，即核空间或零空间（null space）。在机器学习中，“kernel”还有第三种常见的用法，即核密度估计（11.5 节）中的平滑核（smoothing kernel）。</strong></p><p>由于<strong>显式表示 <span class="math inline">\(\phi(x)\)</span> 在数学上与核表示 <span class="math inline">\(k(x_i,x_j)\)</span> 等价</strong>，实践中往往会设计核函数使其比显式特征映射之间的内积计算更高效。例如，多项式核（Schölkopf 和 Smola, 2002），当输入维度很大时，即便是低阶多项式，其显式展开的项数也会迅速增长；而<strong>核函数</strong>每个输入维度只需要一次乘法，从而显著<strong>节省计算量</strong>。另一个例子是 Gaussian 径向基核函数（Schölkopf 和 Smola, 2002；Rasmussen 和 Williams, 2006），其对应的特征空间是无限维的。在这种情况下，我们无法显式表示特征空间，但仍然可以通过核函数计算任意一对样本之间的相似度。</p><p>核技巧的另一个有用之处在于：原始数据不必已经表示为多元实值数据。注意，内积是定义在函数 <span class="math inline">\(\phi(\cdot)\)</span> 的输出上的，但并不限制输入必须是实数。因此，函数 <span class="math inline">\(\phi(\cdot)\)</span> 和核函数 <span class="math inline">\(k(\cdot,\cdot)\)</span> 可以定义在任意对象上，例如集合、序列、字符串、图、分布等（Ben-Hur 等, 2008；Gärtner, 2008；Shi 等, 2009；Sriperumbudur 等, 2010；Vishwanathan 等, 2010）。</p><h3 id="数值求解">12.5 数值求解</h3><p>我们通过考察如何将本章推导出的 SVM 问题用第 7 章介绍的概念来表达，来结束对 SVM 的讨论。我们考虑两种不同的方法来求解 SVM 的最优解。首先，我们考虑 SVM 的损失函数视角（8.2.2 节），并把它表达为一个无约束优化问题。然后，我们把 SVM 的原始形式和对偶形式的约束版本都表示为标准形式（7.3.2 节）的二次规划问题。</p><p>考虑 SVM 的损失函数视角（公式 12.31）。这是一个凸的无约束优化问题，但铰链损失（公式 12.28）不可微。因此，我们采用次梯度方法来求解它。不过，铰链损失几乎在所有地方都是可微的，除了铰链处 <span class="math inline">\(t=1\)</span> 的单一点。在该点处，梯度是一组可能值，介于 0 和 -1 之间。因此，铰链损失的次梯度 <span class="math inline">\(g\)</span> 表示为：</p><p><span class="math display">\[g(t)=\begin{cases}-1, &amp; t&lt;1\\[4pt][-1,0], &amp; t=1\\[4pt]0, &amp; t&gt;1\end{cases}\tag{12.54}\]</span></p><p>利用这个次梯度，我们可以应用第 7.1 节介绍的优化方法。</p><p>SVM 的原始形式和对偶形式都可以归结为一个凸二次规划问题（带约束的优化问题）。注意，公式 (12.26a) 中的原始 SVM 的优化变量大小为输入样本的维度 <span class="math inline">\(D\)</span>；而公式 (12.41) 中的对偶 SVM 的优化变量大小为样本数 <span class="math inline">\(N\)</span>。</p><p>为了将原始 SVM（primal SVM）写成标准二次规划（7.45）的形式，我们假设内积使用的是点积 (3.5)。我们重新排列原始 SVM (12.26a) 的公式，使得所有的优化变量都位于右边，并且约束的不等式形式与标准形式匹配。这样得到如下的优化问题：</p><p><span class="math display">\[\begin{aligned}\min_{w,b,\xi} \quad &amp; \frac{1}{2}\|w\|^2 + C\sum_{n=1}^{N}\xi_n \\\text{s.t.}\quad &amp; -y_n x_n^\top w - y_n b - \xi_n \le -1, \\&amp; -\xi_n \le 0, \quad n=1,\dots,N.\end{aligned}\tag{12.55}\]</span></p><p>通过把变量 <span class="math inline">\(w,b,\xi\)</span> 合并成一个单一的向量，并小心地收集各个项，我们得到软间隔 SVM 的如下矩阵形式：</p><p><span class="math display">\[\begin{align}\min_{w,b,\xi} \quad &amp;\begin{bmatrix}b\\[3pt]w\\[3pt]\xi\end{bmatrix}^{\!\top}\begin{bmatrix}I_D &amp; 0_{D,N+1}\\[3pt]0_{N+1,D} &amp; 0_{N+1,N+1}\end{bmatrix}\begin{bmatrix}b\\[3pt]w\\[3pt]\xi\end{bmatrix} \\[5pt]\text{s.t.}\quad &amp;\begin{bmatrix} -Y X - y - I_N &amp; 0_{N,D+1} - I_N\end{bmatrix}\begin{bmatrix}b\\[3pt]w\\[3pt]\xi\end{bmatrix}+ \begin{bmatrix}0_{D+1,1}\\[3pt] C\,1_{N,1}\end{bmatrix}\le \begin{bmatrix}-1_{N,1}\\[3pt]0_{N,1}\end{bmatrix}.\tag{12.56}\end{align}\]</span></p><p>在上述优化问题中，最小化是针对参数</p><p><span class="math display">\[[w^\top,b,\xi^\top]^\top \in \mathbb{R}^{D+1+N},\]</span></p><p>并且我们用以下记号：</p><ul><li><span class="math inline">\(I_m\)</span> 表示大小为 <span class="math inline">\(m\times m\)</span> 的单位矩阵，</li><li><span class="math inline">\(0_{m,n}\)</span> 表示大小为 <span class="math inline">\(m\times n\)</span> 的全零矩阵，</li><li><span class="math inline">\(1_{m,n}\)</span> 表示大小为 <span class="math inline">\(m\times n\)</span> 的全 1 矩阵。</li></ul><p>此外，<span class="math inline">\(y\)</span> 是标签向量 <span class="math inline">\([y_1,\dots,y_N]^\top\)</span>，<span class="math inline">\(Y=\mathrm{diag}(y)\)</span> 是一个 <span class="math inline">\(N\times N\)</span> 的对角矩阵，其对角元素来自 <span class="math inline">\(y\)</span>，而 <span class="math inline">\(X\in \mathbb{R}^{N\times D}\)</span> 是将所有样本拼接得到的矩阵。</p><p>我们同样可以对 SVM (12.41) 的对偶形式进行项的收集。为了将对偶 SVM 写成标准形式，我们首先必须表示核矩阵 <span class="math inline">\(K\)</span>，其中每个元素为 <span class="math inline">\(K_{ij}=k(x_i,x_j)\)</span>。如果我们有显式的特征表示 <span class="math inline">\(x_i\)</span>，那么我们定义<span class="math inline">\(K_{ij}=\langle x_i,x_j\rangle\)</span>。为了方便记号，我们引入一个矩阵 <span class="math inline">\(Y=\mathrm{diag}(y)\)</span>，它在对角线上存储标签，其余位置全为 0。对偶 SVM 可以写为： <span class="math display">\[\begin{align}\min_{\alpha}\quad &amp; \frac{1}{2}\,\alpha^\top YKY\alpha - \mathbf{1}_{N,1}^\top \alpha \\\text{s.t.}\quad &amp;\begin{bmatrix}y^\top\\[3pt]- y^\top\\[3pt]- I_N\\[3pt]I_N\end{bmatrix} \alpha \;\le\;\begin{bmatrix}0_{N+2,1}\\[3pt]C\mathbf{1}_{N,1}\end{bmatrix}.\tag{12.57}\end{align}\]</span></p><p><strong>备注：</strong> 在 7.3.1 和 7.3.2 节，我们介绍了标准形式的约束均写成不等式约束。我们将对偶 SVM 的等式约束表示为两个不等式约束，即 <span class="math display">\[Ax=b\quad \text{被替换为}\quad Ax\le b \quad \text{和}\quad Ax\ge b。 \tag{12.58}\]</span></p><p>某些凸优化算法的软件实现可能直接支持等式约束。</p><p>由于对 SVM 存在多种不同的视角，因此求解相应优化问题的方法也有很多。这里介绍的这种将 SVM 问题写成标准凸优化形式的方法在实践中并不常用。SVM 求解器的两个主要实现分别是 Chang 和 Lin (2011)（开源）以及 Joachims (1999)。由于 SVM 具有清晰且定义良好的优化问题，因此可以应用许多基于数值优化技术（Nocedal 和 Wright, 2006）的方法（Shawe-Taylor 和 Sun, 2011）。</p><h3 id="延伸阅读">12.6 延伸阅读</h3><p>支持向量机（SVM）只是研究二分类问题的众多方法之一。其他方法包括感知机、逻辑回归、Fisher 判别、最近邻、朴素贝叶斯以及随机森林（Bishop，2006；Murphy，2012）。Ben-Hur 等（2008）给出了一个关于离散序列上 SVM 与核方法的简短教程。</p><p>SVM 的发展与第 8.2 节讨论的<strong>经验风险最小化</strong>密切相关，因此 SVM 拥有坚实的理论性质（Vapnik，2000；Steinwart 和 Christmann，2008）。关于核方法的书（Schölkopf 和 Smola，2002）包含了大量有关支持向量机及其优化的细节。Shawe-Taylor 和 Cristianini（2004）撰写的一本更广泛的核方法书籍，也包含了许多针对不同机器学习问题的线性代数方法。</p><p>SVM 的对偶形式可以用 <strong>Legendre–Fenchel 变换</strong>（第 7.3.3 节）的思想推导出来。该推导将 SVM 的无约束形式 (12.31) 中的每一项分别考虑，并计算其凸共轭（Rifkin 和 Lippert，2007）。对 SVM 感兴趣的读者（尤其是函数分析视角或正则化方法视角）可参考 Wahba（1990）的工作。核方法的理论阐述（Aronszajn，1950；Schwartz，1964；Saitoh，1988；Manton 和 Amblard，2015）需要线性算子（Akhiezer 和 Glazman，1993）的基本知识作为基础。核方法的思想已推广到巴拿赫空间（Zhang 等，2009）和克雷因空间（Ong 等，2004；Loosli 等，2016）。</p><p>需要注意的是，hinge 损失（铰链损失）有三种等价表示，如 (12.28) 和 (12.29)，以及 (12.33) 中的约束优化问题。公式 (12.28) 常用于比较 SVM 损失函数与其他损失函数（Steinwart，2007）。两段式表示 (12.29) 便于计算次梯度，因为每一段都是线性的。第三种表示 (12.33)，如 12.5 节所示，使得可以使用凸二次规划（第 7.3.2 节）工具。</p><p>由于二分类在机器学习中是一个研究得非常充分的任务，人们有时也会使用其他词语来描述它，例如“判别”“分离”和“决策”。此外，二分类器的输出可以有三种不同的形式：</p><ul><li><p><strong>第一种</strong>是线性函数本身的输出（通常称为“分数”score），它可以取任意实数值。这个输出可以用来对样本进行排序，而二分类可以被看作是在排好序的样本上选择一个阈值（Shawe-Taylor 和 Cristianini，2004）。</p></li><li><p><strong>第二种</strong>常被视为二分类器输出的，是将线性输出经过一个非线性函数后得到的结果，用来将取值限制在一个有界范围内，例如区间 <span class="math inline">\([0,1]\)</span>。常见的非线性函数是 sigmoid 函数（Bishop，2006）。当这种非线性转换得到的是经过良好校准的概率（Gneiting 和 Raftery，2007；Reid 和 Williamson，2011）时，这称为<strong>类别概率估计</strong>（class probability estimation）。</p></li><li><p><strong>第三种</strong>输出是最终的二元决策 <span class="math inline">\(\{+1, -1\}\)</span>，这也是最常被假设为分类器输出的形式。</p></li></ul><p>支持向量机（SVM）是一种二分类器，它本身并不自然地对应于概率解释。<strong>有多种方法可以将线性函数（分数）的原始输出转化为一个经过良好校准的类别概率估计 <span class="math inline">\(P(Y=1|X=x)\)</span>，这需要一个额外的校准步骤（Platt，2000；Zadrozny 和 Elkan，2001；Lin 等，2007）。</strong></p><p>从训练的角度看，有许多与概率相关的方法。我们在 12.2.5 节的末尾提到过损失函数与似然之间的关系（也可比较 8.2 节和 8.3 节）。在训练过程中对应于良好<strong>校准的转换的最大似然方法称为逻辑回归</strong>，它来自一类称为<strong>广义线性模型</strong>（generalized linear models）的方法。从这一视角看逻辑回归的细节可参见 Agresti（2002，第 5 章）以及 McCullagh 和 Nelder（1989，第 4 章）。</p><p>当然，人们也可以用更<strong>贝叶斯的视角</strong>来看待分类器输出，即通过贝叶斯逻辑回归来估计后验分布。贝叶斯视角还包括<strong>先验的设定</strong>，这其中包含了与似然的共轭性（第 6.6.1 节）等设计选择。此外，人们还可以将潜在函数视为先验，这样就得到<strong>高斯过程分类</strong>（Rasmussen 和 Williams，2006，第 3 章）。</p>]]></content>
      
      
      <categories>
          
          <category> 翻译 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 数学 </tag>
            
            <tag> 算法 </tag>
            
            <tag> 统计 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《机器学习的数学基础》第11章&quot;高斯混合模型的密度估计&quot;</title>
      <link href="/2025/09/09/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E7%AC%AC11%E7%AB%A0%22%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AF%86%E5%BA%A6%E4%BC%B0%E8%AE%A1%22/"/>
      <url>/2025/09/09/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E7%AC%AC11%E7%AB%A0%22%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AF%86%E5%BA%A6%E4%BC%B0%E8%AE%A1%22/</url>
      
        <content type="html"><![CDATA[<h2 id="第11章-高斯混合模型的密度估计">第11章 高斯混合模型的密度估计</h2><p>Density Estimation with Gaussian Mixture Models</p><p>在前几章中，我们已经讨论了机器学习中的两个基本问题：回归（第 9 章）和降维（第 10 章）。在本章中，我们将探讨机器学习的第三个支柱：<strong>密度估计</strong>。在这一过程中，我们会引入一些重要的概念，例如期望最大化（EM）算法，以及通过混合模型来理解密度估计的潜在变量视角。</p><p><img src="/img3/机器学习的数学基础Part2/F11.1.png" alt="F11.1" style="zoom:50%;" /></p><p>当我们将机器学习应用于数据时，往往希望<strong>以某种方式来表示数据</strong>。<strong>一个直接的方法就是将数据点本身作为数据的表示</strong>；例如见图 11.1。 然而，如果数据集非常庞大，或者我们更关心数据的整体特征，那么这种方式可能就不太有用了。<strong>在密度估计中，我们通过使用某个参数化分布族中的分布（例如高斯分布或 Beta 分布），来紧凑地表示数据。</strong>比如，我们可能希望通过数据集的均值和方差，用一个高斯分布来进行紧凑表示。均值和方差可以通过第 8.3 节讨论的工具（最大似然估计或最大后验估计）来求得。接着，<strong>我们就可以用这个高斯分布的均值和方差来表示数据背后的分布，即我们认为该数据集是从这个分布中采样得到的一次典型实现。</strong></p><blockquote><p>个人注：高斯混合模型的<strong>密度估计</strong>是一种表示数据的方法！其实也是一种建模，因为这个分布就是对应该类数据的模型表达。</p></blockquote><span id="more"></span><p>在实际应用中，高斯分布（或类似我们迄今遇到的其他分布）具有有限的建模能力。例如，用一个高斯分布去近似图 11.1 中生成数据的真实密度，就会是一个很差的近似。接下来，我们将研究一种更具表现力的分布族，可以用来进行密度估计：<strong>混合模型</strong>。</p><p>混合模型可以通过 <strong>K 个简单（基）分布的凸组合</strong> 来描述一个分布 <span class="math inline">\(p(x)\)</span>： <span class="math display">\[p(x) = \sum_{k=1}^K \pi_k p_k(x) \tag{11.1}\]</span> 满足： <span class="math display">\[0 \leq \pi_k \leq 1, \quad \sum_{k=1}^K \pi_k = 1 \tag{11.2}\]</span> 其中，各个成分分布 <span class="math inline">\(p_k\)</span> 属于某个基本分布族，例如高斯分布、伯努利分布或伽马分布，而 <span class="math inline">\(\pi_k\)</span> 是混合权重。混合模型比相应的单个基分布更具表现力，因为它们可以表示多峰的数据分布，即能够描述具有多个“簇”的数据集，就像图 11.1 中的例子。</p><p>我们将重点讨论<strong>高斯混合模型（GMMs）</strong>，其中的基本分布是高斯分布。给定一个数据集，我们的目标是通过最大化模型参数的似然来训练 GMM。为此，我们将会用到第 5 章、第 6 章和第 7.2 节的结果。然而，与之前讨论过的应用（线性回归或主成分分析）不同，这里我们无法得到最大似然的闭式解。相反，我们将得到一组相互依赖的联立方程，而这些方程只能通过<strong>迭代方法</strong>来求解。</p><h3 id="高斯混合模型">11.1 高斯混合模型</h3><p>高斯混合模型（Gaussian Mixture Model, GMM）是一种密度模型，它通过组合有限个 <span class="math inline">\(K\)</span> 个高斯分布 <span class="math inline">\(\mathcal{N}(x|\mu_k, \Sigma_k)\)</span> 来表示： <span class="math display">\[p(x|\theta) = \sum_{k=1}^K \pi_k \, \mathcal{N}(x|\mu_k, \Sigma_k) \tag{11.3}\]</span> 其中： <span class="math display">\[0 \leq \pi_k \leq 1, \quad \sum_{k=1}^K \pi_k = 1, \tag{11.4}\]</span> 我们定义 <span class="math display">\[\theta := \{ \mu_k, \Sigma_k, \pi_k : k = 1, \ldots, K \}\]</span> 为该模型的所有参数集合。</p><p>这种对高斯分布的凸组合，相比单一的高斯分布，能为复杂密度的建模提供更强的灵活性（当 <span class="math inline">\(K=1\)</span> 时，(11.3) 就退化为单一高斯分布）。图 11.2 给出了一个示例，展示了加权后的各个分量以及最终的混合密度： <span class="math display">\[p(x|\theta) = 0.5 \, \mathcal{N}(x|-2, \tfrac{1}{2}) + 0.2 \, \mathcal{N}(x|1, 2) + 0.3 \, \mathcal{N}(x|4, 1). \tag{11.5}\]</span> <img src="/img3/机器学习的数学基础Part2/F11.2.png" alt="F11.2" style="zoom:50%;" /></p><h3 id="最大似然参数学习">11.2 最大似然参数学习</h3><p>假设我们给定一个数据集<span class="math inline">\(X = \{x_1, \ldots, x_N\},\)</span>其中 <span class="math inline">\(x_n, n=1,\ldots,N\)</span>，是从未知分布 <span class="math inline">\(p(x)\)</span> 中独立同分布采样得到的。我们的目标是用一个含有 <span class="math inline">\(K\)</span> 个混合分量的 GMM 来近似/表示这个未知分布 <span class="math inline">\(p(x)\)</span>。GMM 的参数包括： <span class="math inline">\(K\)</span> 个均值 <span class="math inline">\(\mu_k\)</span>， 协方差矩阵 <span class="math inline">\(\Sigma_k\)</span>， 混合权重 <span class="math inline">\(\pi_k\)</span>。我们把这些自由参数统一记为：<span class="math inline">\(\theta := \{\pi_k, \mu_k, \Sigma_k : k = 1, \ldots, K\}.\)</span></p><p><img src="/img3/机器学习的数学基础Part2/F11.3.png" alt="F11.3" style="zoom:50%;" /></p><p><strong>例 11.1（初始设定）</strong></p><p>在本章中，我们将用一个简单的贯穿示例来帮助说明和可视化关键概念。我们考虑一个一维数据集</p><p><span class="math display">\[X = \{-3, -2.5, -1, 0, 2, 4, 5\},\]</span> 它包含 7 个数据点。我们希望用一个含有 <span class="math inline">\(K=3\)</span> 个分量的 GMM 来建模该数据的密度。初始化混合分量如下： <span class="math display">\[p_1(x) = \mathcal{N}(x|-4, 1) \tag{11.6}\]</span></p><p><span class="math display">\[p_2(x) = \mathcal{N}(x|0, 0.2) \tag{11.7}\]</span></p><p><span class="math display">\[p_3(x) = \mathcal{N}(x|8, 3) \tag{11.8}\]</span></p><p>并给它们分配相等的权重： <span class="math display">\[\pi_1 = \pi_2 = \pi_3 = \tfrac{1}{3}.\]</span> 相应的模型（以及数据点）如图 11.3 所示。</p><p>在下面的内容中，我们将详细说明如何得到模型参数 θ 的最大似然估计 <span class="math inline">\(θ_{ML}\)</span>。我们首先写出似然函数，即在给定参数的情况下训练数据的预测分布。我们利用独立同分布 (i.i.d.) 假设，这会导致似然函数的分解形式： <span class="math display">\[p(X|\theta) = \prod_{n=1}^N p(x_n|\theta), \quad p(x_n|\theta) = \sum_{k=1}^K \pi_k \, \mathcal{N}(x_n|\mu_k,\Sigma_k), \tag{11.9}\]</span> 其中每个单独的似然项 <span class="math inline">\(p(x_n|\theta)\)</span> 都是一个高斯混合密度。然后我们得到对数似然： <span class="math display">\[\log p(X|\theta) = \sum_{n=1}^N \log p(x_n|\theta) = \sum_{n=1}^N \log \Bigg( \sum_{k=1}^K \pi_k \, \mathcal{N}(x_n|\mu_k,\Sigma_k) \Bigg). \tag{11.10}\]</span> 记作 <span class="math inline">\(L\)</span>。我们的目标是找到能最大化对数似然 <span class="math inline">\(L\)</span> 的参数 <span class="math inline">\(\theta^*_{ML}\)</span>。通常的做法是计算对数似然关于模型参数 θ 的梯度 <span class="math inline">\(dL/d\theta\)</span>，令其等于 0，然后解出 θ。</p><p>然而，与之前最大似然估计的例子（例如第 9.2 节讨论的线性回归）不同，这里无法得到一个封闭解。但我们可以利用一种迭代方法来找到较好的模型参数 <span class="math inline">\(θ_{ML}\)</span>，这实际上就是用于<strong>高斯混合模型 (GMM) 的 EM 算法。关键思想是：在保持其他参数固定的情况下，逐一更新模型参数。</strong></p><p><strong>备注</strong>：如果我们考虑的目标密度只是单个高斯分布，那么式 (11.10) 中对<span class="math inline">\(k\)</span>的求和就消失了，此时对数可以直接作用于高斯分布，从而得到： <span class="math display">\[\log \mathcal{N}(x|\mu,\Sigma) = -\frac{D}{2}\log(2\pi) - \tfrac{1}{2}\log \det(\Sigma) - \tfrac{1}{2}(x-\mu)^\top \Sigma^{-1}(x-\mu). \tag{11.11}\]</span> 这个简单的形式使得我们可以得到 <span class="math inline">\(\mu\)</span> 和 <span class="math inline">\(\Sigma\)</span> 的封闭形式最大似然估计（第 8 章已讨论）。但在 (11.10) 中，我们无法将对数移入对<span class="math inline">\(k\)</span>的求和中，因此无法得到简单的封闭解。</p><p>任何函数的局部最优点都必须满足梯度关于参数为零的性质（必要条件，见第 7 章）。在我们的情形下，对数似然 (11.10) 关于 GMM 参数 <span class="math inline">\(\mu_k, \Sigma_k, \pi_k\)</span> 的优化给出了如下必要条件： <span class="math display">\[\begin{align}\frac{\partial \mathcal{L}}{\partial \mu_k} = \mathbf{0}^\top &amp;\iff \sum_{n=1}^N \frac{\partial \log p(x_n \mid \theta)}{\partial \mu_k} = \mathbf{0}^\top, \tag{11.12}\\[1em]\frac{\partial \mathcal{L}}{\partial \Sigma_k} = 0 &amp;\iff \sum_{n=1}^N \frac{\partial \log p(x_n \mid \theta)}{\partial \Sigma_k} = 0, \tag{11.13}\\[1em]\frac{\partial \mathcal{L}}{\partial \pi_k} = 0 &amp;\iff \sum_{n=1}^N \frac{\partial \log p(x_n \mid \theta)}{\partial \pi_k} = 0. \tag{11.14}\end{align}\]</span> 对于所有这三类必要条件，通过应用链式法则（见 5.2.2 节），我们需要如下形式的偏导数： <span class="math display">\[\frac{\partial \log p(x_n|\theta)}{\partial \theta} = \frac{1}{p(x_n|\theta)} \, \frac{\partial p(x_n|\theta)}{\partial \theta}, \tag{11.15}\]</span> 其中 <span class="math inline">\(\theta = \{\mu_k, \Sigma_k, \pi_k, k=1,\ldots,K\}\)</span> 是模型参数，并且 <span class="math display">\[\frac{1}{p(x_n|\theta)} = \frac{1}{\sum_{j=1}^K \pi_j \, \mathcal{N}(x_n|\mu_j,\Sigma_j)}. \tag{11.16}\]</span> 在接下来的内容中，我们将计算 (11.12) 到 (11.14) 的偏导数。但在此之前，我们先引入一个将在本章余下部分起核心作用的重要量：<strong>责任 (responsibilities)</strong>。</p><h4 id="责任">11.2.1 责任</h4><p>Responsibilities</p><p>我们定义如下量： <span class="math display">\[r_{nk} := \frac{\pi_k \mathcal{N}(x_n|\mu_k, \Sigma_k)}{\sum_{j=1}^K \pi_j \mathcal{N}(x_n|\mu_j, \Sigma_j)} \tag{11.17}\]</span> 它表示<strong>第 <span class="math inline">\(k\)</span> 个混合成分对第 <span class="math inline">\(n\)</span> 个数据点的责任</strong>。第 <span class="math inline">\(k\)</span> 个混合成分对数据点 <span class="math inline">\(x_n\)</span> 的责任 <span class="math inline">\(r_{nk}\)</span>，与下式成正比：</p><p><span class="math display">\[p(x_n|\pi_k, \mu_k, \Sigma_k) = \pi_k \mathcal{N}(x_n|\mu_k, \Sigma_k) \tag{11.18}\]</span> 即该混合成分在给定数据点下的似然。因此，当某个数据点更可能是由某个混合成分生成时，该成分对该数据点的责任就会更大。注意：</p><p><span class="math display">\[r_n := [r_{n1}, \ldots, r_{nK}]^\top \in \mathbb{R}^K\]</span> 是一个<strong>归一化的概率向量</strong>，即满足 <span class="math inline">\(\sum_k r_{nk} = 1\)</span>，且 <span class="math inline">\(r_{nk} \geq 0\)</span>。这个概率向量在 <span class="math inline">\(K\)</span> 个混合成分之间分配概率质量，我们可以把 <span class="math inline">\(r_n\)</span> 看作是对 <span class="math inline">\(x_n\)</span> 在 <span class="math inline">\(K\)</span> 个混合成分之间的“软分配”。因此，公式 (11.17) 中的责任 <span class="math inline">\(r_{nk}\)</span>，就代表了 <span class="math inline">\(x_n\)</span> 是由第 <span class="math inline">\(k\)</span> 个混合成分生成的概率。</p><p><strong>例 11.2 （责任）</strong></p><p>针对图 11.3 中的例子，我们计算得到责任 <span class="math inline">\(r_{nk}\)</span>： <span class="math display">\[\begin{bmatrix}1.0 &amp; 0.0 &amp; 0.0 \\1.0 &amp; 0.0 &amp; 0.0 \\0.057 &amp; 0.943 &amp; 0.0 \\0.001 &amp; 0.999 &amp; 0.0 \\0.0 &amp; 0.066 &amp; 0.934 \\0.0 &amp; 0.0 &amp; 1.0 \\0.0 &amp; 0.0 &amp; 1.0\end{bmatrix}\in \mathbb{R}^{N \times K}. \tag{11.19}\]</span> 这里，第 <span class="math inline">\(n\)</span> 行表示所有混合成分对数据点 <span class="math inline">\(x_n\)</span> 的责任。每个数据点的 <span class="math inline">\(K\)</span> 个责任（即每一行的和）都等于 1。第 <span class="math inline">\(k\)</span> 列则概括了第 <span class="math inline">\(k\)</span> 个混合成分的责任分布。我们可以看到，第三个混合成分（第三列）对前四个数据点没有责任，但在后面的数据点上承担了较多责任。某一列所有元素的和给出了 <span class="math inline">\(N_k\)</span>，即第 <span class="math inline">\(k\)</span> 个混合成分的总责任。在本例中，我们得到：</p><p><span class="math display">\[N_1 = 2.058, \quad N_2 = 2.008, \quad N_3 = 2.934.\]</span> 下面我们来确定在给定责任值的情况下，模型参数<span class="math inline">\(\mu_k, \Sigma_k, \pi_k\)</span> 的更新方式。我们将看到，这些更新方程都依赖于责任值，这使得最大似然估计问题无法得到闭式解。然而，在给定责任值的情况下，我们会一次只更新一个模型参数，同时保持其他参数固定。之后，我们会重新计算责任值。通过反复迭代这两个步骤，最终会收敛到一个局部最优解，这就是 EM 算法的一种具体实现。我们将在第 11.3 节更详细地讨论这一点。</p><h4 id="更新均值">11.2.2 更新均值</h4><p><strong>定理 11.1（GMM 均值的更新）</strong>GMM 的均值参数 <span class="math inline">\(\mu_k, k=1,\dots,K\)</span> 的更新公式为 <span class="math display">\[\mu_k^{\text{new}} = \frac{\sum_{n=1}^N r_{nk} x_n}{\sum_{n=1}^N r_{nk}}, \tag{11.20}\]</span> 其中责任值 <span class="math inline">\(r_{nk}\)</span> 定义在公式 (11.17) 中。</p><p><strong>注释</strong>：在公式 (11.20) 中，每个混合成分的均值 <span class="math inline">\(\mu_k\)</span> 的更新依赖于所有均值、协方差矩阵 <span class="math inline">\(\Sigma_k\)</span> 和混合权重 <span class="math inline">\(\pi_k\)</span>，因为它们都通过 <span class="math inline">\(r_{nk}\)</span>（定义见 (11.17)）联系起来。因此，我们无法一次性求出所有 <span class="math inline">\(\mu_k\)</span> 的闭式解。</p><p><strong>证明</strong>：由 (11.15) 可知，对均值参数 <span class="math inline">\(\mu_k, k=1,\dots,K\)</span> 的对数似然函数梯度需要我们计算偏导数： <span class="math display">\[\frac{\partial p(x_n|\theta)}{\partial \mu_k}= \sum_{j=1}^K \pi_j \frac{\partial \mathcal{N}(x_n|\mu_j,\Sigma_j)}{\partial \mu_k}= \pi_k \frac{\partial \mathcal{N}(x_n|\mu_k,\Sigma_k)}{\partial \mu_k}, \tag{11.21a}\]</span> 因为只有第 <span class="math inline">\(k\)</span> 个混合成分依赖于 <span class="math inline">\(\mu_k\)</span>。进一步得到： <span class="math display">\[= \pi_k (x_n - \mu_k)^\top \Sigma_k^{-1} \mathcal{N}(x_n|\mu_k,\Sigma_k). \tag{11.21b}\]</span> 将 (11.21b) 的结果代入 (11.15)，并整理后可以得到所需的对数似然函数关于 <span class="math inline">\(\mu_k\)</span> 的偏导数： <span class="math display">\[\begin{align}\frac{\partial \mathcal{L}}{\partial \boldsymbol{\mu}_k}&amp;= \sum_{n=1}^{N} \frac{\partial \log p(\mathbf{x}_n \mid \theta)}{\partial \boldsymbol{\mu}_k} \notag\\[6pt]&amp;= \sum_{n=1}^{N} \frac{1}{p(\mathbf{x}_n \mid \theta)}\frac{\partial p(\mathbf{x}_n \mid \theta)}{\partial \boldsymbol{\mu}_k}\tag{11.22a}\\[6pt]&amp;= \sum_{n=1}^{N} \left( \mathbf{x}_n - \boldsymbol{\mu}_k \right)^{\top} \boldsymbol{\Sigma}_k^{-1} \frac{\pi_k \mathcal{N}\!\bigl(\mathbf{x}_n \mid \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k\bigr)}{\sum_{j=1}^{K} \pi_j \mathcal{N}\!\bigl(\mathbf{x}_n \mid \boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j\bigr)}\tag{11.22b}\\[6pt]&amp;= \sum_{n=1}^{N} r_{nk} \left( \mathbf{x}_n - \boldsymbol{\mu}_k \right)^{\top} \boldsymbol{\Sigma}_k^{-1}\tag{11.22c}\end{align}\]</span></p><p>其中，<span class="math inline">\(r_{nk}\)</span> 就是我们在 (11.17) 中定义的责任值。</p><p>我们现在解 (11.22c) 关于 <span class="math inline">\(\mu_k^{\text{new}}\)</span>，使得 <span class="math display">\[\frac{\partial L(\mu_k^{\text{new}})}{\partial \mu_k} = 0^\top\]</span> 并得到 <span class="math display">\[\sum_{n=1}^N r_{nk} x_n = \sum_{n=1}^N r_{nk} \mu_k^{\text{new}}\;\;\Longleftrightarrow\;\;\mu_k^{\text{new}} = \frac{\sum_{n=1}^N r_{nk} x_n}{\sum_{n=1}^N r_{nk}}= \frac{1}{N_k} \sum_{n=1}^N r_{nk} x_n,\tag{11.23}\]</span> 其中我们定义 <span class="math display">\[N_k := \sum_{n=1}^N r_{nk}\tag{11.24}\]</span> 为第 <span class="math inline">\(k\)</span> 个混合成分对整个数据集的总责任值。至此，定理 11.1 的证明完成。</p><p>直观上，(11.20) 可以解释为均值的一个带重要性权重的蒙特卡罗估计，其中数据点 <span class="math inline">\(x_n\)</span> 的重要性权重由第 <span class="math inline">\(k\)</span> 个簇对 <span class="math inline">\(x_n\)</span> 的责任值 <span class="math inline">\(r_{nk}\)</span> 给出，<span class="math inline">\(k = 1, \dots, K\)</span>。</p><p>因此，均值 <span class="math inline">\(\mu_k\)</span> 会被“拉向”数据点 <span class="math inline">\(x_n\)</span>，其强度由 <span class="math inline">\(r_{nk}\)</span> 决定。当对应的混合成分对数据点具有较高责任（即较高似然）时，均值会更强烈地被该数据点所吸引。图 11.4 对此进行了说明。</p><p><img src="/img3/机器学习的数学基础Part2/F11.4.png" alt="F11.4" style="zoom:67%;" /></p><p>我们还可以将 (11.20) 中的均值更新解释为在如下分布下所有数据点的期望： <span class="math display">\[r_k := \frac{[r_{1k}, \dots, r_{Nk}]^\top}{N_k},\tag{11.25}\]</span> 这是一个归一化的概率向量，即 <span class="math display">\[\mu_k \;\;\leftarrow\;\; \mathbb{E}_{r_k}[X].\tag{11.26}\]</span> 例 11.3（均值更新）</p><p>在图 11.3 的例子中，均值的更新如下： <span class="math display">\[\mu_1: -4 \;\to\; -2.7 \tag{11.27}\]</span></p><p><span class="math display">\[\mu_2: 0 \;\to\; -0.4 \tag{11.28}\]</span></p><p><span class="math display">\[\mu_3: 8 \;\to\; 3.7 \tag{11.29}\]</span></p><p>这里可以看到，第一个和第三个混合成分的均值向数据所在的区域移动，而第二个成分的均值变化并不显著。图 11.5 展示了这一变化，其中图 11.5(a) 显示了在均值更新之前的 GMM 密度，图 11.5(b) 显示了均值 <span class="math inline">\(\mu_k\)</span> 更新之后的 GMM 密度。</p><p><img src="/img3/机器学习的数学基础Part2/F11.5.png" alt="F11.5" style="zoom:67%;" /></p><p>式 (11.20) 中均值参数的更新看起来相当直接。然而，需要注意的是，责任值 <span class="math inline">\(r_{nk}\)</span> 是 <span class="math inline">\(\pi_j, \mu_j, \Sigma_j\)</span> （其中 <span class="math inline">\(j = 1, \dots, K\)</span>）的函数，因此 (11.20) 中的更新依赖于 GMM 的所有参数。而像我们在第 9.2 节（线性回归）或第 10 章（PCA）中得到的那种闭式解，在这里是无法得到的。</p><h4 id="协方差的更新">11.2.3 协方差的更新</h4><p><strong>定理 11.2（GMM 协方差的更新）</strong>GMM 中第 <span class="math inline">\(k\)</span> 个分量的协方差参数 <span class="math inline">\(\Sigma_k, \; k = 1, \dots, K\)</span> 的更新公式为： <span class="math display">\[\Sigma_k^{\text{new}} = \frac{1}{N_k} \sum_{n=1}^N r_{nk}(x_n - \mu_k)(x_n - \mu_k)^\top,\tag{11.30}\]</span> 其中 <span class="math inline">\(r_{nk}\)</span> 和 <span class="math inline">\(N_k\)</span> 分别定义在 (11.17) 和 (11.24) 中。</p><p><strong>证明</strong> 为了证明定理 11.2，我们的方法是对对数似然函数 <span class="math inline">\(L\)</span> 关于协方差 <span class="math inline">\(\Sigma_k\)</span> 求偏导，将其设为 0，然后解出 <span class="math inline">\(\Sigma_k\)</span>。</p><p>首先写出一般形式： <span class="math display">\[\frac{\partial L}{\partial \Sigma_k} = \sum_{n=1}^N \frac{\partial \log p(x_n|\theta)}{\partial \Sigma_k} = \sum_{n=1}^N \frac{1}{p(x_n|\theta)} \frac{\partial p(x_n|\theta)}{\partial \Sigma_k}. \tag{11.31}\]</span> 我们已经从 (11.16) 知道了 <span class="math inline">\(1/p(x_n|\theta)\)</span>。为了得到剩余的偏导 <span class="math inline">\(\partial p(x_n|\theta)/\partial \Sigma_k\)</span>，我们写出高斯分布的定义 <span class="math inline">\(p(x_n|\theta)\)</span>（见 (11.9)），并只保留第 <span class="math inline">\(k\)</span> 项： <span class="math display">\[\frac{\partial p(x_n|\theta)}{\partial \Sigma_k} = \frac{\partial}{\partial \Sigma_k} \pi_k (2\pi)^{-\frac{D}{2}} \det(\Sigma_k)^{-\frac{1}{2}} \exp\left(-\tfrac{1}{2}(x_n - \mu_k)^\top \Sigma_k^{-1}(x_n - \mu_k)\right). \tag{11.32}\]</span> 利用以下恒等式： <span class="math display">\[\frac{\partial}{\partial \Sigma_k} \det(\Sigma_k)^{-\tfrac{1}{2}} = -\tfrac{1}{2} \det(\Sigma_k)^{-\tfrac{1}{2}} \Sigma_k^{-1}, \tag{11.33}\]</span></p><p><span class="math display">\[\frac{\partial}{\partial \Sigma_k} (x_n - \mu_k)^\top \Sigma_k^{-1} (x_n - \mu_k) = -\Sigma_k^{-1}(x_n - \mu_k)(x_n - \mu_k)^\top \Sigma_k^{-1}, \tag{11.34}\]</span></p><p>得到（整理后）： <span class="math display">\[\frac{\partial p(x_n|\theta)}{\partial \Sigma_k} = \pi_k \mathcal{N}(x_n|\mu_k, \Sigma_k) \cdot \left[-\tfrac{1}{2}\big(\Sigma_k^{-1} - \Sigma_k^{-1}(x_n - \mu_k)(x_n - \mu_k)^\top \Sigma_k^{-1}\big)\right]. \tag{11.35}\]</span> 代入 (11.31)，对数似然函数关于 <span class="math inline">\(\Sigma_k\)</span> 的偏导为： <span class="math display">\[\frac{\partial L}{\partial \Sigma_k} = \sum_{n=1}^N r_{nk} \cdot \left[-\tfrac{1}{2}\big(\Sigma_k^{-1} - \Sigma_k^{-1}(x_n - \mu_k)(x_n - \mu_k)^\top \Sigma_k^{-1}\big)\right], \tag{11.36}\]</span> 其中 <span class="math inline">\(r_{nk}\)</span> 就是责任度（见 (11.17)）。</p><p>将其设为 0，得到最优条件： <span class="math display">\[N_k \Sigma_k^{-1} = \Sigma_k^{-1} \left(\sum_{n=1}^N r_{nk}(x_n - \mu_k)(x_n - \mu_k)^\top \right)\Sigma_k^{-1}. \tag{11.37}\]</span> 解得： <span class="math display">\[\Sigma_k^{\text{new}} = \frac{1}{N_k} \sum_{n=1}^N r_{nk}(x_n - \mu_k)(x_n - \mu_k)^\top. \tag{11.38}\]</span> 其中 <span class="math inline">\(r_k\)</span> 是定义在 (11.25) 中的概率向量。这样就得到了一个简单的更新规则，从而证明了定理 11.2。</p><p>类似于均值更新公式 (11.20)，协方差更新公式 (11.30) 可以解释为中心化数据平方的加权期望，其中权重为责任度： <span class="math display">\[\tilde{X}_k := \{x_1 - \mu_k, \dots, x_N - \mu_k\}.\]</span> 也就是说，<strong>协方差是以责任值为权重的期望估计。</strong></p><p><strong>示例 11.4（方差更新）</strong></p><p>在图 11.3 的例子中，方差的更新如下： <span class="math display">\[\sigma^2_1 : 1 \;\to\; 0.14 \tag{11.39}\]</span></p><p><span class="math display">\[\sigma^2_2 : 0.2 \;\to\; 0.44 \tag{11.40}\]</span></p><p><span class="math display">\[\sigma^2_3 : 3 \;\to\; 1.53 \tag{11.41}\]</span></p><p>在这里我们看到，第一个和第三个分量的方差显著收缩，而第二个分量的方差则略微增加。</p><p><img src="/img3/机器学习的数学基础Part2/F11.6.png" alt="F11.6" style="zoom:67%;" /></p><p>图 11.6 展示了这一情况。图 11.6(a) 与图 11.5(b) 相同（但进行了放大），显示了在更新方差之前的 GMM 密度及其各个分量。图 11.6(b) 显示了更新方差之后的 GMM 密度。</p><p>与均值参数的更新类似，我们可以将 (11.30) 理解为对与第 <span class="math inline">\(k\)</span> 个混合分量相关的数据点 <span class="math inline">\(x_n\)</span> 的加权协方差的蒙特卡罗估计，其中权重是责任度 <span class="math inline">\(r_{nk}\)</span>。同样地，正如均值参数的更新，这一更新依赖于所有 <span class="math inline">\(\pi_j, \mu_j, \Sigma_j\)</span>（<span class="math inline">\(j = 1, \ldots, K\)</span>），通过责任度 <span class="math inline">\(r_{nk}\)</span> 体现出来，从而使得无法得到闭式解。</p><blockquote><p>个人注：Monte Carlo estimate 是什么意思？</p><p>在概率论里，一个期望可以用样本的平均值（或带权平均）来近似，这种近似就叫 <strong>Monte Carlo估计</strong>。 比如，协方差的真实定义是： <span class="math display">\[\text{Cov}_k = \mathbb{E}_{\mathbf{x}\sim p_k}\big[(\mathbf{x}-\boldsymbol{\mu}_k)(\mathbf{x}-\boldsymbol{\mu}_k)^\top\big]\]</span> 我们没有真实的 <span class="math inline">\(p_k\)</span>，只有数据 <span class="math inline">\(\mathbf{x}_n\)</span>，于是我们用样本的加权和来近似它： <span class="math display">\[\hat{\text{Cov}}_k =\frac{\sum_n r_{nk} (\mathbf{x}_n - \boldsymbol{\mu}_k)(\mathbf{x}_n - \boldsymbol{\mu}_k)^\top}{\sum_n r_{nk}}\]</span> 这正是 Monte Carlo 估计的典型形式：用有限样本加权平均来近似真实分布下的期望。</p></blockquote><h4 id="更新混合权重">11.2.4 更新混合权重</h4><p><strong>定理 11.3（GMM 混合权重的更新）。</strong>GMM 的混合权重按如下方式更新： <span class="math display">\[\pi^{\text{new}}_k = \frac{N_k}{N}, \quad k=1,\dots,K, \tag{11.42}\]</span></p><p>其中 <span class="math inline">\(N\)</span> 是数据点的数量，<span class="math inline">\(N_k\)</span> 定义见 (11.24)。</p><p><strong>证明</strong> 为了求对权重参数 <span class="math inline">\(\pi_k, k=1,\dots,K\)</span> 的对数似然函数的偏导数，我们利用拉格朗日乘子法（参见第 7.2 节）来处理约束条件 <span class="math inline">\(\sum_k \pi_k = 1\)</span>。 拉格朗日函数为：</p><p><span class="math display">\[\mathcal{L} = L + \lambda \left(\sum_{k=1}^K \pi_k - 1\right) \tag{11.43a}\]</span></p><p><span class="math display">\[= \sum_{n=1}^N \log\left(\sum_{k=1}^K \pi_k \mathcal{N}(x_n|\mu_k,\Sigma_k)\right)+ \lambda \left(\sum_{k=1}^K \pi_k - 1\right), \tag{11.43b}\]</span></p><p>其中 <span class="math inline">\(L\)</span> 是 (11.10) 的对数似然，第二项表示所有混合权重之和为 1 的约束条件。</p><p>对 <span class="math inline">\(\pi_k\)</span> 求偏导得到： <span class="math display">\[\begin{align}\frac{\partial \mathcal{L}}{\partial \pi_k} &amp;= \sum_{n=1}^{N} \frac{\mathcal{N}(\mathbf{x}_n \mid \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)}{\sum_{j=1}^{K} \pi_j \mathcal{N}(\mathbf{x}_n \mid \boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j)}+ \lambda \tag{11.44a}\\[1em]&amp;= \frac{1}{\pi_k} \sum_{n=1}^{N} \frac{\pi_k \mathcal{N}(\mathbf{x}_n \mid \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)}{\sum_{j=1}^{K} \pi_j \mathcal{N}(\mathbf{x}_n \mid \boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j)}+ \lambda = \frac{N_k}{\pi_k} + \lambda,\tag{11.44b}\end{align}\]</span> 对拉格朗日乘子 <span class="math inline">\(\lambda\)</span> 求偏导：</p><p><span class="math display">\[\frac{\partial \mathcal{L}}{\partial \lambda} = \sum_{k=1}^K \pi_k - 1. \tag{11.45}\]</span></p><p>将两者都置为 0（最优条件）得到方程组：</p><p><span class="math display">\[\pi_k = -\frac{N_k}{\lambda}, \tag{11.46}\]</span></p><p><span class="math display">\[\sum_{k=1}^K \pi_k = 1. \tag{11.47}\]</span></p><p>把 (11.46) 代入 (11.47) 并解 <span class="math inline">\(\pi_k\)</span>：</p><p><span class="math display">\[\sum_{k=1}^K \pi_k = 1 \quad \Leftrightarrow \quad -\frac{\sum_{k=1}^K N_k}{\lambda}=1 \quad \Leftrightarrow \quad -\frac{N}{\lambda}=1 \quad \Leftrightarrow \quad \lambda=-N. \tag{11.48}\]</span></p><p>于是可以将 <span class="math inline">\(-N\)</span> 代入 (11.46)，得到：</p><p><span class="math display">\[\pi^{\text{new}}_k = \frac{N_k}{N}, \tag{11.49}\]</span></p><p>这就是权重参数 <span class="math inline">\(\pi_k\)</span> 的更新公式，证明了定理 11.3。</p><p>我们可以将 (11.42) 中的混合权重理解为：第 <span class="math inline">\(k\)</span> 个聚类的总责任（responsibility）占数据点总数的比例。由于 <span class="math inline">\(N=\sum_k N_k\)</span>，数据点总数也可以看作所有混合成分的总责任，因此 <span class="math inline">\(\pi_k\)</span> 表示第 <span class="math inline">\(k\)</span> 个混合成分对于数据集的相对重要性。</p><p><strong>备注</strong>:由于 <span class="math inline">\(N_k=\sum_{i=1}^N r_{nk}\)</span>，(11.42) 中混合权重 <span class="math inline">\(\pi_k\)</span> 的更新公式也通过责任值 <span class="math inline">\(r_{nk}\)</span> 依赖于所有的 <span class="math inline">\(\pi_j,\mu_j,\Sigma_j,j=1,\dots,K\)</span>。</p><p><strong>例 11.5（权重参数更新）</strong> 在图 11.3 的示例中，混合权重的更新如下： <span class="math display">\[\pi_1 : \frac{1}{3} \rightarrow 0.29 \tag{11.50}\]</span></p><p><span class="math display">\[\pi_2 : \frac{1}{3} \rightarrow 0.29 \tag{11.51}\]</span></p><p><span class="math display">\[\pi_3 : \frac{1}{3} \rightarrow 0.42 \tag{11.52}\]</span></p><p>在这里我们可以看到，第三个分量获得了更多的权重/重要性，而其他分量则稍微变得不那么重要。图 11.7 说明了更新混合权重的效果。图 11.7(a) 与图 11.6(b) 完全相同，展示了在更新混合权重之前的 GMM 密度及其各个分量。图 11.7(b) 则展示了更新混合权重之后的 GMM 密度。</p><p><img src="/img3/机器学习的数学基础Part2/F11.7.png" alt="F11.7" style="zoom:67%;" /></p><p>总体而言，当均值、方差以及权重都更新过一次之后，我们得到图 11.7(b) 所示的 GMM。与图 11.3 的初始化相比，<strong>我们可以看到，参数更新使 GMM 的密度把部分概率质量向数据点方向移动。</strong></p><p>在均值、方差和权重都更新过一次之后，图 11.7(b) 中的 GMM 拟合效果已经明显优于图 11.3 中的初始化。这一点也从对数似然值可以看出：从初始化时的 −28.3 提升到了完整更新循环后的 −14.4。</p><h3 id="em-算法"><strong>11.3 EM 算法</strong></h3><p>不幸的是，(11.20)、(11.30) 和 (11.42) 中的更新并不能构成混合模型参数 <span class="math inline">\(\mu_k, \Sigma_k, \pi_k\)</span> 的一个闭式解，因为职责（responsibilities）<span class="math inline">\(r_{nk}\)</span> 以复杂的方式依赖于这些参数。不过，这些结果提示了一个简单的迭代方案，可以通过极大似然来求解参数估计问题。期望最大化算法（EM 算法）由 Dempster 等人（1977）提出，是一种通用的迭代方法，用于在混合模型以及更一般的潜变量模型中学习参数（极大似然或 MAP）。在高斯混合模型的例子中，我们选择 <span class="math inline">\(\mu_k, \Sigma_k, \pi_k\)</span> 的初始值，并在下述两步之间交替迭代直至收敛：</p><ul><li><strong>E 步（Expectation step）</strong>：计算职责 <span class="math inline">\(r_{nk}\)</span>（即数据点 <span class="math inline">\(n\)</span> 属于混合分量 <span class="math inline">\(k\)</span> 的后验概率）。</li><li><strong>M 步（Maximization step）</strong>：利用更新后的职责重新估计参数 <span class="math inline">\(\mu_k, \Sigma_k, \pi_k\)</span>。</li></ul><p>EM 算法的每一步都会使对数似然函数增加（Neal 和 Hinton，1999）。为了判断收敛，我们可以检查对数似然值或直接检查参数的变化。</p><p>一个具体的 EM 算法用于估计 GMM 的参数如下：</p><ol type="1"><li><p><strong>初始化</strong> <span class="math inline">\(\mu_k, \Sigma_k, \pi_k\)</span>。</p></li><li><p><strong>E 步</strong>：使用当前参数 <span class="math inline">\(\pi_k, \mu_k, \Sigma_k\)</span> 计算每个数据点 <span class="math inline">\(x_n\)</span> 的职责 <span class="math inline">\(r_{nk}\)</span>：</p></li></ol><p><span class="math display">\[r_{nk} = \frac{\pi_k \,\mathcal{N}(x_n|\mu_k,\Sigma_k)}{\sum_j \pi_j \,\mathcal{N}(x_n|\mu_j,\Sigma_j)}\tag{11.53}\]</span></p><ol start="3" type="1"><li><strong>M 步</strong>：利用 E 步得到的职责 <span class="math inline">\(r_{nk}\)</span> 重新估计参数 <span class="math inline">\(\pi_k, \mu_k, \Sigma_k\)</span>：</li></ol><p><span class="math display">\[\mu_k = \frac{1}{N_k}\sum_{n=1}^{N} r_{nk}x_n \tag{11.54}\]</span></p><p><span class="math display">\[\Sigma_k = \frac{1}{N_k}\sum_{n=1}^{N} r_{nk}(x_n-\mu_k)(x_n-\mu_k)^\top \tag{11.55}\]</span></p><p><span class="math display">\[\pi_k = \frac{N_k}{N} \tag{11.56}\]</span></p><p><strong>例 11.6（GMM 拟合）</strong></p><p>当我们在图 11.3 的例子上运行 EM 算法时，经过 5 次迭代，我们得到的最终结果如图 11.8(a) 所示；图 11.8(b) 则显示了负对数似然随 EM 迭代次数的变化情况。最终的 GMM 表达式为</p><p><span class="math display">\[p(x) = 0.29\,\mathcal{N}(x|-2.75, 0.06)+ 0.28\,\mathcal{N}(x|-0.50, 0.25)+ 0.43\,\mathcal{N}(x|3.64, 1.63)\tag{11.57}\]</span></p><p>我们将 EM 算法应用到图 11.1 所示的二维数据集上，设混合分量数 <span class="math inline">\(K=3\)</span>。图 11.9 展示了 EM 算法的一些步骤，并显示了负对数似然随 EM 迭代次数的变化（图 11.9(b)）。图 11.10(a) 展示了最终的 GMM 拟合结果。图 11.10(b) 可视化了在 EM 收敛时混合分量对各个数据点的最终职责（responsibilities）。</p><p><img src="/img3/机器学习的数学基础Part2/F11.8.png" alt="F11.8" style="zoom:67%;" /></p><p><img src="/img3/机器学习的数学基础Part2/F11.9.png" alt="F11.9" style="zoom:67%;" /></p><p><img src="/img3/机器学习的数学基础Part2/F11.10.png" alt="F11.10" style="zoom:67%;" /></p><p><strong>数据集按照混合分量的职责进行着色。</strong>左侧的数据显然主要由单一的混合分量负责，而右侧两个数据簇的重叠部分可能由两个混合分量生成。可以清楚地看到，有些数据点无法唯一地分配给单个分量（既非完全蓝色也非完全黄色），因此这两个簇对这些点的职责大约是 0.5。</p><h3 id="潜变量视角"><strong>11.4 潜变量视角</strong></h3><p>我们可以从<strong>离散潜变量模型</strong>的角度来看待 GMM（高斯混合模型），即潜变量 <span class="math inline">\(z\)</span> 只能取有限个值。这与 PCA 形成对比：在 PCA 中，潜变量是 <span class="math inline">\(\mathbb{R}^M\)</span> 中的连续值。从概率视角来看有以下优点： (i) 它能对我们在前面章节中做出的一些临时性（ad hoc）决定进行合理化； (ii) 它允许我们将<strong>责任度（responsibilities）</strong>解释为后验概率； (iii) 更新模型参数的迭代算法可以以一种有原则的方式导出，即作为潜变量模型中最大似然参数估计的 EM 算法。</p><h4 id="生成过程与概率模型"><strong>11.4.1 生成过程与概率模型</strong></h4><p>为了推导 GMM 的概率模型，把它看作<strong>生成过程</strong>是有用的，也就是使用一个概率模型生成数据的过程。</p><p>我们假设一个具有 <span class="math inline">\(K\)</span> 个成分的混合模型，并且一个数据点 <span class="math inline">\(x\)</span> 由其中恰好一个混合成分生成。我们引入一个<strong>二元指示变量</strong> <span class="math inline">\(z_k \in \{0,1\}\)</span>（见第 6.2 节），用以指示第 <span class="math inline">\(k\)</span> 个混合成分是否生成了该数据点，使得：</p><p><span class="math display">\[p(x|z_k=1)=\mathcal{N}\bigl(x\mid \mu_k,\Sigma_k\bigr). \tag{11.58}\]</span></p><blockquote><p>个人注：式 (11.58) 并非从别的式子推导，而是<strong>模型的条件分布定义</strong>——“若隐藏变量指示第 <span class="math inline">\(k\)</span> 个分量被选中，则 <span class="math inline">\(x\)</span> 就从该分量的高斯分布中产生”。</p></blockquote><p>我们定义 <span class="math display">\[z := [z_1,\ldots,z_K]^\top \in \mathbb{R}^K\]</span></p><p>为一个概率向量，它包含 <span class="math inline">\(K-1\)</span> 个 0 和恰好一个 1。例如，当 <span class="math inline">\(K=3\)</span> 时，一个合法的 <span class="math inline">\(z\)</span> 可以是</p><p><span class="math display">\[z=[z_1,z_2,z_3]^\top=[0,1,0]^\top,\]</span></p><p>这就选择了第二个混合成分，因为 <span class="math inline">\(z_2=1\)</span>。</p><p><strong>注：</strong>这种概率分布有时称为“多项伯努利分布（multinoulli）”，是伯努利分布向两个以上值的推广（Murphy，2012）。</p><p>由 <span class="math inline">\(z\)</span> 的性质可知</p><p><span class="math display">\[\sum_{k=1}^K z_k =1。\]</span></p><p>因此，<span class="math inline">\(z\)</span> 是一种<strong>独热编码（one-hot encoding，也叫 1-of-K 表示）</strong>。</p><p>到目前为止，我们假设指示变量 <span class="math inline">\(z_k\)</span> 是已知的。然而在实际中并非如此，因此我们对潜变量 <span class="math inline">\(z\)</span> 施加先验分布：</p><p><span class="math display">\[p(z)=\pi=[\pi_1,\ldots,\pi_K]^\top,\quad\sum_{k=1}^K \pi_k=1。 \tag{11.59}\]</span></p><p>其中第 <span class="math inline">\(k\)</span> 个分量</p><p><span class="math display">\[\pi_k=p(z_k=1) \tag{11.60}\]</span></p><p>描述了第 <span class="math inline">\(k\)</span> 个混合成分生成数据点 <span class="math inline">\(x\)</span> 的概率。</p><p><img src="/img3/机器学习的数学基础Part2/F11.11.png" alt="F11.11" style="zoom:67%;" /></p><p><strong>注（从 GMM 采样）</strong>这种潜变量模型的构造（见图 11.11 中对应的图形模型）自然地导出一个非常简单的<strong>采样过程（生成过程）来生成数据</strong>：</p><ol type="1"><li>从 <span class="math inline">\(p(z)\)</span> 中采样 <span class="math inline">\(z^{(i)}\)</span>。</li><li>从 <span class="math inline">\(p\bigl(x\mid z^{(i)}=1\bigr)\)</span> 中采样 <span class="math inline">\(x^{(i)}\)</span>。</li></ol><p>在第一步，我们根据 <span class="math inline">\(p(z)=\pi\)</span> 随机选择一个混合成分 <span class="math inline">\(i\)</span>（通过 one-hot 编码 <span class="math inline">\(z\)</span>）；在第二步，我们从对应的混合成分中抽取一个样本。当我们丢弃潜变量的样本，仅保留 <span class="math inline">\(x^{(i)}\)</span> 时，我们就得到了来自 GMM 的有效样本。这种采样方式，即随机变量的样本依赖于其在图模型中父节点的样本，被称为<strong>祖先采样（ancestral sampling）</strong>。</p><p>一般地，一个概率模型是由数据和潜变量的联合分布定义的（见第 8.4 节）。结合 (11.59)、(11.60) 中的先验 <span class="math inline">\(p(z)\)</span> 以及 (11.58) 中的条件分布 <span class="math inline">\(p(x|z)\)</span>，我们得到该联合分布的所有 <span class="math inline">\(K\)</span> 个成分：</p><p><span class="math display">\[p(x,z_k=1)=p(x|z_k=1)p(z_k=1)=\pi_k\mathcal{N}\bigl(x\mid \mu_k,\Sigma_k\bigr)\tag{11.61}\]</span></p><p>其中 <span class="math inline">\(k=1,\ldots,K\)</span>，因此</p><p><span class="math display">\[p(x,z)=\begin{bmatrix}p(x,z_1=1)\\\vdots\\p(x,z_K=1)\end{bmatrix}=\begin{bmatrix}\pi_1\mathcal{N}\bigl(x\mid \mu_1,\Sigma_1\bigr)\\\vdots\\\pi_K\mathcal{N}\bigl(x\mid \mu_K,\Sigma_K\bigr)\end{bmatrix} \tag{11.62}\]</span></p><p>这就完全刻画了该概率模型。</p><h4 id="似然"><strong>11.4.2 似然</strong></h4><p>为了在潜变量模型中得到似然 <span class="math inline">\(p(x|\theta)\)</span>，我们需要把潜变量积分（求和）掉（见第 8.4.3 节）。在我们的例子中，可以通过对 (11.62) 中的联合分布 <span class="math inline">\(p(x,z)\)</span> 对所有潜变量求和得到：</p><p><span class="math display">\[p(x|\theta)=\sum_{z}p(x|\theta,z)p(z|\theta),\quad \theta:=\{\mu_k,\Sigma_k,\pi_k: k=1,\ldots,K\}. \tag{11.63}\]</span></p><p>这里我们显式地对概率模型的参数 <span class="math inline">\(\theta\)</span> 进行了条件化，而在之前省略了这一点。 在 (11.63) 中，我们对所有 <span class="math inline">\(K\)</span> 种可能的 one-hot 编码 <span class="math inline">\(z\)</span> 求和（记作 <span class="math inline">\(\sum_z\)</span>）。因为每个 <span class="math inline">\(z\)</span> 中只有一个非零条目，所以 <span class="math inline">\(z\)</span> 只有 <span class="math inline">\(K\)</span> 种可能的配置。例如，当 <span class="math inline">\(K=3\)</span> 时，<span class="math inline">\(z\)</span> 的配置可以是： <span class="math display">\[\begin{bmatrix}1\\0\\0\end{bmatrix},\quad\begin{bmatrix}0\\1\\0\end{bmatrix},\quad\begin{bmatrix}0\\0\\1\end{bmatrix}. \tag{11.64}\]</span></p><p>对 (11.63) 中所有可能的 <span class="math inline">\(z\)</span> 求和等价于取 <span class="math inline">\(z\)</span>-向量中非零的那个位置并写成：</p><p><span class="math display">\[p(x|\theta)=\sum_{z}p(x|\theta,z)p(z|\theta)\tag{11.65a}\]</span></p><p><span class="math display">\[=\sum_{k=1}^K p(x|\theta,z_k=1)p(z_k=1|\theta)\tag{11.65b}\]</span></p><p>从而所需的边缘分布为：</p><p><span class="math display">\[p(x|\theta)=\sum_{k=1}^K p(x|\theta,z_k=1)p(z_k=1|\theta)\tag{11.66a}\]</span></p><p><span class="math display">\[=\sum_{k=1}^K \pi_k\mathcal{N}\bigl(x\mid \mu_k,\Sigma_k\bigr), \tag{11.66b}\]</span></p><p>我们可以把它识别为 (11.3) 中的 GMM 模型。给定一个数据集 <span class="math inline">\(X\)</span>，我们立刻得到其似然：</p><p><span class="math display">\[p(X|\theta)=\prod_{n=1}^N p(x_n|\theta)=\prod_{n=1}^N \sum_{k=1}^K \pi_k\mathcal{N}\bigl(x_n\mid \mu_k,\Sigma_k\bigr). \tag{11.67}\]</span></p><p>这正是 (11.9) 中的 GMM 似然。因此，带有潜在指示变量 <span class="math inline">\(z_k\)</span> 的潜变量模型，是理解高斯混合模型的一个等价方式。</p><h4 id="后验分布"><strong>11.4.3 后验分布</strong></h4><p>我们简单看一下潜在变量 <span class="math inline">\(z\)</span> 的后验分布。根据贝叶斯定理，第 <span class="math inline">\(k\)</span> 个分量生成数据点 <span class="math inline">\(x\)</span> 的后验为</p><p><span class="math display">\[p(z_k = 1 \mid x)=\frac{p(z_k=1)p(x\mid z_k=1)}{p(x)} \tag{11.68}\]</span></p><p>其中边际分布 <span class="math inline">\(p(x)\)</span> 在 (11.66b) 中给出。这就得到了第 <span class="math inline">\(k\)</span> 个指示变量 <span class="math inline">\(z_k\)</span> 的后验分布：</p><p><span class="math display">\[p(z_k = 1 \mid x)=\frac{p(z_k=1)p(x\mid z_k=1)}{\sum_{j=1}^K p(z_j=1)p(x\mid z_j=1)}=\frac{\pi_k\mathcal{N}(x\mid\mu_k,\Sigma_k)}{\sum_{j=1}^K\pi_j\mathcal{N}(x\mid\mu_j,\Sigma_j)}\tag{11.69}\]</span></p><p>我们把它识别为第 <span class="math inline">\(k\)</span> 个混合成分对数据点 <span class="math inline">\(x\)</span> 的“责任”。注意我们省略了对 GMM 参数 <span class="math inline">\(\pi_k,\mu_k,\Sigma_k\)</span> （<span class="math inline">\(k=1,\dots,K\)</span>）的显式条件。</p><h4 id="扩展到整个数据集"><strong>11.4.4 扩展到整个数据集</strong></h4><p>到目前为止，我们只讨论了数据集只包含单个数据点 <span class="math inline">\(x\)</span> 的情形。然而，先验和后验的概念可以直接扩展到 <span class="math inline">\(N\)</span> 个数据点的情况 <span class="math inline">\(X=\{x_1,\dots,x_N\}\)</span>。在 GMM 的概率解释中，每个数据点 <span class="math inline">\(x_n\)</span> 都有自己的潜在变量</p><p><span class="math display">\[z_n=[z_{n1},\dots,z_{nK}]^\top\in\mathbb{R}^K \tag{11.70}\]</span></p><p>之前（当我们只考虑一个数据点 <span class="math inline">\(x\)</span> 时）我们省略了下标 <span class="math inline">\(n\)</span>，但现在它变得重要了。</p><p>我们在所有潜在变量 <span class="math inline">\(z_n\)</span> 上共享相同的先验分布 <span class="math inline">\(\pi\)</span>。对应的图模型如图 11.12 所示，我们使用了 plate 符号。</p><p><img src="/img3/机器学习的数学基础Part2/F11.12.png" alt="F11.12" style="zoom:67%;" /></p><p>条件分布 <span class="math inline">\(p(x_1,\dots,x_N\mid z_1,\dots,z_N)\)</span> 在数据点上是可分解的：</p><p><span class="math display">\[p(x_1,\dots,x_N\mid z_1,\dots,z_N)=\prod_{n=1}^N p(x_n\mid z_n) \tag{11.71}\]</span></p><p>为了得到后验分布 <span class="math inline">\(p(z_{nk}=1\mid x_n)\)</span>，我们像 11.4.3 节一样应用贝叶斯定理得到：</p><p>$$ <span class="math display">\[\begin{align}p(z_{nk} = 1 \mid \boldsymbol{x}_n) &amp; = \frac{p(\boldsymbol{x}_n \mid z_{nk} = 1) p(z_{nk} = 1)}{\sum_{j=1}^{K} p(\boldsymbol{x}_n \mid z_{nj} = 1) p(z_{nj} = 1)} \tag{11.72a} \\&amp; = \frac{\pi_k \mathcal{N}(\boldsymbol{x}_n \mid \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)}{\sum_{j=1}^{K} \pi_j \mathcal{N}(\boldsymbol{x}_n \mid \boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j)} = r_{nk}. \tag{11.72b}\end{align}\]</span> $$</p><p>这意味着 <span class="math inline">\(p(z_k=1\mid x_n)\)</span> 是第 <span class="math inline">\(k\)</span> 个混合成分生成数据点 <span class="math inline">\(x_n\)</span> 的（后验）概率，并对应于我们在 (11.17) 中引入的责任 <span class="math inline">\(r_{nk}\)</span>。现在这些责任不仅有了直观含义，而且也有了作为后验概率的数学解释。</p><h4 id="em-算法再探"><strong>11.4.5 EM 算法再探</strong></h4><p>我们前面介绍过 EM 算法，作为最大似然估计的一种迭代方案；其实，它可以从潜在变量的视角以一种更有原则的方法推导出来。给定模型参数的当前设置 <span class="math inline">\(\theta^{(t)}\)</span>，E 步计算期望对数似然： <span class="math display">\[Q(\theta|\theta^{(t)}) = \mathbb{E}_{z|x,\theta^{(t)}}[\log p(x,z|\theta)] \tag{11.73a}\]</span></p><p><span class="math display">\[= \int \log p(x,z|\theta)\,p(z|x,\theta^{(t)})\,dz \tag{11.73b}\]</span></p><p>其中 <span class="math inline">\(\log p(x,z|\theta)\)</span> 的期望是关于潜在变量的后验 <span class="math inline">\(p(z|x,\theta^{(t)})\)</span> 取的。M 步则通过最大化式 (11.73b) 来选择更新后的模型参数 <span class="math inline">\(\theta^{(t+1)}\)</span>。</p><p>虽然每次 EM 迭代都会增加对数似然，但并没有保证 EM 会收敛到最大似然解。EM 算法有可能收敛到对数似然的局部最大值。<strong>为了减少陷入糟糕局部最优的风险，可以在多次 EM 运行中使用不同的 <span class="math inline">\(\theta\)</span> 初始值。</strong>我们在此不作进一步展开，读者可以参考 Rogers 和 Girolami (2016) 以及 Bishop (2006) 的精彩阐述。</p><h3 id="延伸阅读"><strong>11.5 延伸阅读</strong></h3><p>高斯混合模型（GMM）可以被视为生成式模型，因为可以很容易地通过祖先采样来生成新数据（Bishop, 2006）。给定 GMM 的参数 <span class="math inline">\(\pi_k,\mu_k,\Sigma_k, k=1,\ldots,K\)</span>，我们先从概率向量 <span class="math inline">\([\pi_1,\ldots,\pi_K]^\top\)</span> 中采样一个索引 <span class="math inline">\(k\)</span>，再从正态分布 <span class="math inline">\(x \sim \mathcal{N}(\mu_k,\Sigma_k)\)</span> 中采样一个数据点。如果重复该过程 <span class="math inline">\(N\)</span> 次，就能得到一个由 GMM 生成的数据集。图 11.1 就是用这种方法生成的。</p><p>在本章中，我们始终假设混合成分数 <span class="math inline">\(K\)</span> 是已知的。但在实践中往往不是这样。我们可以用 8.6.1 节讨论过的嵌套交叉验证来寻找合适的模型。高斯混合模型与 K-means 聚类算法密切相关。K-means 同样使用 EM 算法将数据点分配到簇。如果把 GMM 中的均值视为聚类中心并忽略协方差（或设为 <span class="math inline">\(I\)</span>），就得到 K-means。正如 MacKay (2003) 所描述的，K-means 对数据点作“硬”分配到簇中心 <span class="math inline">\(\mu_k\)</span>，而 GMM 则通过责任值作“软”分配。</p><p>我们只略微涉及了 GMM（高斯混合模型）和 EM 算法的潜在变量视角。需要注意的是，EM 算法可以用于一般潜在变量模型的参数学习，例如非线性状态空间模型（Ghahramani 和 Roweis, 1999；Roweis 和 Ghahramani, 1999）以及 Barber (2012) 讨论的强化学习。因此，从潜在变量的角度理解 GMM，有助于以更有原则的方法推导出对应的 EM 算法（Bishop, 2006；Barber, 2012；Murphy, 2012）。</p><p>我们只讨论了通过 EM 算法进行最大似然估计来求解 GMM 参数。对最大似然的标准批评同样适用于此处：</p><ul><li>就像在线性回归中一样，最大似然可能会出现严重的过拟合。在 GMM 情形下，当某个混合成分的均值恰好与某个数据点重合且协方差趋于 0 时，就会发生这种情况。此时似然值趋于无穷大。Bishop (2006) 和 Barber (2012) 对这一问题有详细讨论。</li><li>我们只得到参数 <span class="math inline">\(\pi_k,\mu_k,\Sigma_k, k=1,\ldots,K\)</span> 的点估计，这并不能反映参数值的不确定性。贝叶斯方法会对参数施加先验，从而得到参数的后验分布。这个后验可以用来计算模型证据（即边际似然），并可用于模型比较，从而以更有原则的方式确定混合成分的个数。不幸的是，在此模型中没有共轭先验，因此无法得到封闭形式的推断。但可以使用近似方法（如变分推断）来获得参数后验的近似（Bishop, 2006）。</li></ul><p>本章我们讨论了<strong>用于密度估计的混合模型。可用的密度估计技术非常多。在实际中，我们常使用直方图和核密度估计。</strong>直方图提供了一种非参数方式来表示连续密度，最早由 Pearson (1895) 提出。直方图的构建方法是对数据空间进行“分箱”，并统计每个箱中有多少数据点。然后在每个箱的中心画出一个矩形柱，其高度与该箱内数据点的数量成正比。箱宽是一个关键的超参数，不合适的选择会导致过拟合或欠拟合。可以通过第 8.2.4 节讨论过的交叉验证来确定一个合适的箱宽。核密度估计是由 Rosenblatt (1956) 和 Parzen (1962) 各自独立提出的一种非参数密度估计方法。给定 <span class="math inline">\(N\)</span> 个独立同分布样本，核密度估计器将底层分布表示为</p><p><span class="math display">\[p(x) = \frac{1}{Nh}\sum_{n=1}^{N} k\!\left(\frac{x-x_n}{h}\right) \tag{11.74}\]</span></p><p>其中 <span class="math inline">\(k\)</span> 是核函数，即一个非负且积分为 1 的函数，<span class="math inline">\(h&gt;0\)</span> 是平滑/带宽参数，它的作用类似于直方图的箱宽。注意，我们对数据集中每一个样本点 <span class="math inline">\(x_n\)</span> 都放置了一个核。常用的核函数有均匀分布核和高斯核。核密度估计与直方图密切相关，但通过选择合适的核函数，我们可以保证密度估计的平滑性。图 11.13 展示了在一个包含 250 个数据点的数据集上，直方图与高斯核密度估计的差异。</p><p><img src="./F11.13.png" alt="F11.13" style="zoom:67%;" /></p>]]></content>
      
      
      <categories>
          
          <category> 翻译 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 数学 </tag>
            
            <tag> 算法 </tag>
            
            <tag> 统计 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《机器学习的数学基础》第10章&quot;主成分分析的降维&quot;</title>
      <link href="/2025/09/09/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E7%AC%AC10%E7%AB%A0%22%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%E7%9A%84%E9%99%8D%E7%BB%B4%22/"/>
      <url>/2025/09/09/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E7%AC%AC10%E7%AB%A0%22%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%E7%9A%84%E9%99%8D%E7%BB%B4%22/</url>
      
        <content type="html"><![CDATA[<h2 id="第10章-主成分分析的降维">第10章 主成分分析的降维</h2><p>Dimensionality Reduction with Principal Component Analysis</p><p>直接处理高维数据（例如图像）会带来一些困难：它很难分析、难以解释、几乎无法可视化，并且（从实际角度来看）存储这些数据向量的代价可能很高。然而，高维数据往往具有一些可以利用的性质。例如，高维数据通常是 <strong>过完备的（overcomplete）</strong>，即许多维度是冗余的，可以由其他维度的组合来解释。此外，高维数据中的各个维度往往是相关的，因此数据实际上存在一个 <strong>内在的低维结构</strong>。<strong>降维</strong> 就是利用这种结构和相关性，使我们能够以更紧凑的方式表示数据，理想情况下还能避免信息丢失。我们可以将<strong>降维看作是一种压缩技术</strong>，类似于 <strong>jpeg</strong> 或 <strong>mp3</strong>，它们分别是图像和音乐的压缩算法。</p><p>在本章中，我们将讨论 <strong>主成分分析（PCA），这是一种<em><u>线性的降维算法</u></em></strong>。PCA 由 Pearson (1901) 和 Hotelling (1933) 提出，至今已有一百多年历史，但仍然是数据压缩和数据可视化中最常用的技术之一。它还被广泛用于识别高维数据中的简单模式、潜在因子以及数据结构。在信号处理领域，PCA 也被称为 Karhunen-Loève 变换。在本章中，我们将从最基本的原理推导 PCA，依赖于我们对 <strong>基和基变换</strong>（第 2.6.1 和 2.7.2 节）、<strong>投影</strong>（第 3.8 节）、<strong>特征值</strong>（第 4.2 节）、<strong>高斯分布</strong>（第 6.5 节）以及 <strong>约束优化</strong>（第 7.2 节）的理解。</p><p>降维通常利用高维数据（例如图像）的一个性质：它们往往位于低维子空间上。图 10.1 给出了一个二维的示例说明。虽然图 10.1(a) 中的数据并不完全落在一条直线上，但它在 <span class="math inline">\(x_2\)</span>-方向上的变化很小，因此我们几乎可以把它看作是落在一条直线上——几乎没有信息损失；见图 10.1(b)。为了描述图 10.1(b) 中的数据，只需要 <span class="math inline">\(x_1\)</span>-坐标即可，此时数据位于 <span class="math inline">\(\mathbb{R}^2\)</span> 的一个一维子空间中。</p><p><img src="/img3/机器学习的数学基础Part2/F10.1.png" alt="F10.1" style="zoom:50%;" /></p><span id="more"></span><h3 id="问题设定">10.1 问题设定</h3><p>在主成分分析（PCA）中，我们希望找到数据点 <span class="math inline">\(x_n\)</span> 的投影 <span class="math inline">\(\tilde{x}_n\)</span>，使得这些投影尽可能地接近原始数据点，但其内在维度却显著降低。图 10.1 给出了这种情形的一个示意。更具体地说，我们考虑一个独立同分布的数据集</p><p><span class="math display">\[X = \{x_1, \dots, x_N\}, \quad x_n \in \mathbb{R}^D,\]</span></p><p>其均值为 0，并具有数据协方差矩阵 (式 6.42)</p><p><span class="math display">\[S = \frac{1}{N} \sum_{n=1}^N x_n x_n^\top. \tag{10.1}\]</span></p><p>此外，我们假设存在一个低维压缩表示（编码）</p><p><span class="math display">\[z_n = B^\top x_n \in \mathbb{R}^M, \tag{10.2}\]</span></p><p>其中投影矩阵（projection matrix）定义为</p><p><span class="math display">\[B := [b_1, \dots, b_M] \in \mathbb{R}^{D \times M}. \tag{10.3}\]</span></p><p>我们假设矩阵 <span class="math inline">\(B\)</span> 的列向量是正交归一的（定义 3.7），即当且仅当 <span class="math inline">\(i \neq j\)</span> 时，<span class="math inline">\(b_i^\top b_j = 0\)</span>，并且 <span class="math inline">\(b_i^\top b_i = 1\)</span>。我们希望找到一个 <span class="math inline">\(M\)</span> 维子空间</p><p><span class="math display">\[U \subseteq \mathbb{R}^D, \quad \dim(U) = M &lt; D,\]</span></p><p>并将数据投影到该子空间上。我们记投影后的数据为 <span class="math inline">\(\tilde{x}_n \in U\)</span>，而它们相对于子空间 <span class="math inline">\(U\)</span> 的基向量 <span class="math inline">\(b_1, \dots, b_M\)</span> 的坐标（即编码）为 <span class="math inline">\(z_n\)</span>。我们的目标是找到投影<span class="math inline">\(\tilde{x}_n \in \mathbb{R}^D\)</span>（或等价地，找到编码 <span class="math inline">\(z_n\)</span> 以及基向量 <span class="math inline">\(b_1, \dots, b_M\)</span>），使得这些投影尽可能接近原始数据 <span class="math inline">\(x_n\)</span>，并使因压缩导致的损失最小化。</p><blockquote><p>个人注：关于投影详见章节 “3.8 正交投影”</p><p>投影是一种特殊的线性变换。得出的最终投影的坐标还是原空间的表达！</p></blockquote><p><strong>例 10.1（坐标表示/编码）</strong></p><p>考虑 <span class="math inline">\(\mathbb{R}^2\)</span>，其标准基为</p><p><span class="math display">\[e_1 = [1, 0]^\top, \quad e_2 = [0, 1]^\top.\]</span></p><p>在第 2 章中，我们知道任意 <span class="math inline">\(x \in \mathbb{R}^2\)</span> 都可以表示为这些基向量的线性组合，例如：</p><p><span class="math display">\[\begin{bmatrix} 5 \\ 3 \end{bmatrix} = 5 e_1 + 3 e_2. \tag{10.4}\]</span></p><p>然而，当我们只考虑如下形式的向量：</p><p><span class="math display">\[\tilde{x} = \begin{bmatrix} 0 \\ z \end{bmatrix} \in \mathbb{R}^2, \quad z \in \mathbb{R}, \tag{10.5}\]</span></p><p>它们总可以写作 <span class="math inline">\(0 e_1 + z e_2\)</span>。因此，要表示这些向量，仅需记住/存储 <span class="math inline">\(\tilde{x}\)</span> 在基向量 <span class="math inline">\(e_2\)</span> 上的坐标（即编码）<span class="math inline">\(z\)</span>。更准确地说，这些 <span class="math inline">\(\tilde{x}\)</span> 向量的集合（配合标准的向量加法和数乘运算）构成一个向量子空间 <span class="math inline">\(U\)</span>（参见第 2.4 节），其维度为</p><p><span class="math display">\[\dim(U) = 1, \quad 因为 \; U = \text{span}[e_2].\]</span></p><p>在 第 10.2 节 中，我们将寻找能尽可能保留信息、并最小化压缩损失的低维表示。另一种推导 PCA 的方法将在 第 10.3 节 中给出，在那里我们将通过最小化原始数据 <span class="math inline">\(x_n\)</span> 与其投影 <span class="math inline">\(\tilde{x}_n\)</span> 之间的平方重建误差</p><p><span class="math display">\[\|x_n - \tilde{x}_n\|^2\]</span></p><p>来得到结果。</p><p><img src="/img3/机器学习的数学基础Part2/F10.2.png" alt="F10.2" style="zoom:50%;" /></p><p>图 10.2 展示了 PCA 中所考虑的设定，其中 <span class="math inline">\(z\)</span> 表示压缩数据 <span class="math inline">\(\tilde{x}\)</span> 的低维表示，<strong>并扮演“瓶颈”的角色</strong>，控制着信息在 <span class="math inline">\(x\)</span> 与 <span class="math inline">\(\tilde{x}\)</span> 之间的流动量。在 PCA 中，我们考虑原始数据 <span class="math inline">\(x\)</span> 与其低维编码 <span class="math inline">\(z\)</span> 之间的线性关系，即 <span class="math display">\[z = B^\top x, \quad \tilde{x} = Bz,\]</span></p><p>其中 <span class="math inline">\(B\)</span> 是一个合适的矩阵。基于将 PCA 理解为数据压缩技术的动机，我们可以将图 10.2 中的箭头解释为表示“编码器”和“解码器”的一对操作。矩阵 <span class="math inline">\(B\)</span> 表示的线性映射可以看作 <strong>解码器</strong>，它将低维编码 <span class="math inline">\(z \in \mathbb{R}^M\)</span> 映射回原始数据空间 <span class="math inline">\(\mathbb{R}^D\)</span>。类似地，<span class="math inline">\(B^\top\)</span> 可以看作 <strong>编码器</strong>，它将原始数据 <span class="math inline">\(x\)</span> 编码为低维（压缩）表示 <span class="math inline">\(z\)</span>。</p><p>在本章中，我们将多次使用 MNIST 手写数字数据集 作为示例。该数据集包含 60,000 个手写数字（0 到 9）的样本。每个数字是一个大小为 <span class="math inline">\(28 \times 28\)</span> 的灰度图像，即包含 784 个像素，因此我们可以将数据集中的每张图片看作一个向量</p><p><span class="math display">\[x \in \mathbb{R}^{784}.\]</span></p><p>这些数字的示例如图 10.3 所示。</p><p><img src="/img3/机器学习的数学基础Part2/F10.3.png" alt="F10.3" style="zoom:67%;" /></p><h3 id="最大方差视角">10.2 最大方差视角</h3><p>图 10.1 给出了一个示例，展示了如何用单一坐标表示一个二维数据集。在图 10.1(b) 中，我们选择忽略了数据的 <span class="math inline">\(x_2\)</span> 坐标，因为它并没有带来太多信息，从而使压缩后的数据仍与原始数据（图 10.1(a)）非常接近。我们本可以选择忽略 <span class="math inline">\(x_1\)</span> 坐标，但那样得到的压缩数据将与原始数据差别很大，许多信息都会丢失。</p><p>如果我们<strong>将数据中的信息含量理解为数据“填充空间”的程度</strong>，那么可以通过观察数据的分布范围来描述其中包含的信息量。根据 第 6.4.1 节，我们知道<strong>方差是数据分布范围的一个指标</strong>。由此可以将 PCA 推导为一种维度约简算法，它通过在低维表示中最大化方差来保留尽可能多的信息。图 10.4 说明了这一点。</p><blockquote><p>个人注：所以为了获得最大的信息量，从而保留方差最大的方向。</p></blockquote><p><img src="/img3/机器学习的数学基础Part2/F10.4.png" alt="F10.4" style="zoom:50%;" /></p><p>结合 第 10.1 节 的设定，我们的目标是找到一个矩阵 <span class="math inline">\(B\)</span>（见 (10.3)），当数据被投影到由 <span class="math inline">\(B\)</span> 的列向量 <span class="math inline">\(b_1, \dots, b_M\)</span> 所张成的子空间时，能够尽可能保留信息。在数据压缩后保留最多信息等价于在低维编码中捕获最大的方差（Hotelling, 1933）。</p><p><strong>备注（居中数据）：</strong> 在 (10.1) 中的数据协方差矩阵里，我们假设了数据是中心化的。这个假设不会带来普遍性损失。假设 <span class="math inline">\(\mu\)</span> 是数据的均值。利用 第 6.4.4 节 中讨论过的方差性质，我们有： <span class="math display">\[V_z[z] = V_x[B^\top (x - \mu)] = V_x[B^\top x - B^\top \mu] = V_x[B^\top x], \tag{10.6}\]</span></p><p>即<strong>低维编码的方差与数据的均值无关</strong>。因此，我们可以不失一般性地假设数据均值为 0。在此假设下，低维编码的均值也为 0，因为</p><p><span class="math display">\[E_z[z] = E_x[B^\top x] = B^\top E_x[x] = 0.\]</span></p><h4 id="最大方差方向">10.2.1 最大方差方向</h4><p>我们采用一种 <strong>逐步方法</strong>（a sequential approach） 来最大化低维编码的方差。首先，我们寻找一个向量</p><p><span class="math display">\[b_1 \in \mathbb{R}^D\]</span></p><p>使得投影数据的方差最大化。换句话说，我们希望最大化低维表示 <span class="math inline">\(z \in \mathbb{R}^M\)</span> 的第一个坐标 <span class="math inline">\(z_1\)</span> 的方差：</p><p><span class="math display">\[V_1 := V[z_1] = \frac{1}{N} \sum_{n=1}^N z_{1n}^2, \tag{10.7}\]</span></p><p>其中我们利用了数据的 i.i.d. 假设，并定义 <span class="math inline">\(z_{1n}\)</span> 为样本 <span class="math inline">\(x_n \in \mathbb{R}^D\)</span> 的低维表示 <span class="math inline">\(z_n \in \mathbb{R}^M\)</span> 的第一个坐标。注意到，<span class="math inline">\(z_n\)</span> 的第一个分量由下式给出：</p><p><span class="math display">\[z_{1n} = b_1^\top x_n, \tag{10.8}\]</span></p><p>即，它是向量 <span class="math inline">\(x_n\)</span> 在由 <span class="math inline">\(b_1\)</span> 张成的一维子空间上的正交投影的坐标（见第 3.8 节）。将 (10.8) 代入 (10.7)，得到：</p><p><span class="math display">\[\begin{align}V_1 &amp;= \frac{1}{N} \sum_{n=1}^N (b_1^\top x_n)^2 \\&amp;= b_1^\top \left(\frac{1}{N} \sum_{n=1}^N x_n x_n^\top \right) b_1 \tag{10.9a} \\&amp;= b_1^\top S b_1, \tag{10.9b}\end{align}\]</span></p><p>其中 <span class="math inline">\(S\)</span> 是在 (10.1) 中定义的数据协方差矩阵。在 (10.9a) 中，我们利用了向量内积关于参数对称的性质，即</p><p><span class="math display">\[b_1^\top x_n = x_n^\top b_1.\]</span></p><p><strong>注意，如果任意放大向量 <span class="math inline">\(b_1\)</span> 的长度，就会使 <span class="math inline">\(V_1\)</span> 增大。例如，将 <span class="math inline">\(b_1\)</span> 的长度变为原来的两倍，可能使 <span class="math inline">\(V_1\)</span> 增大到原来的四倍。因此，我们必须施加约束条件 <span class="math inline">\(\|b_1\|^2 = 1\)</span>，</strong>从而将问题转化为一个约束优化问题： <span class="math display">\[\max_{b_1} \; b_1^\top S b_1  \\\quad \text{subject to } \|b_1\|^2 = 1. \tag{10.10}\]</span></p><p>根据 第 7.2 节，该约束优化问题的拉格朗日函数为：</p><p><span class="math display">\[L(b_1, \lambda_1) = b_1^\top S b_1 + \lambda_1 (1 - b_1^\top b_1). \tag{10.11}\]</span></p><p>分别对 <span class="math inline">\(b_1\)</span> 和 <span class="math inline">\(\lambda_1\)</span> 求偏导数：</p><p><span class="math display">\[\frac{\partial L}{\partial b_1} = 2 b_1^\top S - 2 \lambda_1 b_1^\top, \quad\frac{\partial L}{\partial \lambda_1} = 1 - b_1^\top b_1. \tag{10.12}\]</span></p><p>令偏导为 0，可得：</p><p><span class="math display">\[S b_1 = \lambda_1 b_1, \tag{10.13}\]</span></p><p><span class="math display">\[b_1^\top b_1 = 1. \tag{10.14}\]</span></p><p>对比 第 4.4 节 的特征值分解定义，可以发现：</p><ul><li><span class="math inline">\(b_1\)</span> 是数据协方差矩阵 <span class="math inline">\(S\)</span> 的特征向量；</li><li>拉格朗日乘子 <span class="math inline">\(\lambda_1\)</span> 即对应的特征值。</li></ul><p>因此，方差目标 (10.10) 可以改写为：</p><p><span class="math display">\[V_1 = b_1^\top S b_1 = \lambda_1 b_1^\top b_1 = \lambda_1. \tag{10.15}\]</span></p><p>也就是说，<strong>将数据投影到某一维子空间后的<u><em>方差，等于</em></u>该子空间所<em><u>对应的基向量 <span class="math inline">\(b_1\)</span> 的特征值</u></em>。为了最大化低维编码的方差，我们应选择<em><u>数据协方差矩阵最大特征值所对应的特征向量作为基向量</u></em>。这个特征向量被称为 第一主成分（first principal component）。</strong></p><blockquote><p>个人注：在标准 PCA 中，<strong>主成分方向</strong>就是协方差矩阵的特征向量（<strong>必须是单位长度</strong>），<span class="math inline">\(\lambda_1\)</span> 是沿该方向的方差。</p><p><strong>主成分本身是“投影出来的新坐标”</strong></p><p>由于 <span class="math inline">\(S\)</span> 是实对称矩阵，<strong>不同特征值对应的特征向量必定正交</strong>。再加上我们通常会把每个特征向量单位化，得到一组<strong>正交单位向量</strong>： <span class="math display">\[b_i^\top b_j = \delta_{ij}\]</span></p></blockquote><p>我们可以通过将坐标 <span class="math inline">\(z_{1n}\)</span> 映射回数据空间，来观察主成分 <span class="math inline">\(b_1\)</span> 在原始数据空间中的作用/贡献，从而得到投影后的数据点： <span class="math display">\[\tilde{x}_n = b_1 z_{1n} = b_1 b_1^\top x_n \in \mathbb{R}^D. \tag{10.16}\]</span></p><p><strong>备注：</strong> 尽管 <span class="math inline">\(\tilde{x}_n\)</span> 是一个 <span class="math inline">\(D\)</span>-维向量，但它只需要一个坐标 <span class="math inline">\(z_{1n}\)</span> 就能在基向量 <span class="math inline">\(b_1 \in \mathbb{R}^D\)</span> 下表示出来。</p><h4 id="最大方差的-m-维子空间">10.2.2 最大方差的 M 维子空间</h4><p>假设我们已经找到了前 <span class="math inline">\(m-1\)</span> 个主成分，即协方差矩阵 <span class="math inline">\(S\)</span> 的前 <span class="math inline">\(m-1\)</span> 个最大特征值所对应的特征向量。由于 <span class="math inline">\(S\)</span> 是对称矩阵，根据 <strong>谱定理（定理 4.15）</strong>，我们可以用这些特征向量构造出一个 <span class="math inline">\(\mathbb{R}^D\)</span> 中维度为 <span class="math inline">\(m-1\)</span> 的正交规范特征子空间。</p><p>一般而言，第 <span class="math inline">\(m\)</span> 个主成分可以通过从数据中去除前 <span class="math inline">\(m-1\)</span> 个主成分 <span class="math inline">\(b_1, \ldots, b_{m-1}\)</span> 的影响而得到，从而在剩余信息中继续寻找主成分。这样，我们得到新的数据矩阵：</p><p><span class="math display">\[\hat{X} := X - \sum_{i=1}^{m-1} b_i b_i^\top X \;=\; X - B_{m-1} X, \tag{10.17}\]</span></p><p>其中 <span class="math inline">\(X = [x_1, \ldots, x_N] \in \mathbb{R}^{D \times N}\)</span> 的列向量为数据点， 而</p><p><span class="math display">\[B_{m-1} := \sum_{i=1}^{m-1} b_i b_i^\top\]</span></p><p>是一个投影矩阵，将数据投影到由 <span class="math inline">\(b_1, \ldots, b_{m-1}\)</span> 张成的子空间上。</p><blockquote><p>个人注：注意定义，<span class="math inline">\(b_i b_i^\top X\)</span>是<span class="math inline">\(X\)</span>投影到<span class="math inline">\(b_i\)</span>后的坐标（原空间的）。</p></blockquote><p><strong>备注（符号约定）</strong>：<strong>在本章中，我们不采用将数据点 <span class="math inline">\(x_1, \ldots, x_N\)</span> 作为矩阵行向量的惯例，而是将它们作为矩阵 <span class="math inline">\(X\)</span> 的列向量。这意味着我们的数据矩阵 <span class="math inline">\(X\)</span> 是一个 <span class="math inline">\(D \times N\)</span> 矩阵，而非常见的 <span class="math inline">\(N \times D\)</span> 矩阵。我们这样选择的原因是，在后续代数运算中可以避免频繁转置或将列向量重定义为行向量来左乘矩阵</strong>。</p><blockquote><p>个人注：在线性回归分析中，拟合的模型是<span class="math inline">\(X \theta\)</span>，<span class="math inline">\(X\)</span>作为数据的时候，放在左边，这时候可以把数据看作行。</p></blockquote><p>为了找到第 <span class="math inline">\(m\)</span> 个主成分，我们希望最大化：</p><p><span class="math display">\[V_m = V[z_m] = \frac{1}{N} \sum_{n=1}^N z_{mn}^2 = \frac{1}{N} \sum_{n=1}^N (b_m^\top \hat{x}_n)^2= b_m^\top \hat{S} b_m, \tag{10.18}\]</span></p><p>其中约束为 <span class="math inline">\(\|b_m\|^2 = 1\)</span>。这里我们遵循 (10.9b) 的相同步骤，并定义 <span class="math inline">\(\hat{S}\)</span> 为变换后数据集<span class="math inline">\(\hat{X} := \{\hat{x}_1, \ldots, \hat{x}_N\}\)</span> 的协方差矩阵。与第一主成分类似，我们得到一个约束优化问题，其最优解 <span class="math inline">\(b_m\)</span> 是 <span class="math inline">\(\hat{S}\)</span> 最大特征值所对应的特征向量。</p><p>结果表明，<span class="math inline">\(b_m\)</span> 也是 <span class="math inline">\(S\)</span> 的特征向量。<strong>更一般地说，<span class="math inline">\(S\)</span> 和 <span class="math inline">\(\hat{S}\)</span> 的特征向量集合是相同的。</strong>由于 <span class="math inline">\(S\)</span> 和 <span class="math inline">\(\hat{S}\)</span> 都是对称矩阵，根据谱定理 (4.15)，它们都有 <span class="math inline">\(D\)</span> 个两两正交的特征向量。下面我们来证明：每一个 <span class="math inline">\(S\)</span> 的特征向量同时也是 <span class="math inline">\(\hat{S}\)</span> 的特征向量。</p><p>假设我们已经得到了 <span class="math inline">\(\hat{S}\)</span> 的前 <span class="math inline">\(m-1\)</span> 个特征向量 <span class="math inline">\(b_1, \ldots, b_{m-1}\)</span>。考虑 <span class="math inline">\(S\)</span> 的某个特征向量 <span class="math inline">\(b_i\)</span>，即：</p><p><span class="math display">\[S b_i = \lambda_i b_i.\]</span></p><p>一般情况下：</p><p><span class="math display">\[\hat{S} b_i = \frac{1}{N} \hat{X} \hat{X}^\top b_i = \frac{1}{N} (X - B_{m-1} X)(X - B_{m-1} X)^\top b_i \tag{10.19a}\]</span></p><p><span class="math display">\[= (S - S B_{m-1} - B_{m-1} S + B_{m-1} S B_{m-1}) b_i. \tag{10.19b}\]</span></p><p>我们区分两种情况：</p><ul><li><strong>情况 1： <span class="math inline">\(i \geq m\)</span>。</strong> 此时，<span class="math inline">\(b_i\)</span> 不是前 <span class="math inline">\(m-1\)</span> 个主成分中的一个，因此与它们正交，即</li></ul><p><span class="math display">\[B_{m-1} b_i = 0.\]</span></p><ul><li><strong>情况 2： <span class="math inline">\(i &lt; m\)</span>。</strong> 此时，<span class="math inline">\(b_i\)</span> 是前 <span class="math inline">\(m-1\)</span> 个主成分之一，因此属于由 <span class="math inline">\(b_1, \ldots, b_{m-1}\)</span> 张成的子空间。由于它们构成该子空间的 ONB，有：</li></ul><p><span class="math display">\[B_{m-1} b_i = b_i.\]</span></p><p>总结如下：</p><p><span class="math display">\[B_{m-1} b_i =\begin{cases}b_i, &amp; i &lt; m \\0, &amp; i \geq m\end{cases} \tag{10.20}\]</span></p><p>在 <strong>情况 1</strong> (<span class="math inline">\(i \geq m\)</span>) 下，将 (10.20) 代入 (10.19b)，得：</p><p><span class="math display">\[\hat{S} b_i = (S - B_{m-1} S) b_i = S b_i = \lambda_i b_i,\]</span></p><p>即 <span class="math inline">\(b_i\)</span> 也是 <span class="math inline">\(\hat{S}\)</span> 的特征向量，特征值为 <span class="math inline">\(\lambda_i\)</span>。特别地：</p><p><span class="math display">\[\hat{S} b_m = S b_m = \lambda_m b_m. \tag{10.21}\]</span></p><p>这表明，<span class="math inline">\(b_m\)</span> 既是 <span class="math inline">\(S\)</span> 的特征向量，也是 <span class="math inline">\(\hat{S}\)</span> 的特征向量。更具体地说，<span class="math inline">\(\lambda_m\)</span> 是 <span class="math inline">\(\hat{S}\)</span> 的最大特征值，同时也是 <span class="math inline">\(S\)</span> 的第 <span class="math inline">\(m\)</span> 大特征值，它们的特征向量相同。</p><p>在 <strong>情况 2</strong> (<span class="math inline">\(i &lt; m\)</span>) 下，将 (10.20) 代入 (10.19b)，得：</p><p><span class="math display">\[\hat{S} b_i = (S - S B_{m-1} - B_{m-1} S + B_{m-1} S B_{m-1}) b_i = 0 = 0 b_i. \tag{10.22}\]</span></p><p>这意味着 <span class="math inline">\(b_1, \ldots, b_{m-1}\)</span> 也是 <span class="math inline">\(\hat{S}\)</span> 的特征向量，但它们对应的特征值为 0，因此张成了 <span class="math inline">\(\hat{S}\)</span> 的零空间。</p><p>综上所述，<span class="math inline">\(S\)</span> 的每个特征向量都是 <span class="math inline">\(\hat{S}\)</span> 的特征向量。但如果这些特征向量属于前 <span class="math inline">\(m-1\)</span> 个主成分子空间，那么它们在 <span class="math inline">\(\hat{S}\)</span> 中对应的特征值为 0。由 (10.21) 以及约束 <span class="math inline">\(b_m^\top b_m = 1\)</span>，数据投影到第 <span class="math inline">\(m\)</span> 个主成分上的方差为：</p><p><span class="math display">\[V_m = b_m^\top S b_m = \lambda_m b_m^\top b_m = \lambda_m. \tag{10.23}\]</span></p><p>这意味着，<strong>当数据投影到一个 <span class="math inline">\(M\)</span> 维子空间时，其方差等于数据协方差矩阵中与该子空间对应的特征向量的特征值之和</strong></p><p><img src="/img3/机器学习的数学基础Part2/F10.5.png" alt="F10.5" style="zoom:67%;" /></p><p><strong>例 10.2（MNIST 中数字 “8” 的特征值）</strong> 取出 MNIST 训练集中所有的数字 “8”，我们计算这些数据的协方差矩阵的特征值。图 10.5(a) 展示了数据协方差矩阵最大的 200 个特征值。我们看到，只有很少的一部分特征值与 0 有显著差异。因此，当把数据投影到由相应特征向量所张成的子空间时，大部分的方差仅由少数几个主成分捕捉，如图 10.5(b) 所示。</p><p>总体而言，为了在 <span class="math inline">\(\mathbb{R}^D\)</span> 中找到一个尽可能保留信息的 <span class="math inline">\(M\)</span> 维子空间，PCA 告诉我们，应选择矩阵 <span class="math inline">\(B_{in}\)</span>（公式 (10.3) 中）对应的 <span class="math inline">\(M\)</span> 个特征向量——这些特征向量对应于数据协方差矩阵 <span class="math inline">\(S\)</span> 的最大的 <span class="math inline">\(M\)</span> 个特征值。PCA 在前 <span class="math inline">\(M\)</span> 个主成分中能捕捉到的最大方差为</p><p><span class="math display">\[V_M = \sum_{m=1}^M \lambda_m, \tag{10.24}\]</span></p><p>其中 <span class="math inline">\(\lambda_m\)</span> 是数据协方差矩阵 <span class="math inline">\(S\)</span> 的最大的 <span class="math inline">\(M\)</span> 个特征值。相应地，通过 PCA 压缩数据所丢失的方差为</p><p><span class="math display">\[J_M := \sum_{j=M+1}^D \lambda_j = V_D - V_M. \tag{10.25}\]</span></p><p>与其使用这些绝对量，我们也可以定义相对方差：</p><ul><li>被捕捉的相对方差：<span class="math inline">\(\frac{V_M}{V_D}\)</span>，</li><li>压缩导致的相对方差损失：<span class="math inline">\(1 - \frac{V_M}{V_D}\)</span>。</li></ul><h3 id="投影视角">10.3 投影视角</h3><p>在本节中，我们将把 PCA 推导为一种直接<strong>最小化平均重构误差</strong>的算法。这种视角使我们能够把 PCA 理解为一种最优的线性自编码器。这里我们将大量借鉴第 2 章和第 3 章的内容。</p><p>we will derive PCA as an algorithm that directly <strong>mini-mizes the average reconstruction error.</strong> This perspective allows us to interpret PCA as <strong>implementing an optimal linear auto-encoder.</strong></p><p>在上一节中，我们是通过最大化投影空间中的方差来推导 PCA 的，以尽可能保留信息。而在这里，我们将考察原始数据点 <span class="math inline">\(x_n\)</span> 与其重构 <span class="math inline">\(\tilde{x}_n\)</span> 之间的差异向量，并最小化这一距离，使得 <span class="math inline">\(x_n\)</span> 与 <span class="math inline">\(\tilde{x}_n\)</span> 尽可能接近。图 10.6 展示了这一设置。</p><p><img src="/img3/机器学习的数学基础Part2/F10.6.png" alt="F10.6" style="zoom:67%;" /></p><h4 id="设置与目标">10.3.1 设置与目标</h4><p>假设在 <span class="math inline">\(\mathbb{R}^D\)</span> 中有一个（有序的）正交归一基（ONB）<span class="math inline">\(B = (b_1, \dots, b_D),\)</span>即当且仅当 <span class="math inline">\(i=j\)</span> 时 <span class="math inline">\(b_i^\top b_j = 1\)</span>，否则为 0。根据第 2.5 节，我们知道，对于 <span class="math inline">\(\mathbb{R}^D\)</span> 的一组基 <span class="math inline">\((b_1,\dots,b_D)\)</span>，任意 <span class="math inline">\(x \in \mathbb{R}^D\)</span> 都可以表示为基向量的线性组合，即 <span class="math display">\[x = \sum_{d=1}^D \zeta_d b_d = \sum_{m=1}^M \zeta_m b_m + \sum_{j=M+1}^D \zeta_j b_j, \tag{10.26}\]</span> 其中 <span class="math inline">\(\zeta_d \in \mathbb{R}\)</span> 为相应的坐标。我们感兴趣的是寻找向量 <span class="math inline">\(\tilde{x} \in \mathbb{R}^D\)</span>，它位于某个低维子空间<span class="math inline">\(U \subseteq \mathbb{R}^D, \ \dim(U)=M\)</span>，使得 <span class="math display">\[\tilde{x} = \sum_{m=1}^M z_m b_m \in U \subseteq \mathbb{R}^D \tag{10.27}\]</span> 与原始数据 <span class="math inline">\(x\)</span> 尽可能相似。注意，此时我们必须假设 <span class="math inline">\(\tilde{x}\)</span> 的坐标 <span class="math inline">\(z_m\)</span> 与 <span class="math inline">\(x\)</span> 的坐标 <span class="math inline">\(\zeta_m\)</span> 不完全相同。</p><p>接下来，我们正是利用这种形式的 <span class="math inline">\(\tilde{x}\)</span>，来寻找最优的坐标 <span class="math inline">\(z\)</span> 和基向量 <span class="math inline">\(b_1,\dots,b_M\)</span>，使得 <span class="math inline">\(\tilde{x}\)</span> 与原始数据点 <span class="math inline">\(x\)</span> 尽可能接近，即最小化两者之间的欧几里得距离 <span class="math inline">\(\|x - \tilde{x}\|\)</span>。图 10.7 展示了这一情况。</p><p><img src="/img3/机器学习的数学基础Part2/F10.7.png" alt="F10.7" style="zoom:50%;" /></p><p>为了简化推导，不失一般性地，我们假设数据集<span class="math inline">\(X = \{x_1,\dots,x_N\}, \quad x_n \in \mathbb{R}^D,\)</span>是零均值的，即 <span class="math inline">\(E[X]=0\)</span>。如果不做零均值的假设，最终我们也会得到完全相同的解，但符号表达会变得非常繁琐。</p><p>我们的目标是：找到把 <span class="math inline">\(X\)</span> 投影到低维子空间 <span class="math inline">\(U \subseteq \mathbb{R}^D\)</span>（<span class="math inline">\(\dim(U)=M\)</span>）的最佳线性投影，其中子空间由正交归一基向量 <span class="math inline">\(b_1,\dots,b_M\)</span> 张成。我们称该子空间 <span class="math inline">\(U\)</span> 为<strong>主子空间</strong>(principal subspace.)。数据点的投影表示为</p><p><span class="math display">\[\tilde{x}_n := \sum_{m=1}^M z_{mn} b_m = B z_n \in \mathbb{R}^D, \tag{10.28}\]</span> 其中<span class="math inline">\(z_n := [z_{1n}, \dots, z_{Mn}]^\top \in \mathbb{R}^M\)</span>是 <span class="math inline">\(\tilde{x}_n\)</span> 在基 <span class="math inline">\((b_1,\dots,b_M)\)</span> 下的坐标向量。换句话说，我们希望 <span class="math inline">\(\tilde{x}_n\)</span> 尽可能接近原始点 <span class="math inline">\(x_n\)</span>。</p><p>在这里，我们采用的相似度度量是 <span class="math inline">\(x\)</span> 与 <span class="math inline">\(\tilde{x}\)</span> 之间的平方欧几里得距离 <span class="math inline">\(\|x - \tilde{x}\|^2\)</span>。因此，我们定义目标函数为<strong>最小化平均平方欧几里得距离（重构误差）</strong>（Pearson, 1901）： <span class="math display">\[J_M := \frac{1}{N} \sum_{n=1}^N \|x_n - \tilde{x}_n\|^2, \tag{10.29}\]</span> 其中我们明确指出，数据投影的子空间维度是 <span class="math inline">\(M\)</span>。为了找到最优的线性投影，我们需要确定主子空间的正交归一基，以及投影点在该基下的坐标向量 <span class="math inline">\(z_n \in \mathbb{R}^M\)</span>。</p><p>为此，我们采用两步策略：</p><ol type="1"><li>在给定 ONB <span class="math inline">\((b_1,\dots,b_M)\)</span> 的前提下，先优化坐标 <span class="math inline">\(z_n\)</span>；</li><li>再寻找最优的 ONB。</li></ol><h4 id="寻找最优坐标">10.3.2 寻找最优坐标</h4><p>我们先从寻找投影 <span class="math inline">\(\tilde{x}_n\)</span> 的最优坐标 <span class="math inline">\(z_{1n}, \dots, z_{Mn}\)</span> 开始（<span class="math inline">\(n=1,\dots,N\)</span>）。考虑图 10.7(b)，其中主子空间由单个向量 <span class="math inline">\(b\)</span> 张成。几何上，寻找最优坐标 <span class="math inline">\(z\)</span> 对应于：在基向量 <span class="math inline">\(b\)</span> 上找到 <span class="math inline">\(\tilde{x}\)</span> 的表示，使得 <span class="math inline">\(\tilde{x}-x\)</span> 的距离最小。从图 10.7(b) 可以直观看出，这将是<strong>正交投影</strong>，接下来我们将严格证明这一点。</p><p>假设 <span class="math inline">\(U \subseteq \mathbb{R}^D\)</span> 的 ONB（正交归一基）为 <span class="math inline">\((b_1,\dots,b_M)\)</span>。要在该基下找到最优坐标 <span class="math inline">\(z_m\)</span>，我们需要计算偏导数： <span class="math display">\[\begin{align}\frac{\partial J_{M}}{\partial z_{in}} &amp;= \frac{\partial J_{M}}{\partial \tilde{x}_{n}} \frac{\partial \tilde{x}_{n}}{\partial z_{in}}, \tag{10.30a}\\[0.5em]\frac{\partial J_{M}}{\partial \tilde{x}_{n}} &amp;= -\frac{2}{N}\,(x_{n}-\tilde{x}_{n})^{\top} \in \mathbb{R}^{1 \times D}, \tag{10.30b}\end{align}\]</span> 另一方面， <span class="math display">\[\frac{\partial \tilde{x}_n}{\partial z_{in}} = b_i, \quad i=1,\dots,M. \tag{10.30c}\]</span> 结合 (10.28)，可得： <span class="math display">\[\frac{\partial J_M}{\partial z_{in}} = -\frac{2}{N} (x_n - \sum_{m=1}^M z_{mn} b_m)^\top b_i. \tag{10.31a}\]</span> 由于 <span class="math inline">\(b_i^\top b_i = 1\)</span>，化简得： <span class="math display">\[\frac{\partial J_M}{\partial z_{in}} = -\frac{2}{N}(x_n^\top b_i - z_{in}). \tag{10.31b}\]</span> 令该偏导为 0，立即得到最优坐标：<span class="math inline">\(U^\perp = \text{span}[b_{M+1}, \dots, b_D]\)</span>是<span class="math inline">\(U = \text{span}[b_1,\dots,b_M]\)</span>的正交补（参见第 3.6 节）。</p><p><strong>备注（带正交归一基向量的正交投影）</strong></p><p>简要回顾第 3.8 节的正交投影。如果 <span class="math inline">\((b_1,\dots,b_D)\)</span> 是 <span class="math inline">\(\mathbb{R}^D\)</span> 的正交归一基，那么 <span class="math display">\[\tilde{x} = b_j(b_j^\top b_j)^{-1} b_j^\top x = b_j b_j^\top x \in \mathbb{R}^D \tag{10.33}\]</span> 就是 <span class="math inline">\(x\)</span> 在第 <span class="math inline">\(j\)</span> 个基向量张成的一维子空间上的正交投影。而此时坐标 <span class="math display">\[z_j = b_j^\top x\]</span> 正是该投影在基向量 <span class="math inline">\(b_j\)</span> 下的坐标，因为 <span class="math inline">\(z_j b_j = \tilde{x}\)</span>。图 10.8(b) 展示了这种情况。</p><p><img src="/img3/机器学习的数学基础Part2/F10.8.png" alt="F10.8" style="zoom:50%;" /></p><p>更一般地，若我们希望把 <span class="math inline">\(x\)</span> 投影到 <span class="math inline">\(\mathbb{R}^D\)</span> 的一个 M 维子空间上，且该子空间的 ONB 为 <span class="math inline">\(b_1,\dots,b_M\)</span>，则正交投影为： <span class="math display">\[\tilde{x} = B(B^\top B)^{-1}B^\top x = BB^\top x, \tag{10.34}\]</span> 其中 <span class="math display">\[B = [b_1,\dots,b_M] \in \mathbb{R}^{D\times M}.\]</span> 在有序基 <span class="math inline">\((b_1,\dots,bM)\)</span> 下，该投影的坐标为 <span class="math display">\[z = B^\top x,\]</span> 这与第 3.8 节的讨论一致。</p><p>我们可以将这些坐标理解为在新的坐标系 <span class="math inline">\((b_1,\dots,b_M)\)</span> 下对投影向量的表示。需要注意的是，虽然 <span class="math inline">\(\tilde{x} \in \mathbb{R}^D\)</span>，但仅需用 <span class="math inline">\(M\)</span> 个坐标 <span class="math inline">\(z_1,\dots,z_M\)</span> 来表示；其余相对于基 <span class="math inline">\((b_{M+1},\dots,b_D)\)</span> 的 <span class="math inline">\(D-M\)</span> 个坐标恒为 0。</p><p>到目前为止，我们已经证明：在给定 ONB 的情况下，可以通过正交投影找到 <span class="math inline">\(\tilde{x}\)</span> 的最优坐标。接下来，我们将确定最优基是什么。</p><h4 id="寻找主子空间的基">10.3.3 寻找主子空间的基</h4><p>为了确定主子空间的基向量 <span class="math inline">\(b_1, \ldots, b_M\)</span>，我们利用目前已有的结果重新表述损失函数 (10.29)。这样会使得寻找基向量更为容易。为了重新写损失函数，我们使用之前的推导，得到 <span class="math display">\[\tilde{x}_{n} = \sum_{m=1}^{M} z_{mn} \mathbf{b}_{m} \overset{(10.32)}{=} \sum_{m=1}^{M} \left(x_{n}^{\top} \mathbf{b}_{m}\right) \mathbf{b}_{m}. \tag{10.35}\]</span> 接下来，我们利用点积的对称性，得到 <span class="math display">\[\tilde{x}_n = \left(\sum_{m=1}^M b_m b_m^\top \right) x_n. \tag{10.36}\]</span> 由于我们通常可以把原始数据点 <span class="math inline">\(x_n\)</span> 表示为所有基向量的线性组合，因此有 <span class="math display">\[\begin{align}x_{n} &amp;= \sum_{d=1}^{D} z_{dn} \mathbf{b}_{d} \overset{(10.32)}{=} \sum_{d=1}^{D} \left(x_{n}^{\top} \mathbf{b}_{d}\right) \mathbf{b}_{d} = \left(\sum_{d=1}^{D} \mathbf{b}_{d} \mathbf{b}_{d}^{\top}\right) x_{n}, \tag{10.37a} \\[0.75em]&amp;= \left(\sum_{m=1}^{M} \mathbf{b}_{m} \mathbf{b}_{m}^{\top}\right) x_{n} + \left(\sum_{j=M+1}^{D} \mathbf{b}_{j} \mathbf{b}_{j}^{\top}\right) x_{n},\tag{10.37b}\end{align}\]</span> 其中，我们将含 <span class="math inline">\(D\)</span> 项的和拆分为含 <span class="math inline">\(M\)</span> 项的部分与含 <span class="math inline">\(D-M\)</span> 项的部分。由此可得，位移向量 <span class="math inline">\(\tilde{x}_n - x_n\)</span>，即原始数据点与其投影的差向量为 <span class="math display">\[\tilde{x}_n - x_n = \sum_{j=M+1}^D b_j b_j^\top x_n \tag{10.38a}\]</span></p><p><span class="math display">\[= \sum_{j=M+1}^D (x_n^\top b_j)b_j. \tag{10.38b}\]</span> 这意味着，该差向量正好是数据点在主子空间的正交补上的投影：我们将 (10.38a) 中的矩阵 <span class="math inline">\(\sum_{j=M+1}^D b_j b_j^\top\)</span> 识别为执行该投影的投影矩阵。因此，位移向量 <span class="math inline">\(x_n - \tilde{x}_n\)</span> 位于与主子空间正交的子空间中，如图 10.9 所示。</p><p><img src="/img3/机器学习的数学基础Part2/F10.9.png" alt="F10.9" style="zoom:50%;" /></p><p><strong>备注（低秩近似）</strong>在 (10.38a) 中，我们看到将 <span class="math inline">\(x\)</span> 投影到 <span class="math inline">\(\tilde{x}\)</span> 的投影矩阵为 <span class="math display">\[\sum_{m=1}^M b_m b_m^\top = BB^\top. \tag{10.39}\]</span> 由构造可知，作为秩一矩阵 <span class="math inline">\(b_m b_m^\top\)</span> 的和，<span class="math inline">\(BB^\top\)</span> 是对称的，并且秩为 <span class="math inline">\(M\)</span>。因此，平均平方重构误差可以写为 <span class="math display">\[\begin{align}\frac{1}{N} \sum_{n=1}^N \|x_n - \tilde{x}_n\|^2= \frac{1}{N} \sum_{n=1}^N \|x_n - BB^\top x_n\|^2 \tag{10.40a} \\= \frac{1}{N} \sum_{n=1}^N \|(I - BB^\top)x_n\|^2. \tag{10.40b}\end{align}\]</span> 寻找使原始数据 <span class="math inline">\(x_n\)</span> 与其投影 <span class="math inline">\(\tilde{x}_n\)</span> 之间差异最小的正交归一基向量 <span class="math inline">\(b_1, \ldots, b_M\)</span>，等价于寻找单位矩阵 <span class="math inline">\(I\)</span> 的最佳秩-<span class="math inline">\(M\)</span> 近似 <span class="math inline">\(BB^\top\)</span>（参见第 4.6 节。现在我们已经具备了重新表述损失函数 (10.29) 的所有工具：</p><p><span class="math display">\[J_M = \frac{1}{N} \sum_{n=1}^{N} \|\boldsymbol{x}_n - \tilde{\boldsymbol{x}}_n\|^2 \overset{\text{(10.38b)}}{=} \frac{1}{N} \sum_{n=1}^{N} \left\| \sum_{j=M+1}^{D} (\boldsymbol{b}_j^\top \boldsymbol{x}_n) \boldsymbol{b}_j \right\|^2. \tag{10.41}\]</span> 我们现在显式计算平方范数，并利用基向量 <span class="math inline">\(b_j\)</span> 构成正交归一基 (ONB) 的事实，得到： <span class="math display">\[\begin{align}J_M &amp; = \frac{1}{N} \sum_{n=1}^{N} \sum_{j=M+1}^{D} (\boldsymbol{b}_j^\top \boldsymbol{x}_n)^2 = \frac{1}{N} \sum_{n=1}^{N} \sum_{j=M+1}^{D} \boldsymbol{b}_j^\top \boldsymbol{x}_n \boldsymbol{b}_j^\top \boldsymbol{x}_n \tag{10.42a} \\&amp; = \frac{1}{N} \sum_{n=1}^{N} \sum_{j=M+1}^{D} \boldsymbol{b}_j^\top \boldsymbol{x}_n \boldsymbol{x}_n^\top \boldsymbol{b}_j, \tag{10.42b}\end{align}\]</span> 其中，在最后一步中我们利用了点积的对称性，将 <span class="math inline">\(b_j^\top x_n = x_n^\top b_j\)</span> 写出。 接下来交换求和次序： <span class="math display">\[J_M = \sum_{j=M+1}^D b_j^\top \Big( \frac{1}{N}\sum_{n=1}^N x_n x_n^\top \Big) b_j=: \sum_{j=M+1}^D b_j^\top S b_j, \tag{10.43a}\]</span></p><p><span class="math display">\[= \sum_{j=M+1}^D \operatorname{tr}(b_j^\top S b_j) = \sum_{j=M+1}^D \operatorname{tr}(S b_j b_j^\top) = \operatorname{tr}\Big(S \sum_{j=M+1}^D b_j b_j^\top \Big), \tag{10.43b}\]</span> 这里我们利用了迹算子 <span class="math inline">\(\operatorname{tr}(\cdot)\)</span> 的性质（见公式 (4.18)）：其是线性的，并且对循环置换不变。由于我们假设数据集已经中心化，即 <span class="math inline">\(E[X] = 0\)</span>，因此矩阵 <span class="math inline">\(S\)</span> 就是数据的协方差矩阵。而 (10.43b) 中的投影矩阵由秩一矩阵 <span class="math inline">\(b_j b_j^\top\)</span> 的和构成，因此其秩为 <span class="math inline">\(D-M\)</span>。等式 (10.43a) 表明：我们可以将<strong>平均平方重构误差</strong>等价地表述为数据协方差矩阵在主子空间正交补上的投影。因此，最小化平均平方重构误差就等价于最小化数据在被舍弃子空间（即主子空间正交补）上的投影方差。换句话说，这也等价于<strong>最大化</strong>我们保留的主子空间上的投影方差。这就把“投影损失”的视角和第 10.2 节讨论的 PCA 最大方差公式直接联系起来。因此，我们会得到和最大方差视角相同的解。于是，这里不再重复推导，而是结合投影视角总结之前的结果。当投影到 <span class="math inline">\(M\)</span> 维主子空间时，平均平方重构误差为： <span class="math display">\[J_M = \sum_{j=M+1}^D \lambda_j, \tag{10.44}\]</span> 其中 <span class="math inline">\(\lambda_j\)</span> 是数据协方差矩阵的特征值。因此，为了最小化 (10.44)，我们需要舍弃最小的 <span class="math inline">\(D-M\)</span> 个特征值，其对应的特征向量就构成主子空间的正交补的基。于是，<strong>主子空间的基就是协方差矩阵对应于最大 <span class="math inline">\(M\)</span> 个特征值的特征向量 <span class="math inline">\(b_1, \ldots, b_M\)</span>。</strong></p><p><strong>例 10.3（MNIST 数字嵌入）</strong> 图 10.10 将 MNIST 数字 “0” 和 “1” 的训练数据可视化，这些数据被嵌入到由前两个主成分所张成的向量子空间中。我们可以观察到“0”（蓝点）和“1”（橙点）之间有相对清晰的分离，同时也能看到每个簇内部的变化情况。图中用红色标出了四个数字 “0” 和 “1” 的嵌入点，并展示了它们对应的原始数字。图像揭示了“0”的数据集内部变化明显大于“1”的数据集内部变化。</p><p><img src="/img3/机器学习的数学基础Part2/F10.10.png" alt="F10.10" style="zoom:50%;" /></p><h3 id="特征向量计算与低秩近似"><strong>10.4 特征向量计算与低秩近似</strong></h3><p>在前面的章节中，我们得到了主子空间的基，它们是<strong>数据协方差矩阵</strong>中与最大特征值相关联的特征向量： <span class="math display">\[S = \frac{1}{N} \sum_{n=1}^{N} x_n x_n^\top \;=\; \frac{1}{N} X X^\top , \quad X = [x_1,\ldots, x_N] \in \mathbb{R}^{D \times N}\tag{10.45-10.46}\]</span> 需要注意的是，<span class="math inline">\(X\)</span> 是一个 <span class="math inline">\(D \times N\)</span> 的矩阵，即它是“常见”数据矩阵的转置（Bishop, 2006; Murphy, 2012）。为了得到 <span class="math inline">\(S\)</span> 的特征值（及对应的特征向量），我们有两种方法：</p><ol type="1"><li>对 <span class="math inline">\(S\)</span> 做特征分解（见第 4.2 节），直接计算其特征值与特征向量。</li><li>使用奇异值分解（见第 4.5 节）。由于 <span class="math inline">\(S\)</span> 是对称矩阵，并且可以分解为 <span class="math inline">\(X X^\top\)</span>（忽略因子 <span class="math inline">\(\tfrac{1}{N}\)</span>），因此 <span class="math inline">\(S\)</span> 的特征值就是 <span class="math inline">\(X\)</span> 的奇异值的平方。</li></ol><p>更具体地，<span class="math inline">\(X\)</span> 的奇异值分解为： <span class="math display">\[X_{D \times N} = U_{D \times D} \; \Sigma_{D \times N} \; V^\top_{N \times N} \tag{10.47}\]</span> 其中，<span class="math inline">\(U \in \mathbb{R}^{D \times D}\)</span> 和 <span class="math inline">\(V^\top \in \mathbb{R}^{N \times N}\)</span> 是正交矩阵，<span class="math inline">\(\Sigma \in \mathbb{R}^{D \times N}\)</span> 是一个只有奇异值 <span class="math inline">\(\sigma_{ii} \geq 0\)</span> 为非零的矩阵。于是我们有： <span class="math display">\[S = \frac{1}{N} X X^\top = \frac{1}{N} U \Sigma V^\top V \Sigma^\top U^\top = \frac{1}{N} U \Sigma \Sigma^\top U^\top\tag{10.48}\]</span> 根据第 4.5 节的结果，矩阵 <span class="math inline">\(U\)</span> 的列向量就是 <span class="math inline">\(X X^\top\)</span>（因此也是 <span class="math inline">\(S\)</span>）的特征向量。进一步地，<span class="math inline">\(S\)</span> 的特征值 <span class="math inline">\(\lambda_d\)</span> 与 <span class="math inline">\(X\)</span> 的奇异值的关系为： <span class="math display">\[\lambda_d = \frac{\sigma_d^2}{N}\tag{10.49}\]</span> <strong>这种 <span class="math inline">\(S\)</span> 的特征值与 <span class="math inline">\(X\)</span> 的奇异值之间的关系，建立了最大方差视角（第 10.2 节）与奇异值分解之间的联系。</strong></p><h4 id="使用低秩矩阵近似的-pca"><strong>10.4.1 使用低秩矩阵近似的 PCA</strong></h4><p>为了<strong>最大化投影数据的方差（或最小化平均平方重构误差）</strong>，PCA 选择式 (10.48) 中矩阵 <span class="math inline">\(U\)</span> 的列向量作为数据协方差矩阵 <span class="math inline">\(S\)</span> 的前 <span class="math inline">\(M\)</span> 大特征值对应的特征向量。这样，我们可以将 <span class="math inline">\(U\)</span> 识别为公式 (10.3) 中的投影矩阵 <span class="math inline">\(B\)</span>，它把原始数据投影到一个维度为 <span class="math inline">\(M\)</span> 的低维子空间上。</p><p><strong>Eckart–Young 定理</strong>（见第 4.6 节的定理 4.25）提供了一种直接的方法来估计低维表示。考虑 <span class="math inline">\(X\)</span> 的最佳秩-<span class="math inline">\(M\)</span> 近似： <span class="math display">\[\tilde{X}_M := \arg\min_{\text{rk}(A) \leq M} \|X - A\|_2 \;\;\in \mathbb{R}^{D \times N}\tag{10.50}\]</span> 其中，<span class="math inline">\(\|\cdot\|_2\)</span> 是在公式 (4.93) 中定义的谱范数。Eckart–Young 定理指出，<span class="math inline">\(\tilde{X}_M\)</span> 可以通过截断奇异值分解到前 <span class="math inline">\(M\)</span> 个奇异值来获得。换句话说： <span class="math display">\[\tilde{X}_M = U_M \; \Sigma_M \; V_M^\top \;\;\in \mathbb{R}^{D \times N}\tag{10.51}\]</span> 其中：</p><ul><li><span class="math inline">\(U_M := [u_1, \ldots, u_M] \in \mathbb{R}^{D \times M}\)</span>，</li><li><span class="math inline">\(V_M := [v_1, \ldots, v_M] \in \mathbb{R}^{N \times M}\)</span>，</li><li><span class="math inline">\(\Sigma_M \in \mathbb{R}^{M \times M}\)</span> 是对角矩阵，对角线上包含了 <span class="math inline">\(X\)</span> 的前 <span class="math inline">\(M\)</span> 个最大奇异值。</li></ul><h4 id="实际问题"><strong>10.4.2 实际问题</strong></h4><p>求解特征值和特征向量在许多需要矩阵分解的基础机器学习方法中也非常重要。理论上，如第 4.2 节所述，我们可以通过特征多项式的根来解特征值。然而，当矩阵大于 <span class="math inline">\(4 \times 4\)</span> 时，这种方法就不可行了，因为这需要求解 5 次或更高次多项式的根。<strong>而 Abel–Ruffini 定理（Ruffini, 1799；Abel, 1826）表明，对于 5 次或更高次的多项式，不存在代数解。</strong>因此，在实际应用中，我们通过<strong>迭代方法</strong>来求解特征值或奇异值，这些方法已在所有现代线性代数库中实现。</p><p>在许多应用中（例如本章介绍的 PCA），我们只需要前几个特征向量。如果先计算完整分解，然后丢弃除前几个之外的所有特征向量，就会造成计算资源的浪费。事实证明，如果我们只对前几个特征向量（对应最大特征值）感兴趣，那么直接优化这些特征向量的迭代方法比完整的特征分解（或 SVD）更加高效。</p><p>在极端情况下，如果我们只需要第一个特征向量，那么一种简单的方法——<strong>幂迭代（power iteration）</strong> 就非常高效。幂迭代方法选择一个不在 <span class="math inline">\(S\)</span> 零空间中的随机向量 <span class="math inline">\(x_0\)</span>，并进行以下迭代： <span class="math display">\[x_{k+1} = \frac{S x_k}{\| S x_k \|}, \quad k = 0,1,\ldots\tag{10.52}\]</span> 这意味着在每次迭代中，向量 <span class="math inline">\(x_k\)</span> 都会被矩阵 <span class="math inline">\(S\)</span> 乘一次，然后再进行归一化，即始终保持 <span class="math inline">\(\|x_k\| = 1\)</span>。这列向量最终会收敛到 <span class="math inline">\(S\)</span> 的最大特征值对应的特征向量。</p><p>最初的 <strong>Google PageRank 算法</strong>（Page 等人, 1999）就使用了这种算法来根据网页的超链接关系对网页进行排序。</p><h3 id="高维中的-pca"><strong>10.5 高维中的 PCA</strong></h3><p>为了执行 PCA，我们需要计算数据协方差矩阵。在 <span class="math inline">\(D\)</span> 维空间中，数据协方差矩阵是一个 <span class="math inline">\(D \times D\)</span> 的矩阵。计算该矩阵的特征值和特征向量在计算上代价很高，因为其计算复杂度随 <span class="math inline">\(D\)</span> 呈立方增长。因此，正如我们之前讨论的那样，在非常高维的情况下，PCA 是不可行的。</p><p>例如，如果我们的数据点 <span class="math inline">\(x_n\)</span> 是具有 10,000 个像素的图像（例如 <span class="math inline">\(100 \times 100\)</span> 的图像），那么我们就需要对一个 <span class="math inline">\(10,000 \times 10,000\)</span> 的协方差矩阵进行特征分解。接下来，我们将针对 <strong>数据点数量远小于维度数量的情况（即 <span class="math inline">\(N \ll D\)</span>）</strong> 提供一种解决方案。</p><p>假设我们有一个中心化后的数据集 <span class="math inline">\(x_1, \ldots, x_N\)</span>，其中 <span class="math inline">\(x_n \in \mathbb{R}^D\)</span>。那么数据协方差矩阵为： <span class="math display">\[S = \frac{1}{N} XX^\top \in \mathbb{R}^{D \times D},\tag{10.53}\]</span> 其中 <span class="math inline">\(X = [x_1, \ldots, x_N]\)</span> 是一个 <span class="math inline">\(D \times N\)</span> 的矩阵，每一列都是一个数据点。</p><p>现在我们假设 <span class="math inline">\(N \ll D\)</span>，即数据点的数量小于数据的维度。如果没有重复的数据点，那么协方差矩阵 <span class="math inline">\(S\)</span> 的秩为 <span class="math inline">\(N\)</span>，因此它有 <span class="math inline">\(D-N+1\)</span> 个特征值为 0。直观上，这意味着数据中存在冗余。接下来，我们将利用这一点，把原来的 <span class="math inline">\(D \times D\)</span> 协方差矩阵转化为一个 <strong><span class="math inline">\(N \times N\)</span></strong> 的协方差矩阵，其特征值全为正数。</p><p>在 PCA 中，我们最终得到如下的特征向量方程： <span class="math display">\[S b_m = \lambda_m b_m, \quad m = 1, \ldots, M,\tag{10.54}\]</span> 其中 <span class="math inline">\(b_m\)</span> 是主子空间的一个基向量。利用 (10.53) 中定义的 <span class="math inline">\(S\)</span>，我们得到： <span class="math display">\[S b_m = \frac{1}{N} XX^\top b_m = \lambda_m b_m.\tag{10.55}\]</span> 我们现在从左边乘上 <span class="math inline">\(X^\top \in \mathbb{R}^{N \times D}\)</span>，得到： <span class="math display">\[\frac{1}{N} \underbrace{\boldsymbol{X}^\top \boldsymbol{X}}_{N \times N} \underbrace{\boldsymbol{X}^\top \boldsymbol{b}_m}_{=:\boldsymbol{c}_m} = \lambda_m \boldsymbol{X}^\top \boldsymbol{b}_m \iff \frac{1}{N} \boldsymbol{X}^\top \boldsymbol{X} \boldsymbol{c}_m = \lambda_m \boldsymbol{c}_m, \tag{10.56}\]</span> 令 <span class="math display">\[c_m := X^\top b_m,\]</span> 那么我们得到一个新的特征值/特征向量方程： <span class="math display">\[X^\top X c_m = \lambda_m c_m.\]</span> 这里 <span class="math inline">\(\lambda_m\)</span> 仍然是特征值，这验证了我们在<strong>第 4.5.3 节中的结论：<span class="math inline">\(XX^\top \;\text{的非零特征值等于}\; X^\top X \;\text{的非零特征值。}\)</span></strong> 因此，对于矩阵 <span class="math inline">\(\frac{1}{N} X^\top X \in \mathbb{R}^{N \times N}\)</span>，我们得到了与 <span class="math inline">\(\lambda_m\)</span> 相关的特征向量： <span class="math display">\[c_m := X^\top b_m.\]</span> 假设没有重复的数据点，该矩阵的秩为 <span class="math inline">\(N\)</span>，因此它是可逆的。这也意味着 <span class="math inline">\(\frac{1}{N} X^\top X\)</span> 拥有与数据协方差矩阵 <span class="math inline">\(S\)</span> 相同的非零特征值。但它是一个 <span class="math inline">\(N \times N\)</span> 的矩阵，因此比原始的 <span class="math inline">\(D \times D\)</span> 协方差矩阵计算特征值和特征向量要高效得多。</p><p>现在我们已经得到了 <span class="math inline">\(\frac{1}{N} X^\top X\)</span> 的特征向量，接下来我们需要恢复 <strong>原始协方差矩阵 <span class="math inline">\(S\)</span> 的特征向量</strong>，因为 PCA 仍然需要它们。此时我们知道 <span class="math inline">\(\frac{1}{N} X^\top X\)</span> 的特征向量。如果我们对特征值方程左乘 <span class="math inline">\(X\)</span>，就得到：</p><p><span class="math display">\[\frac{1}{N} \underbrace{\boldsymbol{X} \boldsymbol{X}^\top}_{S} \boldsymbol{X} \boldsymbol{c}_m = \lambda_m \boldsymbol{X} \boldsymbol{c}_m \tag{10.57}\]</span></p><p>这正是数据协方差矩阵 <span class="math inline">\(S\)</span>，同时我们也得出 <strong><span class="math inline">\(X c_m\)</span> 是 <span class="math inline">\(S\)</span> 的一个特征向量</strong>。</p><p><strong>备注</strong>：如果我们要应用第 10.6 节中介绍的 PCA 算法，需要将 <span class="math inline">\(S\)</span> 的特征向量 <span class="math inline">\(X c_m\)</span> 归一化，使其范数为 1。</p><h3 id="pca-在实践中的关键步骤">10.6 PCA 在实践中的关键步骤</h3><p>下面我们将通过一个运行示例逐步讲解 PCA 的各个步骤，该示例在图 10.11 中进行了总结。我们有一个二维数据集（图 10.11(a)），希望使用 PCA 将其投影到一个一维子空间上。</p><p><img src="/img3/机器学习的数学基础Part2/F10.11.png" alt="F10.11" style="zoom:67%;" /></p><p><strong>1. 均值去除</strong> 首先通过计算数据集的均值 <span class="math inline">\(\mu\)</span>，并从每个数据点中减去它，从而使数据居中。这保证了数据集的均值为 0（图 10.11(b)）。均值去除并不是严格必须的，但它能减少数值计算问题的风险。</p><p><strong>2. 标准化</strong> 对每个维度 <span class="math inline">\(d = 1, \ldots, D\)</span>，用该维度的标准差 <span class="math inline">\(\sigma_d\)</span> 去除数据点的单位。这样，数据变得无量纲，并且在每个坐标轴方向上的方差均为 1，如图 10.11(c) 中的两个箭头所示。至此，数据的标准化过程完成。</p><p><strong>3. 协方差矩阵的特征分解</strong> 计算数据的协方差矩阵，以及它的特征值和对应的特征向量。由于协方差矩阵是对称的，谱定理（定理 4.15）保证我们可以找到一组正交规范基（ONB）的特征向量。在图 10.11(d) 中，<strong>特征向量根据其对应特征值的大小进行了缩放</strong>。较长的向量张成了主子空间，我们记作 <span class="math inline">\(U\)</span>。<strong>数据协方差矩阵被表示为椭圆。</strong></p><p><strong>4. 投影</strong> 我们可以将任意数据点 <span class="math inline">\(x_* \in \mathbb{R}^D\)</span> 投影到主子空间上。为了正确完成这一过程，需要用训练数据在第 <span class="math inline">\(d\)</span> 个维度的均值 <span class="math inline">\(\mu_d\)</span> 和标准差 <span class="math inline">\(\sigma_d\)</span> 对 <span class="math inline">\(x_*\)</span> 进行标准化： <span class="math display">\[x_*^{(d)} \; \leftarrow \; \frac{x_*^{(d)} - \mu_d}{\sigma_d}, \quad d = 1, \ldots, D\tag{10.58}\]</span> 其中 <span class="math inline">\(x_*^{(d)}\)</span> 是 <span class="math inline">\(x_*\)</span> 的第 <span class="math inline">\(d\)</span> 个分量。投影得到： <span class="math display">\[\tilde{x}_* = BB^\top x_* \tag{10.59}\]</span> <strong>其在主子空间基底下的坐标为：</strong> <span class="math display">\[z_* = B^\top x_* \tag{10.60}\]</span> 这里，<span class="math inline">\(B\)</span> 是一个矩阵，列向量为与数据协方差矩阵最大特征值对应的特征向量。PCA 返回的是坐标式 (10.60)，而不是投影点 <span class="math inline">\(x_*\)</span>。</p><p>由于我们对数据集进行了标准化，公式 (10.59) 只在标准化数据集的上下文中产生投影。若要得到原始数据空间（即标准化之前）的投影，就需要“反标准化”，即先乘以标准差，再加回均值： <span class="math display">\[\tilde{x}_*^{(d)} \; \leftarrow \; \tilde{x}_*^{(d)} \sigma_d + \mu_d, \quad d = 1, \ldots, D\tag{10.61}\]</span> 图 10.11(f) 展示了原始数据空间中的投影效果。</p><p><strong>例 10.4（MNIST 数字：重建）</strong> 接下来，我们将 PCA 应用于 MNIST 手写数字数据集。该数据集包含 60,000 个手写数字（0 到 9）的样本。每个数字是一个大小为 <span class="math inline">\(28 \times 28\)</span> 的图像，即它包含 784 个像素，因此我们可以把该数据集中的每幅图像看作是 <span class="math inline">\(\mathbf{x} \in \mathbb{R}^{784}\)</span> 的一个向量。部分数字的示例如图 10.3 所示。</p><p>为了说明问题，我们将 PCA 应用于 MNIST 数据集中的一个子集，并且专注于数字 “8”。我们使用了 5,389 张训练集中的 “8” 图像，并根据本章介绍的方法确定了主子空间。然后我们使用所学习到的投影矩阵去重建一组测试图像，结果如图 10.12 所示。图 10.12 的第一行显示了来自测试集的四个原始数字。后续的各行展示了这四个数字在使用维度分别为 1、10、100 和 500 的主子空间时的重建结果。我们可以看到，即使只使用一维的主子空间，也能得到一个勉强算得上不错的原始数字重建，但图像会模糊而且比较通用。随着主成分（PCs）数量的增加，重建结果逐渐变得更清晰，细节也逐渐被捕捉到。当使用 500 个主成分时，我们几乎得到了完美的重建。如果我们选择 784 个主成分，就可以毫无压缩损失地恢复出原始数字。</p><p><img src="/img3/机器学习的数学基础Part2/F10.12.png" alt="F10.12" style="zoom:50%;" /></p><p><img src="/img3/机器学习的数学基础Part2/F10.13.png" alt="F10.13" style="zoom:50%;" /></p><p>图 10.13 展示了平均平方重建误差： <span class="math display">\[\frac{1}{N}\sum_{n=1}^N \|x_n - \tilde{x}_n\|^2 = \sum_{i=M+1}^D \lambda_i, \tag{10.62}\]</span> 它是主成分个数 <span class="math inline">\(M\)</span> 的函数。我们可以看到，主成分的重要性下降得非常快，增加更多的主成分只能带来微小的收益。这与图 10.5 中的观察完全一致：我们发现投影数据的大部分方差仅由少数几个主成分捕获。当使用大约 550 个主成分时，我们基本上就能完整重建训练集中的 “8” 图像（在图像边缘的一些像素上没有变化，因为它们始终是黑色的）。</p><h3 id="潜变量视角">10.7 潜变量视角</h3><p>Latent Variable Perspective</p><p>在前面的章节中，我们通过最大方差和投影的视角推导了 PCA，而没有涉及任何概率模型的概念。这样做的一个优点是：它让我们避开了概率论中复杂的数学细节；但另一方面，概率模型则能为我们提供更多的灵活性和有用的见解。更具体地说，概率模型可以：</p><ul><li>提供一个似然函数，使我们能够显式地处理带噪声的观测数据（之前我们甚至没有讨论过噪声问题）；</li><li>允许我们通过边际似然来进行贝叶斯模型比较（参见第 8.6 节）；</li><li>将 PCA 看作是一个生成模型，从而能够模拟新的数据；</li><li>让我们能够直接建立与相关算法的联系；</li><li>通过应用贝叶斯定理来处理随机缺失的数据维度；</li><li>给予我们一个衡量新数据点“新颖性”的概念；</li><li>提供一种有原则的方式来扩展模型，例如推广为 PCA 混合模型；</li><li>使之前推导得到的 PCA 结果成为一个特殊情形；</li><li>允许通过边缘化模型参数来实现完整的贝叶斯推断。</li></ul><p>通过引入一个连续值潜变量 <span class="math inline">\(z \in \mathbb{R}^M\)</span>，我们可以将 PCA 表述为一个<strong>概率潜变量模型</strong>(probabilistic latent-variable model)。Tipping 和 Bishop (1999) 提出了这种潜变量模型，称为<strong>概率 PCA (Probabilistic PCA, PPCA)</strong>。PPCA 解决了上面大多数问题，<strong>而我们之前通过最大化投影空间方差或最小化重构误差得到的 PCA 解，实际上就是在无噪声设定下最大似然估计的一个特殊情况。</strong></p><blockquote><p>个人注：见10.8节：“在我们对 PPCA 的讨论中，我们假设模型的参数，即 <span class="math inline">\(B, \mu\)</span> 和似然参数 <span class="math inline">\(\sigma^2\)</span>，是已知的。”</p></blockquote><h4 id="生成过程与概率模型">10.7.1 生成过程与概率模型</h4><p>在 PPCA 中，我们显式写出线性降维的概率模型。为此，我们假设存在一个连续潜变量 <span class="math inline">\(z \in \mathbb{R}^M\)</span>，其先验分布为标准正态： <span class="math display">\[p(z) = \mathcal{N}(0, I),\]</span> 并假设潜变量与观测数据 <span class="math inline">\(x\)</span> 存在线性关系： <span class="math display">\[x = Bz + \mu + \epsilon \in \mathbb{R}^D, \tag{10.63}\]</span> 其中 <span class="math inline">\(\epsilon \sim \mathcal{N}(0, \sigma^2 I)\)</span> 表示高斯观测噪声，<span class="math inline">\(B \in \mathbb{R}^{D \times M}\)</span> 和 <span class="math inline">\(\mu \in \mathbb{R}^D\)</span> 描述了潜变量到观测变量的线性/仿射映射。于是，PPCA 将潜变量和观测变量联系起来： <span class="math display">\[p(x|z, B, \mu, \sigma^2) = \mathcal{N}(x|Bz + \mu, \sigma^2 I). \tag{10.64}\]</span> 整体上，PPCA 诱导的生成过程如下(PPCA induces the following generative process:)： <span class="math display">\[z_n \sim \mathcal{N}(z|0, I), \tag{10.65}\]</span></p><p><span class="math display">\[x_n | z_n \sim \mathcal{N}(x|Bz_n + \mu, \sigma^2 I). \tag{10.66}\]</span> 要生成一个符合模型参数的数据点，可以按照<strong>祖先采样</strong>（ancestral sampling）的方式：首先从 <span class="math inline">\(p(z)\)</span> 中采样一个潜变量 <span class="math inline">\(z_n\)</span>，然后将 <span class="math inline">\(z_n\)</span> 代入式 (10.64)，再采样得到对应的 <span class="math inline">\(x_n \sim p(x|z_n, B, \mu, \sigma^2)\)</span>。这个生成过程使我们能够写出完整的概率模型（即所有随机变量的联合分布，参见第 8.4 节）：</p><p><span class="math display">\[p(x, z | B, \mu, \sigma^2) = p(x|z, B, \mu, \sigma^2) p(z), \tag{10.67}\]</span> 并且根据第 8.5 节的结果，可以立刻得到如图 10.14 所示的图模型。</p><p><img src="/img3/机器学习的数学基础Part2/F10.14.png" alt="F10.14" style="zoom:67%;" /></p><p><strong>备注</strong>：请注意连接潜在变量 <span class="math inline">\(z\)</span> 和观测数据 <span class="math inline">\(x\)</span> 的箭头方向：箭头是从 <span class="math inline">\(z\)</span> 指向 <span class="math inline">\(x\)</span> 的，这意味着 PPCA 模型假设高维观测 <span class="math inline">\(x\)</span> 来源于一个低维的潜在原因 <span class="math inline">\(z\)</span>。最终，我们显然对在给定一些观测值的情况下推断 <span class="math inline">\(z\)</span> 感兴趣。为了实现这一点，我们将<strong>应用贝叶斯推断来“反转”箭头</strong>（以一种隐含的方式），从观测推到潜在变量。</p><p><strong>例 10.5（使用潜在变量生成新数据）</strong></p><p>图 10.15 展示了在使用二维主子空间时，PCA 找到的 MNIST 数字 “8” 的潜在坐标（蓝点）。我们可以在该潜在空间中查询任意一个向量 <span class="math inline">\(z_*\)</span>，并生成一幅类似数字 “8” 的图像 <span class="math inline">\(\tilde{x}_* = Bz_*\)</span>。我们展示了八个这样的生成图像及其在潜在空间中的对应表示。根据我们在潜在空间查询的位置不同，生成的图像也有所不同（形状、旋转、大小等）。如果我们在远离训练数据的地方进行查询，就会出现越来越多的伪影，例如左上角和右上角的数字。需要注意的是，这些生成图像的内在维度仅仅是 <strong>2</strong>。</p><p><img src="/img3/机器学习的数学基础Part2/F10.15.png" alt="F10.15" style="zoom:50%;" /></p><h4 id="似然函数与联合分布">10.7.2 似然函数与联合分布</h4><p>利用第 6 章中的结果，我们通过对潜在变量 <span class="math inline">\(z\)</span> 积分（见 8.4.3 节）来得到该概率模型的似然函数： $$ <span class="math display">\[\begin{align}p(x \mid B, \mu, \sigma^2) &amp; = \int p(x \mid z, B, \mu, \sigma^2)\, p(z)\, dz \tag{10.68a} \\&amp; = \int \mathcal{N}\!\big(x \mid Bz+\mu, \sigma^2 I\big)\, \mathcal{N}(z \mid 0, I)\, dz. \tag{10.68b}\end{align}\]</span> <span class="math display">\[根据 6.5 节，我们知道该积分的解是一个高斯分布，其均值为\]</span> _x[x] = <em>z[Bz+] + </em>=  <span class="math display">\[其协方差矩阵为\]</span> <span class="math display">\[\begin{align}\mathbb{V}[x] &amp; = \mathbb{V}_z[Bz+\mu] + \mathbb{V}_\epsilon[\epsilon] = \mathbb{V}_z[Bz] + \sigma^2 I  \tag{10.70ab}\\&amp; = B\, \mathbb{V}_z[z]\, B^\top + \sigma^2 I= BB^\top + \sigma^2 I. \tag{10.70b}\end{align}\]</span> $$ (10.68b) 中的似然函数可用于模型参数的极大似然（MLE）或最大后验（MAP）估计。</p><p><strong>备注。</strong> 我们不能使用 (10.64) 中的条件分布来进行极大似然估计，因为它仍然依赖于潜在变量。<strong>极大似然（或 MAP）估计所需的似然函数应该只依赖于观测数据 <span class="math inline">\(x\)</span> 和模型参数，而不能依赖于潜在变量。</strong></p><p>根据 6.5 节，我们知道高斯随机变量 <span class="math inline">\(z\)</span> 及其线性/仿射变换 <span class="math inline">\(x = Bz\)</span> 是联合高斯分布的。我们已经知道其边缘分布为 <span class="math display">\[p(z) = \mathcal{N}(z \mid 0, I), \quad p(x) = \mathcal{N}(x \mid \mu, BB^\top + \sigma^2 I).\]</span> 缺失的交叉协方差为 <span class="math display">\[\mathrm{Cov}[x, z] = \mathrm{Cov}_z[Bz+\mu, z] = B\, \mathrm{Cov}_z[z,z] = B. \tag{10.71}\]</span> 因此，<strong>PPCA 的概率模型（即潜在变量与观测变量的联合分布）</strong>可明确写为 <span class="math display">\[p(x, z \mid B, \mu, \sigma^2) = \mathcal{N} \!\left( \begin{bmatrix} x \\ z \end{bmatrix} \,\middle|\, \begin{bmatrix} \mu \\ 0 \end{bmatrix}, \begin{bmatrix} BB^\top + \sigma^2 I &amp; B \\ B^\top &amp; I \end{bmatrix}\right), \tag{10.72}\]</span> 其中均值向量的维度为 <span class="math inline">\(D+M\)</span>，协方差矩阵的大小为 <span class="math inline">\((D+M) \times (D+M)\)</span>。</p><h4 id="后验分布">10.7.3 后验分布</h4><p>在 (10.72) 中的联合高斯分布 <span class="math inline">\(p(x,z \mid B, \mu, \sigma^2)\)</span>，使我们能够直接利用第 6.5.1 节的高斯条件分布规则来确定后验分布 <span class="math inline">\(p(z \mid x)\)</span>。给定观测值 <span class="math inline">\(x\)</span>，潜在变量的后验分布为 <span class="math display">\[p(z \mid x) = \mathcal{N}(z \mid m, C), \tag{10.73}\]</span> 其中 <span class="math display">\[m = B^\top (BB^\top + \sigma^2 I)^{-1}(x - \mu), \tag{10.74}\]</span></p><p><span class="math display">\[C = I - B^\top (BB^\top + \sigma^2 I)^{-1}B. \tag{10.75}\]</span> 需要注意的是，后验协方差并不依赖于观测数据 <span class="math inline">\(x\)</span>。对于数据空间中的一个新观测 <span class="math inline">\(x_\ast\)</span>，我们使用 (10.73) 来确定对应潜在变量 <span class="math inline">\(z_\ast\)</span> 的后验分布。协方差矩阵 <span class="math inline">\(C\)</span> 允许我们评估嵌入的置信度。若 <span class="math inline">\(C\)</span> 的行列式很小（行列式度量体积），说明潜在嵌入 <span class="math inline">\(z_\ast\)</span> 相对确定。若我们得到的后验分布 <span class="math inline">\(p(z_\ast \mid x_\ast)\)</span> 方差很大，则可能遇到一个异常点。</p><blockquote><p>个人注：虽然 <span class="math inline">\(C\)</span> 公式上是常量，但当数据 <span class="math inline">\(x_*\)</span> <strong>远离模型学到的子空间或均值 <span class="math inline">\(\mu\)</span></strong> 时，后验分布的<strong>有效方差</strong>（比如沿特定方向的不确定性）会显得更大。</p></blockquote><p>不过，我们可以进一步研究该后验分布，以理解在此后验下哪些其他数据点 <span class="math inline">\(x\)</span> 是合理的。为此，我们利用 PPCA 的生成过程，该过程允许通过生成新的、在该后验下合理的数据来探索潜在变量的后验分布：</p><ol type="1"><li>从潜在变量的后验分布 (10.73) 中采样一个潜在变量 <span class="math display">\[z_\ast \sim p(z \mid x_\ast)。\]</span></li><li>从 (10.64) 中的条件分布采样一个重构向量 <span class="math display">\[\tilde{x}_\ast \sim p(x \mid z_\ast, B, \mu, \sigma^2)。\]</span> 如果我们重复上述过程多次，就可以探索潜在变量 <span class="math inline">\(z_\ast\)</span> 的后验分布 (10.73) 及其对观测数据的含义。这个采样过程实际上就是在假设一些在该后验分布下合理的数据。</li></ol><h3 id="延伸阅读">10.8 延伸阅读</h3><p>我们从两个角度推导了 PCA：（a）在投影空间中最大化方差；（b）最小化平均重构误差。然而，PCA 还可以从不同的视角进行解释。让我们回顾一下已完成的工作：我们取一个高维数据 <span class="math inline">\(x \in \mathbb{R}^D\)</span>，并使用一个矩阵 <span class="math inline">\(B^\top\)</span> 来找到其低维表示 <span class="math inline">\(z \in \mathbb{R}^M\)</span>。矩阵 <span class="math inline">\(B\)</span> 的列向量是数据协方差矩阵 <span class="math inline">\(S\)</span> 的特征向量，这些特征向量对应于最大的特征值。得到低维表示 <span class="math inline">\(z\)</span> 之后，我们可以在原始数据空间中重构一个近似的高维版本： <span class="math display">\[x \approx \tilde{x} = Bz = BB^\top x \in \mathbb{R}^D,\]</span> 其中 <span class="math inline">\(BB^\top\)</span> 是一个投影矩阵。<strong>我们还可以把 PCA 看作是一种线性自编码器（如图 10.16 所示）。自编码器将数据 <span class="math inline">\(x_n \in \mathbb{R}^D\)</span> 编码为一个代码 <span class="math inline">\(z_n \in \mathbb{R}^M\)</span>，再将其解码为与 <span class="math inline">\(x_n\)</span> 相似的 <span class="math inline">\(\tilde{x}_n\)</span>。</strong>从数据到代码的映射称为<strong>编码器</strong>，从代码回到原始数据空间的映射称为<strong>解码器</strong>。如果我们考虑线性映射，即代码由<span class="math inline">\(z_n = B^\top x_n \in \mathbb{R}^M\)</span>给出，并且我们希望最小化数据 <span class="math inline">\(x_n\)</span> 与其重构 <span class="math inline">\(\tilde{x}_n = Bz_n\)</span> 的平均平方误差（<span class="math inline">\(n=1,\dots,N\)</span>），则得到： <span class="math display">\[\frac{1}{N} \sum_{n=1}^N \|x_n - \tilde{x}_n\|^2 = \frac{1}{N} \sum_{n=1}^N \|x_n - BB^\top x_n\|^2. \tag{10.76}\]</span> 这意味着我们得到的目标函数与 10.3 节中 (10.29) 相同，因此通过最小化平方自编码损失，我们得到的就是 PCA 解。<strong>如果我们将 PCA 的线性映射替换为非线性映射，就得到非线性自编码器。一个典型的例子是深度自编码器，其中线性函数被深度神经网络替代。在这种情况下，编码器也被称为识别网络或推理网络，而解码器也被称为生成器。</strong></p><p><img src="/img3/机器学习的数学基础Part2/F10.16.png" alt="F10.16" style="zoom:50%;" /></p><p>PCA 的另一种解释与信息论相关。我们可以将代码看作是原始数据点的一个较小或压缩版本。当我们利用代码重构原始数据时，并不能完全恢复原始数据，而是得到一个稍有失真或带噪声的版本。这意味着我们的压缩是<strong>有损的</strong>。直观上，我们希望最大化原始数据与低维代码之间的相关性。更正式地，这与互信息相关。通过最大化互信息（信息论的核心概念，MacKay, 2003），我们将得到与 10.3 节中讨论的 PCA 相同的解。</p><p><strong>在我们对 PPCA 的讨论中，我们假设模型的参数，即 <span class="math inline">\(B, \mu\)</span> 和似然参数 <span class="math inline">\(\sigma^2\)</span>，是已知的。</strong>Tipping 和 Bishop (1999) 描述了如何在 PPCA 框架下推导这些参数的极大似然估计（注意本章的符号体系与他们的不同）。当将 <span class="math inline">\(D\)</span> 维数据投影到 <span class="math inline">\(M\)</span> 维子空间时，极大似然参数为： <span class="math display">\[\mu_{\text{ML}} = \frac{1}{N} \sum_{n=1}^N x_n, \tag{10.77}\]</span></p><p><span class="math display">\[B_{\text{ML}} = T(\Lambda - \sigma^2 I)^{\tfrac{1}{2}} R, \tag{10.78}\]</span></p><p><span class="math display">\[\sigma^2_{\text{ML}} = \frac{1}{D-M} \sum_{j=M+1}^D \lambda_j, \tag{10.79}\]</span> 其中，<span class="math inline">\(T \in \mathbb{R}^{D \times M}\)</span> 包含数据协方差矩阵的 <span class="math inline">\(M\)</span> 个特征向量，<span class="math inline">\(\Lambda = \text{diag}(\lambda_1,\dots,\lambda_M) \in \mathbb{R}^{M \times M}\)</span> 是一个对角矩阵，其对角元为主轴对应的特征值，<span class="math inline">\(R \in \mathbb{R}^{M \times M}\)</span> 是一个任意正交矩阵。极大似然解 <span class="math inline">\(B_{\text{ML}}\)</span> 在任意正交变换下是唯一的，例如我们可以将 <span class="math inline">\(B_{\text{ML}}\)</span> 右乘任意旋转矩阵 <span class="math inline">\(R\)</span>，因此 (10.78) 本质上是一个奇异值分解（见 4.5 节）。该证明的概要由 Tipping 和 Bishop (1999) 给出。在 (10.77) 中给出的 <span class="math inline">\(\mu\)</span> 的极大似然估计就是数据的样本均值。根据 (10.79)，观测噪声方差 <span class="math inline">\(\sigma^2\)</span> 的极大似然估计是主子空间正交补中的平均方差，即无法通过前 <span class="math inline">\(M\)</span> 个主成分捕获的剩余方差被当作观测噪声来处理。在无噪声极限 <span class="math inline">\(\sigma \to 0\)</span> 时，PPCA 和 PCA 给出的解是相同的：由于数据协方差矩阵 <span class="math inline">\(S\)</span> 是对称的，它可以被对角化（见 4.4 节），即存在一个由 <span class="math inline">\(S\)</span> 的特征向量组成的矩阵 <span class="math inline">\(T\)</span>，使得</p><p><span class="math display">\[S = T \Lambda T^{-1}. \tag{10.80}\]</span> 在 PPCA 模型中，数据协方差矩阵就是高斯似然 <span class="math inline">\(p(x \mid B, \mu, \sigma^2)\)</span> 的协方差矩阵，即 <span class="math inline">\(BB^\top + \sigma^2 I\)</span>（见 (10.70b)）。当 <span class="math inline">\(\sigma \to 0\)</span> 时，我们得到 <span class="math inline">\(BB^\top\)</span>，因此这个数据协方差必须等于 PCA 的数据协方差（及其在 (10.80) 中给出的分解），即 <span class="math display">\[\mathrm{Cov}[X] = T \Lambda T^{-1} = BB^\top \;\;\Longleftrightarrow\;\; B = T \Lambda^{\tfrac{1}{2}} R, \tag{10.81}\]</span> 这就是说，在 <span class="math inline">\(\sigma = 0\)</span> 的情况下，我们得到了 (10.78) 中的极大似然估计。由 (10.78) 和 (10.80) 可以清楚地看出，(P)PCA 实际上执行的是数据协方差矩阵的分解。在流式场景中（数据按顺序到达），推荐使用迭代的期望最大化（EM）算法来进行极大似然估计（Roweis, 1998）。为了确定潜在变量的维度（即代码的长度，或投影到的低维子空间的维度），Gavish 和 Donoho (2014) 提出了一种启发式方法：如果我们能够估计数据的噪声方差 <span class="math inline">\(\sigma^2\)</span>，就应该舍弃所有小于</p><p><span class="math display">\[\frac{4\sigma \sqrt{D}}{\sqrt{3}}\]</span> 的奇异值。另一种方法是使用（嵌套的）交叉验证（见 8.6.1 节）或贝叶斯模型选择准则（见 8.6.2 节）来确定数据的内在维度的合理估计（Minka, 2001b）。</p><p>与第 9 章中关于线性回归的讨论类似，我们可以在模型参数上放置一个先验分布，并将其积分消去。这样做有两个好处：（a）避免参数的点估计以及相关问题（见 8.6 节）；（b）能够自动选择潜在空间的适当维度 <span class="math inline">\(M\)</span>。在 Bishop (1999) 提出的<strong>贝叶斯 PCA</strong> 中，我们在模型参数上放置一个先验 <span class="math inline">\(p(\mu, B, \sigma^2)\)</span>。该生成过程允许我们对模型参数进行积分，而不是对其条件化，从而解决了过拟合问题。由于这种积分在解析上是不可解的，Bishop (1999) 建议使用近似推断方法，例如 MCMC 或变分推断。关于这些近似推断技术的更多细节，可以参考 Gilks 等人 (1996) 和 Blei 等人 (2017) 的工作。</p><p>在 PPCA 中，我们考虑了线性模型 <span class="math display">\[p(x_n|z_n) = \mathcal{N}\big(x_n \mid Bz_n + \mu, \ \sigma^2 I \big)\]</span> 其中先验分布为 <span class="math display">\[p(z_n) = \mathcal{N}(0, I),\]</span> 即所有观测维度受到相同强度的噪声影响。如果我们允许每个观测维度 <span class="math inline">\(d\)</span> 具有不同的方差 <span class="math inline">\(\sigma_d^2\)</span>，就得到了因子分析（FA）（Spearman, 1904; Bartholomew 等, 2011）。这意味着 FA 比 PPCA 给似然函数提供了更多灵活性，但仍然要求数据由模型参数 <span class="math inline">\(B, \mu\)</span> 来解释。然而，FA 不再允许存在封闭形式（closed-form）的最大似然解，因此需要使用迭代算法（如期望最大化算法 EM）来估计模型参数。在 PPCA 中，所有的平稳点都是全局最优解，而在 FA 中则不再成立。与 PPCA 相比，FA 在缩放数据时不变，但在旋转数据时会得到不同的解。另一个与 PCA 密切相关的算法是独立成分分析（ICA）（Hyvärinen 等, 2001）。 同样从潜变量的角度出发： <span class="math display">\[p(x_n|z_n) = \mathcal{N}\big(x_n \mid Bz_n + \mu, \ \sigma^2 I \big),\]</span> 但这次我们将 <span class="math inline">\(z_n\)</span> 的先验分布改为非高斯分布。ICA 可用于盲源分离。想象你在一个嘈杂的火车站，周围很多人都在说话。你的耳朵就像麦克风，它们会线性混合火车站内的不同语音信号。盲源分离的目标就是识别出混合信号中的各个组成部分。如前面在 PPCA 的最大似然估计中讨论过的，原始 PCA 解对任何旋转都是不变的。因此，PCA 可以识别信号所在的最佳低维子空间，但无法识别信号本身（Murphy, 2012）。ICA 通过修改潜在源的先验分布 <span class="math inline">\(p(z)\)</span>，要求其为非高斯分布，从而解决了这个问题。关于 ICA 的更多细节可参考 Hyvärinen 等 (2001) 和 Murphy (2012) 的著作。PCA、因子分析和 ICA 是三种线性模型下的降维方法示例。Cunningham 和 Ghahramani (2015) 对线性降维方法提供了更广泛的综述。我们这里讨论的 (P)PCA 模型可以扩展出一些重要方向。在第 10.5 节中，我们解释了当输入维度 <span class="math inline">\(D\)</span> 远大于数据点个数 <span class="math inline">\(N\)</span> 时如何做 PCA。通过利用“PCA 可以通过计算（大量的）内积来实现”的洞见，这个思路可以进一步扩展到无限维特征的情况。核技巧（kernel trick）正是核 PCA 的基础，它允许我们在无限维特征之间隐式地计算内积（Schölkopf 等, 1998; Schölkopf 和 Smola, 2002）。还有一些非线性降维方法是从 PCA 推导出来的（Burges (2010) 提供了很好的综述）。我们在本节前面讨论的“PCA 的自编码器视角”可以将 PCA 表示为深度自编码器的特例。在深度自编码器中，编码器和解码器由多层前馈神经网络表示，而神经网络本身就是非线性映射。如果我们把这些神经网络的激活函数设为恒等函数，模型就等价于 PCA。</p><p>另一种非线性降维方法是 Lawrence (2005) 提出的高斯过程潜变量模型（GP-LVM）。GP-LVM 从我们用来推导 PPCA 的潜变量视角出发，将潜变量 <span class="math inline">\(z\)</span> 与观测 <span class="math inline">\(x\)</span> 之间的线性关系替换为高斯过程（GP）。在 GP-LVM 中，我们不是估计映射的参数（像在 PPCA 中那样），而是边缘化掉模型参数，并对潜变量 <span class="math inline">\(z\)</span> 进行点估计。类似于贝叶斯 PCA，Titsias 和 Lawrence (2010) 提出的贝叶斯 GP-LVM 在潜变量 <span class="math inline">\(z\)</span> 上维持一个分布，并使用近似推断方法将其积分掉。</p>]]></content>
      
      
      <categories>
          
          <category> 翻译 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 数学 </tag>
            
            <tag> 算法 </tag>
            
            <tag> 统计 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《机器学习的数学基础》第9章&quot;线性回归&quot;</title>
      <link href="/2025/09/09/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E7%AC%AC9%E7%AB%A0%22%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%22/"/>
      <url>/2025/09/09/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E7%AC%AC9%E7%AB%A0%22%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%22/</url>
      
        <content type="html"><![CDATA[<h2 id="第-9-章-线性回归">第 9 章 线性回归</h2><p>Linear Regression</p><p>在本章中，我们将应用第 2、5、6 和 7 章中的数学概念来解决<strong>线性回归（曲线拟合 curve fitting）</strong>问题。在回归中，我们的目标是找到一个函数 <span class="math inline">\(f\)</span>，它将输入 <span class="math inline">\(x \in \mathbb{R}^D\)</span> 映射到相应的函数值 <span class="math inline">\(f(x) \in \mathbb{R}\)</span>。</p><p>我们假设给定了一组训练输入 <span class="math inline">\(x_n\)</span> 及其对应的带噪观测值 <span class="math inline">\(y_n = f(x_n) + \varepsilon\)</span>，其中 <span class="math inline">\(\varepsilon\)</span> 是独立同分布（i.i.d.）的随机变量，用来描述测量/观测噪声以及可能未建模的过程（本章中我们不再考虑后者）。<strong>在整个章节中，我们假设噪声是零均值高斯噪声。</strong></p><p><img src="/img3/机器学习的数学基础Part2/F9.1.png" alt="F9.1" style="zoom:67%;" /></p><p>我们的任务是找到一个函数，该函数不仅能够拟合训练数据，还能够很好地推广到预测训练数据之外输入位置的函数值（见第 8 章）。图 9.1 展示了这样一个回归问题的例子。典型的回归场景如图 9.1(a)：对于一些输入值 <span class="math inline">\(x_n\)</span>，我们观测到带噪声的函数值 <span class="math inline">\(y_n = f(x_n) + \varepsilon\)</span>。任务是推断出生成这些数据的函数 <span class="math inline">\(f\)</span>，并且能很好地推广到新的输入位置。图 9.1(b) 给出了一个可能的解答，其中我们同时展示了以函数值 <span class="math inline">\(f(x)\)</span> 为中心的三个分布，用来表示数据中的噪声。</p><p>回归是机器学习中的一个基本问题，回归问题广泛出现在各种研究领域和应用中，包括：</p><ul><li>时间序列分析（如系统辨识）</li><li>控制与机器人（如强化学习、前向/逆向模型学习）</li><li>优化（如线搜索、全局优化）</li><li>深度学习应用（如电子游戏、语音转文本转换、图像识别、自动视频标注）</li></ul><span id="more"></span><p>此外，回归还是分类算法中的关键组成部分。寻找回归函数需要解决多种问题，包括：</p><ol type="1"><li><strong>模型选择与参数化</strong><ul><li>给定一个数据集，哪些函数类别（如多项式）是建模的良好候选？</li><li>具体的参数化（如多项式的次数）应该如何选择？</li><li>第 8.6 节讨论的模型选择方法，可以帮助我们比较不同模型，从中找到能够合理解释训练数据的最简单模型。</li></ul></li><li><strong>寻找合适的参数</strong><ul><li>在选定回归函数模型后，如何找到合适的模型参数？</li><li>我们需要研究不同的损失/目标函数（它们决定了“拟合得好”的含义），以及能够最小化该损失的优化算法。</li></ul></li><li><strong>过拟合与模型选择</strong><ul><li>当回归函数对训练数据“拟合得过好”但无法推广到未见过的测试数据时，就会出现过拟合。</li><li>过拟合通常出现在底层模型（或其参数化）过于灵活、表达能力过强的情况下（见第 8.6 节）。</li><li>我们将探讨过拟合的根本原因，并在线性回归的背景下讨论减轻过拟合的方法。</li></ul></li><li><strong>损失函数与参数先验的关系</strong><ul><li>损失函数（优化目标）通常由概率模型引导并激发。</li><li>我们将探讨损失函数与其背后引入这些损失的先验假设之间的联系。</li></ul></li><li><strong>不确定性建模</strong><ul><li>在实际问题中，我们只能获得有限（尽管可能较大）数量的训练数据，用于选择模型类别和相应参数。</li><li>由于这些有限的训练数据无法覆盖所有可能情况，我们可能需要刻画剩余的参数不确定性，以便在测试时得到模型预测的置信度度量。</li><li>训练集越小，不确定性建模就越重要。</li><li><strong>一致的不确定性建模能够为模型预测提供置信区间</strong>。</li></ul></li></ol><p>在接下来的内容中，我们将使用第 3、5、6 和 7 章中的数学工具来解决线性回归问题。我们将讨论极大似然估计（MLE）和最大后验估计（MAP）以找到最优模型参数。利用这些参数估计，我们将简要考察泛化误差和过拟合。在本章的最后，我们将讨论贝叶斯线性回归，它能在更高层次上推理模型参数，从而避免极大似然和最大后验估计中遇到的一些问题。</p><h3 id="问题表述">9.1 问题表述</h3><p>Problem Formulation</p><p><strong>由于存在观测噪声，我们将采用一种概率方法，并通过似然函数显式地对噪声进行建模。</strong>更具体地说，在本章中，我们考虑一个回归问题，其似然函数为: <span class="math display">\[p(y \mid x) = \mathcal{N}\big(y \mid f(x), \sigma^2 \big). \tag{9.1}\]</span> 这里，<span class="math inline">\(x \in \mathbb{R}^D\)</span> 是输入，<span class="math inline">\(y \in \mathbb{R}\)</span> 是带有噪声的函数值（目标）。根据式 (9.1)，<span class="math inline">\(x\)</span> 与 <span class="math inline">\(y\)</span> 之间的函数关系为</p><p><span class="math display">\[y = f(x) + \varepsilon , \tag{9.2}\]</span> 其中 <span class="math inline">\(\varepsilon \sim \mathcal{N}(0, \sigma^2)\)</span> 是独立同分布 (i.i.d.) 的高斯测量噪声，均值为 0，方差为 <span class="math inline">\(\sigma^2\)</span>。</p><blockquote><p>个人注：</p><p>严格来说，<strong>似然函数（likelihood function）</strong> 是指在给定参数 <span class="math inline">\(\theta\)</span> 的情况下，观测数据出现的概率。因此形式应该写成 <span class="math display">\[p(y \mid x, \theta) = \mathcal{N}\big(y \mid f(x;\theta), \sigma^2 \big),\tag{9.1}\]</span> 其中 <span class="math inline">\(\theta\)</span> 表示模型参数（比如线性回归中的权重向量和偏置，或者神经网络中的权重）。在很多教材（尤其是深度学习、贝叶斯建模的入门部分），作者会 <strong>省略参数 <span class="math inline">\(\theta\)</span></strong>，默认 <span class="math inline">\(f(x)\)</span> 依赖于 <span class="math inline">\(\theta\)</span>。这是出于简化符号的考虑，因为 <span class="math inline">\(\theta\)</span> 总是“隐含存在”的。</p></blockquote><p>我们的目标是找到一个函数，它既接近（类似于）生成数据的未知函数 <span class="math inline">\(f\)</span>，又具有良好的泛化能力。</p><p>在本章中，我们专注于参数化模型。也就是说，我们选择一个参数化的函数，并寻找合适的参数 <span class="math inline">\(\theta\)</span>，使其能够很好地建模数据。暂时我们假设噪声方差 <span class="math inline">\(\sigma^2\)</span> 已知，并专注于学习模型参数 <span class="math inline">\(\theta\)</span>。在线性回归中，我们<strong>考虑参数 <span class="math inline">\(\theta\)</span> 以线性形式出现在模型中的特殊情况</strong>。线性回归的一个例子为：</p><p><span class="math display">\[p(y \mid x, \theta) = \mathcal{N}\big(y \mid x^{\top}\theta, \sigma^2 \big), \tag{9.3}\]</span></p><p>$$ <span class="math display">\[\begin{equation}\;\;\Longleftrightarrow\;\;y = x^{\top}\theta + \varepsilon, \quad \varepsilon \sim \mathcal{N}(0, \sigma^{2}). \tag{9.4}\end{equation}\]</span> $$</p><p>其中 <span class="math inline">\(\theta \in \mathbb{R}^D\)</span> 是我们需要估计的参数。由式 (9.4) 描述的函数类是穿过原点的直线。在式 (9.4) 中，我们选择了如下参数化形式： <span class="math display">\[f(x) = x^{\top}\theta.\]</span> 式 (9.3) 中的似然函数是 <span class="math inline">\(y\)</span> 的概率密度函数，在 <span class="math inline">\(x^{\top}\theta\)</span> 处进行评估。需要注意的是，不确定性的唯一来源是观测噪声（因为在式 (9.3) 中，<span class="math inline">\(x\)</span> 和 <span class="math inline">\(\theta\)</span> 被假设为已知）。如果没有观测噪声，<span class="math inline">\(x\)</span> 与 <span class="math inline">\(y\)</span> 之间的关系将是确定性的，而式 (9.3) 将退化为一个狄拉克 delta 函数。</p><figure><img src="/img3/机器学习的数学基础Part2/F9.2.png" alt="F9.2" /><figcaption aria-hidden="true">F9.2</figcaption></figure><p><strong>例 9.1</strong> 对于 <span class="math inline">\(x, \theta \in \mathbb{R}\)</span>，线性回归模型 (9.4) 描述的是直线（线性函数），参数 <span class="math inline">\(\theta\)</span> 就是直线的斜率。图 9.2(a) 展示了不同参数取值下的一些示例函数。</p><p><strong>线性回归模型 (9.3)–(9.4) 不仅在参数上是线性的，在输入 <span class="math inline">\(x\)</span> 上也是线性的。</strong>图 9.2(a) 展示了此类函数的例子。<strong>我们将在后面看到，对于非线性变换 <span class="math inline">\(\phi\)</span>，模型</strong> <span class="math display">\[y = \phi^{\top}(x)\theta\]</span> <strong>依然是一个线性回归模型，因为<em><u>“线性回归”指的是在参数上是线性的模型</u></em>，即模型通过输入特征的线性组合来描述函数。这里的“特征”是输入 <span class="math inline">\(x\)</span> 的一种表示 <span class="math inline">\(\phi(x)\)</span>。</strong>在接下来的内容中，我们将更详细地讨论如何找到合适的参数 <span class="math inline">\(\theta\)</span>，以及如何评估某组参数是否“效果良好”。在目前的讨论中，我们假设噪声方差 <span class="math inline">\(\sigma^2\)</span> 是已知的。</p><h3 id="参数估计">9.2 参数估计</h3><p>考虑线性回归的设定 (9.4)，假设我们给定一个训练集<span class="math inline">\(D := \{(x_1, y_1), \ldots, (x_N, y_N)\}\)</span>其中包含 <span class="math inline">\(N\)</span> 个输入 <span class="math inline">\(x_n \in \mathbb{R}^D\)</span> 以及对应的观测/目标 <span class="math inline">\(y_n \in \mathbb{R}, \; n = 1, \ldots, N\)</span>。对应的图模型如图 9.3 所示。注意到，给定各自的输入 <span class="math inline">\(x_i, x_j\)</span> 时，<span class="math inline">\(y_i\)</span> 和 <span class="math inline">\(y_j\)</span> 条件独立，因此似然可以因子化为: <span class="math display">\[\begin{align}p(Y \mid X, \theta) &amp;= p(y_{1}, \ldots, y_{N} \mid x_{1}, \ldots, x_{N}, \theta) \tag{9.5a} \\&amp;= \prod_{n=1}^{N} p(y_{n} \mid x_{n}, \theta) = \prod_{n=1}^{N} \mathcal{N}\!\left(y_{n} \mid x_{n}^{\top}\theta, \sigma^{2}\right). \tag{9.5b}\end{align}\]</span> 其中我们定义 <span class="math inline">\(X := \{x_1, \ldots, x_N\}\)</span>，<span class="math inline">\(Y := \{y_1, \ldots, y_N\}\)</span> 分别为训练输入集合和对应的目标集合。由于噪声分布的存在，似然以及各个因子 <span class="math inline">\(p(y_n \mid x_n, \theta)\)</span> 都是高斯分布；见 (9.3)。</p><p>接下来，我们将讨论如何为线性回归模型 (9.4) 找到最优参数 <span class="math inline">\(\theta^* \in \mathbb{R}^D\)</span>。一旦找到参数 <span class="math inline">\(\theta^*\)</span>，我们就可以利用该参数估计来进行预测：对于任意测试输入 <span class="math inline">\(x^*\)</span>，其对应目标 <span class="math inline">\(y^*\)</span> 的分布为 <span class="math display">\[p(y^* \mid x^*, \theta^*) = \mathcal{N}\!\big(y^* \mid (x^*)^\top \theta^*, \sigma^2\big). \tag{9.6}\]</span> 在下文中，我们将考察通过最大化似然来进行参数估计的方法，这是我们在第 8.3 节中已经部分涉及过的一个主题。</p><p><img src="/img3/机器学习的数学基础Part2/F9.3.png" alt="F9.3" style="zoom:67%;" /></p><h4 id="极大似然估计">9.2.1 极大似然估计</h4><p>Maximum Likelihood Estimation</p><p>一种广泛使用的求解期望参数 <span class="math inline">\(\theta_{\text{ML}}\)</span> 的方法是 <strong>极大似然估计</strong>，其思想是找到能最大化似然函数 (9.5b) 的参数 <span class="math inline">\(\theta_{\text{ML}}\)</span>。直观上，最大化似然就意味着：在给定模型参数的条件下，使训练数据的预测分布最大。最大似然参数可以写为： <span class="math display">\[\theta_{\text{ML}} = \arg\max_{\theta} p(Y \mid X, \theta). \tag{9.7}\]</span></p><p><strong>备注</strong>：似然函数 <span class="math inline">\(p(y \mid x, \theta)\)</span> 并不是关于 <span class="math inline">\(\theta\)</span> 的概率分布：它只是参数 <span class="math inline">\(\theta\)</span> 的一个函数，但并不一定积分为 1（即它是未归一化的），甚至可能不可积。然而，在 (9.7) 中的似然函数，是关于 <span class="math inline">\(y\)</span> 的一个归一化的概率分布。</p><p>为了找到能最大化似然的参数 <span class="math inline">\(\theta_{\text{ML}}\)</span>，我们通常会进行 梯度上升（或者对负似然做 梯度下降）。但是，在这里讨论的线性回归情形下，存在一个 闭式解，因此无需迭代的梯度下降。实际应用中，我们通常不会直接最大化似然，而是对似然函数取对数，并最小化 负对数似然。</p><p><strong>备注（对数变换）</strong>：由于似然函数 (9.5b) 是 <span class="math inline">\(N\)</span> 个高斯分布的乘积，因此取对数变换非常有用，因为： (a) 它避免了数值下溢的问题； (b) 它使得求导规则更简单。</p><p>更具体地说，当我们相乘 <span class="math inline">\(N\)</span> 个概率时（<span class="math inline">\(N\)</span> 是数据点的个数），会出现数值下溢的问题，因为我们无法表示极小的数，例如 <span class="math inline">\(10^{-256}\)</span>。此外，对数变换能把乘积转化为对数概率的和，这样相应的梯度就是单个梯度的和，而不必反复应用乘积法则 (5.46) 去计算 <span class="math inline">\(N\)</span> 项乘积的梯度。</p><p>为了找到我们线性回归问题的最优参数 <span class="math inline">\(\theta_{\text{ML}}\)</span>，我们最小化 负对数似然： <span class="math display">\[- \log p(Y \mid X, \theta) = - \log \prod_{n=1}^{N} p(y_n \mid x_n, \theta) = - \sum_{n=1}^{N} \log p(y_n \mid x_n, \theta), \tag{9.8}\]</span> 其中我们利用了在训练集上独立性假设，使得似然函数 (9.5b) 可以在数据点数目上进行因子分解。在 线性回归模型 (9.4) 中，由于高斯加性噪声项的存在，似然函数是高斯分布，因此我们得到：</p><p><span class="math display">\[\log p(y_n \mid x_n, \theta) = -\frac{1}{2\sigma^2} (y_n - x_n^\top \theta)^2 + \text{const}, \tag{9.9}\]</span> 其中常数项包括了所有与 <span class="math inline">\(\theta\)</span> 无关的部分。将 (9.9) 代入负对数似然 (9.8)，并忽略常数项后，我们得到：</p><p><span class="math display">\[\begin{align}L(\theta) &amp;:= \frac{1}{2\sigma^{2}} \sum_{n=1}^{N} \left( y_{n} - x_{n}^{\top}\theta \right)^{2} \tag{9.10a} \\&amp;= \frac{1}{2\sigma^{2}} (y - X\theta)^{\top}(y - X\theta) = \frac{1}{2\sigma^{2}} \, \lVert y - X\theta \rVert^{2}. \tag{9.10b}\end{align}\]</span></p><p>其中我们定义 <strong>设计矩阵</strong>（design matrix） <span class="math display">\[X := [x_1, \dots, x_N]^\top \in \mathbb{R}^{N \times D}\]</span> 为训练输入的集合，且 <span class="math display">\[y := [y_1, \dots, y_N]^\top \in \mathbb{R}^N\]</span> 为收集所有训练目标的向量。注意，设计矩阵 <span class="math inline">\(X\)</span> 的第 <span class="math inline">\(n\)</span> 行对应于训练输入 <span class="math inline">\(x_n\)</span>。在公式 (9.10b) 中，<strong>我们利用了这样一个事实：观测值 <span class="math inline">\(y_n\)</span> 与对应模型预测 <span class="math inline">\(x_n^\top \theta\)</span> 之间的平方误差之和，等于向量 <span class="math inline">\(y\)</span> 与 <span class="math inline">\(X\theta\)</span> 之间的平方距离。</strong></p><p>根据公式 (9.10b)，我们现在得到了需要优化的负对数似然函数的具体形式。我们立刻可以看到，(9.10b) 是关于参数 <span class="math inline">\(\theta\)</span> 的二次函数。这意味着我们可以找到唯一的全局解 <span class="math inline">\(\theta_{\text{ML}}\)</span>，从而最小化负对数似然 <span class="math inline">\(L\)</span>。找到全局最优解的方法是：计算 <span class="math inline">\(L\)</span> 对 <span class="math inline">\(\theta\)</span> 的梯度，将其设为 0，然后解出 <span class="math inline">\(\theta\)</span>。</p><p>利用第 5 章的结果，我们计算 <span class="math inline">\(L\)</span> 关于参数的梯度如下： <span class="math display">\[\begin{align}\frac{dL}{d\theta} &amp;= \frac{d}{d\theta} \left[ \frac{1}{2\sigma^{2}} (y - X\theta)^{\top}(y - X\theta) \right] \tag{9.11a} \\[6pt]&amp;= \frac{1}{2\sigma^{2}} \frac{d}{d\theta} \left( y^{\top}y - 2y^{\top}X\theta + \theta^{\top}X^{\top}X\theta \right) \tag{9.11b} \\[6pt]&amp;= \frac{1}{\sigma^{2}} \left( -y^{\top}X + \theta^{\top}X^{\top}X \right) \in \mathbb{R}^{1 \times D}. \tag{9.11c}\end{align}\]</span> 极大似然估计量 <span class="math inline">\(\theta_{\text{ML}}\)</span> 通过解方程 <span class="math inline">\(\frac{dL}{d\theta} = 0^\top\)</span>（必要的最优条件）得到： <span class="math display">\[\begin{align}\frac{dL}{d\theta} = 0^{\top} &amp;\;\;\;\;\;\;\;\;\; \Longleftrightarrow \;\;\;\;\;\;\;\;\;\theta^{\top}_{\mathrm{ML}} X^{\top}X = y^{\top}X \tag{9.12a} \\[6pt]&amp;\;\;\;\;\;\;\;\;\; \Longleftrightarrow \;\;\;\;\;\;\;\;\;\theta^{\top}_{\mathrm{ML}} = y^{\top}X (X^{\top}X)^{-1} \tag{9.12b} \\[6pt]&amp;\;\;\;\;\;\;\;\;\; \Longleftrightarrow \;\;\;\;\;\;\;\;\;\theta_{\mathrm{ML}} = (X^{\top}X)^{-1} X^{\top} y . \tag{9.12c}\end{align}\]</span></p><p>我们之所以能在 (9.12a) 的等式右乘 <span class="math inline">\((X^\top X)^{-1}\)</span>，是因为当矩阵 <span class="math inline">\(X\)</span> 的秩 <span class="math inline">\(\mathrm{rk}(X) = D\)</span> 时，<span class="math inline">\(X^\top X\)</span> 是正定的。</p><p><strong>备注 1</strong>：将梯度设为 <span class="math inline">\(0^\top\)</span> 是充分必要条件，并且我们得到的是全局最小值，因为 Hessian 矩阵 <span class="math display">\[\nabla^2_\theta L(\theta) = X^\top X \in \mathbb{R}^{D \times D}\]</span> 是正定的。</p><p><strong>备注 2</strong>：最大似然解 (9.12c) 要求我们解一个形如 <span class="math inline">\(A\theta = b\)</span> 的线性方程组，其中 <span class="math display">\[A = X^\top X, \quad b = X^\top y.\]</span></p><p><strong>例 9.2（拟合直线）</strong> 我们来看图 9.2，其中我们的目标是用极大似然估计将一条直线<span class="math inline">\(f(x) = \theta x\)</span>（<span class="math inline">\(\theta\)</span> 是未知的斜率）拟合到一个数据集上。该模型类别中的一些函数（直线）展示在图 9.2(a) 中。对于图 9.2(b) 所示的数据集，我们利用公式 (9.12c) 得到斜率参数 <span class="math inline">\(\theta\)</span> 的极大似然估计，并在图 9.2(c) 中得到对应的最大似然直线函数。</p><p><strong>带特征的极大似然估计</strong></p><p>Maximum Likelihood Estimation with Features</p><p>到目前为止，我们考虑的是 (9.4) 中描述的线性回归设定，它允许我们用极大似然估计将直线拟合到数据上。然而，在拟合更复杂的数据时，直线的表达能力是不够的。幸运的是，线性回归在其框架下为我们提供了一种方法，可以拟合非线性函数：</p><p><strong>由于“线性回归”仅仅指的是“对参数是线性的”</strong>，<strong>我们可以对输入 <span class="math inline">\(x\)</span> 进行任意的非线性变换 <span class="math inline">\(\phi(x)\)</span>，然后再对这个变换后的分量进行线性组合。</strong>相应的线性回归模型为 <span class="math display">\[p(y \mid x, \theta) = \mathcal{N}\big(y \mid \phi^\top(x)\theta, \sigma^2 \big) \quad \Longleftrightarrow \quady = \phi^\top(x)\theta + \varepsilon= \sum_{k=0}^{K-1} \theta_k \phi_k(x) + \varepsilon ,\tag{9.13}\]</span> 其中 <span class="math inline">\(\phi : \mathbb{R}^D \to \mathbb{R}^K\)</span> 是输入 <span class="math inline">\(x\)</span> 的一个（非线性）变换，而 <span class="math inline">\(\phi_k : \mathbb{R}^D \to \mathbb{R}\)</span> 是特征向量 <span class="math inline">\(\phi\)</span> 的第 <span class="math inline">\(k\)</span> 个分量。需要注意的是，模型参数 <span class="math inline">\(\theta\)</span> 仍然只以线性形式出现。</p><p><strong>例 9.3（多项式回归）</strong> 我们关心的是一个回归问题： <span class="math display">\[y = \phi^\top(x)\theta + \varepsilon,\]</span> 其中 <span class="math inline">\(x \in \mathbb{R}\)</span>，<span class="math inline">\(\theta \in \mathbb{R}^K\)</span>。在这种情况下常用的一种变换是 <span class="math display">\[\phi(x) =\begin{bmatrix}\phi_0(x) \\\phi_1(x) \\\vdots \\\phi_{K-1}(x)\end{bmatrix}=\begin{bmatrix}1 \\x \\x^2 \\x^3 \\\vdots \\x^{K-1}\end{bmatrix}\in \mathbb{R}^K. \tag{9.14}\]</span> <strong>这意味着我们将原本一维的输入空间“提升”到一个 <span class="math inline">\(K\)</span> 维特征空间</strong>，该空间由所有单项式 <span class="math inline">\(x^k\)</span> （其中 <span class="math inline">\(k = 0, \ldots, K-1\)</span>）组成。利用这些特征，我们可以在线性回归框架下建模次数不超过 <span class="math inline">\(K-1\)</span> 的多项式：</p><p>一个 <span class="math inline">\(K-1\)</span> 次多项式是 <span class="math display">\[f(x) = \sum_{k=0}^{K-1} \theta_k x^k = \phi^\top(x)\theta, \tag{9.15}\]</span> 其中 <span class="math inline">\(\phi\)</span> 定义如 (9.14)，<span class="math inline">\(\theta = [\theta_0, \ldots, \theta_{K-1}]^\top \in \mathbb{R}^K\)</span> 包含了参数 <span class="math inline">\(\theta_k\)</span>（线性参数）。</p><p>现在我们来看看线性回归模型 (9.13) 中参数 <span class="math inline">\(\theta\)</span> 的最大似然估计。我们考虑训练输入 <span class="math inline">\(x_n \in \mathbb{R}^D\)</span> 和目标 <span class="math inline">\(y_n \in \mathbb{R}, \; n = 1, \dots, N\)</span>，并定义特征矩阵（设计矩阵）为 <span class="math display">\[\Phi :=\begin{bmatrix}\phi^{\top}(x_1) \\\vdots \\\phi^{\top}(x_N)\end{bmatrix}=\begin{bmatrix}\phi_0(x_1) &amp; \cdots &amp; \phi_{K-1}(x_1) \\\phi_0(x_2) &amp; \cdots &amp; \phi_{K-1}(x_2) \\\vdots &amp; \ddots &amp; \vdots \\\phi_0(x_N) &amp; \cdots &amp; \phi_{K-1}(x_N)\end{bmatrix}\in \mathbb{R}^{N \times K},\tag{9.16}\]</span> 其中 <span class="math inline">\(\Phi_{ij} = \phi_j(x_i)\)</span>，并且 <span class="math inline">\(\phi_j : \mathbb{R}^D \to \mathbb{R}\)</span>。</p><p><strong>例 9.4 （二次多项式的特征矩阵）</strong> 对于二次多项式以及训练点 <span class="math inline">\(x_n \in \mathbb{R}, \; n=1,\dots,N\)</span>，其特征矩阵为 <span class="math display">\[\Phi =\begin{bmatrix}1 &amp; x_1 &amp; x_1^2 \\1 &amp; x_2 &amp; x_2^2 \\\vdots &amp; \vdots &amp; \vdots \\1 &amp; x_N &amp; x_N^2\end{bmatrix}.\tag{9.17}\]</span></p><p>在 (9.16) 定义的特征矩阵 <span class="math inline">\(\Phi\)</span> 下，线性回归模型 (9.13) 的负对数似然可以写为 <span class="math display">\[- \log p(Y \mid X, \theta) = \frac{1}{2\sigma^2} (y - \Phi \theta)^{\top}(y - \Phi \theta) + \text{const}.\tag{9.18}\]</span> 将 (9.18) 与 “无特征” 模型的负对数似然 (9.10b) 对比，我们可以立刻看到，只需要用 <span class="math inline">\(\Phi\)</span> 替换 <span class="math inline">\(X\)</span> 即可。由于 <span class="math inline">\(X\)</span> 和 <span class="math inline">\(\Phi\)</span> 都与我们希望优化的参数 <span class="math inline">\(\theta\)</span> 无关，我们立刻得到非线性特征下的线性回归问题的最大似然估计： <span class="math display">\[\theta_{\mathrm{ML}} = (\Phi^{\top}\Phi)^{-1} \Phi^{\top} y .\tag{9.19}\]</span></p><p><strong>备注</strong>：当我们在没有特征的情况下工作时，我们要求 <span class="math inline">\(X^{\top}X\)</span> 可逆，这在 <span class="math inline">\(\mathrm{rk}(X) = D\)</span>（即 <span class="math inline">\(X\)</span> 的列向量线性无关）时成立。在 (9.19) 中，我们同样要求 <span class="math inline">\(\Phi^{\top}\Phi \in \mathbb{R}^{K \times K}\)</span> 可逆。这当且仅当 <span class="math inline">\(\mathrm{rk}(\Phi) = K\)</span> 时成立。</p><p><img src="/img3/机器学习的数学基础Part2/F9.4.png" alt="F9.4" style="zoom:67%;" /></p><p><strong>例 9.5（最大似然多项式拟合）</strong> 考虑图 9.4(a) 中的数据集。该数据集由 <span class="math inline">\(N=10\)</span> 对 <span class="math inline">\((x_n, y_n)\)</span> 组成，其中<span class="math inline">\(x_n \sim U[-5, 5], \quad y_n = -\sin(x_n/5) + \cos(x_n) + \epsilon,\)</span>且 <span class="math inline">\(\epsilon \sim \mathcal{N}(0, 0.2^2)\)</span>。</p><p>我们使用最大似然估计拟合一个 4 次多项式，即参数 <span class="math inline">\(\theta_{\text{ML}}\)</span> 由公式 (9.19) 给出。最大似然估计在任意测试点 <span class="math inline">\(x^*\)</span> 的预测值为<span class="math inline">\(\phi^\top(x^*) \theta_{\text{ML}}.\)</span>结果如图 9.4(b) 所示。</p><blockquote><p>个人注：例子里，真实的数据生成函数是： <span class="math display">\[y_n = -\sin(x_n/5) + \cos(x_n) + \epsilon\]</span> 其中 <span class="math inline">\(\epsilon \sim N(0, 0.2^2)\)</span> 是噪声。注意，这里的 <strong>真实函数是非多项式的</strong>，它是由正弦和余弦函数组成的。</p><p>但是在这个例子中，作者选择用 <strong>4次多项式</strong> 来拟合这些数据： <span class="math display">\[y \approx \theta_0 + \theta_1 x + \theta_2 x^2 + \theta_3 x^3 + \theta_4 x^4\]</span></p></blockquote><p><strong>噪声方差的估计</strong></p><p>到目前为止，我们一直假设噪声方差 σ² 是已知的。然而，我们同样可以利用最大似然估计的原理来得到噪声方差的最大似然估计量 <span class="math inline">\(σ²_{ML}\)</span>。为此，我们遵循标准步骤：写出对数似然函数，对 <span class="math inline">\(σ² &gt; 0\)</span> 求导，使其等于 0，并解方程。</p><blockquote><p>个人注：<strong>噪声方差的最大似然估计告诉你模型拟合后，数据中剩余的不确定性有多大</strong>。</p><p>噪声方差的最大似然估计（MLE for noise variance）在统计建模中有非常直观的意义，它告诉我们 <strong>模型预测值与实际观测值之间偏差的典型大小</strong>，也就是 <strong>数据中的随机波动有多大</strong>。换句话说，它量化了模型拟合“剩余误差”的平均能量。剩余误差就是数据本身的随机波动。</p><p><strong>意义</strong>：</p><ol type="1"><li><strong>平均残差平方</strong>：它就是残差平方的平均值，也就是模型拟合后剩余误差的能量。</li><li><strong>不确定性度量</strong>：在高斯假设下，方差越大，说明数据波动越大，模型预测的不确定性越高。</li><li><strong>概率解释</strong>：在高斯模型下，<span class="math inline">\(\hat{\sigma}^2\)</span> 最大化了观测数据出现的概率（即似然函数最大化）。</li></ol></blockquote><p>对数似然函数为： <span class="math display">\[\begin{align}\log p(Y \mid X, \theta, \sigma^2) &amp;= \sum_{n=1}^N \log \mathcal{N}\!\left(y_n \mid \phi^\top(x_n)\theta, \sigma^2 \right) \tag{9.20a} \\[6pt]&amp;= \sum_{n=1}^N \left(-\tfrac{1}{2}\log(2\pi) -\tfrac{1}{2}\log\sigma^2-\tfrac{1}{2\sigma^2}(y_n - \phi^\top(x_n)\theta)^2\right)\tag{9.20b} \\[6pt]&amp;= -\tfrac{N}{2}\log\sigma^2 -\tfrac{1}{2\sigma^2} \underbrace{\sum_{n=1}^N (y_n - \phi^\top(x_n)\theta)^2\,}_{=:s} + \text{const.}\tag{9.20c}\end{align}\]</span></p><p>对数似然函数关于 σ² 的偏导数为： <span class="math display">\[\begin{align}\frac{\partial \log p(Y \mid X, \theta, \sigma^2)}{\partial \sigma^2}&amp;= -\frac{N}{2\sigma^2} + \frac{1}{2\sigma^4}s = 0 \tag{9.21a} \\[6pt]\Longleftrightarrow \quad \frac{N}{2\sigma^2} &amp;= \frac{s}{2\sigma^4} \tag{9.21b}\end{align}\]</span></p><p>因此我们得到： <span class="math display">\[\sigma^2_{ML} = \frac{s}{N} = \frac{1}{N}\sum_{n=1}^N \big(y_n - \phi^\top(x_n)\theta \big)^2 .\tag{9.22}\]</span> 因此，噪声方差的最大似然估计就是在输入位置 <span class="math inline">\(x_n\)</span> 上，噪声自由的函数值 <span class="math inline">\(\phi^\top(x_n)\theta\)</span> 与相应带噪观测值 <span class="math inline">\(y_n\)</span> 之间平方差的经验均值。</p><h4 id="线性回归中的过拟合">9.2.2 线性回归中的过拟合</h4><p>我们刚刚讨论了如何使用最大似然估计来将线性模型（例如多项式）拟合到数据上。我们可以通过计算模型产生的误差/损失来评估模型的质量。一种方法是计算负对数似然（公式 9.10b），我们通过最小化它来确定最大似然估计器。另一种方法是注意到噪声参数 <span class="math inline">\(\sigma^2\)</span> 不是一个自由的模型参数，因此我们可以忽略 <span class="math inline">\(1/\sigma^2\)</span> 的缩放，从而得到平方误差损失函数 <span class="math display">\[\|y - \Phi \theta\|^2.\]</span></p><blockquote><p>个人注：取负对数似然（negative log-likelihood, NLL）得到： <span class="math display">\[-\log p(y \mid \theta, \sigma^2) = \frac{N}{2} \log (2\pi \sigma^2) + \frac{1}{2\sigma^2} \sum_{n=1}^{N} (y_n - (\Phi \theta)_n)^2\]</span> 注意这里有两部分：</p><ol type="1"><li><span class="math inline">\(\frac{N}{2} \log (2\pi \sigma^2)\)</span> —— 与 <span class="math inline">\(\theta\)</span> 无关</li><li><span class="math inline">\(\frac{1}{2\sigma^2} \sum (y_n - (\Phi \theta)_n)^2\)</span> —— 与 <span class="math inline">\(\theta\)</span> 有关，但前面有 <span class="math inline">\(1/(2\sigma^2)\)</span> 的缩放</li></ol><p>为什么可以忽略 <span class="math inline">\(1/\sigma^2\)</span></p><ul><li>当我们<strong>只对 <span class="math inline">\(\theta\)</span> 求最大似然估计</strong>时，<span class="math inline">\(\sigma^2\)</span> 被认为是一个已知常数或独立处理的参数。</li><li>对于优化 <span class="math inline">\(\theta\)</span> 来说，<strong>乘以一个正的常数 <span class="math inline">\(1/\sigma^2\)</span> 并不会改变最小点的位置</strong>。</li><li>因此，为了简化计算，我们可以<strong>忽略 <span class="math inline">\(1/\sigma^2\)</span> 的缩放因子</strong>，直接最小化平方误差：</li></ul><p><span class="math display">\[\sum_{n=1}^{N} (y_n - (\Phi \theta)_n)^2 = \|y - \Phi \theta\|^2\]</span></p><p><strong>这就是为什么在线性回归中，平方误差损失等价于最大似然估计的原因。</strong></p></blockquote><p>除了使用平方损失外，我们通常使用<strong>均方根误差（root mean square error）（RMSE）</strong>： <span class="math display">\[\sqrt{\frac{1}{N}\|y - \Phi\theta\|^2} \;=\; \sqrt{\frac{1}{N} \sum_{n=1}^N (y_n - \phi(x_n)^\top \theta)^2}, \tag{9.23}\]</span> <strong>它具有以下优点</strong>： (a) 允许我们比较不同规模数据集上的误差； (b) 与观测的函数值 <span class="math inline">\(y_n\)</span> 具有相同的量纲和单位。</p><p>例如，如果我们拟合一个模型，将邮政编码（<span class="math inline">\(x\)</span> 由经纬度表示）映射到房价（<span class="math inline">\(y\)</span> 的单位是欧元），那么 RMSE 的单位也是欧元，而平方误差的单位则是欧元平方（EUR<span class="math inline">\(^2\)</span>）。如果我们选择在目标函数中包含原始负对数似然（公式 9.10b）中的因子 <span class="math inline">\(\sigma^2\)</span>，那么最终目标将不再有单位；在前述例子中，目标函数将不再是以欧元或欧元平方为单位。</p><p>在模型选择中（见第 8.6 节），我们可以使用 RMSE（或负对数似然）来确定多项式的最佳次数，即找到使目标函数最小化的多项式次数 <span class="math inline">\(M\)</span>。由于多项式次数是自然数，我们可以通过穷举搜索来枚举所有（合理的）<span class="math inline">\(M\)</span> 值。对于一个大小为 <span class="math inline">\(N\)</span> 的训练集，测试 <span class="math inline">\(0 \leq M \leq N-1\)</span> 就足够了。</p><p>当 <span class="math inline">\(M &lt; N\)</span> 时，最大似然估计是唯一的；而当 <span class="math inline">\(M \geq N\)</span> 时，模型的参数数量多于数据点数量，我们就需要解一个欠定的线性方程组（此时 (9.19) 中的 <span class="math inline">\(\Phi^\top \Phi\)</span> 也将不可逆），因此会有无穷多个可能的最大似然估计解。</p><figure><img src="/img3/机器学习的数学基础Part2/F9.5.png" alt="F9.5" /><figcaption aria-hidden="true">F9.5</figcaption></figure><p>图 9.5 展示了利用最大似然方法在图 9.4(a) 中包含 <span class="math inline">\(N = 10\)</span> 个观测点的数据集上得到的一系列多项式拟合结果。我们可以注意到，低阶多项式（例如常数函数 <span class="math inline">\(M=0\)</span> 或一次函数 <span class="math inline">\(M=1\)</span>）对数据的拟合效果很差，因此对真实的底层函数表现力不足。对于 <span class="math inline">\(M = 3, \ldots, 6\)</span> 的情况，拟合结果看起来合理，能够平滑地插值通过这些数据点。当阶数进一步增大时，我们发现多项式对数据的拟合越来越好。在极端情况下，当 <span class="math inline">\(M = N - 1 = 9\)</span> 时，拟合函数会精确通过每一个数据点。然而，这类高阶多项式会出现剧烈振荡，无法很好地表示生成数据的真实底层函数，因而会产生严重的过拟合问题。</p><p>请记住，我们的目标是通过对新的（未见过的）数据进行准确预测，从而实现良好的泛化性能。为了更直观地了解泛化性能与多项式阶数 <span class="math inline">\(M\)</span> 之间的关系，我们考虑了一个独立的测试集，该测试集由 200 个数据点组成，数据生成方式与训练集完全相同。测试输入点选取为区间 <span class="math inline">\([-5, 5]\)</span> 内的 200 个等间隔点。对于每一个 <span class="math inline">\(M\)</span>，我们分别在训练集和测试集上计算 RMSE (公式 9.23)。</p><p><strong>观察测试误差（它是评价对应多项式泛化能力的一个定性指标）</strong>，我们发现其初始阶段会下降；参见图 9.6（橙色曲线）。对于四阶多项式，测试误差相对较小，并且在 <span class="math inline">\(M=5\)</span> 之前基本保持稳定。然而，从 <span class="math inline">\(M=6\)</span> 开始，测试误差显著上升，高阶多项式的泛化能力非常差。在这个例子中，这一点也可以从图 9.5 中的最大似然拟合曲线清楚地看出。需要注意的是，训练误差（图 9.6 中的蓝色曲线）在多项式阶数增加时从不会上升。在本例中，最好的泛化效果（测试误差最小的点）出现在 <span class="math inline">\(M=4\)</span> 的多项式上。</p><p><img src="/img3/机器学习的数学基础Part2/F9.6.png" alt="F9.6" style="zoom:67%;" /></p><h4 id="最大后验估计">9.2.3 最大后验估计</h4><p>Maximum A Posteriori Estimation, MAP</p><p>我们刚刚看到，最大似然估计（MLE）容易产生过拟合。<strong>当出现过拟合时，参数值的幅度往往会变得相对较大（Bishop, 2006）</strong>。为了减轻参数值过大的影响，我们可以在参数上引入一个先验分布 <span class="math inline">\(p(\theta)\)</span>。先验分布明确地编码了哪些参数值在观测数据之前是合理的。<strong>例如，对于单个参数 <span class="math inline">\(\theta\)</span>，如果我们选择一个高斯先验 <span class="math inline">\(p(\theta) = \mathcal{N}(0, 1)\)</span>，就表示我们期望 <span class="math inline">\(\theta\)</span> 的取值落在区间 <span class="math inline">\([-2, 2]\)</span> 内（均值的两个标准差范围内）。</strong>一旦数据集 <span class="math inline">\(X, Y\)</span> 可用，我们不再只最大化似然函数，而是寻找能够最大化后验分布 <span class="math inline">\(p(\theta | X, Y)\)</span> 的参数。这一过程称为 <strong>最大后验估计（MAP 估计）</strong>。根据贝叶斯定理（参见第 6.3 节），给定训练数据 <span class="math inline">\(X, Y\)</span>，参数 <span class="math inline">\(\theta\)</span> 的后验分布为</p><p><span class="math display">\[p(\theta | X, Y) = \frac{p(Y | X, \theta)p(\theta)}{p(Y | X)} . \tag{9.24}\]</span> <strong>由于后验分布显式依赖于参数的先验 <span class="math inline">\(p(\theta)\)</span>，因此先验会影响我们得到的后验最大化参数向量。</strong>后面我们会更清楚地看到这一点。能够最大化 (9.24) 式后验分布的参数向量 <span class="math inline">\(\theta_{\text{MAP}}\)</span> 就是 <strong>MAP 估计</strong>。</p><p>求解 MAP 估计的步骤与最大似然估计类似。我们先对后验取对数，得到对数后验： <span class="math display">\[\log p(\theta | X, Y) = \log p(Y | X, \theta) + \log p(\theta) + \text{const}, \tag{9.25}\]</span> 其中常数项不依赖于 <span class="math inline">\(\theta\)</span>。<strong>我们可以看到，对数后验 (9.25) 是对数似然项 <span class="math inline">\(\log p(Y | X, \theta)\)</span> 与对数先验 <span class="math inline">\(\log p(\theta)\)</span> 的和，因此 MAP 估计实际上是 先验（观测数据之前对参数的合理性假设）与数据驱动的似然函数之间的折中。</strong></p><blockquote><p>个人注：所以说先验分布是一种正则化手段。</p></blockquote><p>为了求解 MAP 估计 <span class="math inline">\(\theta_{\text{MAP}}\)</span>，我们需要最小化关于 <span class="math inline">\(\theta\)</span> 的负对数后验： <span class="math display">\[\theta_{\text{MAP}} \in \arg \min_{\theta} \big\{ -\log p(Y | X, \theta) - \log p(\theta) \big\}. \tag{9.26}\]</span> 其梯度为： <span class="math display">\[- \frac{d \log p(\theta | X, Y)}{d\theta} = - \frac{d \log p(Y | X, \theta)}{d\theta} - \frac{d \log p(\theta)}{d\theta}, \tag{9.27}\]</span> 其中右边第一个项就是 (9.11c) 中负对数似然的梯度。若在参数 <span class="math inline">\(\theta\)</span> 上使用共轭的高斯先验</p><p><span class="math display">\[p(\theta) = \mathcal{N}(0, b^2 I),\]</span> 则在 (9.13) 的线性回归设定下，负对数后验为： <span class="math display">\[- \log p(\theta | X, Y) = \frac{1}{2\sigma^2} (y - \Phi \theta)^\top (y - \Phi \theta) + \frac{1}{2b^2} \theta^\top \theta + \text{const}. \tag{9.28}\]</span> 其中第一项来自对数似然，第二项来自对数先验。其关于参数 <span class="math inline">\(\theta\)</span> 的梯度为： <span class="math display">\[- \frac{d \log p(\theta | X, Y)}{d\theta} = \frac{1}{\sigma^2}(\theta^\top \Phi^\top \Phi - y^\top \Phi) + \frac{1}{b^2} \theta^\top. \tag{9.29}\]</span></p><p>我们将通过令该梯度为 <span class="math inline">\(0^\top\)</span>，并解出 <span class="math inline">\(\theta_{\text{MAP}}\)</span>，来找到最大后验估计 <span class="math inline">\(\theta_{\text{MAP}}\)</span>。我们得到：</p><p>$$ <span class="math display">\[\begin{align}&amp; \quad \quad \quad \frac{1}{\sigma^2}(\theta^\top \Phi^\top \Phi - y^\top \Phi) + \frac{1}{b^2}\theta^\top = 0^\top \tag{9.30a} \\&amp; \Longleftrightarrow \quad \theta^\top \left( \frac{1}{\sigma^2}\Phi^\top \Phi + \frac{1}{b^2}I \right)- \frac{1}{\sigma^2}y^\top \Phi = 0^\top \tag{9.30b} \\&amp; \Longleftrightarrow \quad \theta^\top \left( \Phi^\top \Phi + \frac{\sigma^2}{b^2}I \right)= y^\top \Phi \tag{9.30c} \\&amp; \Longleftrightarrow \quad \theta^\top = y^\top \Phi \left( \Phi^\top \Phi + \frac{\sigma^2}{b^2}I \right)^{-1}\tag{9.30d}\end{align}\]</span> $$</p><p>因此，MAP 估计为（通过对最后一个等式两边取转置）： <span class="math display">\[\theta_{\text{MAP}} = \left( \Phi^\top \Phi + \frac{\sigma^2}{b^2} I \right)^{-1} \Phi^\top y .\tag{9.31}\]</span> 将 (9.31) 中的 MAP 估计与 (9.19) 中的最大似然估计进行比较，我们可以看到，两者解的唯一差别在于逆矩阵中额外的项 <span class="math inline">\(\tfrac{\sigma^2}{b^2} I\)</span>。这个附加项保证了 <span class="math display">\[\Phi^\top \Phi + \frac{\sigma^2}{b^2} I\]</span> 是对称的并且严格正定的（即其逆矩阵存在，并且 MAP 估计是该线性方程组的唯一解）。此外它反映了正则化项的影响。</p><p><strong>例 9.6 （多项式回归的 MAP 估计）</strong> 在 9.2.1 节的多项式回归例子中，我们对参数 <span class="math inline">\(\theta\)</span> 施加一个高斯先验 <span class="math inline">\(p(\theta) = \mathcal{N}(0, I)\)</span>，并根据公式 (9.31) 来确定 MAP 估计。在图 9.7 中，我们展示了 6 次多项式（左图）和 8 次多项式（右图）的最大似然估计与 MAP 估计。对于低阶多项式，先验（正则化项）的作用不显著，但在高阶多项式时，它能使函数保持相对光滑。<strong>虽然 MAP 估计可以在一定程度上缓解过拟合，但它并不是该问题的通用解决方案，因此我们需要一种更为系统的方法来应对过拟合。</strong></p><p><img src="/img3/机器学习的数学基础Part2/F9.7.png" alt="F9.7" style="zoom:67%;" /></p><blockquote><p>个人注：正则化是针对同一个模型，能让参数靠近原点，而不是改变参数的个数（模型）。</p></blockquote><h4 id="最大后验估计作为正则化">9.2.4 最大后验估计作为正则化</h4><p>MAP Estimation as Regularizatio</p><blockquote><p>个人注：感觉标题翻译为 “最大后验估计作和正则化” 更容易理解一点。</p></blockquote><p>除了在参数 <span class="math inline">\(θ\)</span> 上放置一个先验分布外，还可以通过<strong>正则化</strong>来惩罚参数的幅度，从而减轻过拟合的影响。在正则化的最小二乘法中，我们考虑如下的损失函数： <span class="math display">\[\|y - \Phi \theta\|^2 + \lambda \|\theta\|^2_2 , \tag{9.32}\]</span> 并使其对 <span class="math inline">\(θ\)</span> 最小化（参见第 8.2.3 节）。其中，第一个项是数据拟合项（也称为失配项 misfit term），它与负对数似然成正比（见公式 (9.10b)）。第二个项称为<strong>正则项</strong> regularizer，正则化参数 <span class="math inline">\(\lambda \geq 0\)</span> 控制了正则化的“严格程度”。</p><p><strong>备注：</strong> 在 (9.32) 中，我们不一定要使用欧几里得范数 <span class="math inline">\(\|\cdot\|_2\)</span>，而是可以选择任意的 p-范数 <span class="math inline">\(\|\cdot\|_p\)</span>。在实践中，较小的 <span class="math inline">\(p\)</span> 值会导致更稀疏的解。这里，“稀疏”意味着许多参数值满足 <span class="math inline">\(\theta_d = 0\)</span>，这对于变量选择也很有用。当 <span class="math inline">\(p = 1\)</span> 时，正则项被称为 <strong>LASSO</strong>（最小绝对收缩与选择算子）(least absolute shrinkage and selection operator)，由 Tibshirani (1996) 提出。</p><p>在式 (9.32) 中的正则项 <span class="math inline">\(\lambda \lVert \theta \rVert_2^2\)</span> 可以解释为一个高斯先验的负对数形式，我们在 MAP 估计中会使用它；参见 (9.26)。更具体地说，若我们采用高斯先验<span class="math inline">\(p(\theta) = \mathcal{N}(0, b^2 I),\)</span>那么对应的负对数高斯先验为 <span class="math display">\[- \log p(\theta) = \frac{1}{2b^2} \lVert \theta \rVert_2^2 + \text{const}, \tag{9.33}\]</span> 因此，当 <span class="math inline">\(\lambda = \tfrac{1}{2b^2}\)</span> 时，正则项与负对数高斯先验是完全相同的。考虑到在 (9.32) 中的正则化最小二乘损失函数由与负对数似然加负对数先验密切相关的项组成，因此，当我们最小化这个损失函数时，得到的解与 (9.31) 中的 MAP 估计非常相似就不足为奇了。更具体地，最小化正则化最小二乘损失函数得到</p><p><span class="math display">\[\theta_{\text{RLS}} = (\Phi^\top \Phi + \lambda I)^{-1} \Phi^\top y, \tag{9.34}\]</span> 当 <span class="math inline">\(\lambda = \tfrac{\sigma^2}{b^2}\)</span> 时，这个解与 (9.31) 中的 MAP 估计完全一致，其中 <span class="math inline">\(\sigma^2\)</span> 是噪声方差，而 <span class="math inline">\(b^2\)</span> 是各向同性高斯先验<span class="math inline">\(p(\theta) = \mathcal{N}(0, b^2 I)\)</span>的方差。</p><p>到目前为止，我们已经介绍了使用最大似然估计（MLE）和最大后验估计（MAP）的参数估计方法，在这些方法中，我们找到的是使目标函数（似然函数或后验概率）最大化的点估计 <span class="math inline">\(\theta^*\)</span>。我们看到，无论是最大似然估计还是 MAP 估计，都可能导致过拟合。接下来的部分，我们将讨论<strong>贝叶斯线性回归</strong>，在这种方法中，我们使用<strong>贝叶斯推断</strong>（见第 8.4 节）来得到未知参数的后验分布，并进一步用它来进行预测。<strong>更具体地说，在预测时，我们会在所有合理的参数集合上进行平均，而不是仅仅依赖某一个点估计。</strong></p><h3 id="贝叶斯线性回归">9.3 贝叶斯线性回归</h3><p>在之前的内容中，我们研究了线性回归模型，并通过最大似然或 MAP 方法来估计模型参数 <span class="math inline">\(\theta\)</span>。我们发现，MLE 在小数据量的情况下可能会导致严重的过拟合。MAP 方法通过在参数上引入一个先验分布，起到了正则化的作用，从而在一定程度上缓解了这一问题。</p><p>贝叶斯线性回归将“参数先验”的思想进一步推进：它甚至不再尝试计算参数的点估计，而是通过参数的完整后验分布来进行预测。换句话说，我们并不拟合某一个具体的参数，而是根据后验分布，对所有可能的参数设定进行加权平均。</p><h4 id="模型">9.3.1 模型</h4><p>在贝叶斯线性回归中，我们考虑如下模型： <span class="math display">\[\begin{align}&amp;  \text{prior } \;\quad \quad p(\theta) = \mathcal{N}(m_0, S_0), \\&amp; \text{likelihood } \; p(y \mid x, \theta) = \mathcal{N}\big(y \mid \phi^{\top}(x)\theta, \sigma^2\big).\tag{9.35}\end{align}\]</span> 这里我们显式地对参数向量 <span class="math inline">\(\theta\)</span> 施加一个高斯先验 <span class="math inline">\(p(\theta) = \mathcal{N}(m_0, S_0)\)</span>，从而把参数向量转变为一个随机变量。</p><p>这使我们能够写出对应的图模型，如图 9.8 所示，在该图中我们将 <span class="math inline">\(\theta\)</span> 的高斯先验的参数明确表示出来。完整的概率模型（即观测变量与未观测变量的联合分布，分别是 <span class="math inline">\(y\)</span> 和 <span class="math inline">\(\theta\)</span>）可以写为： <span class="math display">\[p(y, \theta \mid x) = p(y \mid x, \theta)\, p(\theta). \tag{9.36}\]</span></p><p><img src="/img3/机器学习的数学基础Part2/F9.8.png" alt="F9.8" style="zoom:67%;" /></p><h4 id="先验预测">9.3.2 先验预测</h4><p>在实践中，我们通常并不是特别关心参数值 <span class="math inline">\(\theta\)</span> 本身。相反，我们更关注的是利用这些参数值所做出的预测。在贝叶斯设定下，当我们进行预测时，会基于参数分布，对所有可能的参数取平均。更具体地说，为了在输入 <span class="math inline">\(x_*\)</span> 处进行预测，我们需要对 <span class="math inline">\(\theta\)</span> 积分，从而得到：</p><p><span class="math display">\[p(y_* \mid x_*) = \int p(y_* \mid x_*, \theta)p(\theta)\, d\theta = \mathbb{E}_\theta\!\left[p(y_* \mid x_*, \theta)\right], \tag{9.37}\]</span> 这可以解释为：对所有依据先验分布 <span class="math inline">\(p(\theta)\)</span> 而合理的参数 <span class="math inline">\(\theta\)</span>，其条件预测 <span class="math inline">\(y_* \mid x_*, \theta\)</span> 的平均。注意，使用先验分布进行预测时，只需要给定输入 <span class="math inline">\(x_*\)</span>，而不需要任何训练数据。</p><p>在我们的模型 (9.35) 中，我们为 <span class="math inline">\(\theta\)</span> 选择了一个共轭（高斯）先验，因此预测分布同样是高斯分布（并且可以解析地计算）。具体来说，若先验分布为<span class="math inline">\(p(\theta) = \mathcal{N}(m_0, S_0),\)</span>则预测分布为 <span class="math display">\[p(y_* \mid x_*) = \mathcal{N}\!\Big(\phi^\top(x_*) m_0, \, \phi^\top(x_*) S_0 \phi(x_*) + \sigma^2\Big), \tag{9.38}\]</span> 这里我们利用了以下事实：</p><ol type="1"><li><p>由于共轭性（见第 6.6 节）和高斯分布的边缘化性质（见第 6.5 节），预测分布仍然是高斯分布；</p></li><li><p>高斯噪声独立，因此 <span class="math display">\[\mathrm{Var}[y_*] = \mathrm{Var}_\theta[\phi^\top(x_*)\theta] + \mathrm{Var}_\epsilon[\epsilon], \tag{9.39}\]</span></p></li><li><p><span class="math inline">\(y_*\)</span> 是 <span class="math inline">\(\theta\)</span> 的线性变换，因此可以直接应用 (6.50) 和 (6.51) 中的规则，解析计算预测的均值和协方差。</p></li></ol><p>在公式 (9.38) 中，预测方差中的项 <span class="math inline">\(\phi^\top(x_*) S_0 \phi(x_*)\)</span> 显式体现了与参数 <span class="math inline">\(\theta\)</span> 相关的不确定性，而 <span class="math inline">\(\sigma^2\)</span> 则反映了测量噪声带来的不确定性。如果我们更关心的是预测无噪声的函数值<span class="math inline">\(f(x_*) = \phi^\top(x_*)\theta,\)</span>而不是被噪声污染的目标 <span class="math inline">\(y_*\)</span>，则我们得到： <span class="math display">\[p(f(x_*)) = \mathcal{N}\!\Big(\phi^\top(x_*) m_0, \, \phi^\top(x_*) S_0 \phi(x_*)\Big), \tag{9.40}\]</span> 与 (9.38) 的唯一区别在于预测方差中去掉了噪声方差 <span class="math inline">\(\sigma^2\)</span>。</p><p><strong>备注（函数分布）</strong>： 由于我们可以用一组样本 <span class="math inline">\(\{\theta_i\}\)</span> 来表示参数分布 <span class="math inline">\(p(\theta)\)</span>，并且每个样本 <span class="math inline">\(\theta_i\)</span> 都对应着一个函数 <span class="math display">\[f_i(\cdot) = \theta_i^\top \phi(\cdot)，\]</span> 因此参数分布 <span class="math inline">\(p(\theta)\)</span> 诱导出了一个关于函数的分布 <span class="math inline">\(p(f(\cdot))\)</span>。这里符号 <span class="math inline">\((\cdot)\)</span> 明确表示了函数关系。</p><p><strong>例 9.7 （函数上的先验）</strong></p><p>考虑一个五次多项式的贝叶斯线性回归问题。我们选择参数先验为<span class="math inline">\(p(\theta) = \mathcal{N}(0, \tfrac{1}{4}I)。\)</span></p><p><img src="/img3/机器学习的数学基础Part2/F9.9.png" alt="F9.9" style="zoom:50%;" /></p><p>图 9.9 展示了该参数先验所诱导的函数先验分布（阴影区域：深灰色表示 67% 置信区间；浅灰色表示 95% 置信区间），并给出了一些从该先验中采样得到的函数。</p><p>一个函数样本的生成方式是：首先从先验中采样一个参数向量 <span class="math display">\[\theta_i \sim p(\theta)，\]</span> 然后计算 <span class="math display">\[f_i(\cdot) = \theta_i^\top \phi(\cdot)。\]</span> 我们使用了 200 个输入点 <span class="math inline">\(x_* \in [-5, 5]\)</span>，并将其映射到特征函数 <span class="math inline">\(\phi(\cdot)\)</span>。图 9.9 中的不确定性（阴影区域）完全来自参数的不确定性，因为这里我们考虑的是无噪声的预测分布 (9.40)。</p><p>到目前为止，我们一直在基于参数先验 <span class="math inline">\(p(\theta)\)</span> 来计算预测。然而，当我们拥有参数后验（给定一些训练数据 <span class="math inline">\(X, Y\)</span>）时，预测与推断的原理与公式 (9.37) 相同——只是需要将先验 <span class="math inline">\(p(\theta)\)</span> 替换为后验 <span class="math inline">\(p(\theta \mid X,Y)\)</span>。在接下来的内容中，我们将详细推导后验分布，然后再利用它进行预测。</p><h4 id="后验分布">9.3.3 后验分布</h4><p>给定一组训练输入 <span class="math inline">\(\{x_n \in \mathbb{R}^D\}\)</span> 及其对应的观测值 <span class="math inline">\(\{y_n \in \mathbb{R}\}, n=1,\dots,N\)</span>，我们使用贝叶斯定理计算参数的后验分布： <span class="math display">\[p(\theta \mid X,Y) = \frac{p(Y \mid X,\theta) p(\theta)}{p(Y \mid X)}, \tag{9.41}\]</span> 其中：<span class="math inline">\(X\)</span> 是训练输入的集合； <span class="math inline">\(Y\)</span> 是对应训练目标的集合；<span class="math inline">\(p(Y \mid X,\theta)\)</span> 是似然函数；<span class="math inline">\(p(\theta)\)</span> 是参数先验；</p><p><span class="math display">\[p(Y \mid X) = \int p(Y \mid X,\theta)p(\theta)\, d\theta = \mathbb{E}_\theta[p(Y \mid X,\theta)] \tag{9.42}\]</span></p><p>称为 <strong>边际似然（证据）</strong>(marginal likelihood/evidence)，它与参数 <span class="math inline">\(\theta\)</span> 无关，并确保后验归一化（积分为 1）。我们可以把边际似然看作是所有可能参数设置下（依据先验分布加权）的似然的平均。</p><p><strong>定理 9.1 （参数后验）</strong> 在我们的模型 (9.35) 中，参数后验 (9.41) 可以解析地计算为： $$ <span class="math display">\[\begin{align}p(\theta \mid X,Y) &amp; = \mathcal{N}(\theta \mid m_N, S_N), \tag{9.43a} \\S_N &amp; = (S_0^{-1} + \sigma^{-2}\Phi^\top \Phi)^{-1}, \tag{9.43b}\\m_N &amp; = S_N \big(S_0^{-1} m_0 + \sigma^{-2}\Phi^\top y \big), \tag{9.43c}\end{align}\]</span> $$</p><p>下标 <span class="math inline">\(N\)</span> 表示训练集大小。</p><p><strong>证明</strong></p><p>贝叶斯定理告诉我们，后验分布 <span class="math inline">\(p(\theta \mid X,Y)\)</span> 正比于似然 <span class="math inline">\(p(Y \mid X,\theta)\)</span> 与先验 <span class="math inline">\(p(\theta)\)</span> 的乘积： $$ <span class="math display">\[\begin{align}&amp; \text{后验: }\qquad p(\theta \mid X,Y) = \frac{p(Y \mid X,\theta)p(\theta)}{p(Y \mid X)} \tag{9.44a} \\&amp; \text{似然: } \qquad p(Y \mid X,\theta) = \mathcal{N}(y \mid \Phi\theta, \sigma^2 I) \tag{9.44b} \\&amp; \text{先验: } \qquad p(\theta) = \mathcal{N}(\theta \mid m_0, S_0) \tag{9.44c}\end{align}\]</span> $$ 与其直接研究先验与似然的乘积，我们可以将问题转化到对数空间，并通过“配方”来求解后验的均值和协方差。先验对数与似然对数之和为：</p><p>$$ <span class="math display">\[\begin{align}&amp; \log \mathcal{N}(y \mid \Phi\theta, \sigma^2 I) + \log \mathcal{N}(\theta \mid m_0, S_0) \tag{9.45a} \\&amp; = -\tfrac{1}{2}\sigma^{-2}(y-\Phi\theta)^\top(y-\Phi\theta) \;-\; \tfrac{1}{2}(\theta - m_0)^\top S_0^{-1} (\theta - m_0) + \text{const}, \tag{9.45b}\end{align}\]</span> $$</p><p>其中常数项与 <span class="math inline">\(\theta\)</span> 无关，我们在后续忽略它。继续展开 (9.45b)，得到： <span class="math display">\[-\tfrac{1}{2}\sigma^{-2} y^\top y \;+\; \sigma^{-2} y^\top \Phi \theta \;-\; \tfrac{1}{2}\theta^\top \sigma^{-2}\Phi^\top \Phi \theta \;-\; \tfrac{1}{2}\theta^\top S_0^{-1}\theta \;+\; m_0^\top S_0^{-1}\theta \;-\; \tfrac{1}{2}m_0^\top S_0^{-1} m_0 + \text{const}。 \tag{9.46a}\]</span> 整理后得到： <span class="math display">\[= -\tfrac{1}{2}\theta^\top(\sigma^{-2}\Phi^\top \Phi + S_0^{-1})\theta \;+\; (\sigma^{-2}\Phi^\top y + S_0^{-1}m_0)^\top \theta + \text{const}, \tag{9.46b}\]</span> 显然这是关于 <span class="math inline">\(\theta\)</span> 的二次型。未归一化的对数后验是一个负二次型，因此后验必然是高斯分布：</p><p>$$ <span class="math display">\[\begin{align}&amp; p(\theta \mid X,Y) = \exp(\log p(\theta \mid X,Y)) \propto \exp(\log p(Y \mid X,\theta) + \log p(\theta)) \tag{9.47a} \\&amp; \propto \exp\!\left(-\tfrac{1}{2}\theta^\top (\sigma^{-2}\Phi^\top \Phi + S_0^{-1})\theta \;+\; (\sigma^{-2}\Phi^\top y + S_0^{-1}m_0)^\top \theta \right), \tag{9.47b}\end{align}\]</span> $$</p><p>由此可见，后验分布为高斯分布，其均值与协方差正是 (9.43b) 和 (9.43c) 中所给。剩下的任务就是将这个（未标准化的）高斯分布写成与<span class="math inline">\(\mathcal{N}(\theta \mid m_N, S_N)\)</span>成比例的形式，也就是说，我们需要确定均值 <span class="math inline">\(m_N\)</span> 和协方差矩阵 <span class="math inline">\(S_N\)</span>。为此，我们使用配方（completing the squares）的技巧。所需的对数后验分布为</p><p>$$ <span class="math display">\[\begin{align}\log \mathcal{N}(\theta \mid m_N, S_N) &amp; = -\tfrac{1}{2} (\theta - m_N)^\top S_N^{-1} (\theta - m_N) + \text{const} \tag{9.48a} \\&amp; = -\tfrac{1}{2}\theta^\top S_N^{-1}\theta - m_N^\top S_N^{-1}\theta + \tfrac{1}{2} m_N^\top S_N^{-1} m_N. \tag{9.48b}\end{align}\]</span> $$</p><p>在这里，我们将二次型 <span class="math inline">\((\theta - m_N)^\top S_N^{-1} (\theta - m_N)\)</span> 拆分成三类项：<strong>关于 <span class="math inline">\(\theta\)</span> 的二次项</strong>（蓝色），<strong>关于 <span class="math inline">\(\theta\)</span> 的一次项</strong>（橙色），<strong>常数项</strong>（黑色）。(个人注：原书的公式是标了颜色。)这样我们就可以通过在 (9.46b) 和 (9.48b) 中对比相同颜色的项来得到 <span class="math inline">\(S_N\)</span> 和 <span class="math inline">\(m_N\)</span>：</p><p>$$ <span class="math display">\[\begin{align}&amp; \quad S_N^{-1} = \Phi^\top \sigma^{-2} I \Phi + S_0^{-1} \tag{9.49a} \\\Longleftrightarrow &amp; \quad S_N = \left(\sigma^{-2}\Phi^\top \Phi + S_0^{-1}\right)^{-1} \tag{9.49b}\end{align}\]</span> $$</p><p>并且有</p><p>$$ <span class="math display">\[\begin{align}&amp; \quad m_N^\top S_N^{-1} = \left(\sigma^{-2}\Phi^\top y + S_0^{-1} m_0 \right)^\top \tag{9.50a} \\\Longleftrightarrow \quad &amp; \quad m_N = S_N \left(\sigma^{-2}\Phi^\top y + S_0^{-1} m_0 \right). \tag{9.50b}\end{align}\]</span> $$</p><p><strong>注释（配方法的一般方法）</strong>。如果我们有如下形式的等式： <span class="math display">\[x^\top A x - 2 a^\top x + \text{const}_1 , \tag{9.51}\]</span></p><p>其中 <span class="math inline">\(A\)</span> 是对称且正定的，我们希望将其改写成如下形式：</p><p><span class="math display">\[(x - \mu)^\top \Sigma (x - \mu) + \text{const}_2 , \tag{9.52}\]</span></p><p>那么我们可以通过以下设定来实现：</p><p><span class="math display">\[\Sigma := A, \tag{9.53}\]</span></p><p><span class="math display">\[\mu := \Sigma^{-1} a, \tag{9.54}\]</span></p><p>并且</p><p><span class="math display">\[\text{const}_2 = \text{const}_1 - \mu^\top \Sigma \mu. \quad \]</span></p><p>我们可以看到，(9.47b) 中指数里的项正是 (9.51) 的形式，其中：</p><p>$$ <span class="math display">\[\begin{align}A &amp; := \sigma^{-2}\Phi^\top \Phi + S_0^{-1}, \tag{9.55}\\a &amp; := \sigma^{-2}\Phi^\top y + S_0^{-1} m_0. \tag{9.56}\end{align}\]</span> $$</p><p>由于在像 (9.46a) 这样的等式中，识别 <span class="math inline">\(A, a\)</span> 可能比较困难，因此把这些等式改写为 (9.51) 的形式通常是有帮助的。这样可以把二次项、一次项和常数项分离开，从而简化所需解的推导。</p><h4 id="后验预测">9.3.4 后验预测</h4><p>在式 (9.37) 中，我们利用参数先验 <span class="math inline">\(p(\theta)\)</span> 计算了测试输入 <span class="math inline">\(x_*\)</span> 处的预测分布 <span class="math inline">\(y_*\)</span>。原则上，使用参数后验 <span class="math inline">\(p(\theta \mid X, Y)\)</span> 进行预测并没有本质上的区别，因为在我们的共轭模型中，先验和后验都是高斯分布（只是参数不同）。因此，沿用第 9.3.2 节中的相同推理，我们得到（后验）预测分布：</p><p>$$ <span class="math display">\[\begin{align}p(y_* \mid X, Y, x_*) &amp; = \int p(y_* \mid x_*, \theta)p(\theta \mid X, Y)\, d\theta \tag{9.57a} \\&amp; = \int \mathcal{N}\!\big(y_* \mid \phi^\top(x_*) \theta, \sigma^2\big)\, \mathcal{N}\!\big(\theta \mid m_N, S_N\big)\, d\theta \tag{9.57b} \\&amp; = \mathcal{N}\!\Big(y_* \mid \phi^\top(x_*) m_N,\; \phi^\top(x_*) S_N \phi(x_*) + \sigma^2\Big). \tag{9.57c}\end{align}\]</span> $$</p><p>其中，项 <span class="math inline">\(\phi^\top(x_*) S_N \phi(x_*)\)</span> 反映了与参数 <span class="math inline">\(\theta\)</span> 相关的后验不确定性。注意，<span class="math inline">\(S_N\)</span> 依赖于训练输入（通过 <span class="math inline">\(\Phi\)</span>）；参见式 (9.43b)。预测均值 <span class="math inline">\(\phi^\top(x_*) m_N\)</span> 与最大后验估计（MAP 估计）<span class="math inline">\(\theta_{\text{MAP}}\)</span> 的预测结果一致。</p><p><strong>注释（边际似然与后验预测分布）</strong>。通过替换式 (9.57a) 中的积分，预测分布也可以等价地写为： <span class="math display">\[\mathbb{E}_{\theta \mid X,Y}[p(y_* \mid x_*, \theta)],\]</span></p><p>其中期望是关于参数后验 <span class="math inline">\(p(\theta \mid X,Y)\)</span> 进行的。这样表述后验预测分布凸显了其与边际似然 (9.42) 的紧密联系。两者的关键区别在于：</p><ol type="1"><li>边际似然可以被看作是在预测训练目标 <span class="math inline">\(y\)</span>，而不是测试目标 <span class="math inline">\(y_*\)</span>；</li><li>边际似然是对参数先验进行平均，而后验预测分布是对参数后验进行平均。</li></ol><p><strong>注释（无噪声函数值的均值与方差）</strong>。在很多情况下，我们并不关心（有噪声的）观测 <span class="math inline">\(y_*\)</span> 的预测分布 <span class="math inline">\(p(y_* \mid X,Y,x_*)\)</span>，而是希望得到（无噪声的）函数值 <span class="math display">\[f(x_*) = \phi^\top(x_*) \theta\]</span></p><p>的分布。我们可以利用均值与方差的性质计算其对应矩，从而得到：</p><p>$$ <span class="math display">\[\begin{align}\mathbb{E}[f(x_*) \mid X,Y] &amp; = \mathbb{E}_\theta[\phi^\top(x_*) \theta \mid X,Y]   = \phi^\top(x_*) \mathbb{E}_\theta[\theta \mid X,Y] \tag{9.58} \\&amp; = \phi^\top(x_*) m_N = m_N^\top \phi(x_*),\end{align}\]</span> $$</p><p>$$ <span class="math display">\[\begin{align}\mathbb{V}_\theta[f(x_*) \mid X,Y] &amp; = \mathbb{V}_\theta[\phi^\top(x_*) \theta \mid X,Y] \tag{9.59} \\&amp; = \phi^\top(x_*) \mathbb{V}_\theta[\theta \mid X,Y] \phi(x_*)  \\&amp; = \phi^\top(x_*) S_N \phi(x_*).\end{align}\]</span> $$</p><p>我们看到，由于噪声的均值为 0，预测均值与含噪观测的预测均值相同；预测方差仅在于多了一个 σ² 的差异，这个 σ² 就是测量噪声的方差：当我们预测含噪的函数值时，需要将 σ² 作为不确定性的来源纳入考虑，但在无噪声预测中则不需要这一项。在这里，唯一剩下的不确定性来自于参数的后验分布。</p><p><strong>备注（函数上的分布）</strong>：我们将参数 θ 积分消去的事实，诱导出了函数上的一个分布：如果我们从参数后验分布 <span class="math inline">\(p(\theta \mid X,Y)\)</span> 中采样 <span class="math inline">\(\theta_i\)</span>，那么我们得到的就是一个函数实现 <span class="math inline">\(\theta_i^\top \phi(\cdot)\)</span>。这个函数分布的均值函数（即所有期望函数值 <span class="math inline">\(\mathbb{E}_\theta[f(\cdot)\mid \theta,X,Y]\)</span> 的集合）为 <span class="math inline">\(m_N^\top \phi(\cdot)\)</span>。其（边际）方差，即函数 <span class="math inline">\(f(\cdot)\)</span> 的方差，则由 <span class="math inline">\(\phi^\top(\cdot) S_N \phi(\cdot)\)</span> 给出。</p><p><strong>例 9.8（函数上的后验分布）</strong> 让我们重新考察多项式次数为 5 的贝叶斯线性回归问题。我们选择参数先验为</p><p><span class="math display">\[p(\theta) = \mathcal{N}(0, \tfrac{1}{4}I).\]</span></p><p>图 9.9 展示了由该参数先验诱导的函数先验分布，以及从该先验中采样得到的一些函数。</p><p><img src="/img3/机器学习的数学基础Part2/F9.10.png" alt="F9.10" style="zoom:67%;" /></p><p>图 9.10 展示了通过 贝叶斯线性回归 得到的函数后验分布。面板 (a) 显示了训练数据集；面板 (b) 展示了函数的后验分布，其中包括通过最大似然估计 (MLE) 和最大后验估计 (MAP) 得到的函数。在贝叶斯线性回归中，通过 MAP 得到的函数也对应于 <strong>后验均值函数</strong>。面板 (c) 展示了在该函数后验分布下可能出现的一些函数实例（样本）。</p><p><img src="/img3/机器学习的数学基础Part2/F9.11.png" alt="F9.11" style="zoom:67%;" /></p><p>图 9.11 展示了由参数后验分布诱导出的若干函数后验分布。对于不同的多项式阶数 <span class="math inline">\(M\)</span>，左侧面板展示了最大似然函数 <span class="math inline">\(\theta^\top_{\text{ML}}\phi(\cdot)\)</span>、MAP 函数 <span class="math inline">\(\theta^\top_{\text{MAP}}\phi(\cdot)\)</span>（它与后验均值函数相同），以及通过贝叶斯线性回归得到的 67% 和 95% 的预测置信区间（用阴影区域表示）。</p><p>右侧面板展示了函数后验分布中的一些采样：在这里，我们从参数后验分布中采样参数 <span class="math inline">\(\theta_i\)</span>，并计算函数 <span class="math inline">\(\phi^\top(x^*)\theta_i\)</span>，这对应于后验分布下的某个函数实例。对于低阶多项式，参数后验分布限制了参数的变化范围，因此采样得到的函数几乎相同。当我们通过增加更多参数使模型更灵活（即更高阶的多项式）时，这些参数在后验分布中受到的约束不足，因而采样的函数可以被轻易区分开来。我们还可以在左侧对应的面板中看到，不确定性随之增加，特别是在边界区域。</p><p><strong>尽管对于七阶多项式，MAP 估计能够给出一个合理的拟合，但贝叶斯线性回归模型还额外告诉我们：后验不确定性非常大。在将这些预测用于决策系统时，这一点信息可能至关重要，因为错误的决策可能会带来严重后果（例如在强化学习或机器人应用中）。</strong></p><blockquote><p>个人注：贝叶斯线性回归还给出不确定性的情况！</p></blockquote><h4 id="计算边际似然">9.3.5 计算边际似然</h4><p>在 第 8.6.2 节 中，我们强调了边际似然在贝叶斯模型选择中的重要性。接下来，我们将在带有共轭高斯先验的参数化贝叶斯线性回归模型中计算边际似然，也就是本章所讨论的设定。</p><p>回顾一下，我们考虑如下的生成过程： $$ <span class="math display">\[\begin{align}\theta &amp; \sim \mathcal{N}(m_0, S_0) \tag{9.60a} \\y_n | x_n, \theta &amp; \sim \mathcal{N}(x_n^\top \theta, \sigma^2), \quad n = 1, \ldots, N. \tag{9.60b}\end{align}\]</span> $$</p><p>边际似然由下式给出：</p><p>$$ <span class="math display">\[\begin{align}p(Y|X) &amp; = \int p(Y|X, \theta)\,p(\theta)\,d\theta \tag{9.61a} \\&amp; = \int \mathcal{N}(y|X\theta, \sigma^2 I)\,\mathcal{N}(\theta|m_0, S_0)\, d\theta, \tag{9.61b}\end{align}\]</span> $$</p><p>其中我们对模型参数 <span class="math inline">\(\theta\)</span> 进行了积分。我们分两步计算边际似然：</p><ol type="1"><li><strong>边际似然是高斯分布</strong></li></ol><p>在 第 6.5.2 节 中，我们知道：</p><ol type="i"><li>两个高斯随机变量的乘积仍然是（未归一化的）高斯分布；</li><li>高斯随机变量的线性变换仍然是高斯分布。</li></ol><p>在 (9.61b) 中，我们需要将 <span class="math inline">\(\mathcal{N}(y|X\theta, \sigma^2 I)\)</span> 转换为某个形式 <span class="math inline">\(\mathcal{N}(\theta|\mu, \Sigma)\)</span>。一旦完成转换，该积分就可以闭式解出，结果就是这两个高斯分布乘积的归一化常数。而归一化常数本身呈现高斯分布形式（见公式 (6.76)）。</p><ol start="2" type="1"><li><strong>均值和协方差</strong></li></ol><p>我们利用仿射变换下随机变量的均值和协方差的标准结果（见第 6.4.4 节）来计算边际似然的均值与协方差矩阵。</p><p>边际似然的均值为： <span class="math display">\[\mathbb{E}[Y|X] = \mathbb{E}_{\theta,\epsilon}[X\theta + \epsilon] = X\mathbb{E}_\theta[\theta] = Xm_0. \tag{9.62}\]</span></p><p>其中 <span class="math inline">\(\epsilon \sim \mathcal{N}(0, \sigma^2 I)\)</span> 是一个独立同分布的随机向量。</p><p>协方差矩阵为： <span class="math display">\[\begin{align}\mathrm{Cov}[Y|X] &amp; = \mathrm{Cov}_{\theta,\epsilon}[X\theta + \epsilon] = \mathrm{Cov}_\theta[X\theta] + \sigma^2 I  \tag{9.63a}\\&amp; = X\mathrm{Cov}_\theta[\theta]X^\top + \sigma^2 I = XS_0X^\top + \sigma^2 I. \tag{9.63b}\end{align}\]</span></p><p>因此，边际似然为：</p><p>$$ <span class="math display">\[\begin{align}p(Y|X) &amp; = (2\pi)^{-\frac{N}{2}} \det(XS_0X^\top + \sigma^2 I)^{-\frac{1}{2}} \exp\!\Bigg(-\tfrac{1}{2}(y - Xm_0)^\top (XS_0X^\top + \sigma^2 I)^{-1} (y - Xm_0)\Bigg) \tag{9.64a} \\&amp; = \mathcal{N}\big(y \,|\, Xm_0,\, XS_0X^\top + \sigma^2 I\big). \tag{9.64b}\end{align}\]</span> $$</p><p>鉴于其与 后验预测分布 的紧密联系（参见本节前面的备注 <em>“边际似然与后验预测分布”</em>），边际似然的函数形式并不令人意外。</p><h3 id="最大似然作为正交投影">9.4 最大似然作为正交投影</h3><p>在通过大量代数运算推导出最大似然和 MAP（最大后验）估计之后，我们现在给出最大似然估计的一种几何解释。考虑一个简单的线性回归情形：</p><p><span class="math display">\[y = x\theta + \epsilon,\quad \epsilon \sim \mathcal{N}(0, \sigma^2), \tag{9.65}\]</span></p><p>其中我们考虑通过原点的线性函数 <span class="math inline">\(f: \mathbb{R} \to \mathbb{R}\)</span>（为了简洁这里省略了特征）。参数 <span class="math inline">\(\theta\)</span> 决定了直线的斜率。图 9.12(a) 显示了一个一维数据集。</p><p><img src="/img3/机器学习的数学基础Part2/F9.12.png" alt="F9.12" style="zoom:50%;" /></p><p>给定训练数据集 <span class="math inline">\(\{(x_1,y_1),\dots,(x_N,y_N)\}\)</span>，回顾 9.2.1 节的结果，可以得到斜率参数的最大似然估计：</p><p><span class="math display">\[\theta_{\text{ML}} = (X^\top X)^{-1}X^\top y = \frac{X^\top y}{X^\top X} \in \mathbb{R}, \tag{9.66}\]</span></p><p>其中 <span class="math inline">\(X = [x_1,\dots,x_N]^\top \in \mathbb{R}^N, \; y = [y_1,\dots,y_N]^\top \in \mathbb{R}^N\)</span>。</p><blockquote><p>个人注：相当于在一个<span class="math inline">\(n\)</span>维空间中，向量<span class="math inline">\(y\)</span>投影到向量<span class="math inline">\(X\)</span>上，因为投影<span class="math inline">\(\bar y\)</span>是落在向量<span class="math inline">\(X\)</span>上，故<span class="math inline">\(\bar{y}_i= ax_i\)</span>；从而拟合的曲线在二维空间中是<span class="math inline">\(\bar y=ax\)</span>；具体详见 “3.8.1 一维子空间（线）上的投影” 的公式（ 3.42）</p></blockquote><p>这意味着对于训练输入 <span class="math inline">\(X\)</span>，我们得到对训练目标的最优（最大似然）重建为： <span class="math display">\[X\theta_{\text{ML}} = X\frac{X^\top y}{X^\top X} = \frac{XX^\top}{X^\top X}y, \tag{9.67}\]</span></p><p>即得到在 <span class="math inline">\(y\)</span> 和 <span class="math inline">\(X\theta\)</span> 之间最小化平方误差的近似。</p><p>由于我们正在寻找方程 <span class="math inline">\(y = X\theta\)</span> 的解，可以将线性回归看作是解线性方程组的问题。因此，我们可以联系第 2、3 章中讨论的线性代数和解析几何概念。特别是仔细观察 (9.67)，可以看到在 (9.65) 的例子中，<strong>最大似然估计 <span class="math inline">\(\theta_{\text{ML}}\)</span> 实际上是将 <span class="math inline">\(y\)</span> 正交投影到由 <span class="math inline">\(X\)</span> 张成的一维子空间上。</strong>回顾 3.8 节关于正交投影的结果，我们可以识别出 <span class="math inline">\(\tfrac{XX^\top}{X^\top X}\)</span> 是投影矩阵，<strong><span class="math inline">\(\theta_{\text{ML}}\)</span> 是投影到 <span class="math inline">\(X\)</span> 张成的一维子空间上的坐标，而 <span class="math inline">\(X\theta_{\text{ML}}\)</span> 则是 <span class="math inline">\(y\)</span> 在该子空间上的正交投影。</strong></p><p>因此，最大似然解也提供了几何上最优的解，它找到由 <span class="math inline">\(X\)</span> 张成的子空间中“最接近”观测值 <span class="math inline">\(y\)</span> 的向量，其中“最接近”指的是函数值 <span class="math inline">\(y_n\)</span> 与 <span class="math inline">\(x_n\theta\)</span> 之间平方距离最小。这一过程是通过正交投影实现的。图 9.12(b) 显示了带噪观测值投影到子空间上的情况，它最小化了原始数据集与投影之间的平方距离（注意 <span class="math inline">\(x\)</span>-坐标是固定的），这正对应于最大似然解。</p><p>在一般的线性回归情形下：</p><p><span class="math display">\[y = \phi^\top(x)\theta + \epsilon,\quad \epsilon \sim \mathcal{N}(0,\sigma^2), \tag{9.68}\]</span></p><p>其中 <span class="math inline">\(\phi(x) \in \mathbb{R}^K\)</span> 是向量值特征，我们同样可以将最大似然结果解释为：</p><p><span class="math display">\[y \approx \Phi \theta_{\text{ML}}, \tag{9.69}\]</span></p><p><span class="math display">\[\theta_{\text{ML}} = (\Phi^\top \Phi)^{-1}\Phi^\top y, \tag{9.70}\]</span></p><p>即投影到由特征矩阵 <span class="math inline">\(\Phi\)</span> 的列所张成的 <span class="math inline">\(\mathbb{R}^N\)</span> 的 <span class="math inline">\(K\)</span> 维子空间上；参见 3.8.2 节。</p><blockquote><p>个人注：特征矩阵详见 “9.2.1 极大似然估计”的公式（9.16）。</p></blockquote><p>如果我们用来构造特征矩阵 <span class="math inline">\(\Phi\)</span> 的特征函数 <span class="math inline">\(\phi_k\)</span> 是正交归一的（见 3.7 节），那么我们得到一种特殊情况，此时 <span class="math inline">\(\Phi\)</span> 的列构成一个正交归一基（见 3.5 节），因而：</p><p><span class="math display">\[\Phi^\top \Phi = I.\]</span></p><p>这将导致投影：</p><p><span class="math display">\[\Phi(\Phi^\top \Phi)^{-1}\Phi^\top y = \Phi \Phi^\top y = \left ( \sum_{k=1}^K \phi_k \phi_k^\top \right ) y, \tag{9.71}\]</span></p><p>因此最大似然投影就只是将 <span class="math inline">\(y\)</span> 投影到各个基向量 <span class="math inline">\(\phi_k\)</span>（即 <span class="math inline">\(\Phi\)</span> 的列）上的投影之和。进一步地，由于基是正交的，不同特征之间的耦合消失了。<strong>信号处理中许多常见的基函数（如小波和傅里叶基）就是正交基函数。</strong></p><p>当基不是正交的时，可以通过 Gram-Schmidt 过程将一组线性无关的基函数转换为正交基；参见 3.8.3 节和 Strang (2003)。</p><h3 id="延伸阅读">9.5 延伸阅读</h3><p>在本章中，我们讨论了针对高斯似然以及模型参数的共轭高斯先验的线性回归。这使得贝叶斯推断可以得到闭式解。然而，在一些应用中，我们可能希望选择不同的似然函数。例如，在二分类问题中，我们只观察到两个可能的（类别型）结果，此时使用高斯似然并不合适。相反，我们可以选择 伯努利（Bernoulli）似然，它会返回预测标签为 1（或 0）的概率。关于分类问题的深入介绍，可参见 Barber (2012)、Bishop (2006) 和 Murphy (2012) 的著作。另一个非高斯似然重要的例子是 计数数据。计数是非负整数，在这种情况下，二项分布或泊松分布的似然比高斯似然更合适。<strong>所有这些例子都属于 广义线性模型（GLM） 的范畴，GLM 是线性回归的一种灵活推广，允许响应变量具有非高斯的误差分布。</strong>GLM 推广了线性回归，使得线性模型可以通过一个光滑且可逆的函数 <span class="math inline">\(\sigma(\cdot)\)</span>（可能是非线性的）与观测值相关联，因此有</p><p><span class="math display">\[y = \sigma(f(x)), \quad f(x) = \theta^\top \phi(x),\]</span></p><p>其中 <span class="math inline">\(f(x)\)</span> 就是 (9.13) 中的线性回归模型。换句话说，可以将<strong>广义线性模型看作函数复合：</strong></p><p><span class="math display">\[y = \sigma \circ f,\]</span></p><p><strong>其中 <span class="math inline">\(f\)</span> 是线性回归模型，<u><span class="math inline">\(\sigma\)</span> 是激活函数</u>（activation function）。需要注意的是，尽管我们称其为“广义线性模型”，但输出 <span class="math inline">\(y\)</span> 不再是参数 <span class="math inline">\(\theta\)</span> 的线性函数。</strong></p><p>在逻辑回归中，我们选择逻辑 sigmoid 函数</p><p><span class="math display">\[\sigma(f) = \frac{1}{1 + \exp(-f)} \in [0,1],\]</span></p><p>它可以解释为伯努利随机变量 <span class="math inline">\(y \in \{0,1\}\)</span> 中观测到 <span class="math inline">\(y=1\)</span> 的概率。函数 <span class="math inline">\(\sigma(\cdot)\)</span> 被称为 传递函数（transfer function）或激活函数（activation function），其逆函数被称为 <strong>规范链接函数（canonical link function）</strong>。从这个角度也可以清楚地看到，广义线性模型是（深度）前馈神经网络的构建模块：如果我们考虑一个广义线性模型</p><p><span class="math display">\[y = \sigma(Ax + b),\]</span></p><p>其中 <span class="math inline">\(A\)</span> 是权重矩阵，<span class="math inline">\(b\)</span> 是偏置向量，我们就可以将该广义线性模型识别为一个带有激活函数 <span class="math inline">\(\sigma(\cdot)\)</span> 的单层神经网络。我们可以递归地复合这些函数：</p><p><span class="math display">\[x_{k+1} = f_k(x_k), \quad f_k(x_k) = \sigma_k(A_k x_k + b_k), \quad k = 0, \dots, K-1, \tag{9.72}\]</span></p><p>其中 <span class="math inline">\(x_0\)</span> 是输入特征，<span class="math inline">\(x_K = y\)</span> 是观测输出，这样 <span class="math inline">\(f_{K-1}\circ \cdots \circ f_0\)</span> 就是一个 <span class="math inline">\(K\)</span> 层深度神经网络。因此，这个深度神经网络的基本构建模块正是 (9.72) 中定义的广义线性模型。神经网络（Bishop, 1995; Goodfellow 等, 2016）比线性回归模型更具表现力和灵活性。然而，最大似然参数估计是一个 <strong>非凸优化问题</strong>，在完全贝叶斯的框架下对参数进行边缘化在解析上是不可行的。</p><p>我们曾简要提到过，参数上的分布会诱导出回归函数上的分布。<strong>高斯过程（Gaussian process）</strong>（Rasmussen 和 Williams, 2006）是一种以函数分布为核心的回归模型。高斯过程不是在参数上放置分布，而是直接在函数空间上放置分布，从而避免了“绕道”参数空间。为了实现这一点，高斯过程利用了 <strong>核技巧（kernel trick）</strong>（Schölkopf 和 Smola, 2002），它允许我们仅通过查看输入 <span class="math inline">\(x_i, x_j\)</span>，就能计算函数值 <span class="math inline">\(f(x_i), f(x_j)\)</span> 之间的内积。高斯过程与贝叶斯线性回归和支持向量回归密切相关，但也可以解释为一个单隐层的贝叶斯神经网络，其单元数趋于无穷（Neal, 1996; Williams, 1997）。优秀的高斯过程入门资料可参见 MacKay (1998) 和 Rasmussen 与 Williams (2006)。</p><p>在本章的讨论中，我们专注于高斯先验，因为它能使线性回归模型的推断得到闭式解。然而，即使在具有高斯似然的回归设定中，我们也可能选择非高斯先验。考虑一种情形：输入为 <span class="math inline">\(x \in \mathbb{R}^D\)</span>，但训练集规模很小，<span class="math inline">\(N \ll D\)</span>。这意味着回归问题是 <strong>欠定的</strong>。在这种情况下，我们可以选择一种强制稀疏性的参数先验，即尽可能将更多参数压缩为 0 的先验（变量选择）。这种先验比高斯先验提供了更强的正则化，通常会带来更高的预测准确率和更好的模型可解释性。</p><p><strong>拉普拉斯先验（Laplace prior）</strong> 是一个常用的例子。带有拉普拉斯先验的线性回归模型等价于带 <span class="math inline">\(L_1\)</span> 正则化的线性回归（LASSO）（Tibshirani, 1996）。拉普拉斯分布在零点处尖锐（其一阶导数不连续），并且将概率质量更集中在零附近，相比高斯分布更鼓励参数为零。因此，非零参数对于回归问题来说才是重要的，这也正是我们称之为“变量选择”的原因。</p>]]></content>
      
      
      <categories>
          
          <category> 翻译 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 数学 </tag>
            
            <tag> 算法 </tag>
            
            <tag> 统计 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《机器学习的数学基础》第8章&quot;当模型遇到数据&quot;</title>
      <link href="/2025/09/09/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E7%AC%AC8%E7%AB%A0%22%E5%BD%93%E6%A8%A1%E5%9E%8B%E9%81%87%E5%88%B0%E6%95%B0%E6%8D%AE%22/"/>
      <url>/2025/09/09/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E7%AC%AC8%E7%AB%A0%22%E5%BD%93%E6%A8%A1%E5%9E%8B%E9%81%87%E5%88%B0%E6%95%B0%E6%8D%AE%22/</url>
      
        <content type="html"><![CDATA[<h2 id="第8章-当模型遇到数据">第8章 当模型遇到数据</h2><p>在本书的第一部分，我们介绍了许多机器学习方法所依赖的数学基础。希望读者能够从第一部分学习到数学语言的基本形式，而我们接下来将利用这些形式来描述和讨论机器学习。本书的第二部分介绍了机器学习的四大支柱：</p><ul><li>回归（第9章）</li><li>降维（第10章）</li><li>密度估计（第11章）</li><li>分类（第12章）</li></ul><p>本书这一部分的主要目标是展示如何利用第一部分介绍的数学概念来设计机器学习算法，从而解决四大支柱范围内的任务。我们无意介绍高级的机器学习概念，而是希望提供一套实用的方法，使读者能够运用第一部分所学的知识。同时，这部分内容也为已经熟悉数学的读者提供了进入更广泛机器学习文献的入口。</p><span id="more"></span><h3 id="数据模型与学习">8.1 数据、模型与学习</h3><p>在这里，我们有必要停下来思考一下：机器学习算法究竟是为了解决什么问题。正如在第 1 章中所讨论的，一个机器学习系统主要由三个部分组成：<strong>数据</strong>、<strong>模型</strong>和<strong>学习</strong>。机器学习的核心问题是：<strong>“什么才是好的模型？”</strong></p><p>“模型”这个词有许多微妙的含义，我们将在本章中多次回顾它。而“好”的定义也并非显而易见。机器学习的一个基本指导原则是：<strong>好的模型应当在未见过的数据上也能表现良好</strong>。这要求我们定义一些性能度量标准，比如准确率或与真实值的距离，并找到在这些标准下表现良好的方法。</p><p>本章将介绍一些必要的数学与统计语言，这些语言通常用于讨论机器学习模型。通过这些内容，我们将简要勾勒如何训练模型的最佳实践，从而使得得到的预测器能够在尚未见过的数据上依然有效。</p><p>正如在第 1 章中提到的，“机器学习算法”这一表述有两层含义：<strong>训练</strong>与<strong>预测</strong>。在本章中，我们将描述这两个概念，以及<strong>在不同模型之间进行选择</strong>的思想。我们将在 <strong>8.2 节</strong>介绍<strong>经验风险最小化</strong>框架，在 <strong>8.3 节</strong>介绍<strong>极大似然原理</strong>，在 <strong>8.4 节</strong>介绍<strong>概率模型</strong>。在 <strong>8.5 节</strong>中，我们将简要概述一种用于指定概率模型的图形化语言，并在 <strong>8.6 节</strong>中讨论<strong>模型选择</strong>。</p><p>本节的剩余部分将进一步展开机器学习的三个核心组成部分：<strong>数据</strong>、<strong>模型</strong>和<strong>学习</strong>。</p><h4 id="数据作为向量">8.1.1 数据作为向量</h4><p>我们假设数据可以被计算机读取，并且能够以数值格式恰当地表示。数据被假设为表格形式（图 8.1），其中我们认为表格的每一行代表一个特定的实例或样本，而每一列对应一个特定的特征。</p><p>近年来，机器学习已经应用于许多并非显然以数值表格形式存在的数据，例如基因序列、网页的文本和图像内容，以及社交媒体图谱。我们在此不讨论<strong>如何识别出良好特征这一重要而具有挑战性的方面。这些方面中的许多依赖于领域专业知识并需要精心设计，而近年来它们被纳入了数据科学的范畴（</strong>Stray, 2016; Adhikari 和 DeNero, 2018）。</p><p>即使我们已经拥有表格格式的数据，仍然需要做出一些选择以获得数值表示。例如，在表 8.1 中，性别这一列（一个分类变量）可以被转换为数字，其中 0 表示“男性”，1 表示“女性”。另一种方法是将性别表示为 −1 和 +1（如表 8.2 所示）。此外，在构建表示时，利用领域知识往往十分重要，例如知道大学学位是从学士到硕士再到博士逐级进展，或者意识到邮编不仅仅是字符的组合，而是实际编码了伦敦的某个区域。在表 8.2 中，我们将表 8.1 的数据转换为数值格式，每个邮编被表示为两个数字：一个纬度和一个经度。</p><p><img src="/img3/机器学习的数学基础Part2/T8.1.png" alt="T8.1" style="zoom:50%;" /></p><p><img src="/img3/机器学习的数学基础Part2/T8.2.png" alt="T8.2" style="zoom:50%;" /></p><p>即便是那些可以直接输入机器学习算法的数值数据，也应当仔细考虑其单位、缩放方式和约束条件。在没有额外信息的情况下，应当将数据集的所有列进行平移和缩放，使其经验均值为 0，经验方差为 1。为了本书的讨论，我们假设领域专家已经适当地完成了数据转换，即每个输入 <span class="math inline">\(x_n\)</span> 是一个 <span class="math inline">\(D\)</span> 维实数向量，被称为特征、属性或协变量。我们认为数据集的形式如表 8.2 所示。</p><p>请注意，在新的数值表示中，我们去掉了表 8.1 中的“姓名”这一列。这么做有两个主要原因：(1) 我们不认为标识符（姓名）对机器学习任务有帮助；(2) 我们可能希望对数据进行匿名化，以帮助保护员工的隐私。</p><p>在本书的这一部分，我们用 <span class="math inline">\(N\)</span>表示数据集中样本的数量，并用小写字母$ n = 1, …, N$ 来索引这些样本。我们假设给定了一组数值型数据，表示为一个向量数组（见表 8.2）。每一行代表一个个体 <span class="math inline">\(x_n\)</span>，在机器学习中通常称为样本或数据点。下标 <span class="math inline">\(n\)</span>表示这是数据集中第<span class="math inline">\(n\)</span>个样本，总共有<span class="math inline">\(N\)</span>个样本。每一列代表该样本的一个特征，我们将特征索引为 <span class="math inline">\(d = 1, …, D\)</span>。回忆一下，数据是用向量表示的，这意味着每个样本（每个数据点）是一个 <span class="math inline">\(D\)</span>维向量。<strong>这种表格的排列方式来源于数据库领域，但在某些机器学习算法（例如第 10 章中的方法）中，将样本表示为列向量会更加方便。</strong></p><p>让我们考虑一个根据年龄预测年薪的问题，基于表 8.2 中的数据。这就是一个监督学习问题，其中每个样本 <span class="math inline">\(xₙ\)</span>（年龄）都有一个对应的标签 <span class="math inline">\(yₙ\)</span>（薪水）。标签 <span class="math inline">\(yₙ\)</span> 还有其他叫法，包括<strong>目标值（target）</strong>、<strong>响应变量（response variable）和注释（annotation）</strong>。一个数据集可以写成一组样本-标签对的集合： <span class="math inline">\({(x_1, y_2), …, (x_n, y_n), …, (x_N, y_N)}\)</span>。样本表$ {x₁, …, x_N} $通常会拼接在一起，写作 <span class="math inline">\(X ∈ ℝ^{N*D}\)</span>。图 8.1 展示了由表 8.2 最右两列构成的数据集，其中 <span class="math inline">\(x\)</span> = 年龄，<span class="math inline">\(y\)</span> = 薪水。</p><figure><img src="/img3/机器学习的数学基础Part2/F8.1.png" alt="F8.1" /><figcaption aria-hidden="true">F8.1</figcaption></figure><p>我们使用本书第一部分介绍的概念来形式化机器学习问题，例如上一段中提到的那种问题。 将数据表示为向量 <span class="math inline">\(x_n\)</span> 使我们能够使用线性代数中的概念（在第 2 章介绍）。在许多机器学习算法中，我们还需要能够比较两个向量。正如我们将在第 9 章和第 12 章看到的那样，计算两个样本之间的相似性或距离，可以帮助我们形式化这样一种直觉：具有相似特征的样本应当有相似的标签。比较两个向量需要我们构建一个几何结构（在第 3 章解释），并使我们能够使用第 7 章中的技术来优化由此产生的学习问题。</p><p><strong>既然我们已经有了数据的向量表示，那么我们就可以对数据进行操作，以寻找潜在的更优表示。我们将通过两种方式来讨论如何找到好的表示：一种是寻找原始特征向量的低维近似，另一种是利用原始特征向量的非线性高维组合。</strong></p><p>在第10章中，我们将看到通过寻找<strong>主成分来得到原始数据空间的低维近似的例子</strong>。寻找主成分与第4章介绍的特征值分解和奇异值分解的概念密切相关。</p><p>对于高维表示，我们将看到一个显式的特征映射 φ(·)，它允许我们将输入向量 <span class="math inline">\(x_n\)</span> 表示为一个高维表示 <span class="math inline">\(\phi(x_n)\)</span>。高维表示的主要动机在于：我们可以<strong>将原始特征的非线性组合构造成新的特征，而这反过来可能会使学习问题变得更容易</strong>。我们将在第9.2节中讨论特征映射，并在第12.4节中展示<strong>这种特征映射是如何引出核函数的</strong>。</p><p>近年来，深度学习方法（Goodfellow 等，2016）在利用数据本身来学习新的、更优的特征方面展现出了巨大潜力，并在计算机视觉、语音识别和自然语言处理等领域取得了非常成功的应用。本书的这一部分不会涉及神经网络，但读者可以参考第5.6节，其中包含了关于反向传播的数学描述，这是训练神经网络的关键概念。</p><h4 id="模型作为函数">8.1.2 模型作为函数</h4><p>一旦我们将数据转换为合适的向量表示，就可以着手构建一个预测函数（predictive function）（称为预测器）。</p><p>在第 1 章中，我们还没有精确描述“模型”的语言。利用本书前半部分介绍的概念，我们现在可以引入“模型”的含义。本书主要介绍两种方法：<strong>预测器作为函数</strong>，以及<strong>预测器作为概率模型</strong>。我们将在这里描述前者，而在下一小节介绍后者。</p><p>预测器是一个函数：当输入一个具体的样本（在我们的情形下，是一个特征向量）时，它会产生一个输出。现在先假设输出是一个单一数值，即实数标量输出。这个关系可以写成： <span class="math display">\[f : \mathbb{R}^D \to \mathbb{R}, \tag{8.1}\]</span> 其中输入向量 <span class="math inline">\(x\)</span> 是 <span class="math inline">\(D\)</span> 维的（即有 <span class="math inline">\(D\)</span> 个特征），而函数 <span class="math inline">\(f\)</span> 作用于它（记作 <span class="math inline">\(f(x)\)</span>）后返回一个实数。图 8.2 展示了一个可能的函数，它可以用来计算输入值 <span class="math inline">\(x\)</span> 的预测结果。</p><p><img src="/img3/机器学习的数学基础Part2/F8.2.png" alt="F8.2" style="zoom:50%;" /></p><p>在本书中，我们不会考虑所有函数的一般情况，因为那需要用到泛函分析。相反，我们只考虑<strong>线性函数</strong>这一特殊情况： <span class="math display">\[f(x) = \theta^\top x + \theta_0 \quad (8.2)\]</span> 其中 <span class="math inline">\(\theta\)</span> 和 <span class="math inline">\(\theta_0\)</span> 是未知的。这个限制意味着，仅凭第 2 章和第 3 章的内容，就足以精确地表述“预测器”的概念（针对非概率的视角，相对于接下来要介绍的概率视角）。<strong>线性函数在“可解决问题的广泛性”和“所需数学背景的复杂程度”之间，达成了一个很好的平衡。</strong></p><h4 id="作为概率分布的模型">8.1.3 作为概率分布的模型</h4><p><strong>我们常常将数据看作是某种真实潜在效应的带噪声观测，并希望通过机器学习来从噪声中识别出信号。这就需要一种能够量化噪声影响的语言。我们也常常希望预测器能够表达某种不确定性</strong>，例如，能够量化在某个特定测试数据点上的预测值的置信程度。正如我们在第 6 章中看到的，概率论提供了一种量化不确定性的语言。图 8.3 展示了函数预测不确定性如何可以用高斯分布来表示。</p><p>与其把预测器看作是单一函数，我们可以把预测器视为<strong>概率模型</strong>，即描述可能函数分布的模型。在本书中，我们将范围限制在具有有限维参数的分布这一特殊情形，这样就可以在不需要随机过程和随机测度的情况下描述概率模型。在这种特殊情况下，我们可以把概率模型看作是<strong>多元概率分布</strong>，而这已经能够涵盖丰富的模型类别。</p><p>我们将在 <strong>8.4 节</strong> 中介绍如何利用概率论（第 6 章的内容）来定义机器学习模型，并在 <strong>8.5 节</strong> 中介绍一种图形化语言，以紧凑的方式描述概率模型。</p><p><img src="/img3/机器学习的数学基础Part2/F8.3.png" alt="F8.3" style="zoom:50%;" /></p><h4 id="学习就是寻找参数"><strong>8.1.4 学习就是寻找参数</strong></h4><p>学习的目标是找到一个模型及其对应的参数，使得由此得到的预测器能够在未见过的数据上表现良好。</p><p>在讨论机器学习算法时，从概念上可以区分出三个不同的算法阶段：</p><ol type="1"><li><strong>预测或推断</strong></li><li><strong>训练或参数估计</strong></li><li><strong>超参数调优或模型选择</strong></li></ol><p>预测阶段是指在已经训练好的预测器上使用以前未见过的测试数据。换句话说，此时参数和模型的选择已经固定，预测器被应用到代表新输入数据点的新向量上。正如第 1 章和前一小节所述，在本书中我们将考虑机器学习的两种学派，对应于预测器是一个函数还是一个概率模型。<strong>当我们拥有一个概率模型时（将在第 8.4 节进一步讨论），预测阶段被称为推断（inference）。</strong></p><p><strong>备注：</strong> 不幸的是，对于这些不同的算法阶段，<strong>并没有统一的命名。“推断（inference）”一词有时也被用来指概率模型的参数估计</strong>（个人注：Bayesian Inference 贝叶斯推断），而在更少的情况下，它也可能被用来指非概率模型的预测。</p><p><strong>训练或参数估计阶段</strong>是我们根据训练数据来调整预测模型的过程。我们希望在给定训练数据的情况下找到好的预测器，而主要有两种方法来实现这一目标：</p><ol type="1"><li>基于某种质量度量找到最优的预测器（有时称为<strong>点估计</strong>），</li><li>使用<strong>贝叶斯推断</strong>。</li></ol><p>点估计可以应用于两类预测器，但贝叶斯推断需要概率模型。</p><p>对于非概率模型，我们遵循<strong>经验风险最小化</strong>(empirical risk minimization)（将在第 8.2 节介绍）。经验风险最小化直接给出了一个用于寻找良好参数的优化问题。对于统计模型，我们使用<strong>极大似然原则</strong>来找到一组合适的参数（见第 8.3 节）。我们还可以利用概率模型来刻画参数的不确定性，这将在第 8.4 节更详细地讨论。</p><p>我们使用数值方法来寻找“拟合”数据的良好参数，大多数训练方法都可以看作是<strong>爬山法</strong>（hill-climbing）的思路，即寻找某个目标函数的最大值，例如似然函数的最大值。</p><p>在应用爬山法时，我们会用到第 5 章中描述的<strong>梯度</strong>，并结合第 7 章中的数值优化方法来实现。</p><blockquote><p>在优化领域的惯例是<strong>最小化目标函数</strong>。因此，在机器学习目标函数中经常会多出一个负号。</p></blockquote><p>正如在第 1 章提到的，我们的目标是基于数据学习一个模型，使其在未来数据上也能有良好表现。仅仅在训练数据上拟合得好是不够的，预测器还需要在未见过的数据上表现良好。为了模拟预测器在未来未知数据上的行为，我们使用交叉验证（第 8.2.4 节）。正如我们将在本章看到的，为了实现这一目标，我们需要在“很好地拟合训练数据”和“找到对现象的简单解释”之间取得平衡。这种权衡可以通过正则化（第 8.2.3 节）或引入先验（第 8.3.2 节）来实现。在哲学中，这既不被视为归纳（induction），也不被视为演绎（deduction），而是被称为溯因（abduction）。根据《斯坦福哲学百科全书》的解释，溯因是“推理出最佳解释的过程”（Douven, 2017）。</p><p>我们经常需要对预测器的结构做出高层次的建模决策，例如使用多少个成分，或者考虑哪一类概率分布。成分数的选择就是超参数（hyperparameter）的一个例子，而这种选择会显著影响模型的性能。在不同模型之间进行选择的问题被称为模型选择（model selection），我们将在第 8.6 节中描述。对于非概率模型，模型选择通常使用嵌套交叉验证来完成，这将在第 8.6.1 节中讲解。我们也使用模型选择来决定模型的超参数。</p><p><strong>备注</strong>：<strong>参数（parameter）与超参数（hyperparameter）的区别在某种程度上是人为的，主要取决于它们是可以通过数值优化得到，还是需要用搜索方法确定。</strong>另一种区分方式是：将参数理解为概率模型的显式参数，而将超参数（更高层次的参数）理解为用来控制这些显式参数分布的参数。</p><p>在接下来的章节中，我们将讨论三种机器学习方法：经验风险最小化（第 8.2 节）、最大似然原理（第 8.3 节）以及概率建模（第 8.4 节）。</p><h3 id="经验风险最小化">8.2 经验风险最小化</h3><p>Empirical Risk Minimization</p><p>在掌握了所有数学工具之后，我们现在可以介绍“学习”到底意味着什么。机器学习中的“学习”部分，归根结底就是基于训练数据来估计参数。</p><p>在本节中，我们考虑预测器是一个函数的情况，而在 8.3 节 中再讨论概率模型的情况。我们将描述 经验风险最小化 的思想，这一思想最初是由支持向量机（将在 第 12 章 介绍）的提出而普及的。然而，它的一般原理具有广泛的适用性，使我们能够在不显式构造概率模型的情况下，提出“什么是学习”这个问题。</p><p>这里有四个主要的设计选择，我们会在接下来的小节中详细讨论：</p><p><strong>8.2.1</strong> 我们允许预测器取的函数集合是什么？</p><p><strong>8.2.2</strong> 我们如何衡量预测器在训练数据上的表现？</p><p><strong>8.2.3</strong> 我们如何仅凭训练数据来构造一个在未见过的测试数据上也表现良好的预测器？</p><p><strong>8.2.4</strong> 我们如何在模型空间中进行搜索？</p><h4 id="假设函数类">8.2.1 假设函数类</h4><p>Hypothesis Class of Functions</p><p>假设我们有 <span class="math inline">\(N\)</span> 个样本 <span class="math inline">\(x_n \in \mathbb{R}^D\)</span>，以及对应的标量标签 <span class="math inline">\(y_n \in \mathbb{R}\)</span>。我们考虑的是 <strong>监督学习（supervised learning）</strong> 的设定，即我们得到样本对 <span class="math display">\[(x_1, y_1), \ldots, (x_N, y_N).\]</span> 在给定这些数据的情况下，我们希望去估计一个预测器 <span class="math display">\[f(\cdot, \theta): \mathbb{R}^D \to \mathbb{R},\]</span> 它由参数 <span class="math inline">\(\theta\)</span> 控制。我们希望能够找到一个较好的参数 <span class="math inline">\(\theta^*\)</span>，使得预测器能够很好地拟合数据，即： <span class="math display">\[f(x_n, \theta^*) \approx y_n, \quad n = 1, \ldots, N. \tag{8.3}\]</span> 在本节中，我们使用符号 <span class="math display">\[\hat{y}_n = f(x_n, \theta^*)\]</span> 来表示预测器的输出。</p><p><strong>备注：</strong> 为了方便表述，我们将在有标签的 <strong>监督学习</strong> 框架下描述经验风险最小化。这简化了假设函数类和损失函数的定义。在机器学习中，一个常见的做法是选择一个参数化的函数类，例如仿射函数。</p><p><strong>例 8.1</strong> 我们引入普通最小二乘回归问题来说明经验风险最小化。关于回归的更全面讨论将在第 9 章中给出。当标签 <span class="math inline">\(y_n\)</span> 是实数时，一类常见的预测函数集合是仿射函数。为了更紧凑地表示仿射函数，我们通过在特征向量 <span class="math inline">\(x_n\)</span> 中拼接一个附加的单位特征 <span class="math inline">\(x^{(0)} = 1\)</span>，得到 <span class="math display">\[x_n = [1, x_n^{(1)}, x_n^{(2)}, \ldots, x_n^{(D)}]^\top .\]</span> 相应地，参数向量为 <span class="math display">\[\theta = [\theta_0, \theta_1, \theta_2, \ldots, \theta_D]^\top ,\]</span> 这样我们就能把预测器写成一个线性函数： <span class="math display">\[f(x_n, \theta) = \theta^\top x_n . \tag{8.4}\]</span> 这个线性预测器等价于仿射模型： <span class="math display">\[f(x_n, \theta) = \theta_0 + \sum_{d=1}^D \theta_d x_n^{(d)} . \tag{8.5}\]</span> 预测器把一个单个样本的特征向量 <span class="math inline">\(x_n\)</span> 作为输入，并输出一个实值，即 <span class="math display">\[f : \mathbb{R}^{D+1} \to \mathbb{R}.\]</span> 本章前面的图像中，预测器是一条直线，这意味着我们假设的是一个仿射函数。</p><p>除了线性函数，我们可能希望考虑非线性函数作为预测器。近年来神经网络的进展使得更复杂的非线性函数类的高效计算成为可能。</p><p>给定一个函数类，我们希望寻找一个好的预测器。接下来我们转向经验风险最小化的第二个要素：如何衡量预测器对训练数据的拟合程度。</p><h4 id="训练的损失函数"><strong>8.2.2 训练的损失函数</strong></h4><p>考虑某个样本的标签 <span class="math inline">\(y_n\)</span> 以及基于特征向量 <span class="math inline">\(x_n\)</span> 做出的预测 <span class="math inline">\(\hat{y}_n\)</span>。为了定义“拟合数据良好”的含义，我们需要指定一个损失函数 <span class="math inline">\(\ell(y_n, \hat{y}_n)\)</span>，它以真实标签和预测值作为输入，并输出一个非负数（称为损失），表示我们在这个预测上犯了多少错误。我们寻找一个好的参数向量 <span class="math inline">\(\theta^*\)</span> 的目标是最小化 <span class="math inline">\(N\)</span> 个训练样本上的平均损失。</p><p>在机器学习中常作的一个假设是，训练样本 <span class="math inline">\((x_1, y_1), \dots, (x_N, y_N)\)</span> 是独立同分布的。<strong>这里的“独立”（参见第 6.4.5 节）意味着任意两个数据点 <span class="math inline">\((x_i, y_i)\)</span> 和 <span class="math inline">\((x_j, y_j)\)</span> 在统计上互不依赖，这也意味着经验均值可以很好地估计总体均值（参见第 6.4.1 节）</strong>。这一点表明，我们可以使用训练数据上的经验均值来衡量损失。</p><p>对于给定的训练集 <span class="math inline">\(\{(x_1, y_1), \dots, (x_N, y_N)\}\)</span>，我们引入矩阵表示法：样本矩阵 <span class="math display">\[X := [x_1, \dots, x_N]^\top \in \mathbb{R}^{N \times D}\]</span> 以及标签向量 <span class="math display">\[y := [y_1, \dots, y_N]^\top \in \mathbb{R}^N .\]</span> 使用这种矩阵表示法，平均损失定义为： <span class="math display">\[R_{\text{emp}}(f, X, y) = \frac{1}{N} \sum_{n=1}^{N} \ell(y_n, \hat{y}_n), \quad \text{其中 } \hat{y}_n = f(x_n, \theta). \tag{8.6}\]</span> <strong>式 (8.6) 称为经验风险</strong>(empirical risk)，它依赖三个参数：预测器 <span class="math inline">\(f\)</span> 和数据 <span class="math inline">\(X, y\)</span>。这种基于经验风险的学习策略称为 <strong>经验风险最小化</strong>（Empirical Risk Minimization）。</p><blockquote><p>个人注：<strong>经验风险</strong>(empirical risk) 和下文的<strong>期望风险</strong>（expected risk）对应！</p></blockquote><p><strong>例 8.2（最小二乘损失）</strong></p><p>延续最小二乘回归的例子，我们指定在训练过程中测量预测误差的代价使用平方损失函数： <span class="math display">\[\ell(y_n, \hat{y}_n) = (y_n - \hat{y}_n)^2.\]</span> 我们的目标是最小化经验风险 (8.6)，也就是对数据上损失的平均值： <span class="math display">\[\min_{\theta \in \mathbb{R}^D} \frac{1}{N} \sum_{n=1}^{N} (y_n - f(x_n, \theta))^2, \tag{8.7}\]</span> 其中我们将预测器代入 <span class="math inline">\(\hat{y}_n = f(x_n, \theta)\)</span>。</p><p>使用线性预测器的选择 <span class="math inline">\(f(x_n, \theta) = \theta^\top x_n\)</span>，我们得到优化问题： <span class="math display">\[\min_{\theta \in \mathbb{R}^D} \frac{1}{N} \sum_{n=1}^{N} (y_n - \theta^\top x_n)^2. \tag{8.8}\]</span> 这个方程也可以等价地用矩阵形式表示为： <span class="math display">\[\min_{\theta \in \mathbb{R}^D} \frac{1}{N} \|y - X\theta\|^2. \tag{8.9}\]</span> 这就是所谓的 <strong>最小二乘问题</strong>( least-squares problem.)。通过解正态方程（normal equations），可以得到这个问题的解析闭式解，这将在第 9.2 节讨论。</p><p>我们并不关心一个仅在训练数据上表现良好的预测器。相反，我们希望得到一个在未见过的测试数据上也能表现良好（即风险低）的预测器。更正式地说，我们希望找到一个预测器 <span class="math inline">\(f\)</span>（参数已固定），使其<strong>期望风险</strong>（expected risk）最小： <span class="math display">\[R_{\text{true}}(f) = \mathbb{E}_{x,y}[\ell(y, f(x))], \tag{8.10}\]</span> 其中 <span class="math inline">\(y\)</span> 是标签，<span class="math inline">\(f(x)\)</span> 是基于样本 <span class="math inline">\(x\)</span> 的预测。符号 <span class="math inline">\(R_{\text{true}}(f)\)</span> 表示如果我们拥有无限量的数据，这就是<strong>真实风险</strong>。期望是对所有可能的数据和标签（无限集合）进行的。</p><p>从我们希望最小化期望风险的角度出发，会产生两个实际问题，我们将在接下来的两个小节中讨论：</p><ol type="1"><li>我们应该如何调整训练过程以实现良好的泛化能力？</li><li>我们如何从有限数据中估计期望风险？</li></ol><p><strong>备注</strong>：许多机器学习任务会指定一个<strong>相关的性能指标，例如预测的准确率或均方根误差（RMSE）</strong>。性能指标可能更复杂，也可能对成本敏感，并捕捉特定应用的细节。<strong>原则上，为经验风险最小化设计的损失函数应直接对应于机器学习任务指定的性能指标</strong>。但在实际操作中，损失函数的设计与性能指标往往存在不匹配。这可能是由于实现方便性或优化效率等问题造成的。</p><h4 id="正则化以减少过拟合">8.2.3 正则化以减少过拟合</h4><p>本节介绍了对经验风险最小化（Empirical Risk Minimization, ERM）的一个补充，使其能够更好地泛化（即近似最小化期望风险）。回想一下，训练机器学习预测器的目标是让其在未见过的数据上表现良好，即预测器具有良好的<strong>泛化能力</strong>。我们通过从整个数据集中保留一部分数据来模拟这些未见过的数据，这部分数据称为<strong>测试集</strong>。</p><p>如果给定了一个足够丰富的函数类来表示预测器 <span class="math inline">\(f\)</span>，我们基本上可以“记住”训练数据，从而得到零经验风险。虽然这在最小化训练数据上的损失（以及风险）方面非常有效，但我们不能期望预测器在未见过的数据上也有良好表现。在实际操作中，我们只有有限的数据集，因此需要将数据分为训练集和测试集。<strong>训练集用于拟合模型，而测试集（在训练期间算法未见过）用于评估泛化性能。重要的是，用户在观察测试集后不要再回到新一轮训练中。</strong>我们用下标 <code>train</code> 和 <code>test</code> 分别表示训练集和测试集。在第 8.2.4 节中，我们将再次讨论使用有限数据集来评估期望风险的思想。</p><p>实际上，经验风险最小化可能导致<strong>过拟合</strong>，即预测器过于贴合训练数据，而无法很好地泛化到新数据（Mitchell, 1997）。当训练数据量少而假设空间复杂时，这种现象尤其常见：训练集的平均损失很小，但测试集的平均损失很大。对于固定参数的特定预测器 <span class="math inline">\(f\)</span>，当训练数据的经验风险 <span class="math inline">\(R_{\text{emp}}(f, X_{\text{train}}, y_{\text{train}})\)</span> 低估了期望风险 <span class="math inline">\(R_{\text{true}}(f)\)</span> 时，就会发生过拟合。如果我们<strong>使用测试集上的经验风险 <span class="math inline">\(R_{\text{emp}}(f, X_{\text{test}}, y_{\text{test}})\)</span> 来估计期望风险</strong>，并发现测试风险远大于训练风险，这就是过拟合的一个信号。我们将在第 8.3.3 节中再次讨论过拟合的概念。</p><p>因此，我们需要通过引入一个<strong>惩罚项</strong>来调整经验风险最小化的搜索方向，使优化器更难返回过于灵活的预测器。在机器学习中，这个惩罚项称为<strong>正则化（regularization）</strong>。<strong>正则化是一种在精确求解经验风险最小化和控制解的大小或复杂度之间进行折衷的方法。</strong></p><p><strong>示例 8.3（正则化最小二乘法）</strong></p><p>正则化是一种方法，用于抑制优化问题中复杂或极端的解。最简单的正则化策略是将前一个例子中的最小二乘问题 <span class="math display">\[\min_{\theta} \frac{1}{N} \|y - X\theta\|^2 \tag{8.11}\]</span> 替换为<strong>正则化</strong>问题，通过添加一个仅涉及 <span class="math inline">\(\theta\)</span> 的惩罚项： <span class="math display">\[\min_{\theta} \frac{1}{N} \|y - X\theta\|^2 + \lambda \|\theta\|^2 \tag{8.12}\]</span> 附加的项 <span class="math inline">\(\|\theta\|^2\)</span> 称为<strong>正则化项</strong>（regularizer），参数 <span class="math inline">\(\lambda\)</span> 称为<strong>正则化参数</strong>。正则化参数用于在最小化训练集上的损失和控制参数 <span class="math inline">\(\theta\)</span> 的大小之间进行折衷。如果出现过拟合，参数值的大小通常会变得相对较大（Bishop, 2006）。</p><p><strong>正则化项有时也称为惩罚项，它使向量 <span class="math inline">\(\theta\)</span> 更接近原点。正则化的思想在概率模型中也出现，表现为参数的先验概率。</strong>回想第 6.6 节，为了使后验分布与先验分布具有相同的形式，先验和似然函数需要是共轭的。我们将在第 8.3.2 节再次讨论这一思想。在第 12 章中，我们将看到正则化项的思想与大间隔（large margin）的思想是等价的。</p><h4 id="交叉验证评估泛化性能">8.2.4 交叉验证评估泛化性能</h4><p>在上一节中，我们提到通过在测试数据上应用预测器来估计泛化误差。这些数据有时也称为验证集。验证集是从可用训练数据中留出的一部分。使用这种方法的一个实际问题是数据量有限，而理想情况下我们希望使用尽可能多的数据来训练模型。这就要求我们将验证集 <span class="math inline">\(\mathcal V\)</span> 保持较小，但这样会导致对预测性能的估计存在较大噪声（高方差）。解决这一矛盾目标（大训练集、大验证集）的一种方法是使用交叉验证。</p><p><span class="math inline">\(K\)</span> 折交叉验证有效地将数据集划分为 <span class="math inline">\(K\)</span> 个子块，每次使用其中最后一块作为验证集 <span class="math inline">\(\mathcal V\)</span>（类似于之前描述的思路）。交叉验证会遍历所有可能的子块分配组合，将某些子块分配给训练集 <span class="math inline">\(R\)</span>，某些分配给验证集 <span class="math inline">\(\mathcal V\)</span>；见图 8.4。该过程对每个验证集的 <span class="math inline">\(K\)</span> 个选择重复进行，并对 <span class="math inline">\(K\)</span> 次运行的模型性能取平均值。</p><figure><img src="./F8.4.png" alt="F8.4" /><figcaption aria-hidden="true">F8.4</figcaption></figure><p>我们将数据集划分为两个集合 <span class="math inline">\(D = R \cup \mathcal V\)</span>，且它们不重叠（<span class="math inline">\(R \cap \mathcal V = \emptyset\)</span>），其中 <span class="math inline">\(\mathcal V\)</span> 是验证集，在 <span class="math inline">\(R\)</span> 上训练模型。训练完成后，我们在验证集 <span class="math inline">\(\mathcal V\)</span> 上评估预测器 <span class="math inline">\(f\)</span> 的性能（例如，计算训练模型在验证集上的均方根误差 RMSE）。更准确地说，对于每个划分 <span class="math inline">\(k\)</span>，训练数据 <span class="math inline">\(R^{(k)}\)</span> 生成预测器 <span class="math inline">\(f^{(k)}\)</span>，然后将其应用于验证集 <span class="math inline">\(\mathcal V^{(k)}\)</span> 来计算经验风险 <span class="math inline">\(R(f^{(k)}, \mathcal V^{(k)})\)</span>。我们遍历所有可能的训练集与验证集划分，并计算预测器的平均泛化误差。</p><p>交叉验证近似期望泛化误差： <span class="math display">\[\mathbb{E}_\mathcal V [R(f, \mathcal V)] \approx \frac{1}{K} \sum_{k=1}^{K} R(f^{(k)}, \mathcal V^{(k)}),\]</span> <strong>其中 <span class="math inline">\(R(f^{(k)}, \mathcal V^{(k)})\)</span> 是预测器 <span class="math inline">\(f^{(k)}\)</span> 在验证集 <span class="math inline">\(\mathcal V^{(k)}\)</span> 上的风险（例如 RMSE）</strong>(<strong>个人注：这里应该是期望风险？</strong>)。这种近似有两个来源：第一，由于训练集有限，导致无法得到最优的 <span class="math inline">\(f^{(k)}\)</span>；第二，由于验证集有限，导致对风险 <span class="math inline">\(R(f^{(k)}, \mathcal V^{(k)})\)</span> 的估计不准确。</p><p><span class="math inline">\(K\)</span> 折交叉验证的一个潜在缺点是需要训练模型$ K $次，如果训练成本高，则计算开销很大。在实践中，单看直接参数通常是不够的。例如，我们需要探索多个复杂度参数（如多个正则化参数），这些参数可能不是模型的直接参数。评估依赖这些超参数的模型质量，可能导致训练次数随着模型参数数量呈指数增长。可以使用嵌套交叉验证（Section 8.6.1）来搜索合适的超参数。</p><p>然而，交叉验证是一个非常容易并行化的问题，即几乎不需要额外努力就可以将问题拆分成多个并行任务。只要有足够的计算资源（例如云计算或服务器集群），交叉验证的耗时不会超过一次性能评估所需的时间。</p><p>在本节中，我们看到经验风险最小化是基于以下几个概念：函数的假设类、损失函数以及正则化。在第 8.3 节中，我们将看到使用概率分布来替代损失函数和正则化这一思想的效果。</p><blockquote><p>个人注：k折交叉验证会训练出K个不同参数的模型。常用于评估模型的泛化性能，而不是确定最终模型。</p><p>详见《k 折交叉验证.md》</p></blockquote><h4 id="延伸阅读">8.2.5 延伸阅读</h4><p>由于经验风险最小化的最初发展（Vapnik, 1998）使用了大量理论化的语言，之后的许多发展也以理论为主。这个研究领域被称为统计学习理论（Vapnik, 1999；Evgeniou 等, 2000；Hastie 等, 2001；von Luxburg 和 Scholkopf, 2011）。最近的一本机器学习教材（Shalev-Shwartz 和 Ben-David, 2014）在理论基础上发展了高效的学习算法。</p><p>正则化的概念起源于病态逆问题的求解（Neumaier, 1998）。这里介绍的方法称为 Tikhonov 正则化，同时还有一个紧密相关的受约束版本称为 Ivanov 正则化。Tikhonov 正则化与偏差-方差权衡和特征选择有着深刻的关系（Buhlmann 和 Van De Geer, 2011）。交叉验证的替代方法包括自助法（bootstrap）和留一法（jackknife）（Efron 和 Tibshirani, 1993；Davidson 和 Hinkley, 1997；Hall, 1992）。</p><p>将经验风险最小化（第 8.2 节）视为“无概率基础”是不正确的。实际上存在一个未知的概率分布 <span class="math inline">\(p(x, y)\)</span> 来支配数据生成过程。然而，经验风险最小化的方法对这个分布的选择是不可知的。这与标准统计方法形成对比，后者通常需要明确知道 <span class="math inline">\(p(x, y)\)</span>。此外，由于该分布是样本 <span class="math inline">\(x\)</span> 与标签 <span class="math inline">\(y\)</span> 的联合分布，标签可以是非确定性的。与标准统计方法不同，我们不需要为标签 <span class="math inline">\(y\)</span> 指定噪声分布。</p><h3 id="参数估计">8.3 参数估计</h3><p>在第 8.2 节中，我们并没有使用概率分布来明确地对问题建模。在本节中，我们将看到如何使用<strong>概率分布来建模，由于观测过程的不确定性以及预测器参数的不确定性</strong>。在第 8.3.1 节中，我们将介绍<strong>似然函数，它类似于经验风险最小化中损失函数（第 8.2.2 节）的概念。先验的概念（第 8.3.2 节）则类似于正则化（第 8.2.3 节）的概念。</strong></p><h4 id="极大似然估计">8.3.1 极大似然估计</h4><p>极大似然估计（<span class="math inline">\(\text{MLE}\)</span>）的核心思想是定义一个<strong>参数的函数</strong>，使我们能够找到能够很好拟合数据的模型。估计问题集中在似然函数上，或者更准确地说，是其负对数。对于由随机变量 <span class="math inline">\(x\)</span> 表示的数据，以及由参数 <span class="math inline">\(\theta\)</span> 参数化的概率密度族 <span class="math inline">\(p(x \mid \theta)\)</span>，<strong>负对数似然函数</strong>定义为： <span class="math display">\[L_x(\theta) = - \log p(x \mid \theta). \tag{8.14}\]</span> <strong>符号 <span class="math inline">\(L_x(\theta)\)</span> 强调了参数 <span class="math inline">\(\theta\)</span> 可变而数据 <span class="math inline">\(x\)</span> 固定的事实</strong>。在书写负对数似然函数时，我们经常省略对 <span class="math inline">\(x\)</span> 的引用，因为它实际上是 <span class="math inline">\(\theta\)</span> 的函数，当上下文中数据的不确定性由随机变量表示时，我们记作 <span class="math inline">\(L(\theta)\)</span>。</p><p>让我们解释在固定 <span class="math inline">\(\theta\)</span> 的情况下，概率密度 <span class="math inline">\(p(x \mid \theta)\)</span> 建模了什么。它是一个用于建模数据不确定性的分布。换句话说，<strong>一旦我们选择了想要作为预测器的函数类型，似然函数就提供了观测数据 <span class="math inline">\(x\)</span> 的概率。</strong></p><p>从另一种角度来看，如果我们认为<strong>数据是固定的（因为已经观测到），而我们改变参数 <span class="math inline">\(\theta\)</span>，那么 <span class="math inline">\(L(\theta)\)</span> 告诉我们什么？它告诉我们，对于观测数据 <span class="math inline">\(x\)</span>，某个特定的 <span class="math inline">\(\theta\)</span> 设置有多可能。基于这种观点，极大似然估计器给出了对于该数据集最可能的参数 <span class="math inline">\(\theta\)</span>。</strong></p><p>我们考虑监督学习设置，其中得到一组样本对 <span class="math inline">\((x_1, y_1), \dots, (x_N, y_N)\)</span>，其中 <span class="math inline">\(x_n \in \mathbb{R}^D\)</span>，标签 <span class="math inline">\(y_n \in \mathbb{R}\)</span>。我们的目标是构建一个预测器，它以特征向量 <span class="math inline">\(x_n\)</span> 作为输入并输出预测 <span class="math inline">\(y_n\)</span>（或接近 <span class="math inline">\(y_n\)</span> 的值）。换句话说，给定向量 <span class="math inline">\(x_n\)</span>，我们希望得到标签 <span class="math inline">\(y_n\)</span> 的概率分布。换句话说，我们为特定的参数设置 <span class="math inline">\(\theta\)</span> 指定了标签在样本条件下的条件概率分布。</p><blockquote><p>个人注：要好好理解上述这段，才能理解最大似然函数的估计。</p></blockquote><p><strong>例</strong> 8.4</p><p>第一个常用的例子是指定在给定样本的情况下，标签的条件概率服从高斯分布。换句话说，我们假设可以用独立的高斯噪声（参考第 6.5 节）来解释观测的不确定性，且噪声均值为零，即 <span class="math inline">\(\varepsilon_n \sim \mathcal{N}(0, \sigma^2)\)</span>。我们进一步假设线性模型 <span class="math inline">\(x_n^\top \theta\)</span> 被用于预测。这意味着我们为每个样本标签对 <span class="math inline">\((x_n, y_n)\)</span> 指定了高斯似然函数： <span class="math display">\[p(y_n \mid x_n, \theta) = \mathcal{N}(y_n \mid x_n^\top \theta, \sigma^2). \tag{8.15}\]</span> 图 8.3 展示了给定参数 <span class="math inline">\(\theta\)</span> 的高斯似然的示意图。我们将在第 9.2 节中看到如何将上述表达式显式地展开为高斯分布的形式。</p><blockquote><p>个人注：<span class="math inline">\(\mathcal{N}(y_n∣μ,σ^2)\)</span>的意思是：<strong>在均值为 <span class="math inline">\(\mu\)</span>、方差为 <span class="math inline">\(\sigma^2\)</span> 的高斯分布下，随机变量取值为 <span class="math inline">\(y_n\)</span> 的概率密度值</strong>。</p><p><span class="math inline">\(p(y_n∣x_n,θ)\)</span>理解为 <strong>“在给定输入 <span class="math inline">\(x_n\)</span> 和参数 <span class="math inline">\(\theta\)</span> 的条件下，输出 <span class="math inline">\(y_n\)</span> 的条件概率密度”</strong>。</p></blockquote><p>我们假设样本集合 <span class="math inline">\((x_1, y_1), \ldots, (x_N, y_N)\)</span> 是<strong>独立同分布</strong>( independent identically distributed and identically distribut)（i.i.d.）的。 “独立”（见第 6.4.5 节）意味着整个数据集的似然（<span class="math inline">\(Y = \{y_1, \ldots, y_N\}\)</span>，<span class="math inline">\(X = \{x_1, \ldots, x_N\}\)</span>）可以分解为各个样本似然的乘积： <span class="math display">\[p(Y \mid X, \theta) = \prod_{n=1}^N p(y_n \mid x_n, \theta), \tag{8.16}\]</span> <strong>其中 <span class="math inline">\(p(y_n \mid x_n, \theta)\)</span> 是一个特定的分布</strong>（在例 8.4 中是高斯分布）。 “同分布”意味着乘积式 (8.16) 中的每一项都是相同类型的分布，并且它们共享相同的参数。从优化的角度来看，能够分解为多个更简单函数之和的函数通常更容易计算。因此，在机器学习中我们常常考虑<strong>负对数似然</strong>： <span class="math display">\[L(\theta) = - \log p(Y \mid X, \theta) = - \sum_{n=1}^N \log p(y_n \mid x_n, \theta). \tag{8.17}\]</span> 虽然直观上可能会倾向于这样理解：在条件概率 <span class="math inline">\(p(y_n \mid x_n, \theta)\)</span> (式 8.15) 中，<span class="math inline">\(\theta\)</span> 出现在条件符号右边，因此应该被看作是“已知且固定”的；但是这种理解是<strong>错误的</strong>。事实上，负对数似然 <span class="math inline">\(L(\theta)\)</span> 是 <span class="math inline">\(\theta\)</span> 的函数。因此，要找到一个能够很好地解释数据 <span class="math inline">\((x_1, y_1), \ldots, (x_N, y_N)\)</span> 的参数向量 <span class="math inline">\(\theta\)</span>，需要对 <span class="math inline">\(\theta\)</span> 最小化负对数似然 <span class="math inline">\(L(\theta)\)</span>。</p><p><strong>备注</strong>：<strong>式 (8.17) 中的负号是历史遗留的约定，这是因为我们希望最大化似然，但数值优化领域通常研究的是函数的最小化。</strong></p><blockquote><p>个人注：<span class="math inline">\(L(\theta)\)</span>表示的是整个数据集的联合概率。</p></blockquote><p><strong>例 8.5</strong> 继续我们关于高斯似然 (8.15) 的例子，负对数似然可以改写为 <span class="math display">\[\begin{align}L(\theta) &amp; = -\sum_{n=1}^N \log p(y_n \mid x_n, \theta)     = -\sum_{n=1}^N \log \mathcal{N}(y_n \mid x_n^\top \theta, \sigma^2) \tag{8.18a} \\&amp; = -\sum_{n=1}^N \log \frac{1}{\sqrt{2\pi\sigma^2}}      \exp\Bigg(-\frac{(y_n - x_n^\top \theta)^2}{2\sigma^2}\Bigg) \tag{8.18b} \\&amp; = -\sum_{n=1}^N \log \exp\Bigg(-\frac{(y_n - x_n^\top \theta)^2}{2\sigma^2}\Bigg)      - \sum_{n=1}^N \log \frac{1}{\sqrt{2\pi\sigma^2}} \tag{8.18c} \\&amp; = \frac{1}{2\sigma^2} \sum_{n=1}^N (y_n - x_n^\top \theta)^2      - \sum_{n=1}^N \log \frac{1}{\sqrt{2\pi\sigma^2}} . \tag{8.18d}\end{align}\]</span></p><p><strong>由于 <span class="math inline">\(\sigma\)</span> 是给定的，式 (8.18d) 中的第二项是常数，因此最小化 <span class="math inline">\(L(\theta)\)</span> 等价于解决最小二乘问题（参见 (8.8)），这个问题体现在第一项。</strong></p><p>事实证明，对于高斯似然，极大似然估计所对应的优化问题是有闭式解的。我们将在第 9 章中看到更多细节。图 8.5 展示了一个回归数据集以及由最大似然参数所决定的函数。极大似然估计可能会遭遇过拟合（第 8.3.3 节），这与无正则化的经验风险最小化（第 9.2.3 节）类似。对于其他似然函数，也就是说，如果我们用非高斯分布来建模噪声，那么极大似然估计可能就没有解析的闭式解。在这种情况下，我们需要借助第 7 章讨论的数值优化方法。</p><p><img src="/img3/机器学习的数学基础Part2/F8.5.png" alt="F8.5" style="zoom:50%;" /></p><p><img src="/img3/机器学习的数学基础Part2/F8.6.png" alt="F8.6" style="zoom:50%;" /></p><h4 id="最大后验估计">8.3.2 最大后验估计</h4><p>Maximum A Posteriori Estimation, MAP</p><p>如果我们对参数 <span class="math inline">\(\theta\)</span> 的分布有先验知识，就可以在似然函数中额外引入一项。这个额外的项就是参数的先验概率分布 <span class="math inline">\(p(\theta)\)</span>。对于给定的先验，在观察到一些数据 <span class="math inline">\(x\)</span> 之后，我们应当如何更新关于 <span class="math inline">\(\theta\)</span> 的分布呢？换句话说，观察到数据 <span class="math inline">\(x\)</span> 后，我们如何表示自己对 <span class="math inline">\(\theta\)</span> 有了更具体的认知？正如第 6.3 节所讨论的那样，贝叶斯定理为我们提供了一种系统的方法来<strong>更新随机变量的概率分布</strong>。它允许我们通过先验分布 <span class="math inline">\(p(\theta)\)</span>（一般性的先验信息）以及把参数 <span class="math inline">\(\theta\)</span> 与观测数据 <span class="math inline">\(x\)</span> 联系起来的函数 <span class="math inline">\(p(x \mid \theta)\)</span>（即似然函数），来计算参数 <span class="math inline">\(\theta\)</span> 的后验分布 <span class="math inline">\(p(\theta \mid x)\)</span>（更具体的认知）： <span class="math display">\[p(\theta \mid x) = \frac{p(x \mid \theta)p(\theta)}{p(x)} \tag{8.19}\]</span> 我们感兴趣的是寻找能最大化后验分布的参数 <span class="math inline">\(\theta\)</span>。由于 <span class="math inline">\(p(x)\)</span> 与 <span class="math inline">\(\theta\)</span> 无关，在优化时可以忽略分母，从而得到： <span class="math display">\[p(\theta \mid x) \propto p(x \mid \theta) p(\theta) \tag{8.20}\]</span> 上述比例关系隐藏了数据的密度 <span class="math inline">\(p(x)\)</span>，而它可能是难以估计的。<strong>与其估计负对数似然的最小值，我们现在估计的是负对数后验的最小值，这被称为最大后验估计（Maximum A Posteriori Estimation, MAP）</strong>。图 8.6 展示了在模型中加入一个均值为零的高斯先验后的效果。</p><p><strong>例 8.6</strong> 在前一个例子中假设似然函数是高斯分布的基础上，我们进一步假设参数向量服从零均值的多元高斯分布，即 <span class="math inline">\(p(\theta) = \mathcal{N}(0, \Sigma)\)</span>，其中 <span class="math inline">\(\Sigma\)</span> 是协方差矩阵（见第 6.5 节）。需要注意的是，高斯分布的共轭先验仍然是高斯分布（见第 6.6.1 节），因此我们预期其后验分布也将是高斯分布。关于最大后验估计的细节将在第 9 章中讨论。</p><p>在机器学习中，引入关于“好的参数大致位于何处”的先验知识是非常常见的。另一种观点（见第 8.2.3 节）是正则化，它引入了一个附加项，使得最终得到的参数更倾向于接近原点（个人注：这里的原点应该是指先验分布？）。<strong>最大后验估计可以被视为在非概率方法与概率方法之间架起的一座桥梁：它明确承认了对先验分布的需求，但仍然只给出参数的一个点估计。</strong></p><blockquote><p>个人注：</p><ul><li><span class="math inline">\(p(\theta \mid X,y)\)</span> 就是 <strong>参数的后验分布</strong>。</li><li>它是贝叶斯统计的核心：不是给一个点估计，而是给一整个分布。</li></ul></blockquote><p><strong>备注</strong>：极大似然估计（<span class="math inline">\(\text{MLE}） \theta_{ML}\)</span> 具有以下性质（Lehmann 和 Casella, 1998; Efron 和 Hastie, 2016）：</p><ul><li><p>渐近一致性：当观测数量趋于无穷大时，<span class="math inline">\(\text{MLE}\)</span> 收敛于真实值，且误差近似服从正态分布。</p></li><li><p>为了达到这些性质，所需的样本量可能非常大。</p></li><li><p>误差的方差按 <span class="math inline">\(1/N\)</span> 的速度衰减，其中 <span class="math inline">\(N\)</span> 是数据点的数量。</p></li><li><p>尤其是在“小数据”情境下，极大似然估计可能会导致过拟合。</p></li></ul><p><strong>极大似然估计（以及最大后验估计）的原理是利用概率建模来处<em><u>理数据和模型参数中的不确定性</u></em>。然而，我们还没有把概率建模发挥到极致。在本节中，训练过程的结果仍然是得到预测器的一个点估计，也就是说，训练会返回一组单一的参数值，作为最佳预测器的代表。而在第 8.4 节中，我们将采取另一种观点：参数值本身也应当被视为随机变量，并且在预测时，不是去估计该分布的“最佳”值，而是利用完整的参数分布来进行预测。</strong></p><h4 id="模型拟合"><strong>8.3.3 模型拟合</strong></h4><p>考虑这样一种情形：我们给定了一个数据集，并且希望将一个参数化模型拟合到这些数据上。所谓“拟合”，通常是指通过优化/学习模型参数，使其最小化某个损失函数，例如负对数似然。在极大似然估计（第 8.3.1 节）和最大后验估计（第 8.3.2 节）中，我们已经讨论过两种常用的模型拟合算法。</p><p>模型的参数化定义了一个模型类 <span class="math inline">\(M_\theta\)</span>，我们可以在其中进行操作。例如，在一个线性回归的场景下，我们可以定义输入 <span class="math inline">\(x\)</span> 与（无噪声）观测值 <span class="math inline">\(y\)</span> 之间的关系为 <span class="math display">\[y = ax + b,\]</span> 其中 <span class="math inline">\(\theta := \{a, b\}\)</span> 是模型参数。在这种情况下，模型参数 <span class="math inline">\(\theta\)</span> 描述了一族仿射函数，也就是说，一组斜率为 <span class="math inline">\(a\)</span> 的直线，它们相对于 0 的截距由 <span class="math inline">\(b\)</span> 决定。假设数据来自某个我们未知的模型 <span class="math inline">\(M^\ast\)</span>。对于一个给定的训练数据集，我们的目标是优化 <span class="math inline">\(\theta\)</span>，使得 <span class="math inline">\(M_\theta\)</span> 尽可能接近 <span class="math inline">\(M^\ast\)</span>，这里的“接近”由我们所优化的目标函数来定义（例如，训练数据上的平方损失）。</p><p>图 8.7 展示了一种情形：我们只有一个较小的模型类（由圆圈 <span class="math inline">\(M_\theta\)</span> 表示），而数据生成模型 <span class="math inline">\(M^\ast\)</span> 位于所考虑模型集合之外。我们从某个初始参数 <span class="math inline">\(M_{\theta_0}\)</span> 开始搜索。在完成优化之后，即得到最优参数 <span class="math inline">\(\theta^\ast\)</span> 之后，我们会区分三种不同的情形：（i）过拟合，（ii）欠拟合，以及（iii）拟合良好。接下来我们将对这三个概念给出高层次的直观解释。</p><p><img src="/img3/机器学习的数学基础Part2/F8.7.png" alt="F8.7" style="zoom:50%;" /></p><p>粗略来说，<strong>过拟合</strong>是指这样一种情况：参数化的模型类过于复杂，足以拟合由 <span class="math inline">\(M^\ast\)</span> 生成的数据集，换句话说，<span class="math inline">\(M_\theta\)</span> 能够拟合比真实数据复杂得多的数据集。举例来说，如果数据集是由一个线性函数生成的，而我们定义的 <span class="math inline">\(M_\theta\)</span> 是七阶多项式函数类，那么它不仅能拟合线性函数，还能拟合二次、三次，甚至更高阶的多项式。通常，过拟合的模型具有大量的参数。我们经常观察到的一种现象是：过于灵活的模型类 <span class="math inline">\(M_\theta\)</span> 会用尽其建模能力来降低训练误差。如果训练数据中存在噪声，它甚至会在噪声中“发现”一些虚假的有用信号。这将导致在训练数据之外进行预测时出现严重问题。图 8.8(a) 给出了一个回归场景下过拟合的例子，其中模型参数是通过最大似然法（见第 8.3.1 节）学习得到的。我们将在第 9.2.2 节中进一步讨论回归中的过拟合问题。</p><figure><img src="./F8.8.png" alt="F8.8" /><figcaption aria-hidden="true">F8.8</figcaption></figure><p>当我们遇到 <strong>欠拟合</strong> 时，情况则正好相反：模型类 <span class="math inline">\(M_\theta\)</span> 不够复杂。举个例子，如果我们的数据集是由正弦函数生成的，但 <span class="math inline">\(\theta\)</span> 只能参数化直线，那么即便采用最优的优化方法，也无法逼近真实模型。不过，我们仍然会去优化参数，找到一条最优的直线来拟合数据集。图 8.8(b) 展示了一个欠拟合的例子，因为模型缺乏足够的灵活性。通常，欠拟合的模型参数较少。第三种情况是 <strong>模型类恰到好处</strong>。此时，我们的模型拟合得比较好，即既没有过拟合，也没有欠拟合。这意味着我们的模型类刚好足够丰富，可以描述给定的数据集。图 8.8(c) 展示了一个较好拟合给定数据集的模型。理想情况下，这就是我们希望使用的模型类，因为它具有良好的泛化能力。在实际应用中，我们经常会定义非常复杂的模型类 <span class="math inline">\(M_\theta\)</span>，其中包含大量参数，例如深度神经网络。为了缓解过拟合问题，我们可以使用 <strong>正则化</strong>（第 8.2.3 节）或 <strong>先验</strong>（第 8.3.2 节）。我们将在第 8.6 节中讨论如何选择模型类。</p><h4 id="延伸阅读-1"><strong>8.3.4 延伸阅读</strong></h4><p>在考虑概率模型时，极大似然估计的原理推广了线性模型中最小二乘回归的思想，我们将在第 9 章中详细讨论这一点。当我们将预测器限制为线性形式，并在输出端再施加一个非线性函数 <span class="math inline">\(\phi\)</span> 时，即 <span class="math display">\[p(y_n \mid x_n, \theta) = \phi(\theta^\top x_n), \tag{8.21}\]</span> 我们就可以考虑其他预测任务的模型，例如二分类问题或计数数据建模（McCullagh 和 Nelder，1989）。另一种观点是把这种情形看作是来自 <strong>指数族分布</strong>（第 6.6 节）的似然函数。这类模型中，参数与数据之间具有线性依赖关系，但同时可能包含一个非线性变换 <span class="math inline">\(\phi\)</span>（称为 <strong>链接函数</strong>），被称为 <strong>广义线性模型</strong>（Agresti，2002，第 4 章）。极大似然估计有着悠久的历史，最初是由 Ronald Fisher 爵士在 20 世纪 30 年代提出的。我们将在第 8.4 节中进一步扩展概率模型的思想。在使用概率模型的研究者中，一个长期的争论点是 <strong>贝叶斯统计</strong> 与 <strong>频率学派统计</strong> 的分歧。正如第 6.1.1 节所提到的，争论的核心在于对概率的定义。回忆第 6.1 节中提到的观点：概率可以看作是对逻辑推理的推广（通过引入不确定性）（Cheeseman，1985；Jaynes，2003）。极大似然估计方法在本质上属于频率学派。对有兴趣的读者，可以参考 Efron 和 Hastie（2016），该书对贝叶斯与频率学派两种统计学方法给出了一个较为平衡的视角。需要注意的是，在某些概率模型中，极大似然估计可能不可行。对于这类情况，读者可以参考更高阶的统计教材，例如 Casella 和 Berger（2002），其中介绍了诸如 <strong>矩估计法</strong>、<strong>M-估计</strong> 和 <strong>估计方程</strong> 等方法。</p><h3 id="概率建模与推断"><strong>8.4 概率建模与推断</strong></h3><p>在机器学习中，我们经常关心的是数据的解释和分析，例如预测未来事件和进行决策。为了让这项任务更易处理，我们通常会建立一些模型来描述生成观测数据的生成过程。例如，我们可以用两步来描述一次抛硬币实验的结果（“正面”或“反面”）。第一步，我们定义一个参数 <span class="math inline">\(\mu\)</span>，它作为伯努利分布（第 6 章）的参数，表示硬币出现“正面”的概率；第二步，我们可以从伯努利分布 <span class="math inline">\(p(x \mid \mu) = \mathrm{Ber}(\mu)\)</span> 中采样一个结果 <span class="math inline">\(x \in {\text{正面}, \text{反面}}\)</span>。参数 <span class="math inline">\(\mu\)</span> 决定了一个特定的数据集 <span class="math inline">\(X\)</span>，并取决于所用的硬币。由于 <span class="math inline">\(\mu\)</span> 是事先未知的，且无法被直接观测到，因此我们需要一些机制来在给定抛硬币实验观测结果的情况下，学习关于 <span class="math inline">\(\mu\)</span> 的信息。在接下来的讨论中，我们将说明如何利用概率建模来实现这一目的。</p><h4 id="概率模型"><strong>8.4.1 概率模型</strong></h4><p>概率模型将实验中不确定的部分表示为概率分布。使用概率模型的好处在于，它们提供了一套统一且一致的概率论工具（第 6 章），可用于建模、推断、预测以及模型选择。在概率建模中，观测变量 <span class="math inline">\(x\)</span> 和隐藏参数 <span class="math inline">\(\theta\)</span> 的联合分布 <span class="math inline">\(p(x, \theta)\)</span> 占据核心地位：它包含了以下几方面的信息：</p><ul><li><strong>先验与似然</strong>（乘法规则，第 6.3 节）；</li><li><strong>边际似然</strong> <span class="math inline">\(p(x)\)</span>，在模型选择中将发挥重要作用（第 8.6 节），它可以通过对联合分布对参数进行积分（求和规则，第 6.3 节）来计算；</li><li><strong>后验分布</strong>，它可以通过将联合分布除以边际似然得到。</li></ul><p>只有联合分布具有这样的性质。因此，一个概率模型就是由其所有随机变量的联合分布所刻画的。</p><h4 id="贝叶斯推断"><strong>8.4.2 贝叶斯推断</strong></h4><p>机器学习中的一个关键任务是，利用模型和数据来揭示模型的隐藏变量（参数）<span class="math inline">\(\theta\)</span> 的取值，前提是我们已经观测到了变量 <span class="math inline">\(x\)</span>。在 8.3.1 节 中，我们已经讨论了两种估计模型参数 <span class="math inline">\(\theta\)</span> 的方法：极大似然估计（MLE）和最大后验估计（MAP）。在这两种情况下，我们都得到 <span class="math inline">\(\theta\)</span> 的一个最优点估计值，因此，参数估计的核心算法问题就转化为一个优化问题。一旦得到了这些点估计 <span class="math inline">\(\theta^*\)</span>，我们就可以利用它们进行预测。更具体地，<em><u><strong>预测分布</strong></u></em>为 <span class="math display">\[p(x \mid \theta^*),\]</span> 其中我们在似然函数中使用 <span class="math inline">\(\theta^*\)</span>。</p><p>然而，正如在 6.3 节 中讨论的，仅仅关注后验分布中的某个统计量（比如最大化后验的参数 <span class="math inline">\(\theta^*\)</span>），会导致信息丢失。而这种信息丢失在需要基于预测 <span class="math inline">\(p(x \mid \theta^*)\)</span> 来进行决策的系统中可能非常关键。<strong>这类决策系统通常具有与似然函数不同的目标函数，比如平方误差损失或分类错误率。因此，保留完整的后验分布会非常有用，并能带来更稳健的决策。贝叶斯推断的核心就是寻找这个后验分布（Gelman 等，2004）。</strong></p><blockquote><p>个人注：似然函数是损失函数的一种，在优化过程中使用；而目标函数是预测结果的准确率。</p></blockquote><p>对于一个数据集 <span class="math inline">\(X\)</span>，给定参数的先验分布 <span class="math inline">\(p(\theta)\)</span> 和似然函数，后验分布为 <span class="math display">\[p(\theta \mid X) = \frac{p(X \mid \theta)p(\theta)}{p(X)}, \quad p(X) = \int p(X \mid \theta)p(\theta) \, d\theta, \tag{8.22}\]</span> 这是通过应用 <strong>贝叶斯定理</strong> 得到的。关键思想是利用贝叶斯定理，把参数 <span class="math inline">\(\theta\)</span> 和数据 <span class="math inline">\(X\)</span> 之间的关系反转过来（原本是似然函数 <span class="math inline">\(p(X \mid \theta)\)</span> 给出的），从而得到后验分布 <span class="math inline">\(p(\theta \mid X)\)</span>。拥有参数的后验分布的意义在于，它可以将参数的不确定性传播到数据层面。更具体地说，当参数具有分布 <span class="math inline">\(p(\theta)\)</span> 时，我们的预测为 <span class="math display">\[p(x) = \int p(x \mid \theta) p(\theta) \, d\theta = \mathbb{E}_\theta \big[ p(x \mid \theta) \big], \tag{8.23}\]</span> 这样，<strong>预测就不再依赖于某个固定的模型参数 <span class="math inline">\(\theta\)</span>，因为它们已经被边缘化/积分消去了。</strong>式 (8.23) 揭示了预测是对所有可能的参数取值 <span class="math inline">\(\theta\)</span> 的加权平均，其中参数的“合理性”由参数分布 <span class="math inline">\(p(\theta)\)</span> 体现。</p><blockquote><p>边缘化/积分；边缘化等价于积分掉！</p></blockquote><blockquote><p>个人注：：<strong>预测分布和预测的“点值”是不同概念</strong>：</p><ul><li><span class="math inline">\(p(x \mid \theta^*)\)</span> 是一个<strong>分布</strong>，描述在固定参数下，新数据 <span class="math inline">\(x\)</span> 出现的可能性。</li><li>即便参数确定了，数据本身可能是随机的（比如高斯回归里 <span class="math inline">\(y \sim \mathcal{N}(x^\top \theta^*, \sigma^2)\)</span>），所以我们仍然得到一个概率分布而不是单个数值。</li></ul></blockquote><p>在 8.3 节 我们讨论了参数估计，而在这里讨论了贝叶斯推断，现在让我们比较一下这两种学习方法。通过极大似然估计（MLE）或最大后验估计（MAP）进行的<strong>参数估计，</strong>会<strong>给出参数的一个一致的点估计 <span class="math inline">\(\theta^*\)</span>，其核心计算问题是一个优化问题</strong>。相比之下，<strong>贝叶斯推断给出的是一个（后验）分布，其核心计算问题则是一个积分问题。</strong>基于点估计的预测相对直接，而在贝叶斯框架下的预测则需要解决另一个积分问题（见公式 (8.23)）。然而，贝叶斯推断为我们提供了一种有原则的方法来：</p><ul><li>融合先验知识，</li><li>考虑额外的辅助信息，</li><li>融合结构化知识，</li></ul><p>这些在参数估计框架中并不容易实现。此外，将参数不确定性传播到预测中，在需要进行风险评估和探索的决策系统（尤其是在数据高效学习的背景下，见 Deisenroth 等，2015；Kamthe 和 Deisenroth，2018）中非常有价值。尽管贝叶斯推断是一个在数学上有原则的框架，用于学习参数并进行预测，但它也带来了一些实际挑战，主要是因为需要解决积分问题（见公式 (8.22) 和 (8.23)）。更具体地说，如果我们没有为参数选择一个共轭先验（见 6.6.1 节），那么 (8.22) 和 (8.23) 中的积分通常在解析上是不可解的，我们就无法以封闭形式计算后验、预测或边际似然。在这种情况下，我们必须借助近似方法。例如，可以采用 <strong>随机近似方法</strong>，如 <strong>马尔可夫链蒙特卡罗（MCMC）</strong>（Gilks 等，1996）；或者 <strong>确定性近似方法</strong>，如 <strong>拉普拉斯近似</strong>（Bishop, 2006; Barber, 2012; Murphy, 2012）、<strong>变分推断</strong>（Jordan 等, 1999; Blei 等, 2017）或 <strong>期望传播</strong>（Minka, 2001a）。尽管存在这些挑战，贝叶斯推断已经在许多问题中取得了成功应用，包括：</p><ul><li>大规模主题建模（Hoffman 等, 2013），</li><li>点击率预测（Graepel 等, 2010），</li><li>控制系统中的高效数据利用的强化学习（Deisenroth 等, 2015），</li><li>在线排序系统（Herbrich 等, 2007），</li><li>大规模推荐系统。</li></ul><p>此外，还存在一些通用工具，例如 <strong>贝叶斯优化</strong>（Brochu 等, 2009; Snoek 等, 2012; Shahriari 等, 2016），它们在高效搜索模型或算法的元参数（meta parameters）时非常有用。</p><p><strong>备注：</strong>在机器学习文献中，“（随机）变量（variables）”与“参数（parameters）”之间的区分有时是相对随意的。通常情况下，参数是通过估计得到的（例如最大似然），而变量则通常被边缘化处理。在本书中，我们并不严格区分二者，因为<strong>原则上我们可以为任何参数设定一个先验并将其积分消去</strong>，这样根据上述区分，该参数就会转化为一个随机变量。</p><blockquote><p>个人注：最小二乘估计的参数和预测的结果都是固定的（点估计），最大似然和最大后验估计的参数是固定的（点估计）而预测结果是一个分布；贝叶斯推断的参数和预测结果都是分布；</p><p>详见《点估计和分布估计.md》</p></blockquote><h4 id="潜变量模型"><strong>8.4.3 潜变量模型</strong></h4><p>Latent-Variable Models</p><p><strong>在实际应用中，有时在模型中加入额外的潜变量 <span class="math inline">\(z\)</span>（除了模型参数 <span class="math inline">\(\theta\)</span> 之外）是有用的（Moustaki 等, 2015）。这些潜变量不同于模型参数 <span class="math inline">\(\theta\)</span>，因为它们并不显式地对模型进行参数化。潜变量可以描述数据生成过程，从而增加模型的可解释性。它们还通常能简化模型结构，使我们可以定义更简单且更丰富的模型结构。</strong>模型结构的简化通常伴随着模型参数数量的减少（Paquet, 2008; Murphy, 2012）。</p><p>在潜变量模型中进行学习（至少通过最大似然方法）可以通过 <strong>期望最大化（EM）算法</strong>（Dempster 等, 1977; Bishop, 2006）以有原则的方式进行。潜变量有帮助的例子包括：</p><ul><li>用于降维的主成分分析（第 10 章）</li><li>用于密度估计的高斯混合模型（第 11 章）</li><li>用于时间序列建模的隐马尔可夫模型（Maybeck, 1979）或动力系统（Ghahramani 和 Roweis, 1999; Ljung, 1999）</li><li>元学习和任务泛化（Hausman 等, 2018; Sæmundsson 等, 2018）</li></ul><p>尽管引入这些潜变量可能使模型结构和生成过程更容易理解，但潜变量模型的学习通常仍然很困难，这将在第 11 章中进一步讨论。</p><p>由于潜变量模型也允许我们定义从参数生成数据的过程，让我们来看这个生成过程。用 <span class="math inline">\(x\)</span> 表示数据，<span class="math inline">\(\theta\)</span> 表示模型参数，<span class="math inline">\(z\)</span> 表示潜变量，则条件分布为 <span class="math display">\[p(x \mid \theta, z) \tag{8.24}\]</span> 它允许我们在任意给定的模型参数和潜变量下生成数据。由于 <span class="math inline">\(z\)</span> 是潜变量，我们会为其设定一个先验 <span class="math inline">\(p(z)\)</span>。</p><p>正如前面讨论的模型一样，具有潜变量的模型也可以在 8.3 和 8.4.2 节讨论的框架下，用于参数学习和推断。为了便于学习（例如通过极大似然估计或贝叶斯推断），我们遵循一个两步程序：</p><ol type="1"><li>首先，计算模型的似然 <span class="math inline">\(p(x \mid \theta)\)</span>，它不依赖于潜变量；</li><li>然后，使用这个似然进行参数估计或贝叶斯推断，此时分别使用 8.3 和 8.4.2 节中完全相同的表达式。</li></ol><p>由于似然函数 <span class="math inline">\(p(x \mid \theta)\)</span> 是在给定模型参数下的预测分布，我们需要对潜变量进行边缘化，使得 <span class="math display">\[p(x \mid \theta) = \int p(x \mid \theta, z) p(z) \, dz, \tag{8.25}\]</span> 其中 <span class="math inline">\(p(x \mid \theta, z)\)</span> 如公式 (8.24) 所示，<span class="math inline">\(p(z)\)</span> 是潜变量的先验。注意，似然函数必须 不依赖潜变量 <span class="math inline">\(z\)</span>，而只是数据 <span class="math inline">\(x\)</span> 和模型参数 <span class="math inline">\(\theta\)</span> 的函数。公式 (8.25) 中的似然函数可以直接用于通过最大似然进行参数估计。在模型参数 <span class="math inline">\(\theta\)</span> 上加上先验后，如 8.3.2 节 所述，最大后验估计也同样直接可行。此外，对于潜变量模型，利用似然函数 (8.25) 进行贝叶斯推断（见 8.4.2 节）也是按常规方式进行的：我们在模型参数上设定先验 <span class="math inline">\(p(\theta)\)</span>，然后利用贝叶斯定理得到数据集 <span class="math inline">\(X\)</span> 下模型参数的后验分布 <span class="math display">\[p(\theta \mid X) = \frac{p(X \mid \theta)p(\theta)}{p(X)} \tag{8.26}\]</span> 公式 (8.26) 中的后验分布可以用于贝叶斯推断框架下的预测，见公式 (8.23)。在潜变量模型中，一个挑战是似然函数 <span class="math inline">\(p(X \mid \theta)\)</span> 需要根据公式 (8.25) 对潜变量进行边缘化。除非我们为 <span class="math inline">\(p(x \mid z, \theta)\)</span> 选择共轭先验 <span class="math inline">\(p(z)\)</span>，否则公式 (8.25) 中的边缘化在解析上是不可解的，我们必须采用近似方法（Bishop, 2006; Paquet, 2008; Murphy, 2012; Moustaki 等, 2015）。类似于参数后验 (8.26)，我们可以计算潜变量的后验分布： <span class="math display">\[p(z \mid X) = \frac{p(X \mid z)p(z)}{p(X)}, \quadp(X \mid z) = \int p(X \mid z, \theta)p(\theta) \, d\theta, \tag{8.27}\]</span> 其中 <span class="math inline">\(p(z)\)</span> 是潜变量的先验，而 <span class="math inline">\(p(X \mid z)\)</span> 需要对模型参数 <span class="math inline">\(\theta\)</span> 进行积分边缘化。<strong>由于解析求解积分非常困难，可以看出一般情况下同时对潜变量和模型参数进行边缘化是不可能的</strong>（Bishop, 2006; Murphy, 2012）。一个相对容易计算的量是条件于模型参数的潜变量后验分布，即 <span class="math display">\[p(z \mid X, \theta) = \frac{p(X \mid z, \theta)p(z)}{p(X \mid \theta)}, \tag{8.28}\]</span> 其中 <span class="math inline">\(p(z)\)</span> 是潜变量的先验，<span class="math inline">\(p(X \mid z, \theta)\)</span> 如公式 (8.24) 所示。在第 10 章和第 11 章中，我们将分别推导 <strong>PCA</strong> 和 <strong>高斯混合模型</strong> 的似然函数。此外，我们还将计算 PCA 和高斯混合模型中潜变量的后验分布 (8.28)。</p><p><strong>备注。</strong> 在后续章节中，我们可能不会对潜变量 <span class="math inline">\(z\)</span> 和不确定的模型参数 <span class="math inline">\(\theta\)</span> 做严格区分，也可能将模型参数称为“潜在”或“隐藏”，因为它们未被观测到。在第 10 章和第 11 章中，当使用潜变量 <span class="math inline">\(z\)</span> 时，我们会注意区分两种不同类型的隐藏变量：模型参数 <span class="math inline">\(\theta\)</span> 和潜变量 <span class="math inline">\(z\)</span>。</p><p>我们可以利用概率模型中所有元素都是随机变量的事实，为它们定义一种统一的表示语言。在 8.5 节 中，我们将看到一种简明的图形语言，用于表示概率模型的结构，并将在后续章节中用这种图形语言描述概率模型。</p><h4 id="延伸阅读-2"><strong>8.4.4 延伸阅读</strong></h4><p>机器学习中的概率模型（Bishop, 2006；Barber, 2012；Murphy, 2012）为用户提供了一种有原则的方式，用于捕捉数据和预测模型的不确定性。Ghahramani（2015）对机器学习中的概率模型做了简要综述。对于一个给定的概率模型，我们有时幸运地可以解析地计算出感兴趣的参数。然而，一般情况下，解析解是很少的，因此通常采用计算方法，例如采样（Gilks 等, 1996；Brooks 等, 2011）和变分推断（Jordan 等, 1999；Blei 等, 2017）。Moustaki 等（2015）和 Paquet（2008）对潜变量模型中的贝叶斯推断提供了很好的概述。近年来，出现了若干编程语言，旨在将软件中定义的变量视为对应概率分布的随机变量。其目标是能够编写概率分布的复杂函数，同时在底层由编译器自动处理贝叶斯推断的规则。这个快速发展的领域被称为概率编程（probabilistic programming）。</p><h3 id="有向图模型"><strong>8.5 有向图模型</strong></h3><p><strong>Directed Graphical Models</strong></p><p>在本节中，我们介绍一种用于指定概率模型的图形化语言，称为 有向图模型（Directed Graphical Model）。它提供了一种紧凑且简明的方式来表示概率模型，并允许读者直观地解析随机变量之间的依赖关系。图模型通过可视化方式捕捉了所有随机变量的联合分布如何分解为仅依赖于部分变量的因子的乘积。</p><p>在 8.4 节 中，我们指出<strong>概率模型的联合分布是关键的量</strong>，因为它包含了关于先验、似然和后验的信息。然而，单独的联合分布可能非常复杂，并且并不能告诉我们概率模型的结构特性。例如，联合分布 <span class="math inline">\(p(a, b, c)\)</span> 并不能告诉我们变量之间的独立性关系。这时，图模型就派上用场了。本节依赖于 <strong>独立性和条件独立性</strong> （ independence and conditional independence）的概念，如 6.4.5 节 所述。</p><p><img src="/img3/机器学习的数学基础Part2/F8.9.png" alt="F8.9" style="zoom:50%;" /></p><p>在图模型中，节点表示随机变量。在图 8.9(a) 中，<strong>节点表示随机变量 <span class="math inline">\(a, b, c\)</span>。边表示变量之间的概率关系，例如条件概率。</strong></p><p><strong>备注</strong>：并非每个分布都可以在特定的图模型中表示。关于这方面的讨论可以参见 Bishop (2006)。概率图模型具有一些便利的特性：</p><ul><li>它们是一种简单的方式，用于可视化概率模型的结构。</li><li>可以用于设计或激发新的统计模型。</li><li>仅通过观察图形，就能对模型的性质（例如条件独立性）获得直观理解。</li><li>统计模型中用于推断和学习的复杂计算，可以通过图形操作来表达。</li></ul><h4 id="图的语义">8.5.1 图的语义</h4><p>Graph Semantics</p><p>有向图模型/贝叶斯网络(Directed graphical models/Bayesian networks)是一种用于表示概率模型中<strong>条件依赖关系</strong>的方法。它们通过图形方式描述条件概率，从而为复杂的相互依赖关系提供了一种简洁的语言。同时，这种模块化的描述也带来了计算上的简化。两个节点（随机变量）之间的有向边（箭头）表示条件概率。例如，图 8.9(a) 中从 <span class="math inline">\(a\)</span> 指向 <span class="math inline">\(b\)</span> 的箭头表示在给定 <span class="math inline">\(a\)</span> 的情况下，<span class="math inline">\(b\)</span> 的条件概率 <span class="math inline">\(p(b \mid a)。\)</span>如果我们了解联合分布的某些分解形式，就可以从中推导出有向图模型。</p><p><strong>例 8.7</strong></p><p>考虑三个随机变量 <span class="math inline">\(a, b, c\)</span> 的联合分布： <span class="math display">\[p(a, b, c) = p(c \mid a, b)\, p(b \mid a)\, p(a) \tag{8.29}\]</span> 联合分布 (8.29) 的分解告诉了我们关于这些随机变量之间关系的一些信息：</p><ul><li><span class="math inline">\(c\)</span> 直接依赖于 <span class="math inline">\(a\)</span> 和 <span class="math inline">\(b\)</span>；</li><li><span class="math inline">\(b\)</span> 直接依赖于 <span class="math inline">\(a\)</span>；</li><li><span class="math inline">\(a\)</span> 不依赖于 <span class="math inline">\(b\)</span> 也不依赖于 <span class="math inline">\(c\)</span>。</li></ul><p>对于分解式 (8.29)，我们得到的有向图模型如图 8.9(a) 所示。</p><p>一般来说，我们可以根据分解后的联合分布来构造相应的有向图模型，方法如下：</p><ol type="1"><li>为所有随机变量创建一个节点。</li><li>对于每一个条件分布，在图中添加一条从条件变量对应节点指向该变量节点的有向边（箭头）。</li></ol><p><strong>图的布局取决于联合分布的分解方式。</strong>我们刚刚讨论了如何从一个已知的联合分布分解推导出相应的有向图模型。接下来，我们将做相反的事情：描述如何从一个给定的图模型中提取一组随机变量的联合分布。</p><p><strong>例 8.8</strong> 观察图 8.9(b) 中的图模型，我们利用以下两个性质：</p><ul><li>我们要求的联合分布 <span class="math inline">\(p(x_1, \ldots, x_5)\)</span> 是一组条件分布的乘积，每个图中的节点对应一个条件分布。在本例中，我们需要五个条件分布（个人注：3个条件分布加上2个独立分布）。</li><li>每个条件分布只依赖于图中该节点的父节点。例如，<span class="math inline">\(x_4\)</span> 依赖于 <span class="math inline">\(x_2\)</span>。</li></ul><p>利用这两个性质，我们可以得到联合分布的分解形式： <span class="math display">\[p(x_1, x_2, x_3, x_4, x_5) = p(x_1)p(x_5)p(x_2 \mid x_5)p(x_3 \mid x_1, x_2)p(x_4 \mid x_2). \tag{8.30}\]</span></p><p>一般来说，联合分布 <span class="math display">\[p(x) = p(x_1, \dots, x_K)\]</span> 可以写作 <span class="math display">\[p(x) = \prod_{k=1}^K p(x_k \mid \text{Pa}_k), \tag{8.31}\]</span> 其中 <span class="math inline">\(\text{Pa}_k\)</span> 表示 “<span class="math inline">\(x_k\)</span> 的父节点”。父节点是指<strong>有</strong>箭头指向 <span class="math inline">\(x_k\)</span> 的节点。</p><p>我们用一个具体的抛硬币实验来结束这一小节。考虑一个伯努利实验（例 6.8），在该实验中结果 <span class="math inline">\(x\)</span> 为“正面”的概率是 <span class="math display">\[p(x \mid \mu) = \text{Ber}(\mu). \tag{8.32}\]</span> 现在我们将这个实验重复 <span class="math inline">\(N\)</span> 次，并观测到结果 <span class="math inline">\(x_1, \dots, x_N\)</span>，于是我们得到联合分布： <span class="math display">\[p(x_1, \dots, x_N \mid \mu) = \prod_{n=1}^N p(x_n \mid \mu). \tag{8.33}\]</span> 右边的表达式是对每个单次结果的伯努利分布的乘积，这是因为这些实验相互独立。回顾第 6.4.5 节，<strong>统计独立意味着分布可以因式分解</strong>(个人注：因式分解是指联合分布)。为了把这种情况写成图模型，我们需要区分 未观测/潜在变量和观测变量（unobserved/latent variables and observed variables）。在图中，观测变量用阴影节点表示，于是我们得到图 8.10(a) 所示的模型。</p><p><img src="/img3/机器学习的数学基础Part2/F8.10.png" alt="F8.10" style="zoom:100%;" /></p><p>我们看到，单一参数 <span class="math inline">\(\mu\)</span> 对所有 <span class="math inline">\(x_n, n=1, \dots, N\)</span> 都相同，因为这些观测结果是同分布的。一个更简洁但等价的图模型如图 8.10(b) 所示，我们使用 <strong>板式记号（plate notation）</strong>。板（方框）表示其中的所有内容（在这里是观测值 <span class="math inline">\(x_n\)</span>）重复 <span class="math inline">\(N\)</span> 次。因此，这两种图模型是等价的，但板式记号更简洁。图模型还可以立即让我们在 <span class="math inline">\(\mu\)</span> 上引入一个 超先验（hyperprior）。超先验是对第一层先验的参数再加一层先验分布。在图 8.10(c) 中，我们对潜在变量 <span class="math inline">\(\mu\)</span> 施加了一个 <span class="math inline">\(\text{Beta}(\alpha, \beta)\)</span> 先验。<strong>如果我们把 <span class="math inline">\(\alpha\)</span> 和 <span class="math inline">\(\beta\)</span> 看作确定性参数（即非随机变量），那么就省略其周围的圆圈。</strong></p><h4 id="条件独立与-d-分离">8.5.2 条件独立与 d-分离</h4><p>有向图模型（Directed Graphical Models）使我们仅通过观察图，就能找到联合分布的条件独立性（第 6.4.5 节）关系属性。一个称为 d-分离（d-separation, Pearl, 1988）的概念对此至关重要。</p><p>考虑一个一般的有向图，其中 A, B, C 是不相交的任意节点集合（它们的并集可能小于整个图的节点集合）。我们希望确定一个特定的条件独立性陈述是否成立：“在给定 C 的条件下，A 与 B 条件独立”，记作</p><p><span class="math display">\[A \perp\!\!\!\perp B \mid C \tag{8.34}\]</span> 该条件独立性是否由一个给定的<strong>有向无环图</strong>所蕴含？为此，我们考虑从 A 中的任意节点到 B 中任意节点的所有可能路径（trail，指忽略箭头方向的路径）。如果一条路径<strong>包含某个节点</strong>，并且满足以下<strong>任意一个条件</strong>，则该路径被称为<strong>阻塞</strong>（blocked）：</p><ol type="1"><li>在该节点上，路径的箭头是 <strong>尾对头</strong> 或 <strong>尾对尾</strong> 相接，并且该节点属于集合 C。</li><li>在该节点上，路径的箭头是 <strong>头对头</strong> 相接，并且该节点以及它的所有后代都不属于集合 C。</li></ol><p>如果所有路径都被阻塞，那么称 A和B被C <strong>d-分离</strong>（d-separated），并且图中所有变量的联合分布将满足 <span class="math display">\[A \perp\!\!\!\perp B \mid C\]</span></p><p><img src="/img3/机器学习的数学基础Part2/F8.11.png" alt="F8.11" style="zoom:60%;" /></p><p><strong>例 8.9 （条件独立）</strong></p><p>考虑图 8.11 中的图模型。通过直接观察，我们得到： <span class="math display">\[\begin{align}b \perp\!\!\!\perp d \mid a, c \tag{8.35}\\a \perp\!\!\!\perp c \mid b \tag{8.36}\\b \not\!\perp\!\!\!\perp d \mid c \tag{8.37}\\a \not\!\perp\!\!\!\perp c \mid b, e \tag{8.38}\end{align}\]</span></p><blockquote><p>个人注：自己对d-分离理解还是不够透彻。详见《d-分离 (d-separation) .md》和《条件独立.md》</p></blockquote><p>有向图模型能够对概率模型进行紧凑表示，我们将在第 9、10 和 11 章中看到有向图模型的实例。这种表示形式结合条件独立的概念，使我们能够将相应的概率模型分解成更容易优化的表达式。概率模型的图示表示还能让我们直观地看到建模设计选择对模型结构的影响。我们通常需要对模型结构做出一些高层次的假设。这些建模假设（即超参数hyperparameters）会影响预测性能，但不能直接通过之前介绍的方法来选择。我们将在第 8.6 节讨论选择模型结构的不同方法。</p><h4 id="延伸阅读-3">8.5.3 延伸阅读</h4><p>关于概率图模型的入门介绍，可以参考 Bishop (2006，第 8 章)；关于其不同应用及相应算法含义的更全面描述，可以参考 Koller 和 Friedman (2009) 的著作。<strong>概率图模型主要有三类</strong>：</p><ul><li>有向图模型（贝叶斯网络）Directed graphical models (Bayesian networks)；见图 8.12(a)</li><li>无向图模型（马尔可夫随机场）Undirected graphical models (Markov random fields)；见图 8.12(b)</li><li>因子图factor graph；见图 8.12(c)</li></ul><p><img src="/Users/hongyuanjiao/Desktop/《机器学习的数学基础》Part2/F8.12.png" alt="F8.12" style="zoom:67%;" /></p><p>概率图模型支持基于图的推断与学习算法，例如局部信息传递。其应用范围十分广泛，从在线游戏排名 (Herbrich 等, 2007)，到计算机视觉（如图像分割、语义标注、图像去噪、图像修复 (Kittler 和 Foglein, 1984; Sucar 和 Gillies, 1994; Shotton 等, 2006; Szeliski 等, 2008)），再到编码理论 (McEliece 等, 1998)、解线性方程组 (Shental 等, 2008)、以及信号处理中迭代的贝叶斯状态估计 (Bickson 等, 2007; Deisenroth 和 Mohamed, 2012)。</p><p>在实际应用中，有一个特别重要但本书未展开讨论的话题是<strong>结构化预测</strong>（structured prediction） (Bakir 等, 2007; Nowozin 等, 2014)。它使机器学习模型能够处理带有结构的预测任务，例如序列、树和图。神经网络模型的普及使更灵活的概率模型得以应用，从而带来了许多结构化模型的有用应用 (Goodfellow 等, 2016，第 16 章)。</p><blockquote><p>个人注：<strong>结构化预测</strong>是指非参数的模型吗？</p></blockquote><p>近年来，概率图模型因其在因果推断中的应用而重新引起了广泛关注 (Pearl, 2009; Imbens 和 Rubin, 2015; Peters 等, 2017; Rosenbaum, 2017)。</p><h3 id="模型选择">8.6 模型选择</h3><p>在机器学习中，我们常常需要做出一些高层次的建模决策，而这些决策会对模型的性能产生关键性的影响。我们所做的选择（例如，似然函数的形式）会影响模型中自由参数的数量和类型，从而也影响模型的灵活性与表达能力。更复杂的模型往往更灵活，因为它们能够描述更多样的数据集。</p><p>例如，次数为 1 的多项式（直线 <span class="math inline">\(y = a_0 + a_1x\)</span>）只能用来描述输入 <span class="math inline">\(x\)</span> 与观测值 <span class="math inline">\(y\)</span> 之间的线性关系。而次数为 2 的多项式则还能额外描述输入与观测之间的二次关系。</p><p>此时，人们可能会认为，越灵活的模型一般越优越，因为它们更具表达能力。然而，一个普遍的问题在于，在训练过程中，我们只能使用训练集来评估模型性能并学习其参数。然而，训练集上的性能并不是我们真正关心的。在第 8.3 节中，我们已经看到极大似然估计可能会导致过拟合，尤其是在训练数据集较小时更为明显。理想情况下，我们的模型在测试集（训练时不可用）上也应当表现良好。<strong>因此，我们需要一些机制来评估模型对未见过的测试数据的泛化能力。<em><u>模型选择</u></em>研究的正是这个问题。</strong></p><h4 id="嵌套交叉验证">8.6.1 嵌套交叉验证</h4><p>我们之前已经看到过一种可用于模型选择的方法（第 8.2.4 节中的交叉验证）。回顾一下，交叉验证通过反复将数据集划分为训练集和验证集，来估计泛化误差。我们可以再应用一次这个想法，也就是说，对每一次划分，我们再进行一轮交叉验证。这有时被称为<strong>嵌套交叉验证</strong>（见图 8.13）。</p><p><img src="/img3/机器学习的数学基础Part2/F8.13.png" alt="F8.13" style="zoom:67%;" /></p><p>内层( inner cross-validation level)用于估计某个特定模型或超参数在内部验证集上的表现。外层( outer level)用于估计由内层选择出的最优模型的泛化性能。我们可以在<strong>内层测试不同的模型和超参数选择</strong>。为了区分这两个层次，用于估计泛化性能的集合通常称为<strong>测试集</strong>( test set)，而用于选择最佳模型的集合称为<strong>验证集</strong>(validation set)。</p><p>内层循环通过近似计算验证集上的经验误差，来估计给定模型的期望泛化误差（公式 8.39）： <span class="math display">\[E_{\mathcal V} [R(\mathcal V | M)] \approx \frac{1}{K} \sum_{k=1}^{K} R(\mathcal V^{(k)} | M), \tag{8.39}\]</span> 其中 <span class="math inline">\(R(\mathcal V | M)\)</span> 是模型 <span class="math inline">\(M\)</span> 在验证集 <span class="math inline">\(\mathcal V\)</span> 上的经验风险（例如均方根误差）。我们对所有模型重复这个过程，并选择表现最好的模型。</p><p>需要注意的是，交叉验证不仅能给出期望泛化误差，还能得到高阶统计量，例如标准误差，它能够衡量均值估计的不确定性。</p><p>一旦模型被选定，我们就可以在测试集上评估最终性能。</p><blockquote><p>个人注：</p><p><strong>普通交叉验证</strong>：常用于评估模型的泛化性能，然后对不同的假设（模型）的性能做对比，不是确定最终模型。</p><p><strong>嵌套交叉验证</strong>：主要用于 <strong>模型选择（超参数调优）+ 模型性能评估</strong>，避免“数据泄漏”导致的性能高估。</p><p>内层循环的作用：</p><ul><li><strong>针对不同超参数组合</strong>，依次进行 <span class="math inline">\(M\)</span>-折交叉验证，计算平均验证性能。</li><li><strong>选出性能最好的超参数。</strong></li></ul><p>详见《嵌套交叉验证.md》</p></blockquote><blockquote><p>个人注：</p><p>引用前文8.2.2节：</p><p>对于给定的训练集 <span class="math inline">\(\{(x_1, y_1), \dots, (x_N, y_N)\}\)</span>，我们引入矩阵表示法：样本矩阵 <span class="math display">\[X := [x_1, \dots, x_N]^\top \in \mathbb{R}^{N \times D}\]</span> 以及标签向量 <span class="math display">\[y := [y_1, \dots, y_N]^\top \in \mathbb{R}^N .\]</span> 使用这种矩阵表示法，平均损失定义为： <span class="math display">\[R_{\text{emp}}(f, X, y) = \frac{1}{N} \sum_{n=1}^{N} \ell(y_n, \hat{y}_n), \quad \text{其中 } \hat{y}_n = f(x_n, \theta). \tag{8.6}\]</span> <strong>式 (8.6) 称为经验风险</strong>(empirical risk)，它依赖三个参数：预测器 <span class="math inline">\(f\)</span> 和数据 <span class="math inline">\(X, y\)</span>。这种基于经验风险的学习策略称为 <strong>经验风险最小化</strong>（Empirical Risk Minimization）。</p></blockquote><h4 id="贝叶斯模型选择"><strong>8.6.2 贝叶斯模型选择</strong></h4><p>模型选择有很多方法，本节会介绍其中的一些。一般来说，它们的共同目标是<strong>在模型复杂度与数据拟合程度之间做权衡</strong>。我们通常假设简单模型比复杂模型更不容易过拟合，因此模型选择的目标是找到一个<strong>足够简单但又能合理解释数据的模型</strong>。这个思想也被称为<strong>奥卡姆剃刀（Occam’s razor）</strong>。</p><p><strong>备注</strong>：<u><em>如果把模型选择看作一个假设检验问题，那么我们就是在寻找与数据相一致的最简单的假设</em></u>（Murphy, 2012）。</p><p>有人可能会考虑在模型上加一个先验分布，偏向于简单模型。不过这并不是必须的：所谓“自动奥卡姆剃刀”已经自然地体现在贝叶斯概率的应用中（Smith and Spiegelhalter, 1980; Jefferys and Berger, 1992; MacKay, 1992）。图 8.14（改编自 MacKay, 2003）给出了基本直觉，即为什么复杂、表达能力很强的模型反而可能对给定数据集 D 的建模来说不太合适。</p><p><img src="/img3/机器学习的数学基础Part2/F8.14.png" alt="F8.14" style="zoom:50%;" /></p><p>可以把横轴看作所有可能数据集 <span class="math inline">\(D\)</span> 的空间。如果我们关注给定数据 <span class="math inline">\(D\)</span> 时模型 <span class="math inline">\(M_i\)</span> 的后验概率 <span class="math inline">\(p(M_i | D)\)</span>，就可以使用贝叶斯定理。假设所有模型的先验 <span class="math inline">\(p(M)\)</span> 是均匀分布，那么贝叶斯定理会奖励那些对实际出现的数据预测得更好的模型。这种给定模型 $M_i $对数据 <span class="math inline">\(D\)</span> 的预测概率 <span class="math inline">\(p(D | M_i)\)</span>，称为模型 $M_i $的<strong>证据（evidence）</strong>。</p><p>一个简单模型 $M_1 $只能预测很少一部分数据集，这用 <span class="math inline">\(p(D | M_1)\)</span> 表示；而一个更强大的模型 <span class="math inline">\(M_2\)</span>（比如拥有比 $M_1 <span class="math inline">\(更多的自由参数）能够预测更广泛的数据集。但这同时意味着，\)</span>M_2$ 对于某些区域 <span class="math inline">\(C\)</span> 的数据集预测得不如 <span class="math inline">\(M_1\)</span> 好。假设我们给两个模型分配了相同的先验概率，如果实际数据落在区域 <span class="math inline">\(C\)</span>，那么较简单的模型 <span class="math inline">\(M_1\)</span> 反而更可能是正确的模型。</p><p>在本章前面我们提到过，<strong>模型必须能够解释数据，即模型应当能够生成数据。</strong>如果模型是从数据中正确学习到的，那么我们希望它生成的数据应当与观测数据相似。为了实现这一点，把模型选择表述为一个<strong>分层推断问题（hierarchical inference problem）</strong>是有帮助的，这样我们就能够计算出模型的后验分布。</p><p><img src="/img3/机器学习的数学基础Part2/F8.15.png" alt="F8.15" style="zoom:67%;" /></p><p>我们考虑一个有限个模型的集合 <span class="math inline">\(M = \{M_1, \dots, M_K\},\)</span> 其中每个模型 <span class="math inline">\(M_k\)</span> 拥有参数 <span class="math inline">\(\theta_k\)</span>。在贝叶斯模型选择（Bayesian model selection）中，我们在模型集合上放置一个先验分布 <span class="math inline">\(p(M)\)</span>。相应的生成过程（generative process）允许我们从该模型中生成数据：</p><p><span class="math display">\[M_k \sim p(M) \tag{8.40}\]</span> 这一过程如图 8.15 所示。给定一个训练集 <span class="math inline">\(D\)</span>，我们应用贝叶斯定理来计算模型的后验分布： <span class="math display">\[p(M_k \mid D) \propto p(M_k)\,p(D \mid M_k). \tag{8.43}\]</span> 注意，这个后验分布不再依赖于模型参数 <span class="math inline">\(\theta_k\)</span>，因为在贝叶斯框架中它们已经被积分掉了： <span class="math display">\[p(D \mid M_k) = \int p(D \mid \theta_k)\,p(\theta_k \mid M_k)\, d\theta_k, \tag{8.44}\]</span> 其中 <span class="math inline">\(p(\theta_k \mid M_k)\)</span> 是模型 <span class="math inline">\(M_k\)</span> 的参数 <span class="math inline">\(\theta_k\)</span> 的先验分布。式 (8.44) 被称为 <strong>模型证据（model evidence）</strong> 或 <strong>边际似然（marginal likelihood）</strong>。根据式 (8.43) 的后验分布，我们可以确定最大后验（MAP）估计：</p><p><span class="math display">\[M^* = \arg\max_{M_k} p(M_k \mid D). \tag{8.45}\]</span> 如果先验是均匀的，即 <span class="math inline">\(p(M_k) = \tfrac{1}{K}\)</span>，也就是说每个模型都有相同的（先验）概率，那么模型的 MAP 估计就等价于选择 使模型证据 (8.44) 最大的模型。</p><p><strong>备注（似然与边际似然 ）</strong>Likelihood and Marginal Likelihood 似然与边际似然（证据）之间存在一些重要区别：</p><ul><li>似然容易过拟合，而边际似然通常不会，因为模型参数已被边际化（即我们不再需要去拟合参数本身）。</li><li>此外，<strong>边际似然会自动体现出模型复杂度与数据拟合之间的权衡（奥卡姆剃刀原则，Occam’s razor）。</strong></li></ul><h4 id="贝叶斯因子用于模型比较">8.6.3 贝叶斯因子用于模型比较</h4><p>考虑在给定数据集 <span class="math inline">\(D\)</span> 的情况下，比较两个概率模型 <span class="math inline">\(M_1\)</span> 和 <span class="math inline">\(M_2\)</span> 的问题。如果我们计算后验概率 <span class="math inline">\(p(M_1 \mid D)\)</span> 和 <span class="math inline">\(p(M_2 \mid D)\)</span>，那么我们可以得到它们的比值：</p><p><span class="math display">\[\begin{equation}\underbrace{\frac{p(M_{1}\mid \mathcal{D})}{p(M_{2}\mid \mathcal{D})}}_{\text{posterior odds}}= \frac{\frac{p(\mathcal{D}\mid M_{1})p(M_{1})}{p(\mathcal{D})}}{\frac{p(\mathcal{D}\mid M_{2})p(M_{2})}{p(\mathcal{D})}}= \underbrace{\frac{p(M_{1})}{p(M_{2})}}_{\text{prior odds}}\underbrace{\frac{p(\mathcal{D}\mid M_{1})}{p(\mathcal{D}\mid M_{2})}}_{\text{Bayes factor}} .\tag{8.46}\end{equation}\]</span></p><p>后验比（posterior odds）就是后验概率的比值。在式 (8.46) 的右边，第一个分式称为 <strong>先验比</strong>，它衡量的是我们在观察数据之前对 <span class="math inline">\(M_1\)</span> 相较于 <span class="math inline">\(M_2\)</span> 的偏好。第二个分式，即边际似然（marginal likelihood）的比值，被称为 <strong>贝叶斯因子</strong>，它衡量的是在两个模型下，数据 <span class="math inline">\(D\)</span> 的可预测程度差异。</p><p><strong>备注（Jeffreys–Lindley 悖论）</strong> Jeffreys–Lindley 悖论指出： “贝叶斯因子总是偏向于更简单的模型，因为在一个复杂模型下，若采用弥散先验（diffuse prior），数据的概率会非常小。”（Murphy, 2012）这里的弥散先验指的是一种没有明显偏好、即对许多模型都先验上认为合理的先验分布。</p><p>如果我们在模型之间采用均匀先验，那么式 (8.46) 中的先验比为 <span class="math inline">\(1\)</span>，即： <span class="math display">\[\text{后验比} \;=\; \frac{p(D \mid M_1)}{p(D \mid M_2)} \tag{8.47}\]</span> 在这种情况下，贝叶斯因子就是边际似然的比值。如果贝叶斯因子大于 1，我们选择模型 <span class="math inline">\(M_1\)</span>；否则选择模型 <span class="math inline">\(M_2\)</span>。与频率学派统计方法类似，人们也提出了一些关于贝叶斯因子大小的经验性指南，用于判断结果是否“显著”（Jeffreys, 1961）。</p><p><strong>备注（计算边际似然）</strong> 边际似然在模型选择中起着重要作用：我们需要它来计算贝叶斯因子 (8.46)，以及模型的后验分布 (8.43)。不幸的是，边际似然的计算涉及积分 (8.44)，而这个积分通常在解析上不可解。因此我们必须借助近似方法，例如：</p><ul><li>数值积分（Stoer and Burlirsch, 2002），</li><li>基于 Monte Carlo 的随机近似（Murphy, 2012），</li><li>或者贝叶斯 Monte Carlo 技术（O’Hagan, 1991; Rasmussen and Ghahramani, 2003）。</li></ul><p>不过，在一些特殊情况下，这个积分是可以解析解出的。在 第 6.6.1 节 中，我们讨论过共轭模型。如果选择一个共轭先验 <span class="math inline">\(p(\theta)\)</span>，则可以直接得到边际似然的闭式解。在 第 9 章 中，我们将在 <strong>线性回归</strong> 的背景下展示这一点。</p><p>我们在本章中已经对机器学习的基本概念进行了简要介绍。在本书接下来的部分中，我们将看到第 8.2、8.3 和 8.4 节中三种不同类型的学习方法是如何应用于机器学习的四大支柱（回归、降维、密度估计和分类 (regression, dimensionality reduction, density estimation, and classification）的。</p><h4 id="延伸阅读-4">8.6.4 延伸阅读</h4><p>我们在本节开头提到，<strong>高层次的建模选择会影响模型的性能。例子包括</strong>：</p><ul><li>回归问题中多项式的次数</li><li>混合模型中的成分数目</li><li>（深度）神经网络的网络结构</li><li>支持向量机中的核函数类型</li><li>主成分分析（PCA）中潜在空间的维度</li><li>优化算法中的学习率（或学习率调度）</li></ul><p>Rasmussen 和 Ghahramani（2001）表明，自动奥卡姆剃刀并不一定惩罚模型中参数的数量，而是作用于函数的复杂性。他们还表明，这种自动奥卡姆剃刀同样适用于具有大量参数的贝叶斯非参数模型，例如高斯过程。</p><p><strong>如果我们<em><u>专注于极大似然估计</u></em>，则存在许多用于模型选择的启发式方法来避免过拟合。这些方法被称为信息准则（information criteria），我们通常选择使其值最大的模型。</strong></p><p><strong>赤池信息准则（Akaike Information Criterion, AIC）</strong>（Akaike, 1974）： <span class="math display">\[\log p(x|\theta) - M \quad (8.48)\]</span> 它通过增加一个惩罚项来修正极大似然估计的偏差，以补偿复杂模型（具有大量参数）所带来的过拟合问题。这里 <span class="math inline">\(M\)</span> 表示模型参数的个数。AIC 用于估计给定模型所丢失的相对信息量。</p><p><strong>贝叶斯信息准则（Bayesian Information Criterion, BIC）</strong>（Schwarz, 1978）： <span class="math display">\[\log p(x) = \log \int p(x|\theta)p(\theta)\, d\theta \approx \log p(x|\theta) - \tfrac{1}{2} M \log N \quad (8.49)\]</span> 它可用于指数族分布。在这里，<span class="math inline">\(N\)</span> 是数据点的数量，<span class="math inline">\(M\)</span> 是参数的数量。与 AIC 相比，BIC 对模型复杂度的惩罚更为严格。</p>]]></content>
      
      
      <categories>
          
          <category> 翻译 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 数学 </tag>
            
            <tag> 算法 </tag>
            
            <tag> 统计 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ESL读书笔记</title>
      <link href="/2025/07/26/ESL%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
      <url>/2025/07/26/ESL%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<p>ESL读书笔记</p><p>《The Elements of Statistical Learning - Data Mining, Inference and Prediction - 2nd Edition (ESLII_print4)》</p><blockquote><p>引用“统计学的那些事” （https://cosx.org/2011/12/stories-about-statistical-learning）</p><p>文章看得差不多了，就反复看他们的那本书“The Elements of Statistical learning”（以下简称 ESL）。说实话，不容易看明白，也没有人指导，我只好把文章和书一起反复看，就这样来来回回折腾。比如为看懂 Efron 的“Least angle regression”，我一个人前前后后折腾了一年时间（个人资质太差）。当时国内还有人翻译了这本书（2006 年），把名字翻译为“统计学习基础”。我的神啦，这也叫“基础”！还要不要人学啊！难道绝世武功真的要练三五十年？其实正确的翻译应该叫“<strong>精要</strong>”。在我看来，这本书所记载的是绝世武功的要义，强调的是整体的理解，联系和把握，绝世武功的细节在他们的文章里。</p></blockquote><p><span id="more"></span></p><h2 id="总结">总结</h2><h3 id="贝叶斯定理">贝叶斯定理</h3><ul><li><p>核心是贝叶斯定理，贝叶斯定理在统计中的应用就像牛顿定理在物理学的地位一样。</p></li><li><p>贝叶斯定理的核心是需要理解<strong>似然函数</strong>。</p></li><li><p>P(A|B) = P(B|A)P(A) / P(B) 这个公式是针对离散的概率。</p></li><li><p>条件概率的核心是根据三个条件：样本总体的分布+先验信息(P(A))+样本的信息(P(B|A)) ， 得到后验概率（分布）（P(A|B））。</p></li><li><p>贝叶斯推断中，我们需要确定一个在给定参数时数据的<strong>采样模型</strong> $(Z;) $(密度函数或者概率质量函数)，以及反映我们在得到数据之前对于 <span class="math inline">\(\theta\)</span> 认知的先验分布 <span class="math inline">\(\Pr(\theta)\)</span>．然后计算后验分布： <span class="math display">\[\Pr(\theta\mid\mathbf Z)=\frac{\Pr(\mathbf Z\mid\theta)\cdot \Pr(\theta)}{\int \Pr(\mathbf Z\mid \theta)\cdot \Pr(\theta)d\theta}\tag{8.23}\]</span> 它表示当我们知道数据后更新对 <span class="math inline">\(\theta\)</span> 的认知．为了理解这一后验分布，可以从中抽取样本或者通过计算均值或众数来描述它．贝叶斯方法与一般推断方法的不同之处在于，用先验分布来表达知道数据之前的这种不确定性，而且在知道数据之后允许不确定性继续存在，将它表示成后验分布．</p></li><li><p>自己学会举一个生活中的例子，就说明你理解了。</p></li></ul><h3 id="tips">Tips</h3><ul><li><p>自由度；也就是无约束参数的个数？</p></li><li><p>有偏无偏估计，个人理解例如Lasso这些方法增加了约束条件就是有偏估计？</p></li><li><p>二次规划问题：<strong>二次规划 = 二次目标函数 + 线性约束 + 有限维变量空间的凸优化问题。</strong></p></li><li><p>广义线性模型类，它们都是以同样的方式扩展为广义可加模型。</p></li><li><p>怎么判断函数的凸性：对于任意两点之间的连线，<strong>总是在函数图像之上或重合</strong>。</p></li><li><p>中心化：使得均值为 0； 标准化:使得均值为 0 、方差为 1</p></li><li><p>为什么引入随机效应后会有如此神奇的疗效？</p></li></ul><h3 id="概念">概念</h3><ul><li><p>假设检验：基于小概率的反证法。 提出假设（置信水平），计算抽样的样本统计量，计算概率，判断是否小概率事件（根据置信水平）；如果是小概率事件则假设不成立。</p></li><li><p>标准差：是衡量样本个体的离散程度； 标准误：<strong>样本统计量</strong>的标准差；是衡量抽样样本水平（样本统计量，均值是其中一个统计量）的离散程度（或者叫抽样误差的程度）。</p></li><li><p>t-检验可用于对回归系数的检验。 t = （样本统计量 - 总体参数）/ 样本统计量标准差（或者叫标准误） t检验本质是：当数据服从t分布的时候，检验某一样本统计量是否与总体参数相等。</p></li><li><p>条件概率和似然函数的区别 同一个表达式 <span class="math inline">\(P(x \mid \theta)\)</span>，既可以是<strong>条件概率</strong>，也可以是<strong>同一个表达式 <span class="math inline">\(P(x \mid \theta)\)</span>，既可以是条件概率，也可以是似然函数，取决于我们把哪个当变量、哪个当已知！</strong>，取决于我们把哪个当变量、哪个当已知！</p></li><li><p>偏差（bias）：真实值 - 预测值（拟合的结果） bias, the amount by which the average of our estimate differs from the true mean。</p></li><li><p>偏差 (deviance)：偏差是用来比较两个不同模型的。我们通过将一个模型的偏差减去另一个模型的偏差来进行比较。 一篇写得很棒的博客，<a href="https://statisticaloddsandends.wordpress.com/2019/03/27/what-is-deviance/">What is deviance? -- by kjytay</a></p></li><li><p>有效参数个数 (effective number of parameters)</p></li><li><p>我们知道了一个变量的分布，要生成一批样本服从这个分布，这个过程就叫采样。 听起来好像很简单，对一些简单的分布函数确实如此，比如，均匀分布、正太分布，但只要分布函数稍微复杂一点，采样这个事情就没那么简单了。为什么要采样在讲具体的采样方法之前，有必要弄清楚采样的目的。为什么要采样呢？有人可能会这样想，样本一般是用来估计分布参数的，现在我都知道分布函数了，还采样干嘛呢？其实采样不只是可以用来估计分布参数，还有其他用途，比如说用来估计分布的期望、高阶动量等。</p></li><li><p>贝叶斯误差（Bayes Error） 是统计学习理论中的一个核心概念，指的是在已知真实分布的最优分类器下，仍然不可避免的分类错误率。它代表了任何分类器都无法超越的理论最小错误率，是分类问题中的“理论下限”。</p></li><li><p>独立与不相关 统计上, 连续型随机变量 <span class="math inline">\(X\)</span> 与 <span class="math inline">\(Y\)</span> 独立的定义为 <span class="math display">\[  p(x, y)=p_X(x)p_Y(y)\;\forall x,y  \]</span> 而不相关的定义为 <span class="math display">\[  \text {Cov}(X, Y)=0  \]</span> 独立意味着不相关，但反之不对．对于二元正态随机变量，两者等价．</p><blockquote><p>不相关但不独立的例子： <span class="math inline">\(X\)</span> 是从区间 <span class="math inline">\([-1, 1]\)</span> 上均匀分布的随机变量； <span class="math inline">\(Y = X^2\)</span> 则： <span class="math inline">\(X\)</span> 和 <span class="math inline">\(Y\)</span> 是<strong>不相关</strong>的（因为 <span class="math inline">\(E[X] = 0\)</span>，<span class="math inline">\(E[XY] = 0\)</span>） 但 <span class="math inline">\(X, Y\)</span> <strong>不是独立的</strong>，因为知道 <span class="math inline">\(X\)</span> 的值后，<span class="math inline">\(Y\)</span> 就完全确定。</p></blockquote></li><li><p><strong>马尔科夫蒙特卡洛法 (Markov chain Monte Carlo)</strong>．我们将要看到吉布斯采样（一个 MCMC 过程）</p></li><li><p>吉布斯采样（Gibbs Sampling） 吉布斯采样是MCMC的一个特例，<strong>吉布斯采样的牛逼之处在于只需要知道条件概率的分布，便可以通过采样得到联合概率分布的样本；核心在七个字：一维一维的采样：</strong></p><p>具体步骤：</p><ol type="1"><li><p>初始化：首先给每个变量一个初始值（通常是随机选择的）。</p></li><li><p>循环抽样：依次更新每个变量，具体过程是：</p><ul><li><p>在给定当前所有其他变量的情况下，从该变量的条件分布中抽样。</p></li><li><p>用新的样本值替代当前变量的值，并更新系统。</p></li></ul></li><li><p>迭代收敛：重复上述抽样过程足够多次，随着迭代进行，样本将会逐渐收敛于目标的联合分布。</p></li></ol></li><li><p>自然三次样条 （详见原书 5.2 分段多项式和样条）</p><p><strong>三次样条（cubic spline）</strong>是将数据区间划分为若干个小区间，每个区间内用一个三次多项式拟合，且整体函数在区间连接点处保持：</p><ul><li>函数值连续（<span class="math inline">\(C^0\)</span>）</li><li>一阶导数连续（<span class="math inline">\(C^1\)</span>）</li><li>二阶导数连续（<span class="math inline">\(C^2\)</span>）</li></ul><p><strong>自然三次样条（Natural cubic spline）</strong>是在此基础上，<strong>对两端的两个点添加了“自然”条件</strong>：<span class="math inline">\(f&#39;&#39;(x_1) = f&#39;&#39;(x_n) = 0\)</span></p><p>也就是说，样条函数在两端的二阶导数为 0，表示在端点处“趋于线性”。</p></li></ul><h2 id="第一章-导言">第一章 导言</h2><h3 id="这本书是如何组织的">这本书是如何组织的</h3><p>我们的观点是一个人在尝试掌握复杂的概念前必须理解简单的方法．因此，当在<strong>第二章</strong>对监督学习给出概要后，我们在<strong>第三和第四章</strong>讨论了回归和分类的线性方法．<strong>第五章</strong>我们描述了对单个自变量的样条，小波和正则/惩罚方法，<strong>第六章</strong>则描述了核方法和局部回归．上述两种方法都是高维学习技术的重要基石．模型评估与选择是<strong>第七章</strong>的主题，包括偏差和方差，过拟合和用于选择模型用的交叉验证．<strong>第八章</strong>讨论了模型推断与平均，概要地介绍了极大似然法、贝叶斯推断、自助法、EM算法、吉布斯抽样和bagging．与此相关的boosting过程是<strong>第十章</strong>的重点．</p><p>在<strong>第九到十三章</strong>我们描述了一系列用于监督学习的结构化方法，其中<strong>第九和第十一章</strong>介绍回归以及<strong>第十二和第十三章</strong>重点在分类．<strong>第十四章</strong>描述了非监督学习的方法．两个最近提出来的技术，随机森林和集成学习将在<strong>第十五和第十六章</strong>讨论．我们在<strong>第十七章</strong>讨论无向图模型以及最后在<strong>第十八章</strong>学习高维问题．</p><p>在每一张结尾我们讨论计算需要考虑的因素，这对於数据挖掘的应用是很重要的，包括计算量级随着观测值和自变量数目的变化．</p><p>我们建议按顺序先阅读第1-4章，第7章也应该当作强制阅读，因为它介绍了关于所有学习方法的中心概念．有了这些概念，这本书的其他章节根据读者的兴趣可以按照顺序读，或选读．</p><p>这个标志表明这是技术上困难的部分，读者可以跳过这部分而且不会影响后续讨论．</p><h3 id="书的网址">书的网址</h3><p>这本书的网站是 <a href="http://www-stat.stanford.edu/ElemStatLearn">http://www-stat.stanford.edu/ElemStatLearn</a> 里面包含很多资源，包括这本书里面用到的很多数据集．</p><h2 id="第二章-监督学习概要">第二章 监督学习概要</h2><h3 id="最小二乘和k-最近邻的核心思想差别">最小二乘和K-最近邻的核心思想差别</h3><p><strong>所以 <span class="math inline">\(k\)</span>-最邻近和最小二乘最终都是根据平均来近似条件期望．但是它们在模型上显著不同</strong>．</p><ul><li>最小二乘假设 <span class="math inline">\(f(x)\)</span> 是某个整体线性函数的良好近似．</li><li><span class="math inline">\(k\)</span>-最近邻假设 <span class="math inline">\(f(x)\)</span> 是局部常值函数的良好近似．</li></ul><h3 id="最小二乘法和k-最近邻">最小二乘法和k-最近邻</h3><p>两种简单但很有用的预测方法：最小二乘法的线性模型拟合和 <span class="math inline">\(k\)</span>-最近邻预测规则．线性模型对结构做出很大的假设而且得出稳定但可能不正确的预测．<span class="math inline">\(k\)</span>-最近邻方法对结构的假设很温和：它的预测通常是准确的但不稳定．</p><ul><li>最小二乘：低方差、高偏差；</li><li>K-最近邻：高方差、低偏差；</li></ul><h3 id="mse的偏差-方差分解">MSE的偏差-方差分解</h3><p><span class="math inline">\(\text{MSE}\)</span> 分解成两个部分，随着我们继续讨论，会越来越熟悉这两个部分，这两部分分别是方差和偏差平方．这一分解总是可行的而且经常有用，并且这一分解被称为 <strong>偏差-方差分解 (bias-variance decomposition)</strong>．</p><p><strong>模型复杂度 (model complexity)</strong> 增加，方差趋于上升，偏差趋于下降．当模型复杂度下降时会发生相反的行为．对于 <span class="math inline">\(k\)</span>-最近邻，模型复杂度由 <span class="math inline">\(k\)</span> 来控制．</p><h3 id="模型选择和偏差-方差的权衡">模型选择和偏差-方差的权衡</h3><p>上面讨论的所有模型以及其他将要在后面章节讨论的模型都有一个 <strong>光滑 (smoothing)</strong> 或 <strong>复杂性 (complexity)</strong> 参数需要确定：</p><ul><li>惩罚项的乘子</li><li>核的宽度</li><li>基函数的个数</li></ul><p><strong>模型复杂度 (model complexity)</strong> 增加，方差趋于上升，偏差趋于下降．当模型复杂度下降时会发生相反的行为． 一般地，我们选择模型复杂度使偏差与方差达到均衡从而使测试误差最小．</p><p>无论何时增加模型复杂度（换句话说，无论何时更精细地(harder)拟合数据），训练误差都趋于下降．然而过度的拟合，模型会自适应使得更加接近训练数据，但不能很好地进行泛化（比如说，测试误差很大）．</p><h3 id="维度的诅咒-curse-of-dimensionality">维度的诅咒 (curse of dimensionality)</h3><p>如果我们希望以在低维中以相同的精度来估计高维中的函数，我们将会需要呈指数增长规模的训练集．</p><p>通过依赖严格的假设，线性模型没有偏差而且方差几乎可以忽略，然后 <span class="math inline">\(1\)</span>-最近邻的误差就会相当的大．然而，如果假设是错误的，所有的东西都不复存在，而 <span class="math inline">\(1\)</span>-最近邻将占主导地位．我们将会看到介于严格的线性模型和非常灵活的 <span class="math inline">\(1\)</span>-最近邻模型之间的模型谱，每个都有它们各自的假设和偏差，这些假设已经具体提到过，通过在很大程度上借鉴这些假设来避免高维下函数复杂度呈指数增长．</p><p>统计判别理论的章节的理论准备中，对于定量的响应，我们看到平方误差损失引导我们得到了回归函数 <span class="math inline">\(f(X)=\text{E}(Y\mid X=x)\)</span>．最近邻方法可以看成是对条件期望的直接估计，但是我们可以看到至少在两个方面它们不起作用</p><ul><li>如果输入空间的维数高，则最近邻不必离目标点近，而且可能导致大的误差</li><li>如果知道存在特殊的结构，可以用来降低估计的偏差与方差．</li></ul><h3 id="极大似然估计">极大似然估计</h3><p>尽管最小二乘在一般情况下非常方便，但并不是唯一的准则，而且在一些情形下没有意义．一个更一般的估计准则是 <strong>极大似然估计 (maximum likelihood estimation)</strong>． 极大似然的原则是假设最合理的 <span class="math inline">\(\theta\)</span> 值会使得观测样本的概率最大．</p><h3 id="约束条件和问题简化">约束条件和问题简化</h3><p>一般地，大多数学习方法施加的约束条件都可以描述为这样或那样对复杂度的限制．这通常也意味着在输入空间的小邻域的一些规则的行为．</p><p>限制的强度由邻域的大小所确定．邻域的规模越大，限制条件则越强，而且解对特定限制条件的选择也很敏感．举个例子，在无穷小邻域内局部常值拟合根本不是限制；在一个比较大的邻域内进行局部线性拟合几乎是全局线性模型，而且限制性是非常强的．</p><p>限制的本质取决于采用的度量．一些像核方法、局部回归和基于树的方法，直接明确了度量以及邻域的规模．至今为止讨论的最近邻方法是基于函数局部为常值的假设； 其它像样条、神经网络以及基于函数的方法隐性定义了邻域的局部行为。</p><p><strong>限制性估计的种类</strong>： 非参回归技巧的多样性或学习方法的类型根据其限制条件的本质可以分成不同的种类．这些种类不是完全不同的，而且确实一些方法可以归为好几种不同的类别．这里我们进行一个简短的概要，因为详细的描述将在后面章节中给出．每个类都有与之对应的一个或多个参数，有时恰当地称之为 <strong>光滑化(smoothing)</strong> 参数，这些参数控制着局部邻域的有效大小．</p><p>这里我们描述三个大的类别．</p><h4 id="粗糙度惩罚和贝叶斯方法">1、粗糙度惩罚和贝叶斯方法</h4><p>惩罚函数，或者说 <strong>正则 (regularization)</strong> 方法，表达了我们的 <strong>先验信仰 (prior belief)</strong>——寻找具有一个特定类型的光滑行为函数类型，而且确实可以套进贝叶斯的模型中．</p><h4 id="核方法和局部回归">2、核方法和局部回归</h4><p>这些方法可以认为是通过确定局部邻域的本质来显式给出回归函数的估计或条件期望，并且属于局部拟合得很好的规则函数类．局部邻域由 <strong>核函数(kernel function)</strong> <span class="math inline">\(K_{\lambda}(x_0,x)\)</span> 确定</p><blockquote><p><strong>内积（dot product）与距离（distance）有密切关系</strong>，尤其是在<strong>欧几里得空间</strong>中，二者经常可以互相表达、转化。</p><p><strong>欧几里得距离（Euclidean distance）</strong> <span class="math display">\[\|\vec{a} - \vec{b}\| = \sqrt{(a_1 - b_1)^2 + (a_2 - b_2)^2 + \cdots + (a_n - b_n)^2}\]</span></p><p><strong>用内积表示</strong> <span class="math display">\[\|\vec{a} - \vec{b}\|^2 = (\vec{a} - \vec{b}) \cdot (\vec{a} - \vec{b})= \vec{a} \cdot \vec{a} - 2\vec{a} \cdot \vec{b} + \vec{b} \cdot \vec{b}= \|\vec{a}\|^2 + \|\vec{b}\|^2 - 2 \vec{a} \cdot \vec{b}\]</span> 在机器学习中的重要意义:</p><ul><li><strong>余弦相似度（cosine similarity）</strong>：基于内积</li><li><strong>欧几里得距离（KNN、聚类）</strong>：基于向量差</li><li>二者在许多模型（SVM、PCA、核方法）中互相关联</li></ul></blockquote><h4 id="基函数和字典方法">3、基函数和字典方法</h4><p>这个方法的类别包括熟悉的线性和多项式展开式，但是最重要的有多种多样的更灵活的模型．这些关于 <span class="math inline">\(f\)</span> 的模型是基本函数的线性展开。 单层的向前反馈的带有线性输出权重的神经网络模型可以认为是一种自适应的基函数方法．（详见文中公式2.45） <strong>激活 (activation)</strong> 函数．作为投射追踪模型。 这些自适应选择基函数的方法也被称作 <strong>字典 (dictionary)</strong> 方法，其中有一个可用的候选基函数的可能无限集或字典 <span class="math inline">\(\cal D\)</span>可供选择，而且通过应用其它的搜索机制来建立模型．</p><h2 id="第三章-线性回归方法">第三章 线性回归方法</h2><p><strong>PS：</strong></p><ul><li><p>对于任意一个有限维的矩阵（实数或复数矩阵），它的行秩 = 列秩。这个值也被称为矩阵的秩（rank）；</p></li><li><p>标准化因数或者 Z-分数，<span class="math inline">\(z_j\)</span> 分布为 <span class="math inline">\(t_{N-p-1}\)</span>（自由度为 <span class="math inline">\(N-p-1\)</span> 的 <span class="math inline">\(t\)</span> 分布）；</p></li><li><p><span class="math inline">\(t\)</span> 分布和标准正态分布在尾概率之间的差异随着样本规模增大可以忽略；</p></li><li><p><span class="math inline">\(F\)</span> 统计量衡量了在大模型中每个增加的系数对残差平方和的改变；</p></li><li><p>当 <span class="math inline">\(N\)</span> 足够大时，<span class="math inline">\(F_{p_1-p_0,N-p_1-1}\)</span> 近似 <span class="math inline">\(\chi^2_{p_1-p_0}\)</span>．</p></li></ul><h3 id="guass-markov-定理">Guass-Markov 定理</h3><ul><li>统计学中一个很有名的结论称参数 <span class="math inline">\(\beta\)</span> 的最小二乘估计在所有的线性无偏估计中有最小的方差．</li></ul><p>考虑 <span class="math inline">\(\theta\)</span> 的估计值 <span class="math inline">\(\tilde{\theta}\)</span> 的均方误差 <span class="math display">\[\begin{align}\text MSE(\tilde{\theta})&amp;=\text E(\tilde{\theta}-\theta)^2\notag\\&amp;=\text Var(\tilde{\theta})+[\text E(\tilde{\theta})-\theta]^2\tag{3.20}\end{align}\]</span> 第一项为方差，第二项为平方偏差．Gauss-Markov 定理表明最小二乘估计在所有无偏线性估计中有最小的均方误差．然而，或许存在有较小均方误差的有偏估计．这样的估计用小的偏差来换取方差大幅度的降低．实际中也会经常使用有偏估计．任何收缩或者将最小二乘的一些参数设为 0 的方法都可能导致有偏估计．我们将在这章的后半部分讨论许多例子，包括 <strong>变量子集选择</strong> 和 <strong>岭回归</strong>．<strong>从一个更加实际的观点来看，许多模型是对事实的曲解，因此是有偏的；</strong></p><ul><li>挑选一个合适的模型意味着要在偏差和方差之间创造某种良好的平衡．</li></ul><h3 id="从简单单变量回归到多重回归">从简单单变量回归到多重回归</h3><p>令 <span class="math inline">\(\mathbf{y}=(y_1,\ldots,y_N)^T\)</span>，<span class="math inline">\(\mathbf{x}=(x_1,\ldots,x_N)^T\)</span>，并且定义 <span class="math display">\[\begin{align}\langle\mathbf{x},\mathbf{y}\rangle &amp;= \sum\limits_{i=1}^Nx_iy_i\notag\\&amp; = \mathbf{x}^T \mathbf{y} \tag{3.25}\end{align}\]</span></p><p><span class="math inline">\(\mathbf{x}\)</span> 和 <span class="math inline">\(\mathbf{y}\)</span> 之间的内积，于是我们可以写成</p><p><span class="math display">\[\begin{align*}\hat{\beta}&amp;=\dfrac{\langle \mathbf{x,y}\rangle}{\langle\mathbf{x,x} \rangle}\\\mathbf{r}&amp;=\mathbf{y}-\mathbf{x}\hat{\beta}\end{align*}\tag{3.26}\label{3.26}\]</span></p><p>!!! note "weiya 注：原书脚注" The inner-product notation is suggestive of generalizations of linear regression to different metric spaces, as well as to probability spaces. 内积表示是线性回归模型一般化到不同度量空间（包括概率空间）建议的方式．</p><blockquote><p>正如我们所看到的，这个简单的单变量回归提供了多重线性回归的框架 (building block)．进一步假设输入变量 <span class="math inline">\(\mathbf{x}_1,\mathbf{x_2,\ldots,x_p}\)</span>（数据矩阵 <span class="math inline">\(\mathbf{X}\)</span> 的列）是<strong>正交</strong>的；也就是对于所有的 <span class="math inline">\(j\neq k\)</span> 有<span class="math inline">\(\langle \text x_j,\text x_k\rangle=0\)</span>．于是很容易得到多重最小二乘估计 <span class="math inline">\(\hat{\beta}_j\)</span> 等于 <span class="math inline">\(\langle \mathbf{x}_j,\mathbf{y}\rangle/\langle\mathbf{x}_j,\mathbf{x}_j\rangle\)</span> ——单变量估计．<strong>换句话说，当输入变量为正交的，它们对模型中其它的参数估计没有影响．</strong></p></blockquote><p>进一步假设我们有<strong>一个截距和单输入 <span class="math inline">\(\bf{x}\)</span></strong>．则 <span class="math inline">\(\bf{x}\)</span> 的最小二乘系数有如下形式</p><p><span class="math display">\[\hat{\beta}_1=\dfrac{\langle \mathbf{x}-\bar{x}\mathbf{1},\mathbf{y}\rangle}{\langle \mathbf{x}-\bar{x}\mathbf{1},\mathbf{x}-\bar{x}\mathbf{1}\rangle}\tag{3.27}\label{3.27}\]</span></p><p>其中，<span class="math inline">\(\bar{x}=\sum_ix_i/N\)</span>,且 <span class="math inline">\(N\)</span> 个元素全为 1 的向量 <span class="math inline">\(\mathbf{1}=\mathbf{x_0}\)</span>．我们可以将式 <span class="math inline">\(\eqref{3.27}\)</span> 的估计看成简单回归 <span class="math inline">\(\eqref{3.26}\)</span> 的两次应用．这两步是：</p><ol type="1"><li>在 <span class="math inline">\(\bf{1}\)</span> 上回归 <span class="math inline">\(\bf{x}\)</span> 产生残差 <span class="math inline">\(\mathbf{z}=\mathbf{x}-\bar{x}\mathbf{1}\)</span>;</li><li>在残差 <span class="math inline">\(\bf{z}\)</span> 上回归 <span class="math inline">\(\bf{y}\)</span> 得到系数 <span class="math inline">\(\hat{\beta}_1\)</span> 在这个过程中，“在 <span class="math inline">\(\bf{a}\)</span> 上回归 <span class="math inline">\(\bf{b}\)</span>”意思是 <span class="math inline">\(\bf{b}\)</span> 在 <span class="math inline">\(\bf{a}\)</span> 上的无截距的简单单变量回归，产生系数 <span class="math inline">\(\hat{\gamma}=\langle\mathbf{a,b}\rangle/\langle\mathbf{a,a}\rangle\)</span> 以及残差向量 <span class="math inline">\(\mathbf{b}-\hat{\gamma}\mathbf{a}\)</span>．我们称 <span class="math inline">\(\bf{b}\)</span> 由 <span class="math inline">\(\bf{a}\)</span> 校正(adjusted)，或者关于 <span class="math inline">\(\bf{a}\)</span> 正交化．</li></ol><blockquote><p>详见原书图3.4 正交输入的最小二乘回归．向量 <span class="math inline">\(\mathbf{x}_2\)</span> 在向量 <span class="math inline">\(\mathbf{x}_1\)</span> 上回归，得到残差向量 <span class="math inline">\(\mathbf{z}\)</span>．<span class="math inline">\(\mathbf{y}\)</span> 在<span class="math inline">\(\mathbf{z}\)</span> 上的回归给出 <span class="math inline">\(\mathbf{x}_2\)</span> 的系数．把 <span class="math inline">\(\mathbf{y}\)</span> 在 <span class="math inline">\(\mathbf{x}_1\)</span> 和 <span class="math inline">\(\mathbf{z}\)</span> 上的投影加起来给出了最小二乘拟合 <span class="math inline">\(\mathbf{\hat{y}}\)</span>．</p><p>多重回归系数 <span class="math inline">\(\hat\beta_j\)</span> 表示 <span class="math inline">\(\mathbf x_j\)</span> 经过 <span class="math inline">\(\mathbf x_0,\mathbf x_1,\ldots,\mathbf x_{j-1},\mathbf x_{j+1},\ldots,\mathbf x_p\)</span> 调整后 <span class="math inline">\(\mathbf x_j\)</span> 对 <span class="math inline">\(\mathbf y\)</span> 额外的贡献</p></blockquote><ul><li>若模型为：<span class="math inline">\(y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p + \varepsilon\)</span></li></ul><p>​ 则：<span class="math inline">\(\hat{\beta}_0 = \bar{y} - \sum_{j=1}^{p} \hat{\beta}_j \bar{x}_j\)</span></p><p>​ 依然体现了：<strong>截距项用于保证模型在均值点的预测值等于 <span class="math inline">\(\bar{y}\)</span></strong>。 截距和样本均值有严格的线性关系。</p><ul><li>在带截距的线性回归中，<strong>回归线总通过样本均值点</strong> <span class="math inline">\((\bar{x}, \bar{y})\)</span>，这是一个非常重要的几何性质。</li></ul><h3 id="子集的选择">子集的选择</h3><p>两个原因使得我们经常不满足最小二乘估计 (3.6)</p><ul><li>第一个是预测的 <strong>精确性 (prediction accuracy)</strong>：最小二乘估计经常有小偏差大方差．预测精确性有时可以通过收缩或者令某些系数为 0 来提高．通过这些方法我们牺牲一点偏差来降低预测值的方差，因此可能提高整个预测的精确性．</li><li>第二个原因是 <strong>可解释性 (interpretation)</strong>：当有大量的预测变量时，我们经常去确定一个小的子集来保持最强的影响．为了得到“big picture”，我们愿意牺牲一些小的细节．</li></ul><p>这节我们描述一些线性回归选择变量子集的方法．在后面的部分中我们讨论用于控制方差的收缩和混合的方法，以及其它降维的策略．这些都属于 <strong>模型选择 (model selection)</strong>．模型选择不局限于线性模型；第 7 章将详细介绍这个主题．</p><p>子集选择意味着我们只保留变量的一个子集，并除去模型中的剩余部分．最小二乘回归用来预测保留下的输入变量的系数．这里有一系列不同的选择子集的策略．</p><h4 id="最优集的选择">最优集的选择</h4><p>AIC 准则是一个受欢迎的选择</p><h4 id="向前和向后逐步选择">向前和向后逐步选择</h4><ul><li>其它传统的包中的选择基于 <span class="math inline">\(F\)</span> 统计量，加入“显著性”的项，然后删掉“非显著性”的项．这些不再流行，因为它们没有合理考虑到多重检验的问题．</li></ul><h4 id="向前逐渐-forward-stagewise-回归">向前逐渐 (Forward-Stagewise) 回归</h4><ul><li>使用“一个标准误差”规则——在最小值的一个标准误差范围内我们选取最简洁的模型。</li></ul><h3 id="收缩的方法">收缩的方法</h3><p>通过保留一部分预测变量而丢弃剩余的变量，<strong>子集选择 (subset selection)</strong> 可得到一个可解释的、预测误差可能比全模型低的模型．然而，因为这是一个离散的过程（变量不是保留就是丢弃），所以经常表现为高方差，因此不会降低全模型的预测误差．而<strong>收缩方法 (shrinkage methods)</strong> 更加连续，因此不会受 <strong>高易变性 (high variability)</strong> 太大的影响．</p><h4 id="岭回归">岭回归</h4><ul><li>系数向零收缩（并且彼此收缩到一起）；</li><li>通过参数的平方和来惩罚的想法也用在了神经网络，也被称作 <strong>权重衰减 (weight decay)</strong></li><li>对输入按比例进行缩放时，岭回归的解不相等，因此求解公式 <span class="math inline">\(\text{3.41}\)</span>前我们需要对输入进行标准化．另外，注意到惩罚项不包含截距 <span class="math inline">\(\beta_0\)</span>．对截距的惩罚会使得过程依赖于 <span class="math inline">\(\mathbf{Y}\)</span> 的初始选择；</li><li>对输入进行中心化（每个 <span class="math inline">\(x_{ij}\)</span> 替换为 <span class="math inline">\(x_{ij}-\bar x_j\)</span>）</li><li>主成分回归与岭回归非常相似：都是通过输入矩阵的主成分来操作的．岭回归对主成分系数进行了收缩，收缩更多地依赖对应特征值的大小；主成分回归丢掉 <span class="math inline">\(p-M\)</span> 个最小的特征值分量．</li></ul><p>中心化输入矩阵 <span class="math inline">\(\mathbf{X}\)</span> 的 <strong>奇异值分解 (SVD)</strong> <span class="math inline">\(\mathbf{X}\)</span> 的 SVD 分解有如下形式 <span class="math display">\[\mathbf{X=UDV^T}\tag{3.45}\label{3.45}\]</span></p><p>得到 <span class="math display">\[\mathbf{X^T X = VD^2V^T} \tag{3.48}\]</span></p><p>上式是 <span class="math inline">\(\mathbf{X}^T \mathbf{X}\)</span>（当忽略因子 <span class="math inline">\(N\)</span> 时，也是 <span class="math inline">\(S\)</span>）的 <strong>特征值分解 (eigen decomposition)</strong>．特征向量 <span class="math inline">\(v_j\)</span>（<span class="math inline">\(\mathbf{V}\)</span> 的列向量）也称作 <span class="math inline">\(\mathbf{X}\)</span> 的 <strong>主成分 (principal components)</strong>（或 Karhunen-Loeve）方向．</p><p>最后一个主成分有最小的方差．因此<strong>越小的奇异值 <span class="math inline">\(d_j\)</span> 对应 <span class="math inline">\(\mathbf{X}\)</span> 列空间中方差越小的方向，并且岭回归在这些方向上收缩得最厉害．</strong></p><p>图 3.9 展示了两个维度下部分数据点的主成分．如果我们考虑在这个区域（<span class="math inline">\(Y\)</span> 轴垂直纸面）内拟合线性曲面，数据的结构形态使得确定梯度时长方向会比短方向更精确．岭回归防止在短方向上估计梯度可能存在的高方差．隐含的假设是响应变量往往在高方差的输入方向上变化．这往往是个合理的假设，因为我们所研究的预测变量随响应变量变化而变化，而不需要保持不变．</p><p>图 3.9 部分输入数据点的主成分．<strong>最大主成分是使得投影数据方差最大的方向，最小主成分是使得方差最小的方向．岭回归将 <span class="math inline">\(\mathbf{y}\)</span> 投射到这些成分上，然后对低方差成分的系数比高方差收缩得更厉害．</strong></p><h4 id="lasso">Lasso</h4><ul><li>由于该约束的本质，令 <span class="math inline">\(t\)</span> 充分小会造成一些参数恰恰等于 0．因此 lasso 完成一个温和的连续子集选择．</li><li>类似在变量子集选择中子集的大小，或者岭回归的惩罚参数，应该自适应地选择 <span class="math inline">\(t\)</span> 使预测误差期望值的估计最小化．</li><li>lasso 曲线会达到 0，然而岭回归不会．曲线是分段线性的</li></ul><h3 id="讨论子集的选择岭回归lasso">讨论：子集的选择，岭回归，Lasso</h3><p>有约束的线性回归模型的三种方法：子集选择、岭回归和 lasso．</p><ul><li>在正交输入矩阵的情况下，三种过程都有显式解．每种方法对最小二乘估计 <span class="math inline">\(\hat{\beta}_j\)</span> 应用简单的变换，详见表 3.4．</li><li>岭回归做等比例的收缩．lasso 通过常数因子 <span class="math inline">\(\lambda\)</span> 变换每个系数，在 0 处截去．这也称作“软阈限”，而且用在 5.9 节中基于小波光滑的内容中．最优子集选择删掉所有系数小于第 <span class="math inline">\(M\)</span> 个大系数的变量；这是“硬阈限”的一种形式．</li><li>lasso、岭回归和最优子集选择是有着不同先验分布的贝叶斯估计,然而，注意到它们取自后验分布的众数，即最大化后验分布．在贝叶斯估计中使用后验分布的均值更加常见．岭回归同样是后验分布的均值，但是 lasso 和最优子集选择不是．</li></ul><h2 id="第四章-线性分类方法">第四章 线性分类方法</h2><p>PS： <strong>贝叶斯定理</strong>（Bayes’ Theorem）是概率论中一个非常重要的定理，用于在已知结果的情况下推断原因（也就是“后验概率”）。</p><h3 id="一句话理解">一句话理解</h3><blockquote><p><strong>贝叶斯定理告诉我们如何根据已有信息更新对某事件的信念。</strong></p></blockquote><h3 id="数学表达式">数学表达式</h3><p>对于两个事件 <span class="math inline">\(A\)</span> 和 <span class="math inline">\(B\)</span>，只要 <span class="math inline">\(P(B) &gt; 0\)</span>，贝叶斯定理公式如下：</p><p><span class="math display">\[P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}\]</span></p><p>其中：</p><ul><li><span class="math inline">\(P(A)\)</span>：<strong>先验概率</strong>，事件 A 发生的原始概率；</li><li><span class="math inline">\(P(B|A)\)</span>：<strong>似然度</strong>，在 A 发生的条件下，观察到 B 的概率；</li><li><span class="math inline">\(P(B)\)</span>：<strong>边缘概率</strong>，B 发生的总概率；</li><li><span class="math inline">\(P(A|B)\)</span>：<strong>后验概率</strong>，在 B 发生的前提下，A 发生的概率。</li></ul><h3 id="通俗理解">通俗理解</h3><p>想象一个医生看到病人有发烧的症状（B），他希望知道这个人是否患有某种病（A）。</p><ul><li><span class="math inline">\(P(A)\)</span>：这个病在人群中的患病率；</li><li><span class="math inline">\(P(B|A)\)</span>：有这种病的人发烧的概率；</li><li><span class="math inline">\(P(B)\)</span>：不管是否得病，总体发烧的概率；</li><li><span class="math inline">\(P(A|B)\)</span>：某人已经发烧了，他患这种病的可能性。</li></ul><h3 id="示例疾病检测">示例：疾病检测</h3><p>假设：</p><ul><li><p>某病患病率 <span class="math inline">\(P(\text{病}) = 1\% = 0.01\)</span></p></li><li><p>检测准确率：</p><ul><li>如果有病，检测呈阳性 <span class="math inline">\(P(\text{阳性}|\text{病}) = 99\%\)</span></li><li>如果无病，检测误报率 <span class="math inline">\(P(\text{阳性}|\text{无病}) = 5\%\)</span></li></ul></li></ul><p>问：<strong>一个人检测阳性后，他实际患病的概率是多少？</strong></p><p><strong>解：</strong>设：</p><ul><li><span class="math inline">\(A\)</span>：有病</li><li><span class="math inline">\(B\)</span>：检测为阳性</li></ul><p><span class="math display">\[P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}\]</span></p><p>计算 <span class="math inline">\(P(B)\)</span>：</p><p><span class="math display">\[P(B) = P(B|A)P(A) + P(B|\text{无病})P(\text{无病}) \\= 0.99 \times 0.01 + 0.05 \times 0.99 = 0.0099 + 0.0495 = 0.0594\]</span></p><p>代入贝叶斯公式：</p><p><span class="math display">\[P(A|B) = \frac{0.99 \times 0.01}{0.0594} \approx 0.1667\]</span></p><p>所以，即使检测呈阳性，患病概率也只有 <strong>约 16.67%</strong>！</p><h3 id="应用场景">应用场景</h3><ul><li>医学诊断</li><li>垃圾邮件识别（朴素贝叶斯）</li><li>金融风控</li><li>机器学习中的贝叶斯推断</li><li>自然语言处理（情感分析、文本分类）</li></ul><h2 id="第五章-基函数扩展与正则化">第五章 基函数扩展与正则化</h2><ul><li><p>据说三次样条是人眼看不出结点不连续的最低阶样条．很少有更好的理由去选择更高次的样条，除非对光滑的微分感兴趣．</p></li><li><p>固定结点的样条也称作 <strong>回归样条 (regression splines)</strong>．我们需要选择样条的阶数，结点的个数以及它们的位置．一种简单方式是用基函数或自由度来参量化样条族，并用观测 <span class="math inline">\(x_i\)</span> 来确定结点的位置．</p></li><li><p><strong>自然三次样条 (natural cubic spline)</strong> 添加额外的限制，具体地，令边界结点之外的函数是线性的．</p></li><li><p>高维特征的预处理是非常普遍的而且对于改善学习算法的效果是很有效的</p></li></ul><h2 id="第六章-核平滑方法">第六章 核平滑方法</h2><ul><li><span class="math inline">\(\Vert \cdot\Vert\)</span> 是欧几里得范数．因为欧式范数取决于每个坐标的单位，所以对每个预测变量进行标准化是有意义的，举个例子，在光滑之前，标准化为单位标准误差．</li><li>当维度与样本大小的比率不是很好，则局部回归对我们没有太大帮助，除非我们想要对模型做出一些结构化的假设．这本书的很多部分是关于结构化回归和分类模型的．</li></ul><h3 id="朴素贝叶斯分类器">朴素贝叶斯分类器</h3><p>不管它的名字（也称为“白痴的贝叶斯”！），这是这些年仍然流行的技巧．当特征空间的维数 <span class="math inline">\(p\)</span> 很高，这种方式特别合适，使得密度估计不再吸引人．朴素贝叶斯模型假设给定类别 <span class="math inline">\(G=j\)</span>，特征 <span class="math inline">\(X_k\)</span> 是独立的:</p><p><span class="math display">\[f_j(X)=\prod\limits_{k=1}^pf_{jk}(X_k)\tag{6.26}\label{6.26}\]</span></p><p>尽管这个假设一般是不对的，但是确实很大程度上简化了估计：</p><ul><li>单独的类别条件的边缘密度 <span class="math inline">\(f_{jk}\)</span> 可以采用一维核密度估计分别估计出来．这实际上是原始朴素贝叶斯过程的泛化，采用单变量高斯分布来表示这些边缘密度．</li><li>如果 <span class="math inline">\(X\)</span> 的组分 <span class="math inline">\(X_j\)</span> 是离散的，则可以使用合适的直方图估计．这提供了在特征向量中混合变量类型的完美方式．</li></ul><p>尽管这些假设过于乐观，朴素贝叶斯分类器经常比更复杂的分类器做得更好．原因与图 6.15 相关：尽管单独的类别密度估计可能是有偏的，但这个偏差或许不会对后验概率不会有太大的影响，特别是在判别区域附近．实际上，这个问题或许可以承受为了节省方差造成的相当大的偏差，比如“天真的”假设得到的．</p><h3 id="计算上的考虑">计算上的考虑</h3><p>核和局部回归以及密度估计都是 <strong>基于内存的 (memory-based)</strong> 方法：模型是整个训练数据集，并且在赋值或者预测的时候完成拟合．对于许多实时的应用，这使得这类方法不可行．</p><h2 id="第七章-模型评估与选择">第七章 模型评估与选择</h2><p>根据<a href="https://baike.baidu.com/item/%E5%AE%9A%E8%B7%9D%E5%8F%98%E9%87%8F/2710255">百度百科</a> 即 <a href="https://www.zhihu.com/question/26201880">知乎回答</a>，定距变量也称间距变量，是取值具有“距离”特征的变量。统计学依据数据的计量尺度将数据分为四大类：</p><ul><li><strong>定距型 (interval scale)</strong>: 数值变量，可以加减运算，但不能乘除；不存在基准 0 值，即当变量值为 0 时不是表示没有，如温度变量。</li><li><strong>定序型 (ordinal scale)</strong>: 类别型变量，如性别。</li><li><strong>定类型 (nominal scale)</strong>: 不可以做四则运算，如满意度（非常满意、满意、一般、不满意、非常不满意）。</li><li><strong>定比型 (ratio scale)</strong>: 数值变量，存在 0 值，比值有意义。</li></ul><h3 id="测试误差和训练误差">测试误差和训练误差</h3><h4 id="测试误差和期望测试误差">测试误差和期望测试误差</h4><p><strong>测试误差 (test error)</strong>，也被称作 <strong>泛化误差 (generalization error)</strong>，它是在独立的测试样本上的预测误差 <span class="math display">\[\text {Err}_{\cal T}=E[L(Y,\hat f(X))\mid {\cal T}]\tag{7.2}\]</span></p><p>其中 <span class="math inline">\(X\)</span> 和 <span class="math inline">\(Y\)</span> 都是随机从它们的（总体）联合分布中选取的．这里训练集 <span class="math inline">\(\cal T\)</span> 是固定的，测试误差指的是对该特定训练集的误差．一个相关的量是 <strong>期望预测误差 (expected prediction error)</strong>（或者 <strong>期望测试误差 (expected test error)</strong>）</p><p><span class="math display">\[\text {Err} = E[L(Y,\hat f(X))]=\text {E}[\text {Err}_{\cal T}]\tag{7.3}\]</span></p><p>注意到这个期望平均的任何项都是随机的，包括产生 <span class="math inline">\(\hat f\)</span> 的训练集的随机性．</p><h4 id="训练误差">训练误差</h4><p><strong>训练误差 (Training error)</strong> 是在训练样本上的平均损失，（<strong>注意：这里是平均值</strong>） <span class="math display">\[\overline{\text {err}}=\frac{1}{N}\sum\limits_{i=1}^NL(y_i,\hat f(x_i))\,.\tag{7.4}\]</span></p><p>我们想要知道估计模型 <span class="math inline">\(\hat f\)</span> 的测试误差的期望值．当模型越来越复杂时，它使用更多的训练数据并且可以适应于更复杂的潜在结构．因此偏差会有降低而方差会有增大．一些中等程度的模型复杂度给出了最小的测试误差期望值．</p><p>不幸的是，正如我们在图 7.1 中看到的那样训练误差不是测试误差一个良好的估计．训练误差随着模型复杂度增大不断降低，一般地如果我们将模型复杂性增到充分大它会降为 0．然而，0 训练误差的模型对训练数据是过拟合的并且一般泛化性很差．</p><h3 id="模型选择">模型选择</h3><p>重要的是要注意，事实上我们可能有两个单独的目标：</p><p><strong>模型选择 (Model selection)：</strong> 估计不同模型的表现来选择最好的那个</p><p><strong>模型评估 (Model assessment)：</strong> 已经选择好了最终模型，估计它在新数据上的预测误差（泛化误差）</p><p>如果我们处在有充足数据的情形中，对于这两个问题的最好的方式是将数据集随机地分成 3 个部分：训练集，验证集，以及测试集．训练集用来拟合模型；验证集用来估计预测误差来进行模型选择；测试集用来评估最终选择的模型的泛化误差．理想情形下，测试集应保存在“黑箱 (valut)”中，并且只在数据分析结束时才会显示出来．否则，假设我们重复采用测试集，选择具有最小测试误差的模型，则最终选择模型的测试误差会低估真实的测试误差，有时候偏差是相当大的．</p><p>在三个部分的每一个中如何选取观测的个数很难给出一个一般性的规则，因为这取决于数据的信噪比和训练样本的规模．一般的分割是 50% 用于训练，25% 用于验证，25% 用于测试：</p><p><strong>本章中的方法是为了没有足够的数据来分成 3 部分的情形设计的．</strong>同样地，给出多少的训练数据是足够的一般规则太难了；此外，这取决于潜在函数的信噪比以及根据数据拟合出的模型的复杂度．</p><p><strong>本章的方法有两类，第一类通过分析的手段 (AIC，BIC，MDL，SRM)，第二类通过有效的样本重利用（交叉验证和自助法）来近似验证过程（验证过程即比较候选模型选出最优的模型）</strong>．除了在模型选择使用它们，我们也检验了每个方法对最终选择的模型的测试误差的估计的可靠性程度．</p><p>在讨论这些之前，我们首先进一步探究测试误差的本质与 <strong>偏差-方差之间的权衡 (the bias-variance tradeoff)</strong>．</p><h4 id="偏差-方差分解">偏差-方差分解</h4><p>和 <a href="../02-Overview-of-Supervised-Learning/2.4-Statistical-Decision-Theory/index.html">第 2 章</a> 一样，如果我们假设 <span class="math inline">\(Y=f(X)+\varepsilon\)</span>，其中 <span class="math inline">\(\text {E}(\varepsilon)=0\)</span>，并且 <span class="math inline">\(\text {Var}(\epsilon)=\sigma_\varepsilon^2\)</span>，我们可以导出在使用平方误差损失的情形下，在输入点 <span class="math inline">\(X=x_0\)</span> 处回归拟合值 <span class="math inline">\(\hat f(X)\)</span> 的期望预测误差：（<strong>个人注：这里是对单独的点，而不是MSE</strong>） <span class="math display">\[\begin{align}\text {Err}(x_0)&amp;=\text {E}[(Y-\hat f(x_0))^2\mid X=x_0]\notag\\&amp;=\sigma_\varepsilon^2+[\text {E}\hat f(x_0)-f(x_0)]^2+\text {E}[\hat f(x_0)-E\hat f(x_0)]^2\notag\\&amp;=\sigma_\varepsilon^2+\text {Bias}^2(\hat f(x_0))+\text {Var}(\hat f(x_0))\notag\\&amp;=\text{Irreducible Error} + \text{Bias}^2+\text{Variance}\tag{7.9}\end{align}\]</span></p><p>第一项是目标在真实均值 <span class="math inline">\(f(x_0)\)</span> 处的方差，无论我们对 <span class="math inline">\(f(x_0)\)</span>（<strong>个人注：观测的数据？</strong>） 的估计有多好，这是不可避免的，除非 <span class="math inline">\(\sigma_\varepsilon^2=0\)</span>，第二项是<strong>偏差的平方</strong>，是我们估计的均值与真实的均值间的偏差量；最后一项是方差，是估计的 <span class="math inline">\(\hat f(x_0)\)</span> 在其均值处的平方偏差的期望值．一般地，我们建立的模型 <span class="math inline">\(\hat f\)</span> 越复杂，（平方）偏差越低但是方差越大。</p><p>对于线性模型族比如岭回归，我们可以更精细地分解偏差。</p><p>接着我们可以将 <strong>偏差平方的平均 (average squared bias)</strong> 写成 <span class="math display">\[\begin{align}&amp;\text {E}_{x_0}[f(x_0)-E\hat f_\alpha(x_0)]^2\notag\\&amp;=\text {E}_{x_0}[f(x_0)-x_0^T\beta_*]^2+\text {E}_{x_0}[x_0^T\beta_*-\text {E} x_0^T\hat\beta_\alpha]^2\notag\\&amp;=\text{Ave[Model Bias]}^2+\text{Ave[Estimation Bias]}^2\tag{7.14}\end{align}\]</span></p><p>右侧的第一项是 <strong>模型偏差 (model bias)</strong> 平方的平均，它是最优线性近似和真实函数之间的误差．第二项是 <strong>估计偏差 (estimation bias)</strong> 平方的平均，它是估计的平均值 <span class="math inline">\(\text {E}(x_0^T\hat\beta)\)</span> 与最优线性近似之间的误差．</p><p><strong>对于通过普通最小二乘拟合的线性模型，估计量的偏差为 0．对于约束的拟合，比如岭回归，它是正的</strong>，而且我们用减小方差的好处进行交易．<strong>模型偏差只可能通过将线性模型类扩大为更广的模型类才能降低，或者通过在模型中加入变量的交叉项以及变换项（通过变量的变换得到的）来降低。</strong></p><p>详见原书图7.2！</p><p>偏差方差间的权衡在 0-1 损失的表现与在平方误差损失的表现不一样．反过来意味着调整参数的最优选择可能在两种设定下本质上不同．正如后面章节中描述的那样，我们应该将调整参数的选择建立于对预测误差的估计之上。</p><h4 id="训练误差率的-optimism">训练误差率的 optimism</h4><p><strong>训练误差 (training error)</strong></p><p><span class="math display">\[\overline{\text {err}} = \frac{1}{N}\sum\limits_{i=1}^NL(y_i,\hat f(x_i))\tag{7.17}\label{7.17}\]</span></p><p>会比真实误差 <span class="math inline">\(\text {Err}\_{\cal T}\)</span> 小，因为数据被同时用来拟合方法并且评估误差（见<a href="https://github.com/szcf-weiya/ESL-CN/issues/176">练习 2.9</a>）．拟合方法一般适应于训练数据，因此 <strong>表面误差 (apparent error)</strong> 或训练误差 <span class="math inline">\(\overline{\text {err}}\)</span> 是对泛化误差 <span class="math inline">\(\mathrm {Err}\_{\cal T}\)</span>过度的乐观估计。</p><h4 id="交叉验证">交叉验证</h4><p>错误的方式： 考虑有许多预测变量的分类问题，举个例子，可能在基因与蛋白质的应用中．一般的分析技巧或许如下：</p><ol type="1"><li>筛选预测变量：选择与类别有着相当强（单变量）相关性的“好”预测变量的一个子集</li><li>运用这个预测变量的子集，建立多维变量分类器．</li><li>采用交叉验证来估计未知调整参数并且估计最终模型的预测误差．</li></ol><p>这是正确地应用交叉验证吗？</p><p>正确的方式： 下面是在这个例子中正确使用交叉验证的方式：</p><ol type="1"><li>将样本随机分成 <span class="math inline">\(K\)</span> 个交叉验证折（群）．</li><li>对于每一折 <span class="math inline">\(k=1,2,\ldots,K\)</span><ol type="1"><li>利用除了第 <span class="math inline">\(k\)</span> 折的所有样本找到与类别标签有相对强（单变量）的相关性的“好”预测变量的一个子集．</li><li>利用除了第 <span class="math inline">\(k\)</span> 折的所有样本仅仅运用找到的预测变量来建立多元分类器．</li><li>运用分类器来预测第 <span class="math inline">\(k\)</span> 折中样本的类别．</li></ol></li></ol><h4 id="自助法">**自助法</h4><ul><li>自助法我觉得是最“独立”，最“自强”的方法了。</li><li>所以这也是我最喜欢的统计方法了。</li><li>自助法是评价统计精确性的通用工具。</li></ul><p>基本思想是从训练集中<strong>有放回</strong>地随机选取数据集，每个样本的大小与原始数据集相同．做 <span class="math inline">\(B\)</span> 次（如 <span class="math inline">\(B=100\)</span>）选取的操作，得到 <span class="math inline">\(B\)</span> 个自助训练集。接着我们对每个自助法数据集重新拟合模型，并且检验在 <span class="math inline">\(B\)</span> 个复制集上拟合的表现．</p><p>我们的结论是，对于这些特定的问题和拟合方法，AIC 和交叉验证的最小化，或者自助法都得到相当接近最好的可行模型．注意到模型选择的目的，这些衡量方式可能都有偏差，但是这不会有影响，只要偏差不会改变这些方法的相对表现．举个例子，对所有的衡量方式都加上常数不会改变最终选择的模型．然而，<strong>对于许多自适应非线性技巧（比如树），估计参数的有效个数是非常困难的．这使得像 AIC 之类的方法不可行，只留下交叉验证和自助法供我们选择．</strong></p><h2 id="第八章-模型推断与平均化">第八章 模型推断与平均化</h2><p>本书的大部分章节中，对于回归而言，模型的拟合（学习）通过最小化平方和实现；或对于分类而言，通过最小化交叉熵实现．事实上，这两种最小化都是用极大似然来拟合的实例．</p><p>这章中，我们给出极大似然法的一个一般性的描述，以及用于推断的贝叶斯方法．在第7章中讨论的自助法在本章中也继续讨论，而且描述了它与极大似然和贝叶斯之间的联系．最后，我们提出模型平均和改善的相关技巧，包括 committee 方法、bagging、stacking 和 bumping．</p><h3 id="自助法和最大似然法">自助法和最大似然法</h3><ul><li>自助法，通过从训练集中有放回地采样，称作<strong>非参</strong>自助法(nonparametric bootstrap),这实际上意味着这个方法是与模型无关的，因为它使用原始数据来得到新的数据集，而不是一个特定的含参数的模型．</li><li><strong>本质上自助法是非参最大似然或者参数最大似然法的计算机实现</strong>．与最大似然法相比自助法的好处是允许我们在没有公式的情况下计算标准误差和其他一些量的最大似然估计． （详见原书的图8.2）</li></ul><h3 id="贝叶斯方法">贝叶斯方法</h3><ul><li>贝叶斯方法与一般推断方法的不同之处在于，用先验分布来表达知道数据之前的这种不确定性，而且在知道数据之后允许不确定性继续存在，将它表示成后验分布．</li><li>最大似然方法会使用在最大概率估计那个点的密度来预测未来的数据．不同于贝叶斯方法的预测分布，它不能说明估计 <span class="math inline">\(\theta\)</span> 的不确定性</li></ul><blockquote><p>个人注：推断中最大似然是预测某个最大的<span class="math inline">\(\theta\)</span>值，贝叶斯方法是预测<span class="math inline">\(\theta\)</span>的分布？</p></blockquote><ul><li>自助法分布表示我们参数的（近似的）非参、无信息后验分布．但是自助法分布可以很方便地得到——不需要正式地确定一个先验而且不需要从后验分布中取样．因此我们或许可以把自助法分布看成一个“穷人的”贝叶斯后验．通过扰动数据，自助法近似于扰动参数后的贝叶斯效应，而且一般实施起来更简单．</li></ul><h3 id="em算法">EM算法</h3><p>EM 算法是简化复杂极大似然问题的一种很受欢迎的工具；</p><blockquote><p>个人注：入门的视频见B站 博主“风中摇曳的小萝卜”的视频“EM算法 你到底是哪个班级的”</p></blockquote><h3 id="bagging">Bagging</h3><p>简单来说，bagging 和随机森林都是针对 bootstrap 样本，且前者可以看成后者的特殊形式；而 boosting 是针对残差样本．</p><h2 id="第九章-加性模型树模型及相关方法">第九章 加性模型、树模型及相关方法</h2><p>这章中我们开始对监督学习中一些特定的方法进行讨论．这里每个技巧都<strong>假设了未知回归函数（不同的）结构形式</strong>，而且通过这样处理巧妙地解决了维数灾难．<strong>当然，它们要为错误地确定模型类型付出可能的代价</strong>，所以在每种情形下都需要做出一个权衡．第 3-6 章留下的问题都将继续讨论．我们描述 5 个相关的技巧：<strong>广义可加模型 (generalized additive models)</strong>，<strong>树 (trees)</strong>，<strong>多元自适应回归样条 (MARS)</strong>，<strong>耐心规则归纳法 (PRIM)</strong>，以及 <strong>混合层次专家 (HME)</strong>．</p><h3 id="广义可加模型">广义可加模型</h3><p>在回归的设定中，广义可加模型有如下形式</p><p><span class="math display">\[\text E(Y\mid X_1,X_2,\ldots,X_p) = \alpha+f_1(X_1)+f_2(X_2)+\cdots+f_p(X_p).\tag{9.1}\]</span></p><h3 id="树模型">树模型</h3><ul><li>为什么二值分割？ 与其在每一步对每个结点只分割成两个群体（如上面讨论的），我们或许可以考虑多重分割成多于两个群体．尽管这个在某些情况下是有用的，但不是一个好的一般策略．问题在于多重分割将数据分得太快，以至于在下一层次没有充分多的数据．因此我们仅仅当需要的时候采用这种分割．因为多重分割可以通过一系列的二值分割实现，所以后者更好一点．</li><li>树的不稳定性 树的一个主要问题是它们的高方差性．经常在数据中的一个小改动导致完全不同的分割点序列，使得解释不稳定．这种不稳定的主要原因是这个过程的层次性：上一个分割点的误差会传递到下面所有的分割点上．可以试图采取更加稳定的分离准则在某种程度上减轻这一影响，但是固有的不稳定性没有移除．这是从数据中估计一个简单的、基于树结构的代价．Bagging（<a href="/08-Model-Inference-and-Averaging/8.7-Bagging/index.html">8.7 节</a>）对很多树进行平均来降低方差．</li><li>缺乏光滑性 树的另一个限制是预测表面缺乏光滑性，如在图 9.2 中的右下图中那样．在 0/1 损失的分类问题中，这不会有太大的损伤，因为类别概率估计的偏差的影响有限．然而，在回归问题中这会降低效果，正常情况下我们期望潜在的函数是光滑的．<a href="9.4-MARS/index.html">9.4 节</a>介绍的 MARS 过程可以看出是为了减轻 CART 缺乏光滑性而做的改动．</li></ul><h3 id="roc">ROC</h3><p>在医学分类问题中，<strong>敏感度 (sensitivity)</strong> 和 <strong>特异度 (specificity)</strong> 经常用来衡量一个准则．它们按如下定义：</p><ul><li>敏感度：给定真实状态为患病预测为患病的概率</li><li>特异度：给定真实状态为未患病预测为未患病的概率</li></ul><p><strong>受试者工作特征曲线 (receiver operating characteristic curve, ROC)</strong> 是用于评估敏感度和特异度之间折中的常用概述．当我们改变分类规则的参数便会得到敏感度关于特异度的图像． 越靠近东北角落的曲线表示越好的分类器． ROC 曲线下的面积有时被称作 <strong><span class="math inline">\(c\)</span> 统计量 (c-statistics)</strong>．</p><h3 id="专家的分层混合-hme"><strong>专家的分层混合 (HME)</strong></h3><p>过程可以看成是基于树方法的变种．主要的差异是树的分割不是<strong>硬决定 (hard decision)</strong>，而是<strong>软概率的决定 (soft probabilistic)</strong>．在每个结点观测往左或者往右的概率取决于输入值．因为最后的参数优化问题是光滑的，所以有一些计算的优势，不像在基于树的方式中的离散分割点的搜索．软分割或许也可以帮助预测准确性，并且提供另外一种有用的数据描述方式．</p><h2 id="第十章-提升方法与加性树模型">第十章 <strong>提升方法与加性树模型</strong></h2><p>Boosting 是最近20年内提出的最有力的学习方法．最初是为了分类问题而设计的，但是我们将在这章中看到，它也可以很好地扩展到回归问题上．Boosting的动机是集合许多弱学习的结果来得到有用的“committee”．</p><p>弱分类器是误差率仅仅比随机猜测要好一点的分类器．Boosting 的目的是依次对反复修改的数据应用弱分类器算法，因此得到弱分类器序列 <span class="math inline">\(G_m(x),m=1,2,\ldots,M\)</span> 根据它们得到的预测再通过一个加权来得到最终的预测</p><blockquote><p><strong>事实上，Breiman(NIPS Workshop，1996) 将树的 AdaBoost 称为“世界上最好的现成分类器”(best off-the-shelf classifier in the world)．</strong> <strong>有人认为决策树是 boosting 是数据挖掘应用中理想的基学习器．</strong></p></blockquote><h3 id="数据挖掘的现货方法">数据挖掘的现货方法</h3><blockquote><p>个人注：<strong>“现货”(off-the-shelf)</strong> 方法．现货方法指的是可以直接应用到数据中而不需要大量时间进行数据预处理或者学习过程的精心调参．</p></blockquote><p><strong>预测学习 (predictive learning)</strong> 是数据挖掘中很重要的一部分．正如在这本书中看到的一样，已经提出了大量的方法对数据进行学习，然后预测．对于每个特定的方法，有些情形特别适用，但在其他情形表现得很差．我们已经试图在每个方法的讨论中明确合适的应用情形．然而，对于给定的问题，我们不会事先知道哪种方法表现得最好．表 10.1 总结了一些学习方法的特点．</p><p>工业和商业数据挖掘应用在学习过程的要求往往特别具有挑战性．数据集中观测值的个数以及每个观测值上衡量的变量个数往往都非常大．因此，需要注意计算的复杂度．并且，数据经常是混乱的 (messy)：输入往往是定量，二值以及类别型变量的混合，而且类别型变量往往有很多层次．一般还会有许多缺失值，完整的观测值是很稀少的．预测变量和响应变量的分布经常是 <strong>长尾 (long-tailed)</strong> 并且 <strong>高偏的 (highly skewed)</strong>．垃圾邮件的数据就是这种情形(<a href="../09-Additive-Models-Trees-and-Related-Methods/9.1-Generalized-Additive-Models/index.html">9.1.2 节</a>)；当拟合一个广义可加模型，我们首先对每个预测变量进行对数变换以期得到合理的拟合．另外，它们通常会包含很大一部分的严重的误测量值（离群值）．预测变量通常在差异很大的尺度下进行测量．</p><p>在数据挖掘应用中，通常只有大量预测变量中的一小部分真正与预测值相关的变量才被包含在分析中．另外，不同于很多应用的是，比如模式识别，很少有可信的专业知识来创建相关的特征，或者过滤掉不相关的， 这些不相关的特征显著降低了很多方法的效果．</p><p>另外，数据挖掘一般需要可解释性的模型．简单地得到预测值是不够的．提供定性 (qualitative) 理解输入变量和预测的响应变量之间的关系的信息是迫切的．因此，<strong>黑箱方法(black box)</strong>，比如神经网络，在单纯的预测情形，比如模式识别中是很有用的，但在数据挖掘中不是很有用．</p><p>这些计算速度、可解释性的要求以及数据的混乱本性严重限制了许多学习过程作为数据挖掘的 <strong>“现货”(off-the-shelf)</strong> 方法．现货方法指的是可以直接应用到数据中而不需要大量时间进行数据预处理或者学习过程的精心调参．</p><p><strong>在所有的有名的学习方法中，决策树最能达到数据挖掘的现货方法的要求．它们相对很快地构造出模型并且得到可解释的模型（如果树很小）</strong>．如 <a href="../09-Additive-Models-Trees-and-Related-Methods/9.2-Tree-Based-Methods/index.html">9.2 节</a> 中讨论的，它们自然地包含数值型和类别型预测变量以及缺失值的混合．它们在对单个预测变量的（严格单调）的变换中保持不变．结果是，尺寸变换和（或）更一般的变换不是问题，并且它们不受预测变量中的离群值的影响．它们将中间的特征选择作为算法过程的一部分．从而它们抑制（如果不是完全不受影响）包含许多不相关预测变量． 决策树的这些性质在很大程度上是它们成为数据挖掘中最受欢迎的学习方法的原因．</p><p><strong>树的不准确性导致其无法作为预测学习的最理想的工具．它们很少达到那个将数据训练得最好的方法的准确性．正如在 <a href="../10-Boosting-and-Additive-Trees/10.1-Boosting-Methods/index.html">10.1 节</a>看到的，boosting 决策树提高了它们的准确性，经常是显著提高．同时它保留着数据挖掘中所需要的性质</strong>．一些树的优势被 boosting 牺牲的是计算速度、可解释性，以及对于 AdaBoost 而言，对重叠类的鲁棒性，特别是训练数据的误分类．<strong>gradient boosted model(GBM)</strong>是 tree boosting 的一般化，它试图减轻这些问题，以便为数据挖掘提供准确且有效的现货方法．</p><h3 id="大小合适的boosting树">大小合适的boosting树</h3><p>曾经，boosting 被认为是一种将模型结合起来(combing models)的技巧，在这里模型是树．同样地，生成树的算法可看成是产生用于 boosting 进行结合的模型的 <strong>原型(primitive)</strong>．这种情形下，在生成树的时候以通常的方式分别估计每棵树的最优大小（<a href="9.2-Tree-Based-Methods/index.html">9.2 节</a>）．首先诱导出非常大（过大的）的一棵树，接着应用自下而上的过程剪枝得到估计的最优终止结点个数的树．这种方式隐含地假设了每棵树是式 公式（10.28） 中的最后一棵．</p><ul><li>解释性</li></ul><p>​ 单个决策树有着很高的解释性．整个模型可以用简单的二维图象（二叉树）完整地表示，其中二叉树也很容易可视化．树的线性组合 公式（10.28） 丢失了这条重要的特性，所以必须考虑用不同的方式来解释．</p><ul><li>预测变量的相对重要性</li></ul><p>​ 在数据挖掘应用中，输入的预测变量与响应变量的相关程度很少是相等的．通常只有一小部分会对响应变量有显著的影响，而绝大部分的变量是不相关的，并且可以简单地不用包含进模型．研究每个输入变量在预测响应变量时的相关重要度或者贡献是很有用的．</p><h2 id="第十一章-神经网络">第十一章 <strong>神经网络</strong></h2><p>这章中我们描述一类学习方法，它是基于在不同的领域（统计和人工智能）中独立发展起来但本质上相同的模型．中心思想是提取输入的线性组合作为<strong>导出特征 (derived features)</strong>，然后将目标看成特征的非线性函数进行建模．这是一个很有效的学习方法，在许多领域都有广泛应用．我们首先讨论<strong>投影寻踪模型 (projection pursuit model)</strong>，这是在半参统计和光滑化领域中发展出来的．本章的剩余部分集中讨论神经网络模型．</p><h3 id="投影寻踪回归">投影寻踪回归</h3><p>Projection Pursuit Regression</p><p>在我们一般监督学习问题中，假设我们有 <span class="math inline">\(p\)</span> 个组分的输入向量 <span class="math inline">\(X\)</span>，以及目标变量 <span class="math inline">\(Y\)</span>．令 <span class="math inline">\(\omega_m,m=1,2,\ldots, M\)</span> 为未知参数的 <span class="math inline">\(p\)</span> 维单位向量．<strong>投影寻踪回归 (PPR)</strong> 模型有如下形式: <span class="math display">\[f(X)=\sum\limits_{m=1}^Mg_m(\omega_m^TX)\tag{11.1}\label{11.1}\]</span> 这是一个可加模型，但是是关于导出特征 <span class="math inline">\(V_m=\omega_m^TX\)</span>，而不是关于输入变量本身．函数 <span class="math inline">\(g_m\)</span> 未定，而是用一些灵活的光滑化方法来估计及 <span class="math inline">\(\omega_m\)</span> 的方向（见下）．</p><p>函数 <span class="math inline">\(g_m(\omega_m^TX)\)</span> 称为 <span class="math inline">\(\mathbb R^p\)</span> 中的<strong>岭函数 (ridge function)</strong>．仅仅在由向量 <span class="math inline">\(\omega_m\)</span> 定义的方向上变化．标量变量 <span class="math inline">\(V_m=\omega_m^TX\)</span> 是 <span class="math inline">\(X\)</span> 在单位向量 <span class="math inline">\(\omega_m\)</span> 上的投影，寻找使得模型拟合好的 <span class="math inline">\(\omega_m\)</span>，因此称为“投影寻踪”．<strong>图 11.1 显示了岭函数的一些例子</strong>。</p><blockquote><p>个人注：详见原书图11.1</p></blockquote><p>实际上，如果 <span class="math inline">\(M\)</span> 任意大，选择合适的 <span class="math inline">\(g_m\)</span>，PPR 模型可以很好地近似 <span class="math inline">\(\mathbb R^p\)</span> 中任意的连续函数．这样的模型类别称为 <strong>通用近似 (universal approximator)</strong>．然而这种一般性需要付出代价．拟合模型的解释性通常很困难，因为每个输入变量都以复杂且多位面的方式进入模型中．结果使得 PPR 模型对于预测非常有用，但是对于产生一个可理解的模型不是很有用．<span class="math inline">\(M=1\)</span> 模型是个例外，也是计量经济学中的 <strong>单指标模型 (single index model)</strong>．这比线性回归模型更加一般，也提供了一个类似（线性回归模型）的解释．</p><p>然而，投影寻踪回归模型在统计领域并没有被广泛地使用，或许是因为在它的提出时间（1981），计算上的需求超出大多数已有计算机的能力．但是它确实代表着重要的智力进步，它是一个在神经网络领域的转世中发展起来的</p><h3 id="拟合神经网络">拟合神经网络</h3><p>神经网络模型中未知的参数，通常称为 <strong>权重 (weights)</strong>，我们需要寻找它们的值使得模型很好地拟合训练数据．我们将参数的全集记为 <span class="math inline">\(\theta\)</span></p><p>对于回归，我们采用误差平方和用于衡量拟合的效果（误差函数） <span class="math display">\[R(\theta)=\sum\limits_{k=1}^K\sum\limits_{i=1}^N(y_{ik}-f_k(x_i))^2\tag{11.9}\]</span></p><p>对于分类，我们可以采用平方误差或者交叉熵（偏差）：</p><p><span class="math display">\[R(\theta)=-\sum\limits_{i=1}^N\sum\limits_{k=1}^Ky_{ik}\mathrm{log}\;f_k(x_i)\tag{11.10}\]</span></p><p>以及对应的分类器 <span class="math inline">\(G(x)=\mathrm{arg\; max}_kf_k(x)\)</span>．有了 softmax 激活函数和交叉熵误差函数，神经网络模型实际上是关于隐藏层的线性逻辑斯蒂回归模型，而且所有的参数通过极大似然来估计．</p><p>一般地，我们不想要 <span class="math inline">\(R(\theta)\)</span> 的全局最小值，因为这可能会是一个过拟合解．而是需要一些正则化：这个可以通过惩罚项来直接实现，或者提前终止来间接实现．下一节中将给出详细的细节．</p><p>最小化 <span class="math inline">\(R(\theta)\)</span> 的一般方法是通过梯度下降，在这种情形下称作 <strong>向后传播 (back-propagation)</strong>．因为模型的组成成分，运用微分的链式法则可以很简单地得到梯度．这个可以通过对网络向前或向后遍历计算得到，仅跟踪每个单元的局部量．</p><p>向后传播的优点在于简单，局部自然．在向后传播算法中，每个隐藏层单元仅仅向（从）有其联系的单元传递（接收）信息．因此可以在并行架构的计算机上高效地实现．</p><p><strong>批量学习 (batch learning)</strong>，参数更新为所有训练情形的和．学习也可以 online 地进行——每次处理一个观测，每个训练情形过后更新梯度，然后对训练情形重复多次．在这种情形下，公式（11.13） 式的和可以替换成单个被加数．一个 <strong>训练时期 (training epoch)</strong> 指的是一次扫描整个训练集．<strong>在线训练 (online training)</strong> 允许网络处理非常大的训练集，而且当新观测加入时更新权重．</p><h3 id="训练神经网络的一些问题">训练神经网络的一些问题</h3><p>训练神经网络真的是一门艺术．模型一般会过参量化，而且优化问题非凸而且不稳定，除非遵循某确定的方式．在这节我们总结一些重要的问题．</p><ul><li>初始值 注意如果权重接近 0，则 sigmoid（图 11.3）起作用的部分近似线性，因此神经网络退化成近似线性模型（<a href="https://github.com/szcf-weiya/ESL-CN/issues/177">练习 11.2</a>）．通常权重系数的初始值取为接近 0 的随机值．因此模型开始时近似线性，当系数增大时变成非线性．需要的时候局部化单个单元的方向并且引入非线性．恰巧为 0 的权重的使用导致 0 微分和完美的对称，而且算法将不会移动．而以较大的值开始经常带来不好的解． <span class="math inline">\(\sigma(sv)\)</span> ，s是权重。</li><li>过拟合 通常神经网络有太多的权重而且在 <span class="math inline">\(R\)</span> 的全局最小处过拟合数据．在神经网络的发展早期，无论是设计还是意外，采用提前终止的规则来避免过拟合．也就是训练一会儿模型，在达到全局最小前终止．因为权重以高正则化（线性）解开始，这有将最终模型收缩成线性模型的效果．验证集对于决定什么时候停止是很有用的，因为我们期望此时验证误差开始增长． 一个更明显的正则化方法是 <strong>权重衰减 (weight decay)</strong>，类似用于线性模型的岭回归</li><li>输入的缩放 因为对输入的缩放决定了在底层中系数缩放的效率，所有它可以对最终解有很大的影响．最开始最好是对所有输入进行标准化使均值为 0，标准差为 1．这保证了在正则化过程中对所有输入公平对待，而且允许为随机的初始权重系数选择一个有意义的区间．有了标准化的输入，一般在 <span class="math inline">\([-0.7,+0.7]\)</span> 范围内均匀随机选择权重系数．</li><li>隐藏单元和层的个数 一般来说，太多的隐藏单元比太少的隐藏单元要好．太少的隐藏单元，模型或许没有足够的灵活性来捕捉数据的非线性；太多的隐藏单元，如果使用了合适的正则化，额外的权重系数可以收缩到 0．一般地，隐藏单元的数量处于 5 到 100 的范围之内，而且随着输入个数、训练情形的种数的增加而增加．最常见的是放入相当大数量的单元并且进行正则化训练．一些研究者采用交叉验证来估计最优的数量，但是当交叉验证用来估计正则化系数这似乎是不必要的．<strong>隐藏层的选择由背景知识和经验来指导</strong>．每一层提取输入的特征用于回归或者分类．多重隐藏层的使用允许在不同的分解层次上构造层次特征．</li><li>多重最小点 误差函数 <span class="math inline">\(R(\theta)\)</span> 为非凸，具有许多局部最小点．后果是最终得到的解取决于权重系数的初始值．至少需要尝试一系列随机的初始配置，然后选择给出最低（惩罚）误差的解．或许更好的方式是在对一系列网络的预测值进行平均作为最终的预测（Ripley，1996[^1]）．这比平均权重系数更好，因为模型的非线性表明平均的解会很差．另一种方式是通过 bagging，它是对不同网络的预测值进行平均，这些网络是对随机扰动版本的训练数据进行训练得到的．</li></ul><h2 id="第十二章-支持向量机与灵活判别方法">第十二章 <strong>支持向量机与灵活判别方法</strong></h2><p><strong>核函数</strong> 所以 公式（12.19） 和 公式（12.20） 仅仅通过内积涉及 <span class="math inline">\(h(x)\)</span>．实际上，我们根本不需要明确变换关系 <span class="math inline">\(h(x)\)</span>，而仅仅要求知道在转换后的空间中计算内积的核函数 <span class="math display">\[K(x,x&#39;)=\langle h(x), h(x&#39;) \rangle\tag{12.21}\]</span></p><p>在 SVM 中有三种流行的 <span class="math inline">\(K\)</span> 可以选择</p><p><span class="math display">\[\begin{array}{rl}d\text{ 阶多项式：} &amp; K(x,x&#39;)=(1+\langle x,x&#39; \rangle)^d\\\text{径向基：} &amp; K(x, x&#39;)=\exp(-\gamma \Vert x-x&#39;\Vert^2)\tag{12.22}\label{12.22}\\\text{神经网络：} &amp; K(x,x&#39;)=\tanh(\kappa_1\langle x,x&#39; \rangle+\kappa_2)\\\end{array}\]</span></p><p>考虑含有两个输入变量 <span class="math inline">\(X_1\)</span> 和 <span class="math inline">\(X_2\)</span> 的特征空间，以及 2 阶的多项式核．则</p><p><span class="math display">\[\begin{array}{ll}K(x,x&#39;)&amp;=(1+\langle X,X&#39; \rangle)^2\\&amp;=(1+X_1X_1&#39;+X_2X_2&#39;)^2\\&amp;=1+2X_1X_1&#39;+2X_2X_2&#39;+(X_1X_1&#39;)^2+(X_2X_2&#39;)^2+2X_1X_1&#39;X_2X_2&#39;\tag{12.23}\label{12.23}\end{array}\]</span></p><p>则 <span class="math inline">\(M=6\)</span>，而且如果我们选择 <span class="math inline">\(h_1(X)=1,h_2(X)=\sqrt{2}X_1,h_3(X)=\sqrt{2}X_2,h_4(X)=X_1^2,h_5(X)=X_2^2\)</span>，以及 <span class="math inline">\(h_6(X)=\sqrt{2}X_1X_2\)</span>，则<span class="math inline">\(K(X,X&#39;)=\langle h(X),h(X&#39;)\rangle\)</span>．</p><h2 id="第十三章-原型方法与最近邻算法">第十三章 <strong>原型方法与最近邻算法</strong></h2><p><strong>不变量和切线距离</strong> 对于每张图像，我们画出了该图像旋转版本的曲线，称为 <strong>不变流形 (invariance manifolds)</strong>．现在，不是用传统的欧氏距离，而是采用两条曲线间的最短距离．换句话说，两张图像间的距离取为第一张图像的任意旋转版本与第二张图像的任意旋转版本间的最短欧氏距离．这个距离称为 <strong>不变度量 (invariant metric)</strong>．</p><p>原则上，可以采用这种不变度量来进行 1 最近邻分类．然而这里有两个问题．第一，对于真实图像很难进行计算．第二，允许大的变换，可能效果很差．举个例子，经过 180° 旋转后，“6” 可能看成是 “9”．<strong>我们需要限制为微小旋转</strong>．</p><p><strong>切线距离 (tangent distance)</strong> 解决了这两个问题．如图 13.10 所示，我们可以用图像“3”在其原图像的切线来近似不变流形．这个切线可以通过从图像的微小旋转中来估计方向向量，或者通过更复杂的空间光滑方法（<a href="https://github.com/szcf-weiya/ESL-CN/issues/187">练习 13.4</a>）．对于较大的旋转，切线图像不再像“3”，所以大程度的旋转问题可以减轻．</p><p>想法是对每个训练图像计算不变切线．对于待分类的 <strong>查询图像 (query image)</strong>，计算其不变切线，并且在训练集的直线中寻找最近的直线．对应最近的直线的类别（数字）是对查询图像类别的预测值．在图 13.11 中，两条切向直线相交，但也只是因为我们是在二维空间中表示 256 维的情形．在 <span class="math inline">\(\mathbb R^{256}\)</span> 中，两条这样的直线相交的概率是 0．</p><p><strong>自适应最近邻方法</strong> 最近邻分类的隐含假设是类别概率在邻域内近似为常值，因此简单的平均会得到不错的估计．然而，在这个例子中，只有水平方向上的类别概率会变化．如果我们提前知道这一点，可以将邻居拉伸为长方形区域．这会降低估计的偏差，同时保持方差不变．</p><p>一般地，这要求最近邻分类中采用自适应的度量，使得得到的邻域沿着类别不会改变太多的方向上拉伸．在高维特征空间中，类别概率可能仅仅只在一个低维的子空间中有所改变，因此自适应度量是很重要的优点．</p><p><strong>判别自适应最近邻 (DANN)</strong> 方法进行了局部维度降低——也就是，在每个查询点单独降低维度．提出通过逐步剔除包含训练数据的盒子的边来自动寻找长方形邻域．这里我们介绍 Hastie and Tibshirani (1996a)[^2] 提出的 <strong>判别自适应最近邻 (discriminant adaptive nearest-neighbor) (DANN)</strong>．在每个查询点，构造其大小为 50 个点的邻域，并且用这些点的类别分布来决定怎么对邻域进行变形——也就是，对度量进行更新．接着更新后的度量用在该查询点的最近邻规则中．因此每一个查询点都可能采用不同的度量．很明显邻域应当沿着垂直类别重心连线的方向拉伸．这个方向也与线性判别边界重合，而且是类别概率改变最少的方向．一般地，类别概率变化最大的方向不会与类别重心连线垂直</p><p><strong>计算上的考虑</strong></p><p>最近邻分类规则的一个缺点通常是它的计算负荷量，无论是寻找最近邻还是存储整个训练集合．</p><h2 id="第十四章-无监督学习">第十四章 <strong>无监督学习</strong></h2><p>流形（manifold）：数学上，流形是一个拓扑空间，在每一点附近局部地近似欧式空间．更精确地，<span class="math inline">\(n\)</span> 维流形的每个点与维度为 <span class="math inline">\(n\)</span> 的欧式空间同态的邻域． 监督学习中，有一个明确的成功或不成功的量度，因此可用于判断特定情况下的<strong>充分性 (adequacy)</strong>，并比较不同方法在各种情况下的<strong>有效性 (effectiveness)</strong>．成功的损失直接用在联合分布 <span class="math inline">\(\Pr(X,Y)\)</span> 上的期望损失来衡量．这个可以用各种方式来衡量，包括交叉验证．在非监督学习中，没有这些直接衡量成功的量度．从大部分非监督学习的算法的输出中评估推断的有效性是很难确定的．必须诉诸于<strong>启发式变量 (heuristic arguments)</strong>，在监督学习也经常使用，这不仅可以激励 (motivating) 算法，而且为了评价结果的质量．因为有效性是主观问题，不能直接加以证实，这种不舒服 (unconfortable) 的情形导致提出的方法激增，</p><p><strong>关联规则分析 (Association rule analysis)</strong> 已经成为挖掘贸易数据的流行工具．目标是寻找变量 <span class="math inline">\(X=(X_1,X_2,\ldots,X_p)\)</span> 在数据中出现最频繁的联合值．在二值数据 <span class="math inline">\(X_j\in\{0,1\}\)</span> 中应用最多，也称作“市场篮子”分析．这种情形下观测值为销售交易，比如出现在商店收银台的商品．变量表示所有在商店中出售的商品．对于观测 <span class="math inline">\(i\)</span>，每个变量 <span class="math inline">\(X_j\)</span> 取值为 0 或 1；如果第 <span class="math inline">\(j\)</span> 个商品作为该次交易购买的一部分则 <span class="math inline">\(x_{ij}=1\)</span>，而如果没有购买则 <span class="math inline">\(x_{ij}=0\)</span>．这些经常有联合值的变量表示物品经常被一起购买．这个信息对于货架、跨营销的促销活动、商品目录的设计，以及基于购买模式的消费者划分都是很有用的． 关联规则成为了在相关的市场篮子的设定下用于分析非常大的交易数据库的流行工具．这是当数据可以转换成多维邻接表的形式时．输出是以容易理解并且可解释的关联规则 公式（14.4）的形式展现的．Apriori 算法允许分析可以用到大的数据库中，更大的数据库适用于其他类型的分析．关联规则是数据挖掘最大的成功之一．</p><p><strong>聚类分析</strong></p><p>聚类分析的所有目标的核心是度量要聚类的单个点间相似（或不相似）的程度．聚类方法试图基于点间相似性的定义来将其分类．相似性的定义只能从关注的主题得到．某种程度上，这个情形与确定预测问题（监督学习）中的损失或花费函数相似．在预测问题中，损失函数与错误的预测有关，而错误的预测取决于数据之外的考虑． 简而言之，聚类方法中相似性的定义就如同监督学习问题中损失函数一样重要. <strong>确定一个合适的不相似性的度量远比选择聚类算法来得重要</strong>．(涉及领域知识。)</p><p><strong>主成分</strong>，<strong>主曲线和主曲面</strong></p><p><strong>流形学习</strong>（manifold learning）</p><p>是机器学习、模式识别中的一种方法，在维数约简方面具有广泛的应用。它的主要思想是将高维的数据映射到低维，使该低维的数据能够反映原高维数据的某些本质结构特征。流形学习的前提是有一种假设，即某些高维数据，实际是一种低维的流形结构嵌入在高维空间中。流形学习的目的是将其映射回低维空间中，揭示其本质。</p><blockquote><p>个人注：关于流行学习知乎上一篇通熟易懂的文章 https://www.zhihu.com/question/24015486/answer/26524937 ，最常用的例子就是瑞士卷</p></blockquote><p><strong>投影是从一个向量空间到其自身的线性变换</strong>，并且投影矩阵满足<span class="math inline">\(\mathbf P^2=\mathbf P\)</span>．</p><blockquote><p>个人注：应该是到子空间吧。<strong>不是所有从一个向量空间到自身的线性变换都是投影</strong>，但<strong>所有投影都是线性变换</strong>，而且满足 <span class="math inline">\(P^2 = P\)</span>（即投影两次不变）。</p></blockquote><p><strong>主成分可以看成是主曲线的特殊情形</strong>。</p><h2 id="第十五章-随机森林">第十五章 <strong>随机森林</strong></h2><blockquote><p>在每次分割时，随机选择 <span class="math inline">\(m\le p\)</span> 个输入变量作为候选变量用来分割</p></blockquote><p>一般地，<span class="math inline">\(m\)</span> 取为 <span class="math inline">\(\sqrt{p}\)</span>，或者甚至小到取 1．</p><p>Bagging 可以看成是特殊的随机森林，即 <span class="math inline">\(m=p\)</span> 的随机森林．</p><p>另外，发明者给出下面两条推荐：</p><ul><li>对于分类，<span class="math inline">\(m\)</span> 的默认值为 <span class="math inline">\(\lfloor \sqrt p \rfloor\)</span>，且最小的结点数为 1．</li><li>对于回归，<span class="math inline">\(m\)</span> 的默认值为 <span class="math inline">\(\lfloor p/3\rfloor\)</span>，且最小的结点数为 5．</li></ul><p>实际中这些参数的最优值取决于具体问题，并且它们应当被视为 <strong>调整参数 (tunning parameters)</strong>．在图 15.3 中，<span class="math inline">\(m=6\)</span> 比默认值 <span class="math inline">\(\lfloor 8/3\rfloor =2\)</span> 更好．</p><h2 id="第十六章-集成学习">第十六章 <strong>集成学习</strong></h2><p>"Bet on Sparsity" 原则 <span class="math inline">\(L_1\)</span> 的收缩能更好地适应稀疏的情形（在所有可能选择中，非零系数的基函数的个数很少）． 当拟合系数时，我们应该使用 <span class="math inline">\(L_2\)</span> 惩罚，而不是 <span class="math inline">\(L_1\)</span> 惩罚．另一方面，如果这里只有少量的（比如，<span class="math inline">\(1000\)</span>）系数非零，则 lasso （<span class="math inline">\(L_1\)</span> 惩罚）会表现得很好．我们将这个看成是 <strong>稀疏 (sparse)</strong> 的情形，而第一种情形（高斯系数）是 <strong>稠密 (dense)</strong> 的．注意到尽管在稠密情形下，<span class="math inline">\(L_2\)</span> 惩罚是最好的，但没有方法能做得很好，因为数据太少，但却要从中估计大量的非零系数．这是维数的灾难造成的损失．稀疏设定中，我们可以用 <span class="math inline">\(L_1\)</span> 惩罚做得很好，因为非零稀疏的个数很少．但 <span class="math inline">\(L_2\)</span> 惩罚便不行．</p><p>换句话说，<span class="math inline">\(L_1\)</span> 惩罚的使用遵循称作 “bet on sparsity” 的这一高维问题的准则：</p><blockquote><p>采用在稀疏问题中表现得好的方法，因为没有方法能在稠密问题中表现得好．</p></blockquote><h2 id="第十七章-无向图模型">第十七章 <strong>无向图模型</strong></h2><p><strong>图 (Graph)</strong> 由顶点（结点）集，以及连接顶点对的边集构成．在图模型中，每个顶点表示一个随机变量，并且图给出了一种理解全体随机变量联合分布的可视化方式．对于监督学习和非监督学习它们都是很有用的．在 <strong>无向图 (undirected graph)</strong> 中，边是没有方向的．我们仅限于讨论无向图模型，也称作 <strong>马尔科夫随机域 (Markov random fields)</strong> 或者 <strong>马尔科夫网络 (Markov networks)</strong>． - 在这些图中，两个顶点间缺失一条边有着特殊的含义：对应的随机变量在给定其它变量下是条件独立的．</p><p>图中的边用值 (value) 或者 <strong>势 (potential)</strong> 参量化，来表示在对应顶点上的随机变量间条件依赖性的强度大小．采用图模型的主要挑战是模型选择（选择图的结构）、根据数据来估计边的参数，并且从联合分布中计算边缘顶点的概率和期望．后两个任务在计算机科学中有时被称作 <strong>学习 (learning)</strong> 和 <strong>推断(inference)</strong>．</p><p>关于 <strong>有向图 (directed graphical models)</strong> 或者 <strong>贝叶斯网络 (Bayesian networks)</strong> 有大量并且活跃的文献；这是边有方向箭头（但是没有有向环）的图模型．有向图模型表示可以分解成条件分布乘积的概率分布，并且有解释因果关系的潜力．</p><p>三种等价的 Markov 性质" pairwise Markov properties: 寻找缺失边，在给定其他结点的情况下，缺失边的两个顶点相互独立； global Markov properties: 寻找分离集，在给定分离集的情况下，被分离的子图相互独立；</p><h3 id="连续变量的无向图模型">连续变量的无向图模型</h3><p>这里我们考虑所有变量都是连续变量的马尔科夫网络．这样的图模型几乎总是用到高斯分布，因为它有方便的分析性质．我们假设观测值服从均值为 <span class="math inline">\(\mu\)</span>，协方差为 <span class="math inline">\(\mathbf \Sigma\)</span> 的多元高斯分布．因为高斯分布至多表示二阶的关系，所以它自动地编码了一个成对马尔科夫图．</p><p>!!! note "weiya 注：" 因为在高斯分布的密度函数中，指数项中关于随机变量的阶数最多是二次，所以说它至多能表示二阶的关系．</p><ul><li>高斯分布有个性质是所有的条件分布也是高斯分布． 协方差矩阵的逆 <span class="math inline">\(\mathbf\Sigma^{-1}\)</span> 包含变量之间的 <strong>偏协方差 (partial covariances)</strong> 信息；也就是，在给定其它变量的条件下，<span class="math inline">\(i\)</span> 与 <span class="math inline">\(j\)</span> 的协方差．特别地，如果 <span class="math inline">\(\mathbf {\Theta=\Sigma^{-1}}\)</span> 的第 <span class="math inline">\(ij\)</span> 个元素为 0，则变量 <span class="math inline">\(i\)</span> 和 <span class="math inline">\(j\)</span> 在给定其它变量情况下是条件独立的．</li></ul><h3 id="图结构的估计">图结构的估计</h3><p>大多数情况下，我们不知道哪些边要从图中去掉，因此想试图从数据本身找出．最近几年很多作者提出用于这个目的的 <span class="math inline">\(L_1\)</span> (lasso) 正则化．</p><p>!!! note "weiya 注：" 省略图中的边，有点类似于做变量选择，而 lasso 正是应对变量选择的“绝世武功”!:joy:</p><p>为了实现这点，它们将每个变量看成响应变量而其它的变量作为预测变量进行拟合 lasso 回归</p><h3 id="限制玻尔兹曼机">限制玻尔兹曼机</h3><p>离散变量的无向马尔科夫网络是很流行的，而且特别地，二值变量的成对马尔科夫网络更普遍．在统计力学领域有时称为 Ising 模型，在机器学习领域称为 <strong>玻尔兹曼机 (Boltzmann machines)</strong>，其中顶点称为“<strong>结点 (nodes)</strong>”或“<strong>单元 (units)</strong>”，取值为 0 或 1.</p><p>这节我们考虑受神经网络影响的一种特殊的图模型结构，该结构中，单元是按层进行组织的．<strong>限制玻尔兹曼机 (RBM)</strong> 包含一层可见单元和一层隐藏单元，单层之间没有联系．如果隐藏单元的连接被移除掉，计算条件期望变得很简单</p><blockquote><p>个人注：<strong>虽然标准 RBM 使用二值神经元，但也存在许多变体，可以处理不同类型的输入：</strong></p><table><thead><tr class="header"><th>变体类型</th><th>描述</th></tr></thead><tbody><tr class="odd"><td><strong>Gaussian-Bernoulli RBM</strong></td><td>可见层为连续实数（高斯分布），隐藏层仍是二值</td></tr><tr class="even"><td><strong>Gaussian-Gaussian RBM</strong></td><td>可见层和隐藏层都为连续变量</td></tr><tr class="odd"><td><strong>Softmax RBM</strong></td><td>可见层或隐藏层是 one-hot 多类别状态</td></tr><tr class="even"><td><strong>ReLU RBM</strong></td><td>使用 ReLU 激活而非二值状态，用于更复杂的连续特征建模</td></tr></tbody></table><p>这些变体适用于图像、音频、自然语言处理等不同类型的数据。</p></blockquote><h2 id="第十八章-高维问题">第十八章 高维问题</h2>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 数学 </tag>
            
            <tag> 算法 </tag>
            
            <tag> ESL </tag>
            
            <tag> 统计 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《金融经济学二十五讲》学习笔记</title>
      <link href="/2025/02/01/%E3%80%8A%E9%87%91%E8%9E%8D%E7%BB%8F%E6%B5%8E%E5%AD%A6%E4%BA%8C%E5%8D%81%E4%BA%94%E8%AE%B2%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
      <url>/2025/02/01/%E3%80%8A%E9%87%91%E8%9E%8D%E7%BB%8F%E6%B5%8E%E5%AD%A6%E4%BA%8C%E5%8D%81%E4%BA%94%E8%AE%B2%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<p><u><strong>金融的主干是定价（均衡定价、无套利定价）</strong></u></p><p><strong><u>金融是研究赚钱的理论，所以重点是研究市场上各类资产的定价，定价又涉及风险和效用的概念。</u></strong></p><ul><li>现实的资产价格（例如股票）由人的情绪来确定；一种投票的机制！！！！<br /></li><li>行为经济学：非理性假设；方法论：使用心理学的结论作为起点<br /></li><li>科学的尽头是神学，行为经济学是玄学 ，也是学术的宿命<br /></li><li>VaR 历史模拟法<br /></li><li>表见代理：例如老朱不认高管签订的合同就违反了这一点；案例见光大（？）证券的萝卜章事件。<br /></li><li>场内市场 场外市场OTC市场 <span id="more"></span><br /></li><li>击台球 的隐喻：<br />客观和主观纠缠一起的状态空间。<br />物理学研究的都是客观的对象（自然科学，观察统计规律、利用规律预测未来（把严格的数学作为工具）；和数学的纯粹演绎的逻辑推导不同）；<br />而经济学研究的人的主观性混杂进去了客观的空间<br />方法1、统计求平均值<br />方法2、通过假设把人的主观方面限定成客观的状态空间<br />方法3、借助神（行家假设，理性人的假设，类似只会计算的计算机，把人性中最重要最激动人心的部分给剥离了）抽象的理性人，不以人（个人）的意志为转移<br /></li><li>主观到客观的转换；客观和主观：能不能观察来分？不可预测<br /></li><li>海德格尔：存在先于本质<br /></li><li>ETF<br /></li><li>量化交易的风险：光大证券下单的乌龙指<br /></li><li>锤子和钉子，就是工具的套用，也是教条主义，需要跳出框架；<br /></li><li>所有的观点只是其中一个角度而已，就像盲人摸象；<br /></li><li>学经济跳出数学的思维框架（确定性的思维），更多要考虑统计的思维<br /></li><li>大胆假设，小心求证 ；假设的能力也很重要，在假设的前提下建模，然后通过结果验证假设的合理性。这其实就是统计的核心思想。<br /></li><li>赛道的选择问题：例如龟兔赛跑，乌龟可以考虑在水中比、或者乌龟开车来比；这其实也是选择比努力重要的例子。<br /></li><li>经济学中的希腊字母，表示资产（组合） 价值对各种参数的偏导数（可能多阶，敏感性）<br /></li><li>mortgage 按揭 down payment 首付<br /></li><li>理想世界只有一个；而金融摩擦世界很多，信息不对称、期限错配、交易成本是摩擦世界的几个要素<br /></li><li>运动：短时间的运动 用微分；运动的长时间的累积 用积分<br /></li><li>因为股票不能有负的，所以不能直接用正态分布来直接表达，用对数正态分布来表达股票的变动符合正态分布<br /></li><li>风险在组合投资后，会由高变小；<br /></li><li>经济学的两次革命：马特维兹的BS公式和布莱克-斯科尔斯公式（衍生品定价，第18讲）<br /></li><li>Option是一个权利非义务<br /></li><li>期权相当于保险，风险管理工具+赌博（投机）工具<br /></li><li>按揭贷款期权是利率的衍生品<br /></li><li>洞察力在模型中体现为各种参数的判断。<br /></li><li>方法论： 不要引入新概念，例如小学低年级不要引入方程一样，要在当前的假设空间去研究问题<br /></li><li>因果，互为因果，例如供需。所以可以根据研究需要，确定谁是因，谁是果<br /></li><li>凸函数，凹函数的判断，从原点看，是凸还是凹<br /></li><li>经济学、金融让你感觉到数学还是有用的，或者说用途很大。<br /></li><li>金融是研究赚钱的理论，所以重点是研究市场上各类资产的定价，定价又涉及风险和效用的概念。<br /></li><li>每个人都要精神寄托，区别是内求于心，还是外求于神<br /></li><li>不确定性下的数学描述，符号体系，假设； 核心是组合优化的问题。 资产组合<br /></li><li>理论的约束，例如从牛顿到爱因斯坦，到量子力学，何况人，还有你的工作，肯定是不完美的，接受不完美。<br /></li><li>不确定下做决策<br /></li><li>数学家和经济学家（实践家）的差别，对严格性的考虑不同； 工程中的近似概念<br /></li><li>经济学中的坐标图，点代表各种样本，而坐标代表样本对应的两个特征<br /></li><li>用对数好算，因为收益和时间序列有关，是指数极的，所以用对数计算方便。<br /></li><li>一阶导数是边际效用，二阶导数是边际效用递减率，一阶导数是正数，二阶导数是负数<br /></li><li>基于未来的不确定性，做现在的决策，以达到自己的预期。<br /></li><li>研究方法论<br />分析问题的时候不要画蛇添足呢，引入新的概念，需要在一个简单的环境和上下文中讲清问题即可。相当于不要引进新的变量。 研究一个模型的时候，前提的假设是其它因素是常数。只研究当前模型提及的变量之间的关系。<br /></li><li>有时候数学证明很复杂，而通过几何意义，让大家有直观的印象来理解，从而简化。<br /></li><li>烟草（烟草大数据分析）<br /></li><li><strong>金融极度不确定 计算机确定性的结合</strong><br /></li><li><strong>利率其实时间价值</strong><br /></li><li><strong>模型：核心符号化，或者说模型就是公式</strong><br /></li><li><strong>股票其实在公式之外，博弈人性还有宏观经济、政治等影响</strong><br /></li><li><strong>对数收益率的时序可加性</strong><br />如果我们考察单一投资品在总共 T 期内的表现，那应该用对数收益率，而非算数收益率。<strong>算术平均值</strong>50%，明年跌了 50%，它的算数平均收益率]为 0；但事实上，两年后该投资品亏损了最初资金的 25%。相反的，<strong>对数收益率由于具备可加性，它的均值可以正确反映出该投资品的真实收益率</strong> 40.5% 和 -69.3%，平均值为 -28.77%，转换为百分比亏损就是 exp{-28.77%} - 1 = -25%。 <strong>对数收益率的最大好处是它的可加性，把单期的对数收益率相加就得到整体的对数收益率。</strong>https://www.zhihu.com/question/30113132</li><li>夏普比率（Sharpe Ratio）<br />夏普比率（Sharpe Ratio）：是衡量投资回报相对于承担的风险的指标。它的计算公式如下：<br />夏普比率 = (回报率 - 无风险回报率) / 投资组合回报率的标准差<br /><strong>夏普比率越高，意味着每单位风险所获得的投资回报越高。</strong><br />如何求股票的风险（标准差）？见下文<br />理财知识汇总1-------夏普、α、β<br />https://blog.csdn.net/p213100/article/details/132403483。</li></ul><h2 id="期权-option">期权 Option</h2><p><strong>期权——四种交易方向</strong></p><ol type="1"><li>买入认购期权（看涨）：预期大涨，通过购买认购期权获利。</li><li>买入认沽期权（看跌）：预期大跌，通过购买认沽期权获利。</li><li>卖出认购期权（看小跌）：预期小幅下跌，通过卖出认购期权获得权利金。</li><li>卖出认沽期权（看小涨）：预期小幅上涨，通过卖出认沽期权获得权利金。</li></ol><ul><li>通过期权，投资者可以在股价上涨、小跌、不涨不跌的情况下获利。</li></ul><p>期权的盈利方式主要包括到期行权和期权合约的转让</p><p>看跌期权交易（现货）。</p><p>看涨期权交易（现货）。</p><p><strong>Put</strong> options transaction (spot).</p><p><strong>Call</strong> options transaction (spot).</p><h2 id="风险的度量">风险的度量</h2><p>beta、 方差 CML SML<br />均衡分析 互为因果 自我强化 价格决定人的行为，而人的行为又影响结果，最终达到了均衡的状态。<br />资产本身的风险 通过组合消除部分的风险，所以形成了beta<br />市场组合的回报可以视为宏观经济的回报 神奇的beta<br />hat 符号代表估计量<br />均衡是变动的，或者说，市场是不均衡的，只是一种理想状态； 临时的均衡</p><h2 id="风险规避相对系数和消费平滑">风险规避相对系数和消费平滑</h2><p>风险规避相对系数（Relative Risk Aversion, RRA）和消费平滑（Consumption Smoothing）之间有密切的关系。这两个概念在经济学，特别是在跨期选择和不确定性下的消费理论中，起着重要作用。</p><h3 id="风险规避相对系数">风险规避相对系数</h3><p>风险规避相对系数用来衡量一个人对风险的态度。它通常定义为效用函数的二阶导数与一阶导数之比，并乘以消费水平 $ c $。如果效用函数为 $ u(c) $，风险规避相对系数 $ RRA $ 的定义如下：<br /><span class="math display">\[RRA(c) = -\frac{c u&#39;&#39;(c)}{u&#39;(c)}\]</span> 其中：</p><ul><li>$ u'(c) $是效用函数的第一阶导数，表示边际效用。<br /></li><li>$ u''(c) $ 是效用函数的第二阶导数，表示边际效用的变化率。</li></ul><p>风险规避相对系数越大，表示该人对风险的规避程度越高。</p><h3 id="消费平滑">消费平滑</h3><p>消费平滑是指在跨期选择中，消费者倾向于在整个生命周期中保持消费的稳定，而不是让消费水平大幅波动。消费者通过储蓄和借贷来调整不同时间点的消费水平，以应对收入的波动和不确定性。</p><h3 id="风险规避和消费平滑的关系">风险规避和消费平滑的关系</h3><ol type="1"><li><p><strong>高风险规避导致更强的消费平滑</strong>：</p><ul><li>当消费者具有较高的风险规避相对系数时，他们对未来的不确定性感到更为担忧。因此，他们会更倾向于通过储蓄和借贷来平滑消费，以减少未来消费的不确定性和波动。<br /></li><li>高风险规避意味着消费者希望在任何可能的未来状态下都能维持一个相对稳定的消费水平，而不愿意在某些状态下消费较多而在其他状态下消费较少。</li></ul></li><li><p><strong>效用函数的形式和消费平滑</strong>：</p><ul><li>常见的效用函数形式之一是CRRA（恒定相对风险规避）效用函数，它的形式为：<br /><span class="math display">\[u(c) = \frac{c^{1-\gamma}}{1-\gamma}\]</span> 其中 <span class="math inline">\(\gamma\)</span> 是风险规避相对系数。当 <span class="math inline">\(\gamma\)</span> 较大时，消费者对风险的厌恶程度较高，他们会更努力地平滑消费。</li></ul></li><li><p><strong>消费平滑的机制</strong>：</p><ul><li><strong>储蓄和借贷</strong>：消费者可以通过在收入高的时候储蓄，在收入低的时候借贷来平滑消费。这种行为在高风险规避者中尤为明显，因为他们希望避免未来的消费不确定性。<br /></li><li><strong>保险和投资组合选择</strong>：高风险规避的消费者更倾向于购买保险或选择低风险的投资组合，以确保未来的消费稳定。</li></ul></li></ol><h3 id="例子说明">例子说明</h3><p>假设一个消费者的效用函数为 $ u(c) =  $，其中 <span class="math inline">\(\gamma\)</span> 是风险规避相对系数。如果 <span class="math inline">\(\gamma\)</span> 很大，消费者在面对未来收入的不确定性时会选择增加储蓄，以在未来收入下降时仍能维持较高的消费水平。相反，如果 <span class="math inline">\(\gamma\)</span> 很小，消费者对未来收入的不确定性较不敏感，他们可能会选择较少的储蓄和更多的当前消费，从而导致未来消费的波动较大。</p><p>综上所述，风险规避相对系数和消费平滑之间有密切的关系。高风险规避相对系数通常会导致消费者更加注重平滑消费，以应对未来的不确定性。</p><h2 id="财务分析-杜邦分析法">财务分析 杜邦分析法</h2><p>杜邦分析法（DuPont Analysis）是一种常用的财务分析工具，用于分解公司的股东权益回报率（ROE），以更好地理解公司的财务状况和经营效率。杜邦分析法通过将ROE分解为多个关键财务比率，帮助分析人员深入研究影响公司盈利能力的各个因素。</p><h3 id="杜邦分析法的基本公式">杜邦分析法的基本公式</h3><p>杜邦分析法最初是由杜邦公司（DuPont Corporation）在20世纪初提出的，原始的三步杜邦公式将ROE分解为以下三个组成部分：</p><p><span class="math display">\[\text{ROE} = \text{Net Profit Margin} \times \text{Asset Turnover} \times \text{Equity Multiplier}\]</span></p><ol type="1"><li><strong>净利润率（Net Profit Margin）</strong>：<ul><li>公式：<span class="math inline">\(\text{Net Profit Margin} = \frac{\text{Net Income}}{\text{Sales}}\)</span><br /></li><li>含义：公司每单位销售收入的净利润，反映了公司成本控制和定价策略的有效性。</li></ul></li><li><strong>资产周转率（Asset Turnover）</strong>：<ul><li>公式：<span class="math inline">\(\text{Asset Turnover} = \frac{\text{Sales}}{\text{Total Assets}}\)</span><br /></li><li>含义：公司利用其资产创造收入的效率，反映了公司的运营效率。</li></ul></li><li><strong>权益乘数（Equity Multiplier）</strong>：<ul><li>公式：<span class="math inline">\(\text{Equity Multiplier} = \frac{\text{Total Assets}}{\text{Total Equity}}\)</span><br /></li><li>含义：衡量公司使用负债的程度，反映了财务杠杆的影响。</li></ul></li></ol><h3 id="五步杜邦分析法">五步杜邦分析法</h3><p>为了更细致地分析ROE，五步杜邦分析法进一步将净利润率和资产周转率分解为更多的财务比率：</p><p><span class="math display">\[\text{ROE} = \left(\frac{\text{Net Income}}{\text{EBT}}\right) \times \left(\frac{\text{EBT}}{\text{EBIT}}\right) \times \left(\frac{\text{EBIT}}{\text{Sales}}\right) \times \left(\frac{\text{Sales}}{\text{Total Assets}}\right) \times \left(\frac{\text{Total Assets}}{\text{Total Equity}}\right)\]</span></p><ol type="1"><li><strong>税后利润率</strong>：<span class="math inline">\(\frac{\text{Net Income}}{\text{EBT}}\)</span>，反映税务管理的影响。<br /></li><li><strong>利息负担比率</strong>：<span class="math inline">\(\frac{\text{EBT}}{\text{EBIT}}\)</span>，衡量利息成本对盈利的影响。<br /></li><li><strong>营业利润率</strong>：<span class="math inline">\(\frac{\text{EBIT}}{\text{Sales}}\)</span>，表示公司的经营利润率。<br /></li><li><strong>资产周转率</strong>：<span class="math inline">\(\frac{\text{Sales}}{\text{Total Assets}}\)</span>，与前述相同。<br /></li><li><strong>权益乘数</strong>：<span class="math inline">\(\frac{\text{Total Assets}}{\text{Total Equity}}\)</span>，与前述相同。</li></ol><h3 id="杜邦分析法的应用">杜邦分析法的应用</h3><ol type="1"><li><strong>盈利能力分析</strong>：通过净利润率和营业利润率，可以了解公司在成本控制和收入管理上的表现。<br /></li><li><strong>运营效率分析</strong>：通过资产周转率，可以评估公司利用其资产的效率。<br /></li><li><strong>财务杠杆分析</strong>：通过权益乘数，可以评估公司通过负债扩大收益的程度。</li></ol><h3 id="杜邦分析法的优势">杜邦分析法的优势</h3><ul><li><strong>综合性强</strong>：它将财务报表的多个指标联系起来，提供了全面的财务状况分析。<br /></li><li><strong>可识别问题</strong>：通过分解ROE，杜邦分析法可以帮助识别财务问题的根源。</li></ul><h3 id="杜邦分析法的局限性">杜邦分析法的局限性</h3><ul><li><strong>依赖历史数据</strong>：它主要基于历史财务报表数据，可能无法准确反映未来的财务表现。<br /></li><li><strong>忽略非财务因素</strong>：如市场竞争、管理质量等非财务因素对公司财务表现的影响。</li></ul><p>杜邦分析法在财务分析中非常有用，但应结合其他分析工具和定性因素进行综合评估。</p><h2 id="应收账款">应收账款</h2><p><strong>应收账款周转率</strong>（Accounts Receivable Turnover Ratio）是一个重要的财务比率，用于衡量公司收回其应收账款的效率。它反映了公司在一个财务期间内将应收账款转化为现金的速度。高的应收账款周转率通常表明公司有效地管理其信用政策和收款过程，低的周转率可能意味着公司在收款方面存在问题，或其信用政策较为宽松。</p><h3 id="应收账款周转率的计算公式">应收账款周转率的计算公式</h3><p>应收账款周转率的基本计算公式为：</p><p><span class="math display">\[\text{应收账款周转率} = \frac{\text{净销售额}}{\text{平均应收账款}}\]</span></p><ul><li><strong>净销售额</strong>：通常指报告期间的信用销售额，即扣除销售折扣和销售退回后的销售收入。<br /></li><li><strong>平均应收账款</strong>：通常使用期初和期末应收账款余额的平均值计算得出。计算公式为：</li></ul><p><span class="math display">\[\text{平均应收账款} = \frac{\text{期初应收账款} + \text{期末应收账款}}{2}\]</span></p><h3 id="应收账款周转天数">应收账款周转天数</h3><p>应收账款周转天数（Days Sales Outstanding, DSO）是一个相关的指标，用于衡量公司在多少天内收回其应收账款。应收账款周转天数可以通过应收账款周转率计算得出，公式如下：</p><p><span class="math display">\[\text{应收账款周转天数} = \frac{365}{\text{应收账款周转率}}\]</span></p><h3 id="示例">示例</h3><p>假设某公司在2023年的净销售额为1,000,000元，期初应收账款为200,000元，期末应收账款为300,000元，则该公司2023年的应收账款周转率和应收账款周转天数可以这样计算：</p><ol type="1"><li><strong>计算平均应收账款</strong>：</li></ol><p><span class="math display">\[\text{平均应收账款} = \frac{200,000 + 300,000}{2} = 250,000\]</span></p><ol start="2" type="1"><li><strong>计算应收账款周转率</strong>：</li></ol><p><span class="math display">\[\text{应收账款周转率} = \frac{1,000,000}{250,000} = 4\]</span></p><p>这意味着公司在2023年将其应收账款周转了4次。</p><ol start="3" type="1"><li><strong>计算应收账款周转天数</strong>：</li></ol><p><span class="math display">\[\text{应收账款周转天数} = \frac{365}{4} \approx 91.25\]</span></p><p>这表示公司平均每91.25天收回一次应收账款。</p><h3 id="应收账款周转率的分析">应收账款周转率的分析</h3><ul><li><strong>高周转率</strong>：通常表明公司在收款方面效率较高，信用管理政策严格，客户支付及时。可能意味着公司拥有更好的现金流和流动性。<br /></li><li><strong>低周转率</strong>：可能表明公司在收款方面存在问题，例如账款逾期较多，客户付款延迟或信用政策过于宽松。这可能会导致公司的现金流紧张，增加坏账风险。</li></ul><h3 id="应用与注意事项">应用与注意事项</h3><ul><li><strong>行业比较</strong>：应收账款周转率通常需要在行业内进行比较，不同行业的信用政策和销售模式差异较大，导致其应收账款周转率有较大不同。<br /></li><li><strong>季节性因素</strong>：在分析应收账款周转率时，需要考虑公司的季节性销售模式，可能会影响应收账款的变化。<br /></li><li><strong>信用政策</strong>：公司可能通过调整信用政策来影响应收账款周转率，例如通过延长或缩短客户的付款期。</li></ul><p>应收账款周转率是公司财务分析中的重要指标之一，它能够帮助公司了解其在信用销售和应收账款管理方面的效率，同时也是评估公司流动性和财务健康状况的重要工具。</p><h2 id="流动资产和非流动资产">流动资产和非流动资产</h2><p>在财务会计中，资产被分为<strong>流动资产</strong>（Current Assets）和<strong>非流动资产</strong>（Non-Current Assets），以帮助更好地了解公司的财务状况和资产的流动性。了解流动资产和非流动资产的区别有助于分析公司在短期和长期的财务能力和稳定性。</p><h3 id="流动资产">流动资产</h3><p><strong>流动资产</strong>是指在一个财务周期（通常为一年）内可以转化为现金或被消耗的资产。这类资产具有较高的流动性，可以在短期内用于支付公司的日常开支或清偿短期债务。</p><h4 id="主要的流动资产类型">主要的流动资产类型：</h4><ol type="1"><li><strong>现金和现金等价物</strong>：<ul><li>包括公司的现金存款、银行存款、可立即变现的短期投资（如国库券、商业票据等）。<br /></li><li>这些资产具有最高的流动性，能够随时用于支付公司需要的开支。</li></ul></li><li><strong>应收账款</strong>：<ul><li>指公司因销售商品或服务而预期在短期内收回的款项。<br /></li><li>它代表了公司未来的现金流入，但也可能存在坏账风险。</li></ul></li><li><strong>存货</strong>：<ul><li>包括原材料、在产品和产成品。<br /></li><li>存货是为了销售或生产过程中消耗的资产。存货的周转速度对公司的流动性有直接影响。</li></ul></li><li><strong>预付费用</strong>：<ul><li>指公司已经支付但尚未享受的服务或获得的产品，如预付租金、保险费等。<br /></li><li>这些费用会在未来的会计期间逐步摊销。</li></ul></li><li><strong>短期投资</strong>：<ul><li>期限在一年以内的金融资产投资，如短期债券、股票等，目的是赚取短期收益。</li></ul></li><li><strong>其他流动资产</strong>：<ul><li>包括在一年内可以变现或被消耗的其他资产，如待摊费用、短期借款的利息等。</li></ul></li></ol><h3 id="非流动资产">非流动资产</h3><p><strong>非流动资产</strong>（又称长期资产）是指预计在超过一个财务周期内不会变现或被消耗的资产。这类资产通常用于长期的投资和生产经营，具有较低的流动性。</p><h4 id="主要的非流动资产类型">主要的非流动资产类型：</h4><ol type="1"><li><strong>固定资产</strong>：<ul><li>包括公司用于生产、经营的长期资产，如土地、建筑物、机器设备、车辆等。<br /></li><li>固定资产通常通过折旧来分摊其使用寿命中的成本。</li></ul></li><li><strong>无形资产</strong>：<ul><li>包括商誉、专利、商标、著作权、软件等无形的非货币性资产。<br /></li><li>无形资产的价值通常通过摊销来反映在财务报表中。</li></ul></li><li><strong>长期投资</strong>：<ul><li>指公司持有的期限超过一年的金融投资，如长期债券、股票投资、关联公司的股权投资等。<br /></li><li>这些投资通常用于战略性持有而非短期变现。</li></ul></li><li><strong>递延所得税资产</strong>：<ul><li>指因暂时性差异和税务损失结转未来可抵扣的税款部分，预计在未来的会计期间可以用于减少所得税支出。</li></ul></li><li><strong>长期应收款</strong>：<ul><li>预计在一年以上才能收回的款项，例如分期付款销售的应收款项、长期的借款等。</li></ul></li><li><strong>其他非流动资产</strong>：<ul><li>包括其他预计在超过一年内实现的资产，如长期预付费用、长期待摊费用等。</li></ul></li></ol><h3 id="流动资产和非流动资产的区别">流动资产和非流动资产的区别</h3><table><thead><tr class="header"><th><strong>类别</strong></th><th><strong>流动资产</strong></th><th><strong>非流动资产</strong></th></tr></thead><tbody><tr class="odd"><td><strong>流动性</strong></td><td>高，通常在一年内变现或被消耗</td><td>低，通常持有超过一年，不易快速变现</td></tr><tr class="even"><td><strong>使用目的</strong></td><td>短期运营和支付日常开支</td><td>长期投资和支持长期生产经营</td></tr><tr class="odd"><td><strong>举例</strong></td><td>现金、应收账款、存货</td><td>机器设备、土地、专利、长期投资</td></tr><tr class="even"><td><strong>报告方式</strong></td><td>作为流动资产在资产负债表的当前资产部分列示</td><td>作为非流动资产在资产负债表的长期资产部分列示</td></tr><tr class="odd"><td><strong>风险和收益</strong></td><td>通常伴随着短期风险和短期收益波动</td><td>通常伴随着长期风险和长期收益潜力</td></tr></tbody></table><h3 id="财务分析中的应用">财务分析中的应用</h3><ul><li><strong>流动资产</strong>和<strong>非流动资产</strong>的比例和结构是分析公司流动性和长期财务健康的重要指标。一个平衡的资产结构可以帮助公司在短期内维持运营，同时支持长期增长。<br /></li><li><strong>流动比率</strong>（Current Ratio）和<strong>速动比率</strong>（Quick Ratio）是分析公司短期偿债能力的重要指标，它们主要关注流动资产的流动性。<br /></li><li>公司的资产结构变化往往反映出其战略调整和市场环境变化。例如，增加固定资产投资可能表明公司在扩张生产能力，而增加流动资产可能表明公司在改善流动性。</li></ul><p>理解流动资产和非流动资产有助于更全面地分析公司的财务状况和管理决策，为投资者和财务分析师提供了有效的分析框架。</p><h2 id="速动比率和流动比率">速动比率和流动比率</h2><p><strong>速动比率</strong>（Quick Ratio），又称<strong>酸性测试比率</strong>（Acid-Test Ratio），是衡量公司短期偿债能力的重要财务比率。与流动比率相比，速动比率更加严格地评估公司的流动性，因为它只考虑那些可以在短期内迅速变现的资产，不包括流动性较差的存货。</p><h3 id="速动比率的计算公式">速动比率的计算公式</h3><p>速动比率的计算公式为：</p><p><span class="math display">\[\text{速动比率} = \frac{\text{速动资产}}{\text{流动负债}}\]</span></p><p>其中：</p><ul><li><p><strong>速动资产</strong>：指可以在短期内迅速变现的流动资产，通常包括现金、现金等价物、应收账款和其他速动资产。不包括存货和预付费用等流动性较差的资产。</p><p>速动资产的计算方式为：</p><p><span class="math display">\[\text{速动资产} = \text{现金及现金等价物} + \text{应收账款} + \text{短期投资}\]</span></p></li><li><p><strong>流动负债</strong>：指需要在一个财务周期（通常为一年）内偿还的负债，包括短期借款、应付账款、应付票据、应付费用等。</p></li></ul><h3 id="示例-1">示例</h3><p>假设某公司的财务报表数据如下：</p><ul><li>现金及现金等价物：100,000元<br /></li><li>应收账款：150,000元<br /></li><li>存货：200,000元<br /></li><li>预付费用：50,000元<br /></li><li>短期借款：180,000元<br /></li><li>应付账款：120,000元</li></ul><p>根据这些数据，可以计算速动比率：</p><ol type="1"><li><strong>计算速动资产</strong>：</li></ol><p><span class="math display">\[\text{速动资产} = 100,000 + 150,000 = 250,000 \text{元}\]</span></p><ol start="2" type="1"><li><strong>计算流动负债</strong>：</li></ol><p><span class="math display">\[\text{流动负债} = 180,000 + 120,000 = 300,000 \text{元}\]</span></p><ol start="3" type="1"><li><strong>计算速动比率</strong>：</li></ol><p><span class="math display">\[\text{速动比率} = \frac{250,000}{300,000} \approx 0.83\]</span></p><p>这意味着该公司的速动比率为0.83，表明每1元的流动负债仅由0.83元的速动资产覆盖。</p><h3 id="速动比率的分析">速动比率的分析</h3><ul><li><strong>速动比率大于1</strong>：通常被认为是良好的财务健康标志，表明公司有足够的速动资产来偿还短期负债，有较好的短期偿债能力。<br /></li><li><strong>速动比率等于1</strong>：表示公司的速动资产刚好能够覆盖其流动负债，虽然可以偿还短期负债，但没有多余的速动资产。<br /></li><li><strong>速动比率小于1</strong>：可能表示公司在短期内无法依靠速动资产来偿还流动负债，这可能是流动性风险的信号。公司可能需要变现存货或依赖其他方式来获得现金，以满足短期负债。</li></ul><h3 id="应用与注意事项-1">应用与注意事项</h3><ol type="1"><li><p><strong>行业特性</strong>：速动比率应与行业平均水平进行比较，因为不同行业的公司速动比率标准不同。例如，制造业公司通常需要持有大量的存货，其速动比率可能低于服务业公司。</p></li><li><p><strong>公司政策</strong>：公司的信用政策、应收账款管理策略等都会影响速动比率。严格的应收账款管理政策可能会提高速动比率，而较为宽松的政策可能会导致比率较低。</p></li><li><p><strong>资产质量</strong>：速动比率虽然排除了存货，但应收账款的可回收性同样重要。如果应收账款中有大量难以收回的账款，即使速动比率看似较高，公司仍可能面临流动性问题。</p></li><li><p><strong>季节性因素</strong>：某些行业可能存在季节性销售高峰和低谷，这会导致速动比率的波动。在分析速动比率时，应考虑这些季节性因素。</p></li><li><p><strong>动态观察</strong>：速动比率应动态观察，关注其变化趋势。例如，速动比率持续下降可能预示着公司的财务状况恶化，流动性变差，需要采取措施改善资产管理或优化财务结构。</p></li></ol><h3 id="与流动比率的对比">与流动比率的对比</h3><ul><li><strong>流动比率</strong>：包含所有流动资产，包括存货和预付费用等。因此，它提供了一个更广泛的视角来看待公司在短期内偿还债务的能力，但可能不如速动比率那么严格。<br /></li><li><strong>速动比率</strong>：不包含存货和预付费用等流动性较差的资产，因此更严格地衡量公司的流动性，适合用于关注公司能否快速变现资产来偿还短期负债的情况。</li></ul><p>速动比率是公司财务分析中的一个重要指标，它可以帮助投资者和管理者了解公司短期偿债能力和财务稳健性。在财务决策中，速动比率通常与其他流动性指标一起使用，以获得更全面的财务状况分析。</p><h2 id="营业利润和净利润">营业利润和净利润</h2><p><strong>净利润</strong>（Net Profit）和<strong>营业利润</strong>（Operating Profit）是财务报表中两个不同的利润指标，它们反映了公司不同层次的盈利能力。了解这两个指标的区别有助于更全面地分析公司的财务状况和经营成果。</p><h3 id="营业利润operating-profit">营业利润（Operating Profit）</h3><p><strong>营业利润</strong>，又称<strong>经营利润</strong>（Operating Income）或<strong>息税前利润</strong>（EBIT, Earnings Before Interest and Taxes），是公司在扣除主营业务相关的所有成本和费用后所获得的利润。它反映了公司核心业务的盈利能力，不包括非经营性收入和费用。</p><h4 id="营业利润的计算公式">营业利润的计算公式</h4><p><span class="math display">\[\text{营业利润} = \text{营业总收入} - \text{营业成本} - \text{营业税金及附加} - \text{销售费用} - \text{管理费用} - \text{研发费用}\]</span></p><p>其中：</p><ul><li><strong>营业总收入</strong>：指公司通过主营业务获得的收入，包括产品销售收入、服务收入等。<br /></li><li><strong>营业成本</strong>：与营业总收入直接相关的成本，如生产制造成本或服务提供成本。<br /></li><li><strong>营业税金及附加</strong>：公司在营业过程中需缴纳的税金，如增值税、营业税等。<br /></li><li><strong>销售费用</strong>：与销售活动相关的费用，如广告费用、销售人员工资等。<br /></li><li><strong>管理费用</strong>：公司管理活动相关的费用，如办公费用、行政人员工资等。<br /></li><li><strong>研发费用</strong>：用于研发活动的费用。</li></ul><h3 id="净利润net-profit">净利润（Net Profit）</h3><p><strong>净利润</strong>，也称为<strong>净收益</strong>（Net Income）或<strong>税后净利润</strong>，是公司在扣除所有费用、成本、利息支出、税款及其他非经常性损益后的最终利润。它反映了公司整体的盈利能力，是衡量公司财务表现的关键指标。</p><h4 id="净利润的计算公式">净利润的计算公式</h4><p><span class="math display">\[\text{净利润} = \text{营业利润} + \text{营业外收入} - \text{营业外支出} - \text{所得税费用}\]</span></p><p>其中：</p><ul><li><strong>营业外收入</strong>：指公司主营业务之外的收入，如利息收入、投资收益、固定资产处置收益等。<br /></li><li><strong>营业外支出</strong>：指公司主营业务之外的支出，如固定资产处置损失、罚款支出等。<br /></li><li><strong>所得税费用</strong>：公司当期应支付的所得税费用。</li></ul><h3 id="营业利润和净利润的区别">营业利润和净利润的区别</h3><table><thead><tr class="header"><th><strong>指标</strong></th><th><strong>营业利润（Operating Profit）</strong></th><th><strong>净利润（Net Profit）</strong></th></tr></thead><tbody><tr class="odd"><td><strong>定义</strong></td><td>公司从主营业务中获得的利润，扣除与核心业务相关的成本和费用。</td><td>公司从总收入中扣除所有费用后的最终利润，包括非经营性收入和费用。</td></tr><tr class="even"><td><strong>关注点</strong></td><td>关注公司核心业务的盈利能力和经营效率。</td><td>反映公司整体的财务表现和盈利能力。</td></tr><tr class="odd"><td><strong>包含内容</strong></td><td>包含营业收入、营业成本、营业费用，不包括非经营性项目。</td><td>包括所有收入和费用，涵盖经营性和非经营性项目。</td></tr><tr class="even"><td><strong>计算公式</strong></td><td>营业总收入 - 营业成本 - 营业费用 - 其他与主营业务相关的费用</td><td>营业利润 + 营业外收入 - 营业外支出 - 所得税费用</td></tr><tr class="odd"><td><strong>使用场景</strong></td><td>用于评估公司核心业务的盈利状况，不受非经营因素干扰。</td><td>用于评估公司整体的盈利状况，考虑了所有的收入和费用。</td></tr></tbody></table><h3 id="举例说明">举例说明</h3><p>假设某公司2023年的财务数据如下：</p><ul><li>营业总收入：10,000,000元<br /></li><li>营业成本：6,000,000元<br /></li><li>销售费用：1,000,000元<br /></li><li>管理费用：500,000元<br /></li><li>营业外收入：200,000元（如投资收益）<br /></li><li>营业外支出：100,000元（如固定资产处置损失）<br /></li><li>所得税费用：600,000元</li></ul><p><strong>营业利润</strong>的计算：</p><p><span class="math display">\[\text{营业利润} = 10,000,000 - 6,000,000 - 1,000,000 - 500,000 = 2,500,000 \text{元}\]</span></p><p><strong>净利润</strong>的计算：</p><p><span class="math display">\[\text{净利润} = 2,500,000 + 200,000 - 100,000 - 600,000 = 2,000,000 \text{元}\]</span></p><p>在此例中，营业利润为2,500,000元，而净利润为2,000,000元。</p><h3 id="分析和应用">分析和应用</h3><ol type="1"><li><strong>营业利润的分析</strong>：<ul><li>营业利润能够直接反映公司主营业务的盈利能力，是分析公司核心业务健康状况的重要指标。<br /></li><li>如果营业利润较高且稳定，说明公司的核心业务运作良好，盈利能力较强。<br /></li><li>如果营业利润下降，可能意味着公司面临成本上升、销售下滑或管理效率低下等问题。</li></ul></li><li><strong>净利润的分析</strong>：<ul><li>净利润能够全面反映公司整体的盈利状况，包括所有的收入和费用。<br /></li><li>净利润指标包括了非经营性收入和支出及税收的影响，因此更适合用于全面评估公司的整体财务表现。<br /></li><li>净利润的波动可能受到营业外收入或支出的影响，需要结合营业利润一同分析，才能全面了解公司的盈利能力。</li></ul></li><li><strong>综合使用</strong>：<ul><li>在财务分析中，营业利润和净利润应结合使用。营业利润用于评估公司核心业务的盈利能力，而净利润用于评估公司整体的盈利能力。<br /></li><li>投资者和分析师通常会比较这两个指标的变化，分析营业外项目和税收对公司整体盈利的影响。</li></ul></li></ol><p>通过理解营业利润和净利润的区别，投资者和财务分析师能够更准确地评估公司在核心业务和整体财务表现上的表现。这对于制定投资决策和了解公司经营战略的有效性都是非常重要的。</p><h2 id="非经常性损益">非经常性损益</h2><p><strong>非经常性损益</strong>（Non-recurring Gains and Losses）是指公司在正常经营活动之外发生的、具有偶发性或非持续性的损益。这些损益通常不反映公司的核心业务经营状况，因此在进行公司财务分析和评估时，通常需要将非经常性损益从财务报表中剥离出来，以更准确地了解公司的持续经营能力和盈利能力。</p><h3 id="非经常性损益的构成">非经常性损益的构成</h3><p>非经常性损益通常包括以下几类：</p><ol type="1"><li><strong>非经常性收益</strong>：<ul><li><strong>资产处置收益</strong>：如固定资产或长期投资的出售收益。<br /></li><li><strong>政府补助</strong>：如政府为特定项目或政策提供的补贴。<br /></li><li><strong>债务重组收益</strong>：如债务豁免或减免带来的收益。<br /></li><li><strong>投资收益</strong>：如短期或长期投资的非正常收益（如股票或债券投资的浮动盈利）。<br /></li><li><strong>偶发性收入</strong>：如一次性的合同补偿、保险赔偿等。</li></ul></li><li><strong>非经常性损失</strong>：<ul><li><strong>资产处置损失</strong>：如固定资产或长期投资的处置损失。<br /></li><li><strong>自然灾害损失</strong>：如因不可抗力的自然灾害造成的损失。<br /></li><li><strong>诉讼赔偿</strong>：如涉及法律诉讼的赔偿支出。<br /></li><li><strong>坏账损失</strong>：非常规的坏账核销，如重大债务人破产导致的大额坏账。<br /></li><li><strong>重组费用</strong>：公司重组过程中发生的费用，如裁员费、合并费用等。<br /></li><li><strong>偶发性支出</strong>：如一次性的大额罚款、处罚等。</li></ul></li></ol><h3 id="非经常性损益的计算">非经常性损益的计算</h3><p>非经常性损益通常在财务报表中作为单独的项目列示，其计算公式为：</p><p><span class="math display">\[\text{非经常性损益} = \text{非经常性收益} - \text{非经常性损失}\]</span></p><h3 id="示例-2">示例</h3><p>假设某公司在2023年的非经常性损益明细如下：</p><ul><li>资产处置收益：200,000元<br /></li><li>政府补助：150,000元<br /></li><li>投资收益：50,000元<br /></li><li>诉讼赔偿支出：100,000元<br /></li><li>重组费用：80,000元</li></ul><p>则该公司的非经常性损益计算为：</p><p><span class="math display">\[\text{非经常性损益} = (200,000 + 150,000 + 50,000) - (100,000 + 80,000) = 300,000 \text{元}\]</span></p><p>这意味着公司在2023年的非经常性损益为300,000元的净收益。</p><h3 id="非经常性损益的分析">非经常性损益的分析</h3><ol type="1"><li><strong>剥离非经常性损益的必要性</strong>：<ul><li>在评估公司核心业务的经营状况时，应剥离非经常性损益，因为它们不反映公司的持续经营能力。剥离后得到的利润可以更好地反映公司在正常经营条件下的盈利水平。</li></ul></li><li><strong>对比分析</strong>：<ul><li>如果一家公司非经常性收益较多，而主营业务利润较少甚至亏损，这可能表明公司的核心业务经营状况不佳，需要关注主营业务的改善和提升。<br /></li><li>相反，如果非经常性损失较多，且是由于不可控的外部因素造成，投资者可以适当考虑剔除这些因素，重新评估公司的经营能力。</li></ul></li><li><strong>未来影响的分析</strong>：<ul><li>需要分析非经常性损益对公司未来财务状况的潜在影响。例如，大额的政府补助可能无法持续；而一次性的大额罚款或赔偿也可能不会在未来重复发生。</li></ul></li><li><strong>风险评估</strong>：<ul><li>频繁出现大额非经常性损益的公司可能面临较大的不确定性，这种不确定性可能来自于不稳定的投资回报、频繁的资产重组或不可预见的诉讼风险等。</li></ul></li></ol><h3 id="应用与注意事项-2">应用与注意事项</h3><ol type="1"><li><strong>在利润表中的位置</strong>：<ul><li>非经常性损益通常列在利润表的“营业利润”之后、“利润总额”之前，作为一个单独的科目进行披露，以明确区分经营性损益与非经营性损益。</li></ul></li><li><strong>财务分析中的调整</strong>：<ul><li>投资者和分析师通常会在计算公司的核心盈利能力（如经常性净利润）时剔除非经常性损益，以避免因偶发事件而对公司业绩的判断产生偏差。</li></ul></li><li><strong>与其他财务指标的结合使用</strong>：<ul><li>非经常性损益分析应结合其他财务指标（如营业利润率、净利润率、每股收益等）一起使用，全面评估公司盈利质量和风险水平。</li></ul></li><li><strong>信息披露的透明度</strong>：<ul><li>公司应在财务报表的附注中充分披露非经常性损益的具体项目和金额，以便投资者和分析师对公司的财务状况进行全面了解和准确评估。</li></ul></li></ol><h3 id="总结">总结</h3><p>非经常性损益是公司在正常经营活动之外发生的偶发性或非持续性损益，虽然不反映公司的核心业务状况，但对整体财务表现有一定影响。在财务分析中，投资者和分析师应剔除非经常性损益，聚焦公司核心业务的持续盈利能力，从而更准确地评估公司的长期发展潜力和财务健康状况。</p><h2 id="资产负债表和会计科目">资产负债表和会计科目</h2><p>资产负债表的科目和会计科目虽然在很多方面是相关联的，但它们并不完全相同，二者的用途和结构有一定的差异。</p><h3 id="资产负债表科目">资产负债表科目</h3><p><strong>资产负债表</strong>是一种财务报表，用于反映企业在某一特定日期的财务状况，包括资产、负债和所有者权益三个部分。资产负债表科目是资产负债表上用于分类和列示企业各种资产、负债和所有者权益的项目，通常包括以下主要类别：</p><ol type="1"><li><strong>资产科目</strong>：<ul><li><strong>流动资产</strong>：如货币资金、应收账款、存货等。这些资产通常在一年内或一个正常营业周期内可以变现或耗用。<br /></li><li><strong>非流动资产</strong>：如固定资产、无形资产、长期股权投资等。这些资产的使用期限通常超过一年。</li></ul></li><li><strong>负债科目</strong>：<ul><li><strong>流动负债</strong>：如短期借款、应付账款、应付职工薪酬等。这些负债通常需要在一年内或一个正常营业周期内偿还。<br /></li><li><strong>非流动负债</strong>：如长期借款、长期应付款等，这些负债的偿还期限通常超过一年。</li></ul></li><li><strong>所有者权益科目</strong>：<ul><li>如实收资本（股本）、资本公积、盈余公积、未分配利润等。这些科目反映了企业的净资产或所有者对企业的剩余权益。</li></ul></li></ol><h3 id="会计科目">会计科目</h3><p><strong>会计科目</strong>是会计核算中用来分类、记录、归集经济业务和事项的科目。它是会计凭证和账簿记录的基础，在会计系统中用于归类和跟踪所有的财务交易。会计科目更加细化和详细，它们是企业会计制度中的一个重要部分，用于反映不同类型的资产、负债、收入、费用和所有者权益。</p><p>会计科目通常包括以下几个类别：</p><ol type="1"><li><strong>资产类</strong>：如现金、银行存款、应收账款、预付账款、固定资产、累计折旧等。<br /></li><li><strong>负债类</strong>：如短期借款、应付账款、应付职工薪酬、长期借款等。<br /></li><li><strong>权益类</strong>：如实收资本、资本公积、盈余公积、未分配利润等。<br /></li><li><strong>收入类</strong>：如主营业务收入、其他业务收入、投资收益等。<br /></li><li><strong>费用类</strong>：如主营业务成本、销售费用、管理费用、财务费用等。</li></ol><h3 id="差异与关系">差异与关系</h3><ul><li><strong>用途不同</strong>：<ul><li><strong>资产负债表科目</strong>：用于财务报表，以提供给外部利益相关者（如投资者、债权人、监管机构）的一种简化和概括的方式，反映企业的财务状况。<br /></li><li><strong>会计科目</strong>：用于内部会计记录和核算，包含更加详细的交易数据，帮助企业进行日常的财务管理、决策和报表编制。</li></ul></li><li><strong>详细程度不同</strong>：<ul><li><strong>会计科目</strong>通常更为详细，细化到具体的资产或负债项目（例如：现金、应收账款、短期借款、长期借款等）。<br /></li><li><strong>资产负债表科目</strong>通常是会计科目的合并或汇总。例如，“流动资产”在资产负债表上是一个总类，而会计科目中可能包含了“现金”、“应收账款”、“预付账款”等具体科目。</li></ul></li><li><strong>内容分类不同</strong>：<ul><li><strong>会计科目</strong>还包括收入和费用类科目，而资产负债表科目只包括资产、负债和所有者权益类科目。</li></ul></li></ul><h3 id="示例-3">示例</h3><p>假设公司在某一特定日期的财务状况如下：</p><p><strong>会计科目记录</strong>：</p><ul><li>现金：50,000 元<br /></li><li>应收账款：30,000 元<br /></li><li>存货：20,000 元<br /></li><li>短期借款：40,000 元<br /></li><li>应付账款：10,000 元<br /></li><li>实收资本：50,000 元<br /></li><li>未分配利润：20,000 元</li></ul><p><strong>资产负债表科目</strong>（根据会计科目汇总）：</p><ul><li><strong>资产</strong>：<ul><li>流动资产：100,000 元（包括现金、应收账款、存货的总和）<br /></li></ul></li><li><strong>负债</strong>：<ul><li>流动负债：50,000 元（包括短期借款和应付账款的总和）<br /></li></ul></li><li><strong>所有者权益</strong>：<ul><li>实收资本：50,000 元<br /></li><li>未分配利润：20,000 元</li></ul></li></ul><h3 id="总结-1">总结</h3><p>虽然资产负债表科目和会计科目在许多方面相互关联，但它们在结构、用途和详细程度上存在差异。会计科目是会计核算的基础，涵盖了更详细的分类，而资产负债表科目则是根据会计科目汇总、整理后，用于编制财务报表的项目分类。</p><p>在会计核算中，<strong>收入</strong>和<strong>费用</strong>科目属于损益类科目，它们主要记录企业在一定期间内的经营成果，直接影响当期的<strong>利润</strong>。虽然收入和费用科目本身不直接出现在<strong>资产负债表</strong>中，但它们的累计结果通过<strong>利润</strong>的变化间接影响<strong>资产负债表</strong>中的科目，尤其是<strong>所有者权益</strong>部分。</p><h2 id="收入和费用">收入和费用</h2><h3 id="收入和费用的定义">收入和费用的定义</h3><ul><li><p><strong>收入</strong>：企业在日常经营活动中，因销售商品、提供服务等取得的经济利益的总流入，会计科目包括<strong>主营业务收入</strong>、<strong>其他业务收入</strong>、<strong>投资收益</strong>等。</p></li><li><p><strong>费用</strong>：企业在日常经营活动中，因销售商品、提供服务等发生的经济利益的总流出，会计科目包括<strong>主营业务成本</strong>、<strong>销售费用</strong>、<strong>管理费用</strong>、<strong>财务费用</strong>等。</p></li></ul><h3 id="收入和费用对资产负债表的影响">收入和费用对资产负债表的影响</h3><p>收入和费用影响企业的<strong>净利润</strong>，而净利润最终会计入<strong>所有者权益</strong>部分的<strong>未分配利润</strong>科目。因此，收入和费用科目与资产负债表中的所有者权益部分存在间接的联系。</p><h4 id="损益类科目结转到所有者权益的过程">损益类科目结转到所有者权益的过程</h4><ol type="1"><li><p><strong>期间收入和费用结转</strong>：在会计期末，所有收入和费用科目都会通过<strong>本年利润</strong>科目结转，来计算本期的净利润（或净亏损）。</p></li><li><p><strong>净利润结转</strong>：本年利润的余额（即净利润或净亏损）结转到<strong>未分配利润</strong>科目，反映在所有者权益部分。</p></li></ol><h4 id="会计处理示例">会计处理示例</h4><p><strong>假设：</strong>某公司本期主营业务收入为 100,000 元，主营业务成本为 60,000 元，管理费用为 20,000 元。</p><ul><li><p><strong>期末损益结转分录：</strong></p><ol type="1"><li><strong>结转收入：</strong></li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">借：主营业务收入  100,000 元</span><br><span class="line">    贷：本年利润          100,000 元</span><br></pre></td></tr></table></figure><ol start="2" type="1"><li><strong>结转成本和费用：</strong></li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">借：本年利润               80,000 元</span><br><span class="line">    贷：主营业务成本  60,000 元</span><br><span class="line">    贷：管理费用           20,000 元</span><br></pre></td></tr></table></figure></li><li><p><strong>净利润结转到所有者权益：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">借：本年利润               20,000 元</span><br><span class="line">    贷：未分配利润        20,000 元</span><br></pre></td></tr></table></figure></li></ul><p>在资产负债表中，这些过程的最终影响是：</p><ul><li><strong>未分配利润</strong>（所有者权益科目）增加 20,000 元，这反映了本期净利润的累计结果。</li></ul><h3 id="资产负债表中的所有者权益部分">资产负债表中的所有者权益部分</h3><p>资产负债表中的所有者权益部分包括以下主要项目：</p><ul><li><strong>实收资本（或股本）</strong>：股东实际投入公司的资本。</li><li><strong>资本公积</strong>：股东投入资本的溢价部分或其他资本性收入。</li><li><strong>盈余公积</strong>：从企业净利润中提取的法定盈余公积和任意盈余公积。</li><li><strong>未分配利润</strong>：累计未分配的净利润，收入和费用最终的归宿科目。</li></ul><h3 id="结论">结论</h3><p>收入和费用的会计科目本身不会直接出现在资产负债表中，但它们的累计结果（通过净利润的形成）会影响资产负债表中所有者权益部分的<strong>未分配利润</strong>。在企业财务报表体系中，损益表与资产负债表通过所有者权益部分紧密联系，共同反映企业的财务状况和经营成果。</p><p>Q：如果发了一笔员工工资100万，银行存款减少了100万，在资产负债表中如何变化？</p><p>资产=负债+权益 A=L+E</p><p>第一步：集体工资 E-100（借未分配利润） L+100 （贷应付工资）</p><p>第二步：发工资： A-100（借银行存款） L-100 （贷应付工资）</p><p>当公司发放员工工资 100 万元时，银行存款减少 100 万元，同时企业的利润也会减少，因为工资属于费用类科目。发放工资会对资产负债表中的<strong>资产</strong>和<strong>所有者权益</strong>部分产生影响。</p><h2 id="会计分录">会计分录</h2><h3 id="会计分录-1">会计分录</h3><p>在发放工资时，典型的会计分录如下：</p><ol type="1"><li><strong>发放工资的会计分录</strong>：</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">借：应付职工薪酬         1,000,000 元</span><br><span class="line">    贷：银行存款               1,000,000 元</span><br></pre></td></tr></table></figure><ol start="2" type="1"><li><strong>计提工资的会计分录</strong>（在发放工资之前，需要先计提工资费用）：</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">借：管理费用（或销售费用、生产成本等）  1,000,000 元</span><br><span class="line">    贷：应付职工薪酬                     1,000,000 元</span><br></pre></td></tr></table></figure><h3 id="对资产负债表的影响">对资产负债表的影响</h3><p>发放工资后，资产负债表中的变化如下：</p><h4 id="资产部分的变化"><strong>资产</strong>部分的变化</h4><ul><li><strong>银行存款减少 100 万元</strong>：这是公司的流动资产。发放工资使得公司的银行存款减少 100 万元。</li></ul><h4 id="负债部分的变化"><strong>负债</strong>部分的变化</h4><ul><li><strong>应付职工薪酬减少 100 万元</strong>：发放工资时，应付职工薪酬减少。这一变化不会直接影响资产负债表的总金额，因为应付职工薪酬是发放前的负债减少对应银行存款的减少。</li></ul><h4 id="所有者权益部分的变化"><strong>所有者权益</strong>部分的变化</h4><ul><li><strong>未分配利润减少 100 万元</strong>：支付工资属于费用支出，会影响企业的净利润。当费用增加，净利润减少，未分配利润相应减少。因此，未分配利润减少 100 万元。</li></ul><h3 id="资产负债表的变化总结">资产负债表的变化总结</h3><p>发放工资 100 万元的会计事项在资产负债表中主要表现为以下几部分：</p><ol type="1"><li><strong>资产</strong>：<ul><li><strong>银行存款减少</strong>：资产负债表的流动资产部分中，银行存款减少 100 万元。</li></ul></li><li><strong>负债</strong>：<ul><li><strong>应付职工薪酬减少</strong>：流动负债部分，应付职工薪酬减少 100 万元。</li></ul></li><li><strong>所有者权益</strong>：<ul><li><strong>未分配利润减少</strong>：所有者权益部分中的未分配利润减少 100 万元。</li></ul></li></ol><h3 id="总体影响">总体影响</h3><p>支付工资 100 万元会导致<strong>资产</strong>（银行存款）减少和<strong>所有者权益</strong>（未分配利润）减少，而<strong>负债</strong>（应付职工薪酬）减少后对整体资产负债表无影响。</p><p>最终，资产和所有者权益的减少保持了资产负债表的平衡。具体变化如下：</p><ul><li><strong>资产减少 100 万元</strong>（银行存款减少）。</li><li><strong>负债减少 100 万元</strong>（应付职工薪酬减少）。</li><li><strong>所有者权益减少 100 万元</strong>（未分配利润减少）。</li></ul><p>总结一下，发放员工工资会导致企业流动资产的减少，流动负债的减少，以及所有者权益（未分配利润）的减少，使得资产负债表依然保持平衡。</p><h2 id="会计分录有负数吗">会计分录有负数吗？</h2><p>在会计凭证的分录中，通常不会直接出现负数。会计分录主要采用借贷记账法，通过<strong>借方</strong>和<strong>贷方</strong>来表示不同类型的会计科目的增加或减少，所有的金额都以正数形式记录。</p><h3 id="借贷记账法的基本规则">借贷记账法的基本规则</h3><ol type="1"><li><strong>借方（Debit）</strong>：<ul><li>资产增加记借方（例如：现金、银行存款、固定资产等）。</li><li>费用增加记借方（例如：管理费用、销售费用等）。</li><li>负债减少记借方（例如：应付账款的减少）。</li><li>所有者权益减少记借方（例如：股东分红）。</li><li>收入减少记借方（例如：冲销收入）。</li></ul></li><li><strong>贷方（Credit）</strong>：<ul><li>资产减少记贷方（例如：现金、银行存款的减少）。</li><li>费用减少记贷方（例如：费用的冲回）。</li><li>负债增加记贷方（例如：借入款项、应付账款的增加）。</li><li>所有者权益增加记贷方（例如：实收资本的增加）。</li><li>收入增加记贷方（例如：主营业务收入、其他收入）。</li></ul></li></ol><h3 id="负数的处理方式">负数的处理方式</h3><p>在会计分录中，不会直接记录负数。如果需要表示一个账户的减少，通常是通过将减少金额记入与增加相反的方向来实现。例如：</p><ul><li><strong>资产的减少</strong>：记入贷方。</li><li><strong>负债的减少</strong>：记入借方。</li><li><strong>收入的减少</strong>：记入借方。</li><li><strong>费用的减少</strong>：记入贷方。</li></ul><h3 id="特殊情况">特殊情况</h3><p>虽然在会计凭证中通常不使用负数，但在某些特殊情况下，会计软件或财务报表中可能会以负数形式展示：</p><ol type="1"><li><p><strong>冲销和调整</strong>：为了反映前期的错误或调整某项交易，负数可能用于冲销先前记录的交易。例如，发现多计了一笔应收账款，可能需要通过一笔相同金额的负数进行冲销。</p></li><li><p><strong>净额展示</strong>：在财务报表中，有时为了清晰展示净额变化，负数可能用于反映费用或支出的减少。例如，展示本期收入减少时，可能在报表中显示为负数。</p></li></ol><h3 id="示例-4">示例</h3><p>假设公司在上一期记录了一笔应收账款 10,000 元，但后来发现实际金额应为 8,000 元。因此，需要冲销多记的 2,000 元：</p><p><strong>原始会计分录</strong>：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">借：应收账款            10,000 元</span><br><span class="line">    贷：主营业务收入        10,000 元</span><br></pre></td></tr></table></figure><p><strong>调整会计分录（冲销多记的 2,000 元）</strong>：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">借：主营业务收入         2,000 元</span><br><span class="line">    贷：应收账款               2,000 元</span><br></pre></td></tr></table></figure><p>在这个调整分录中，收入减少和应收账款的减少都没有直接用负数，而是通过借贷方向的变化来反映调整。</p><h3 id="结论-1">结论</h3><p>总之，会计凭证分录中一般不会使用负数。相反，会计通过借贷记账法的借方和贷方表示账户的增加和减少，确保账目清晰、准确。负数的使用通常仅出现在财务报表的展示层面，用于更加直观地反映数据的增减变化。</p><h2 id="资产负债表中equity下一般有哪些科目">资产负债表中Equity下一般有哪些科目？</h2><p>在资产负债表中，<strong>Equity（所有者权益）</strong>部分反映了企业资产减去负债后的净值，即股东在企业中的权益。所有者权益是资产负债表的三大组成部分之一，通常位于资产负债表的右侧（负债和所有者权益部分）。在所有者权益部分，一般包括以下主要科目：</p><h3 id="实收资本股本"><strong>实收资本（股本）</strong></h3><ul><li><strong>实收资本（Paid-in Capital / Contributed Capital / Share Capital）：</strong><br />也称为<strong>股本（Share Capital）</strong>，是企业在成立时或增资时股东实际投入的资本。这部分反映了企业最初和后续融资时股东投入的资金。根据公司类型不同，可能细分为普通股股本和优先股股本。</li></ul><h3 id="资本公积capital-surplus-additional-paid-in-capital"><strong>资本公积（Capital Surplus / Additional Paid-In Capital）</strong></h3><ul><li><strong>资本公积（Capital Surplus）：</strong><br />是指股东出资超过其票面价值（面值）的部分，也包括在股份公司发行股份时股票溢价部分。这部分是股东出资中超过股票面值部分的累积，或者是其他资本性收入（如捐赠、财产重估等）的累积。</li></ul><h3 id="盈余公积retained-earnings-reserve-reserve-fund"><strong>盈余公积（Retained Earnings Reserve / Reserve Fund）</strong></h3><ul><li><strong>盈余公积（Reserves）：</strong><br />是企业从净利润中提取并积累的资金，通常根据公司章程或法律要求提取，用于防范风险或未来的扩展投资。常见的盈余公积包括<strong>法定盈余公积金</strong>（法定公积金）和<strong>任意盈余公积金</strong>（任意公积金）。</li></ul><h3 id="未分配利润retained-earnings"><strong>未分配利润（Retained Earnings）</strong></h3><ul><li><strong>未分配利润（Retained Earnings）：</strong><br />是企业在扣除各项费用、税金、支付股东红利后的累积净利润部分。这些利润未被分配给股东，通常用于再投资或增加企业资本。它反映了企业自成立以来的累计盈利状况。</li></ul><h3 id="其他综合收益other-comprehensive-income"><strong>其他综合收益（Other Comprehensive Income）</strong></h3><ul><li><strong>其他综合收益（OCI）：</strong><br />是直接计入所有者权益但不计入当期损益的收入和费用的净额。它包括了未实现的利得或损失，如可供出售金融资产的公允价值变动、外币财务报表折算差额、现金流量套期工具的利得和损失等。OCI通常反映在权益变动表中，并计入所有者权益的总额。</li></ul><h3 id="库存股treasury-stock"><strong>库存股（Treasury Stock）</strong></h3><ul><li><strong>库存股（Treasury Stock）：</strong><br />是企业回购的自家股票，这些股票在会计上作为所有者权益的一个抵减项目。回购的股份减少了企业的已发行股份总数，也减少了所有者权益的总额。</li></ul><h3 id="外币报表折算差额foreign-currency-translation-adjustments"><strong>外币报表折算差额（Foreign Currency Translation Adjustments）</strong></h3><ul><li><strong>外币报表折算差额（Translation Adjustments）：</strong><br />当企业的子公司或分支机构位于其他国家，其财务报表以外币报告时，需要将这些报表折算为母公司的本币进行合并。在折算过程中可能会出现汇率差异，这些差异累积形成外币报表折算差额。</li></ul><h3 id="归属于母公司股东权益equity-attributable-to-owners-of-the-parent"><strong>归属于母公司股东权益（Equity Attributable to Owners of the Parent）</strong></h3><ul><li><strong>归属于母公司股东权益：</strong><br />在合并财务报表中，企业还需要展示归属于母公司股东的权益，这部分包含母公司自身的权益和对子公司投资形成的权益。</li></ul><h3 id="非控制性权益minority-interest-non-controlling-interest"><strong>非控制性权益（Minority Interest / Non-Controlling Interest）</strong></h3><ul><li><strong>非控制性权益（Minority Interest）：</strong><br />也称少数股东权益，是指在子公司中，非母公司股东所拥有的权益。它反映了在合并报表中，对子公司有控制权的母公司股东之外的其他股东的权益。</li></ul><h3 id="总结-2">总结</h3><p>资产负债表中的所有者权益部分通常包括上述科目，这些科目详细展示了企业自成立以来股东投入的资金、盈利的累积、资本的增减及股东权益的变化情况。这部分为投资者提供了公司资本结构和财务健康状况的相关信息。具体科目的设定可能会根据企业所在国家的会计准则（如GAAP、IFRS）和企业的实际情况有所不同。</p><h2 id="预售房款财务报表的处理">预售房款财务报表的处理</h2><p>中国大陆房地产企业在香港联交所上市时，对于预收了房款但尚未交楼的经济活动，会根据国际财务报告准则（IFRS）或香港财务报告准则（HKFRS）进行财务报表的确认和列示。这些准则通常要求企业在确认收入之前需要满足一定的条件，即主要是商品或服务的控制权已经转移给客户。</p><h3 id="预收房款的确认"><strong>预收房款的确认</strong></h3><p>对于房地产企业来说，预收的房款代表客户提前支付的一部分或全部购房款项，但在房地产企业完成房产交付之前，这部分金额不能作为收入确认。根据国际财务报告准则第15号《客户合同收入》（IFRS 15 / HKFRS 15），企业在将商品或服务的控制权转移给客户之前，不应确认收入。因此，预收的房款会被列为<strong>负债</strong>，而不是收入。</p><h4 id="会计处理">会计处理：</h4><ul><li><strong>借：现金或银行存款（Cash/Bank Deposit）</strong></li><li><strong>贷：合同负债（Contract Liabilities）</strong></li></ul><p>这笔预收款项的会计处理会增加企业的资产（现金或银行存款），同时在负债栏下增加一个名为“合同负债”（或者称为预收账款）的项目。</p><h3 id="合同负债contract-liabilities"><strong>合同负债（Contract Liabilities）</strong></h3><p>“合同负债”是预收房款的记账方式，它反映了企业因未履行的合约义务而需交付的商品或服务。在房地产行业，这意味着公司已经收到购房者的预付款，但尚未完成商品（房屋）的交付。</p><h4 id="资产负债表中的体现">资产负债表中的体现：</h4><ul><li><strong>资产部分：</strong><ul><li>现金或银行存款增加。</li></ul></li><li><strong>负债部分：</strong><ul><li>合同负债（合同预收款）增加。</li></ul></li></ul><h3 id="收入的确认时点"><strong>收入的确认时点</strong></h3><p>房地产企业在确认收入时，必须等到房产的控制权（即风险和报酬）已实质性地转移给购房者，且企业不再继续控制该房产。通常，控制权的转移发生在房产的实物交付或交楼时，此时企业才会将之前记入“合同负债”的金额转为收入。</p><h4 id="会计处理交楼时">会计处理（交楼时）：</h4><ul><li><strong>借：合同负债（Contract Liabilities）</strong></li><li><strong>贷：收入（Revenue）</strong></li></ul><p>一旦确认收入，企业的负债就减少了，而相应的收入会在利润表中列示。</p><h3 id="预收房款的财务报表反映总结"><strong>预收房款的财务报表反映总结</strong></h3><ul><li><strong>资产负债表：</strong><ul><li>预收房款作为“合同负债”列在负债项下，直到房屋交付才会结转为收入。</li></ul></li><li><strong>利润表：</strong><ul><li>在房屋交付时，合同负债的金额会转入收入，并相应反映在当期的利润表中。</li></ul></li></ul><h3 id="特定会计政策和披露要求"><strong>特定会计政策和披露要求</strong></h3><p>房地产企业还需要披露预收房款的具体信息，包括合同负债的总金额、预计履约时间表、合同的主要条款和条件，以及收入确认的会计政策。这些信息有助于投资者和监管机构更好地理解企业的财务状况和未来的收入预期。</p><h3 id="总结-3">总结</h3><p>中国大陆房地产企业在香港联交所上市时，对于预收房款但未交楼的经济活动，需根据国际财务报告准则或香港财务报告准则确认收入和列示负债。在房屋交付之前，预收房款作为合同负债在资产负债表中列示；在房屋交付后，该金额才会从合同负债转为收入。</p><h2 id="递延所得税">递延所得税</h2><p><strong>递延所得税</strong>（Deferred Tax）是会计中用于处理企业会计利润与税务应纳税所得额之间由于时间差异而产生的税收影响的一种方法。递延所得税在财务报表中以递延所得税资产和递延所得税负债的形式出现，反映了未来可能会发生的税务影响。</p><h3 id="递延所得税的产生原因">递延所得税的产生原因</h3><p>递延所得税主要源于<strong>会计利润</strong>与<strong>应纳税所得额</strong>之间的<strong>暂时性差异</strong>。这些差异通常是由于会计准则与税法规定不同的确认收入、费用、资产或负债的时间而产生的。递延所得税可分为两种情况：</p><ol type="1"><li><strong>暂时性差异（Temporary Differences）：</strong><ul><li><strong>应纳税暂时性差异</strong>：这些差异会导致未来应纳税所得额的增加，例如资产的账面价值大于其计税基础，或负债的账面价值小于其计税基础。这些差异会导致递延所得税负债。</li><li><strong>可抵扣暂时性差异</strong>：这些差异会导致未来应纳税所得额的减少，例如资产的账面价值小于其计税基础，或负债的账面价值大于其计税基础。这些差异会导致递延所得税资产。</li></ul></li><li><strong>永久性差异（Permanent Differences）：</strong><ul><li>永久性差异是指根据会计准则确认的收入或费用，在税务上从未计入应纳税所得额（例如某些税收优惠和罚款等）。永久性差异不产生递延所得税。</li></ul></li></ol><h3 id="递延所得税资产与负债">递延所得税资产与负债</h3><ul><li><p><strong>递延所得税资产（Deferred Tax Assets）</strong>：指由于暂时性差异或税务亏损结转等原因，预计未来可以抵减应纳税所得额，从而减少未来税负的金额。常见的递延所得税资产产生原因包括：坏账准备、资产减值损失、费用预提、税务亏损结转等。</p></li><li><p><strong>递延所得税负债（Deferred Tax Liabilities）</strong>：指由于暂时性差异而导致未来需支付的所得税。常见的递延所得税负债产生原因包括：加速折旧方法、开发成本的资本化、收入确认时间与税法不一致等。</p></li></ul><h3 id="递延所得税的确认和计量">递延所得税的确认和计量</h3><p>根据<strong>国际财务报告准则（IFRS）</strong>和<strong>中国企业会计准则</strong>，递延所得税的确认和计量通常遵循“资产负债表债务法”（Balance Sheet Liability Method），具体要求如下：</p><ol type="1"><li><strong>确认</strong>：<ul><li>企业应当对于所有的应纳税暂时性差异确认递延所得税负债。</li><li>对于所有的可抵扣暂时性差异，企业应当确认递延所得税资产，但须考虑其可实现性，即只有在有充分的应纳税所得额可以利用这些差异时，才确认递延所得税资产。</li></ul></li><li><strong>计量</strong>：<ul><li>递延所得税资产和负债的计量应根据预计在暂时性差异转回期间适用的税率进行，且应考虑税率变动的影响。</li><li>计量时应根据税法对资产或负债的计税基础进行调整，从而确认相应的递延所得税资产或负债。</li></ul></li></ol><h3 id="递延所得税的财务报表列示">递延所得税的财务报表列示</h3><ul><li><p><strong>资产负债表</strong>：递延所得税资产和负债应分别列示，通常列在资产或负债部分的非流动项下。</p></li><li><p><strong>利润表</strong>：当暂时性差异转回时，递延所得税的变化会影响当期所得税费用，这部分费用或收益反映在利润表中的所得税费用科目下。</p></li></ul><h3 id="递延所得税的意义">递延所得税的意义</h3><ol type="1"><li><p><strong>平滑企业利润</strong>：递延所得税帮助平滑企业因会计处理与税法不同而产生的利润波动，使得财务报表更能反映企业的真实经营成果和财务状况。</p></li><li><p><strong>反映未来税务影响</strong>：通过递延所得税，财务报表能够预示未来可能发生的税务负担或税务优惠，提供更全面的财务信息。</p></li><li><p><strong>支持决策分析</strong>：递延所得税信息有助于投资者、管理层和分析师评估企业未来的税务风险和潜在的税收节约，支持更好的决策和评估。</p></li></ol><h3 id="总结-4">总结</h3><p>递延所得税是企业在编制财务报表时用来处理会计准则和税法差异的一种重要方法。它通过确认和计量未来税务影响，帮助企业更准确地反映财务状况和经营成果，为决策提供了重要依据。</p><h2 id="自由现金流">自由现金流</h2><p><strong>自由现金流（Free Cash Flow，FCF）</strong>是衡量公司运营产生的可用于分配给债权人和股东的现金量的一种重要财务指标。自由现金流反映了企业在支付完运营费用和资本支出之后，剩余的现金流情况。它能够帮助投资者评估公司产生现金的能力，以及用于投资、偿还债务或分红的潜力。</p><p>自由现金流的计算公式有两种常见的形式：<strong>企业自由现金流（FCF to Firm，FCFF）</strong>和<strong>股权自由现金流（FCF to Equity，FCFE）</strong>。</p><h3 id="企业自由现金流fcff的计算"><strong>企业自由现金流（FCFF）的计算</strong></h3><p>企业自由现金流（FCFF）是企业从经营活动中产生的，可供债权人和股东使用的现金流。计算公式如下：</p><p><span class="math display">\[ \text{FCFF} = \text{EBIT} \times (1 - \text{税率}) + \text{折旧与摊销} - \text{资本支出} - \Delta \text{营运资金} \]</span></p><p>其中：</p><ul><li><strong>EBIT</strong>：息税前利润（Earnings Before Interest and Taxes）。</li><li><strong>税率</strong>：公司适用的税率。</li><li><strong>折旧与摊销</strong>：反映资产使用的非现金费用，需要加回。</li><li><strong>资本支出（CapEx）</strong>：企业用于购置、维护固定资产的支出。</li><li><strong>Δ营运资金（Working Capital Changes）</strong>：营运资本的变动（营运资本 = 流动资产 - 流动负债）。</li></ul><h3 id="股权自由现金流fcfe的计算"><strong>股权自由现金流（FCFE）的计算</strong></h3><p>股权自由现金流（FCFE）是公司在支付完所有营运成本、资本支出和债务利息等费用后，剩余的可供股东使用的现金流。计算公式如下：</p><p><span class="math display">\[ \text{FCFE} = \text{FCFF} - \text{利息支出} \times (1 - \text{税率}) + \Delta \text{净负债} \]</span></p><p>其中：</p><ul><li><strong>利息支出</strong>：企业支付的债务利息。</li><li><strong>Δ净负债</strong>：净债务的变动（新借入的债务减去偿还的债务）。</li></ul><h3 id="简化计算方法"><strong>简化计算方法</strong></h3><p>有时候可以通过简化公式来计算自由现金流。使用企业的净利润和非现金费用可以快速得出一个大致的自由现金流量：</p><p><span class="math display">\[ \text{FCF} = \text{净利润} + \text{折旧与摊销} - \text{资本支出} - \Delta \text{营运资金} \]</span></p><h3 id="自由现金流的解读"><strong>自由现金流的解读</strong></h3><ul><li><strong>正的自由现金流</strong>：企业在支付完资本支出后，还有剩余的现金流，表明企业具备很强的内部资金生成能力，可以用于投资、偿还债务或分红。</li><li><strong>负的自由现金流</strong>：企业可能在快速扩张，资本支出较大，但如果长期为负，可能会导致企业面临现金流压力，需依赖外部融资。</li></ul><h3 id="自由现金流的重要性"><strong>自由现金流的重要性</strong></h3><ul><li><strong>投资评估</strong>：自由现金流被广泛用于投资分析，因为它能更好地反映公司实际创造的可支配现金。</li><li><strong>估值模型</strong>：在DCF（现金流贴现）估值法中，自由现金流是关键的输入变量，用于预测公司未来的现金流，并折现回当前时间进行估值。</li></ul><p>自由现金流作为财务分析中一个重要的指标，可以帮助分析公司的运营效率、财务健康状况，以及对未来增长和分红的潜力。</p><h2 id="土地增值税">土地增值税</h2><p>在房地产项目开发过程中，如果项目尚未结算，但公司已经销售了一部分房产，<strong>土地增值税</strong>的处理会根据具体的税法规定和财务处理要求来进行。</p><h3 id="土地增值税的分期预缴"><strong>土地增值税的分期预缴</strong></h3><p>在项目未完全结算，但企业已经开始销售房产的情况下，<strong>土地增值税</strong>通常不会等到项目结算后一次性缴纳。相反，根据中国的税法规定，企业应当根据房产销售收入的情况进行<strong>分期预缴</strong>土地增值税。具体规定如下：</p><ul><li><p><strong>分期预缴</strong>：在房地产企业未完成全部项目开发但已有房产销售的情况下，企业需要按房屋销售收入的比例预缴土地增值税。根据《中华人民共和国土地增值税暂行条例》及其实施细则，预缴的土地增值税通常是按销售收入的一定比例缴纳，具体比例由地方税务机关规定。</p><p>例如，某地区可能要求房地产企业按房屋销售收入的<strong>2% - 5%</strong>预缴土地增值税。</p></li><li><p><strong>正式清算</strong>：当项目整体结算时，企业需进行土地增值税的<strong>清算</strong>。此时，企业将根据项目的整体开发成本、收入、增值等情况，进行准确计算，确定应缴纳的土地增值税，并在清算时多退少补。如果预缴的土地增值税不足，企业需补缴；如果多缴，则可申请退税。</p></li></ul><h3 id="财务报表中的体现"><strong>财务报表中的体现</strong></h3><p>即便土地增值税未在项目结算时一次性缴纳，企业在部分销售房产时，<strong>预缴的土地增值税</strong>也需要在当期的财务报表中体现。这通常包括以下几个方面：</p><ul><li><strong>销售房产产生的土地增值税</strong>作为一种<strong>税金及附加</strong>，会反映在利润表的相关科目中，减少企业的净利润。</li><li>预缴的土地增值税通常被归类为负债（例如“应交税费”科目）中的一部分，直到项目最终清算并完成全部纳税。</li></ul><h3 id="土地增值税预缴的依据"><strong>土地增值税预缴的依据</strong></h3><ul><li>《中华人民共和国土地增值税暂行条例》及其实施细则明确规定了土地增值税的预缴制度。</li><li>地方税务机关会根据项目的具体情况确定预缴比例，并要求企业按比例在房屋销售时逐期预缴税款。</li></ul><h3 id="总结-5"><strong>总结</strong></h3><p>如果企业当期部分销售了房产但尚未完成项目整体结算，企业仍需根据房产销售收入的比例<strong>预缴土地增值税</strong>，并在财务报表中体现相应的税费支出和负债。这一预缴税款将持续到项目完成整体结算时，进行清算并做最终的税务调整。</p><h2 id="营业税增值税所得税">营业税、增值税、所得税</h2><p>营业税、增值税、所得税是常见的三种税种，分别针对不同的经济活动和纳税对象。它们的区别在于征税范围、税基、纳税主体等多个方面。以下是三者的主要区别：</p><h3 id="营业税"><strong>营业税</strong></h3><h4 id="概念">概念</h4><p>营业税是一种针对特定服务和销售行为的<strong>流转税</strong>，主要征收对象是从事<strong>提供劳务、转让无形资产或销售不动产</strong>的单位和个人。</p><h4 id="征税范围">征税范围</h4><ul><li>在2016年“营改增”之前，中国的营业税主要征收对象是服务业、建筑业、金融业、房地产等行业。所有这些行业的营业额都会按照不同的税率缴纳营业税。</li><li><strong>营改增（营业税改增值税）</strong>后，营业税已被<strong>增值税</strong>取代，只有某些特定行业还可能适用类似的地方性附加税（如某些地方政府的附加税）。</li></ul><h4 id="税率">税率</h4><ul><li>不同行业的营业税率不同。通常在<strong>3%-5%</strong>之间，如房地产交易为5%。</li></ul><h4 id="计税依据">计税依据</h4><ul><li>以<strong>营业额</strong>作为计税依据，按营业收入的一定比例计算税额。</li><li>不区分成本和利润，企业提供服务或销售的总收入都需缴纳。</li></ul><hr /><h3 id="增值税"><strong>增值税</strong></h3><h4 id="概念-1">概念</h4><p>增值税是一种<strong>流转税</strong>，它针对商品或服务的<strong>增值部分</strong>征税。即企业在生产、流通过程中产生的增值部分才需要缴纳税款。它是基于<strong>商品或服务的附加值</strong>征收的税种。</p><h4 id="征税范围-1">征税范围</h4><ul><li>增值税广泛适用于<strong>商品销售</strong>、<strong>进口商品</strong>、以及提供<strong>加工、修理、修配劳务</strong>等。基本覆盖了所有的商品和服务类交易。</li><li>2016年之后，中国通过“营改增”将服务业的营业税改为增值税，使增值税几乎覆盖了所有行业。</li></ul><h4 id="税率-1">税率</h4><ul><li>增值税的税率通常为<strong>13%</strong>（适用于一般货物和服务），但根据行业不同，可能有<strong>6%、9%</strong>等不同的适用税率。</li><li>一般纳税人可以从销售税额中<strong>抵扣进项税额</strong>，即购买商品或服务时所支付的增值税。</li></ul><h4 id="计税依据-1">计税依据</h4><ul><li>增值税以<strong>商品或服务的增值额</strong>作为计税基础，企业需要缴纳的税款是<strong>销项税额减去进项税额</strong>。</li><li>销项税额 = 销售收入 × 增值税税率</li><li>进项税额 = 购入商品或服务时支付的增值税额</li></ul><hr /><h3 id="所得税"><strong>所得税</strong></h3><h4 id="概念-2">概念</h4><p>所得税是针对企业或个人的<strong>所得或利润</strong>征收的税种。它直接根据纳税主体的<strong>收入</strong>或<strong>利润</strong>计算。</p><h4 id="征税范围-2">征税范围</h4><ul><li><strong>企业所得税</strong>：针对企业的<strong>应纳税所得额</strong>征税，适用于法人单位，如公司和其他经济实体。</li><li><strong>个人所得税</strong>：针对个人的<strong>工资、股息、利息、财产转让收入、租金收入等</strong>各类所得征税。</li></ul><h4 id="税率-2">税率</h4><ul><li><strong>企业所得税</strong>：标准税率为<strong>25%</strong>，但中小企业、科技型企业等可能享受低税率，如<strong>15%</strong>等。</li><li><strong>个人所得税</strong>：按累进税率征收，工资、薪金所得适用的税率从<strong>3%</strong>至<strong>45%</strong>不等，其他类型的收入如股息、利息、财产转让所得等适用不同的固定税率。</li></ul><h4 id="计税依据-2">计税依据</h4><ul><li><strong>企业所得税</strong>：以企业的<strong>净利润</strong>为基础，即<strong>收入减去成本、费用、税金、损失等</strong>。</li><li><strong>个人所得税</strong>：以个人的<strong>应税收入</strong>为基础，计算应纳税所得额。</li></ul><hr /><h3 id="总结对比"><strong>总结对比</strong></h3><table><thead><tr class="header"><th>项目</th><th>营业税</th><th>增值税</th><th>所得税</th></tr></thead><tbody><tr class="odd"><td><strong>征税对象</strong></td><td>服务业、房地产等行业</td><td>商品销售、劳务服务、进口等</td><td>企业利润、个人所得</td></tr><tr class="even"><td><strong>计税依据</strong></td><td>营业收入</td><td>增值部分（销项税额-进项税额）</td><td>应纳税所得额（净利润或个人收入）</td></tr><tr class="odd"><td><strong>税率</strong></td><td>3%-5%</td><td>13%、9%、6%等不同税率</td><td>企业：25%；个人：3%-45%累进税率</td></tr><tr class="even"><td><strong>征税性质</strong></td><td>流转税</td><td>流转税</td><td>所得税</td></tr><tr class="odd"><td><strong>适用对象</strong></td><td>企业和个人</td><td>企业和个人</td><td>企业和个人</td></tr><tr class="even"><td><strong>是否可抵扣</strong></td><td>否</td><td>是（进项税可抵扣销项税）</td><td>否</td></tr></tbody></table><p>通过对比，营业税和增值税都是<strong>流转税</strong>，但增值税更具普遍性，且只针对增值部分征税，营业税则对全部收入征税。所得税则是对<strong>企业利润</strong>或<strong>个人收入</strong>征税，属于直接税，与营业税和增值税的间接征税不同。</p><h2 id="营业利润和毛利润">营业利润和毛利润</h2><p><strong>营业利润</strong>和<strong>毛利润</strong>是企业财务报表中的两个关键指标，但它们反映的盈利能力层次不同，所考虑的成本和费用范围也不同。以下是两者的区别：</p><h3 id="毛利润"><strong>毛利润</strong></h3><h4 id="概念-3">概念</h4><p>毛利润（Gross Profit）是企业的<strong>销售收入减去销售成本</strong>（或称为营业成本）后的差额，反映企业在生产和销售产品或提供服务过程中，去掉直接成本后的盈利能力。</p><h4 id="计算公式">计算公式</h4><p><span class="math display">\[\text{毛利润} = \text{销售收入} - \text{销售成本}\]</span></p><h4 id="代表意义">代表意义</h4><ul><li>毛利润体现的是企业的<strong>基本经营活动</strong>带来的盈利能力，衡量了企业销售产品或提供服务在扣除直接生产成本后的收益。</li><li>它不包括企业的销售费用、管理费用、财务费用等间接成本。</li></ul><h4 id="适用范围">适用范围</h4><ul><li>毛利润适合用于分析<strong>产品的生产或服务的直接盈利能力</strong>，即企业在制造或提供服务的过程中能赚取多少收入。</li></ul><hr /><h3 id="营业利润"><strong>营业利润</strong></h3><h4 id="概念-4">概念</h4><p>营业利润（Operating Profit）是企业的<strong>营业收入减去营业成本和各项期间费用（销售费用、管理费用、财务费用等）</strong>后的利润。它反映的是企业在其<strong>主要经营活动</strong>中获得的实际利润情况。</p><h4 id="计算公式-1">计算公式</h4><p><span class="math display">\[\text{营业利润} = \text{营业收入} - \text{营业成本} - \text{期间费用}\]</span></p><p>其中：</p><ul><li>营业收入 = 销售收入（或主营业务收入）</li><li>营业成本 = 销售成本（或主营业务成本）</li><li>期间费用包括：销售费用、管理费用、财务费用</li></ul><h4 id="代表意义-1">代表意义</h4><ul><li>营业利润更全面地反映了企业<strong>核心业务的盈利能力</strong>，它不仅考虑了直接成本，还包括企业的日常运营成本（如销售费用、管理费用和财务费用）。</li><li>营业利润通常是衡量企业经营活动效率的重要指标。</li></ul><h4 id="适用范围-1">适用范围</h4><ul><li>营业利润适用于评估企业的<strong>整体经营能力</strong>，不仅关注产品或服务的直接收益，还包括企业日常运营的费用控制。</li></ul><hr /><h3 id="毛利润和营业利润的主要区别"><strong>毛利润和营业利润的主要区别</strong></h3><table><thead><tr class="header"><th>项目</th><th>毛利润</th><th>营业利润</th></tr></thead><tbody><tr class="odd"><td><strong>计算范围</strong></td><td>仅考虑销售收入和销售成本</td><td>包括销售收入、销售成本及各种期间费用</td></tr><tr class="even"><td><strong>考虑的费用种类</strong></td><td>只扣除直接成本，如生产成本、进货成本</td><td>扣除所有与主营业务相关的费用，如销售、管理、财务费用</td></tr><tr class="odd"><td><strong>反映的层次</strong></td><td>反映基本生产或服务的盈利能力</td><td>反映公司整体经营活动的盈利能力</td></tr><tr class="even"><td><strong>用途</strong></td><td>适合分析产品或服务的毛利率、盈利能力</td><td>适合分析公司整体运营效率和成本控制</td></tr><tr class="odd"><td><strong>重要性</strong></td><td>是营业利润的基础，但并不全面</td><td>是全面衡量企业经营状况的重要指标</td></tr></tbody></table><hr /><h3 id="总结-6"><strong>总结</strong></h3><ul><li><strong>毛利润</strong>只考虑了产品或服务的<strong>直接成本</strong>，因此它主要用于衡量企业的<strong>基本生产或服务的盈利能力</strong>。</li><li><strong>营业利润</strong>则是在毛利润的基础上，进一步扣除了企业的<strong>期间费用</strong>，如销售费用、管理费用和财务费用，全面反映了企业的<strong>日常经营状况和成本控制</strong>能力。</li></ul><p>简单来说，毛利润是企业通过销售商品或服务获得的基本收益，而营业利润是企业在其核心经营活动中实际获得的收益。</p><h2 id="roe计算">ROE计算</h2><p>ROE（Return on Equity，股东权益回报率）是衡量公司利用股东权益赚取利润的效率的指标。其计算公式为：</p><p><span class="math display">\[ROE = \frac{\text{净利润}}{\text{股东权益}}\]</span> 具体来说，ROE使用的是：</p><ul><li><strong>净利润</strong>：来自<strong>利润表</strong>，通常是税后净利润，也叫归属于母公司股东的净利润。</li><li><strong>股东权益</strong>：来自<strong>资产负债表</strong>，可以是期末股东权益或平均股东权益。股东权益一般包括实收资本、资本公积、盈余公积和未分配利润。</li></ul><p>为了更精确的反映整个期间的情况，通常使用平均股东权益：</p><p><span class="math display">\[\text{平均股东权益} = \frac{\text{期初股东权益} + \text{期末股东权益}}{2}\]</span></p><p>因此，ROE的完整公式可以表示为：</p><p><span class="math display">\[ROE = \frac{\text{净利润}}{\text{平均股东权益}}\]</span></p><p>这是一个重要的财务指标，用来衡量公司对股东的回报能力。</p><h2 id="epsr的计算">EPSR的计算</h2><p><strong>每股营业收入</strong>（EPSR，Earnings Per Share Revenue）是衡量公司每股股票能够带来的营业收入的指标。它的计算公式为：</p><p><span class="math display">\[\text{每股营业收入} = \frac{\text{营业收入}}{\text{总股本}}\]</span> 具体来说，需要从<strong>利润表</strong>和<strong>资产负债表</strong>中的以下项目取值：</p><ol type="1"><li><strong>营业收入</strong>：来自<strong>利润表</strong>中的“营业收入”一栏，反映公司在报告期内从主要业务获得的收入。</li><li><strong>总股本</strong>：来自<strong>资产负债表</strong>中的“股本”或“实收资本”一栏，反映公司在报告期内发行的所有普通股股数。</li></ol><h3 id="公式分解">公式分解</h3><ul><li><strong>营业收入</strong>：在利润表中找到“营业收入”或类似条目，这通常是公司主营业务带来的总收入。</li><li><strong>总股本</strong>：在资产负债表的股东权益部分找到“股本”或“实收资本”，这表示公司发行的股票总数。</li></ul><h3 id="计算步骤">计算步骤</h3><p><span class="math display">\[\text{每股营业收入} = \frac{\text{利润表中的营业收入}}{\text{资产负债表中的总股本}}\]</span></p><p>这个指标主要用来衡量公司每股股票为股东带来的收入规模，能够帮助投资者了解公司股票的价值与潜力。</p><h2 id="财务指标评分标准">财务指标评分标准</h2><p>在财务分析中，不同行业和领域有不同的评分标准，通常会根据企业的财务表现和特定指标来衡量其健康状况和竞争力。以下是常见的财务评分标准和指标，通常用于评估公司在行业中的表现。企业可以根据这些指标制定评分系统，并使用雷达图或其他图表来可视化它们。</p><h3 id="常见的财务评分标准">常见的财务评分标准：</h3><h4 id="盈利能力profitability"><strong>盈利能力（Profitability）</strong></h4><ul><li><strong>毛利率（Gross Profit Margin）</strong>: 衡量销售收入扣除成本后的毛利情况，通常越高越好。<ul><li><strong>评分标准</strong>: &gt; 50% 表示良好，30-50% 中等，&lt; 30% 表示需要改进。</li></ul></li><li><strong>净利率（Net Profit Margin）</strong>: 衡量公司最终净利润与销售收入的比率，反映盈利能力。<ul><li><strong>评分标准</strong>: &gt; 20% 优秀，10-20% 良好，&lt; 10% 可能存在问题。</li></ul></li><li><strong>资产回报率（ROA）</strong>: 衡量公司利用其资产盈利的能力。<ul><li><strong>评分标准</strong>: &gt; 10% 优秀，5-10% 良好，&lt; 5% 需提高。</li></ul></li></ul><h4 id="运营效率operational-efficiency"><strong>运营效率（Operational Efficiency）</strong></h4><ul><li><strong>应收账款周转率（Accounts Receivable Turnover）</strong>: 衡量公司收回应收账款的效率，通常越高越好。<ul><li><strong>评分标准</strong>: &gt; 8 表示优秀，5-8 良好，&lt; 5 表示可能存在问题。</li></ul></li><li><strong>存货周转率（Inventory Turnover）</strong>: 衡量公司存货销售的效率。<ul><li><strong>评分标准</strong>: &gt; 10 次/年 优秀，6-10 次/年 良好，&lt; 6 需改进。</li></ul></li><li><strong>资产周转率（Asset Turnover Ratio）</strong>: 衡量公司使用资产创造收入的效率。<ul><li><strong>评分标准</strong>: &gt; 1 表示有效利用资产，&lt; 1 需提高。</li></ul></li></ul><h4 id="偿债能力solvency"><strong>偿债能力（Solvency）</strong></h4><ul><li><strong>流动比率（Current Ratio）</strong>: 衡量公司短期偿债能力，通常 1.5-2 是健康范围。<ul><li><strong>评分标准</strong>: &gt; 2 优秀，1.5-2 良好，&lt; 1.5 可能存在流动性问题。</li></ul></li><li><strong>速动比率（Quick Ratio）</strong>: 类似流动比率，但剔除了存货。<ul><li><strong>评分标准</strong>: &gt; 1 表示良好，&lt; 1 可能存在流动性风险。</li></ul></li><li><strong>负债权益比率（Debt to Equity Ratio）</strong>: 衡量公司资产中由债务融资的比例。<ul><li><strong>评分标准</strong>: &lt; 0.5 表示良好，0.5-1 中等，&gt; 1 负债较高。</li></ul></li></ul><h4 id="成长性growth"><strong>成长性（Growth）</strong></h4><ul><li><strong>销售增长率（Revenue Growth Rate）</strong>: 衡量公司收入的增长速度。<ul><li><strong>评分标准</strong>: &gt; 15% 表示快速增长，5-15% 稳健增长，&lt; 5% 表示增长缓慢或停滞。</li></ul></li><li><strong>净利润增长率（Net Profit Growth Rate）</strong>: 衡量净利润的增长速度。<ul><li><strong>评分标准</strong>: &gt; 20% 优秀，10-20% 良好，&lt; 10% 需提高。</li></ul></li><li><strong>每股收益增长率（Earnings per Share (EPS) Growth Rate）</strong>: 衡量公司每股收益的增长情况。<ul><li><strong>评分标准</strong>: &gt; 15% 表示良好，5-15% 稳健，&lt; 5% 增长缓慢。</li></ul></li></ul><h4 id="f现金流cash-flow">f<strong>现金流（Cash Flow）</strong></h4><ul><li><strong>经营活动现金流（Operating Cash Flow）</strong>: 衡量公司核心业务产生的现金流量。<ul><li><strong>评分标准</strong>: 正值表示公司有足够现金支持运营，负值可能表明资金压力。</li></ul></li><li><strong>自由现金流（Free Cash Flow）</strong>: 衡量扣除资本支出后的现金流量。<ul><li><strong>评分标准</strong>: &gt; 0 表示健康的财务状况，负值可能需要关注。</li></ul></li></ul><h3 id="行业特定的评分标准">行业特定的评分标准</h3><p>不同的行业可能对某些指标有特殊的关注。以下是一些行业的常用标准：</p><ol type="1"><li><strong>制造业</strong>：<ul><li><strong>设备周转率（Asset Turnover）</strong>：因为制造业通常设备投入较大，资产的周转率是关注重点。</li><li><strong>存货周转率（Inventory Turnover）</strong>：存货积压对制造业影响较大，周转率反映效率。</li></ul></li><li><strong>零售业</strong>：<ul><li><strong>销售增长率（Sales Growth Rate）</strong>：在零售行业中，销售增长是关键指标。</li><li><strong>毛利率（Gross Profit Margin）</strong>：反映销售效率和成本控制能力。</li></ul></li><li><strong>科技行业</strong>：<ul><li><strong>研发投入占比（R&amp;D to Revenue Ratio）</strong>：科技公司通常非常关注研发支出，这是衡量创新能力的指标。</li><li><strong>利润率（Profit Margin）</strong>：科技公司往往具有较高的利润率，因此利润率是衡量其核心竞争力的标准。</li></ul></li><li><strong>金融行业</strong>：<ul><li><strong>资本充足率（Capital Adequacy Ratio）</strong>：衡量银行等金融机构的资本稳定性和风险承受能力。</li><li><strong>不良贷款率（Non-performing Loan Ratio）</strong>：银行业中，较低的不良贷款率是衡量资产质量的重要标准。</li></ul></li></ol><h3 id="综合评分">综合评分</h3><p>在实际应用中，你可以将不同财务指标的评分合并为一个综合评分。比如：</p><ul><li>使用<strong>加权评分</strong>，为每个指标分配权重。</li><li><strong>标准化处理</strong>，将各个指标的评分转换为相同的量级。</li><li>根据不同指标的重要性，设定不同的<strong>权重</strong>。</li></ul><h3 id="综合评分公式示例">综合评分公式示例：</h3><p><span class="math display">\[\text{综合评分} = w_1 \times \text{盈利能力得分} + w_2 \times \text{运营效率得分} + w_3 \times \text{偿债能力得分} + w_4 \times \text{成长性得分} + w_5 \times \text{现金流得分}\]</span></p><p>其中 ( w_1, w_2, w_3, w_4, w_5 ) 为各个维度的权重，评分可以根据不同企业的战略目标进行调整。</p><h3 id="总结-7">总结</h3><p>企业的财务健康状况可以通过不同维度的评分来评估，每个行业有其特定的财务指标。通过这些评分标准，可以帮助分析公司在市场中的表现，并为投资、管理决策提供依据。</p><h2 id="软件公司评价标准">软件公司评价标准</h2><p>对于软件开发公司，财务指标评分标准可能与其他行业有所不同，主要因为软件公司通常具有高增长潜力、低边际成本以及不同的业务模式。以下是一些常见的财务指标和评分标准，特别适用于软件开发公司：</p><h3 id="收入和增长性revenue-and-growth"><strong>收入和增长性（Revenue and Growth）</strong></h3><ul><li><p><strong>年度收入增长率（Annual Revenue Growth Rate）</strong>:</p><ul><li><p><strong>评分标准</strong>:</p><ul><li><blockquote><p>30% 表示高增长</p></blockquote></li><li><p>15-30% 为稳健增长</p></li><li><p>&lt; 15% 增长缓慢或停滞</p></li></ul></li></ul></li><li><p><strong>经常性收入占比（Recurring Revenue Ratio）</strong>:</p><ul><li><p><strong>评分标准</strong>:</p><ul><li><blockquote><p>70% 表示业务稳定</p></blockquote></li><li><p>50-70% 表示一定的经常性收入</p></li><li><p>&lt; 50% 需关注</p></li></ul></li></ul></li></ul><h3 id="盈利能力profitability-1"><strong>盈利能力（Profitability）</strong></h3><ul><li><p><strong>毛利率（Gross Profit Margin）</strong>:</p><ul><li><p><strong>评分标准</strong>:</p><ul><li><blockquote><p>70% 表示高盈利能力</p></blockquote></li><li><p>50-70% 为良好</p></li><li><p>&lt; 50% 表示需要提高</p></li></ul></li></ul></li><li><p><strong>净利率（Net Profit Margin）</strong>:</p><ul><li><p><strong>评分标准</strong>:</p><ul><li><blockquote><p>20% 表示盈利能力强</p></blockquote></li><li><p>10-20% 为稳健</p></li><li><p>&lt; 10% 需关注</p></li></ul></li></ul></li></ul><h3 id="现金流cash-flow"><strong>现金流（Cash Flow）</strong></h3><ul><li><p><strong>经营活动现金流（Operating Cash Flow）</strong>:</p><ul><li><strong>评分标准</strong>:<ul><li>正值且持续增长表示健康</li><li>偶尔负值需关注</li><li>长期负值需改善</li></ul></li></ul></li><li><p><strong>自由现金流（Free Cash Flow）</strong>:</p><ul><li><p><strong>评分标准</strong>:</p><ul><li><blockquote><p>0 表示健康的财务状况</p></blockquote></li><li>负值需分析原因</li></ul></li></ul></li></ul><h3 id="效率指标efficiency-metrics"><strong>效率指标（Efficiency Metrics）</strong></h3><ul><li><strong>客户获取成本（Customer Acquisition Cost, CAC）</strong>:<ul><li><strong>评分标准</strong>:<ul><li>低于同行业平均水平表示高效</li><li>高于平均水平需改进</li></ul></li></ul></li><li><strong>客户终身价值（Customer Lifetime Value, CLV）</strong>:<ul><li><strong>评分标准</strong>:<ul><li>高于 CAC 的多倍数（如 CLV/CAC &gt; 3）表示良好</li><li>较低需关注</li></ul></li></ul></li></ul><h3 id="资本结构capital-structure"><strong>资本结构（Capital Structure）</strong></h3><ul><li><p><strong>负债权益比率（Debt to Equity Ratio）</strong>:</p><ul><li><p><strong>评分标准</strong>:</p><ul><li><p>&lt; 0.5 表示低风险</p></li><li><p>0.5-1 为适中</p></li><li><blockquote><p>1 需关注</p></blockquote></li></ul></li></ul></li><li><p><strong>资本充足率（Capital Adequacy Ratio）</strong>:</p><ul><li><strong>评分标准</strong>:<ul><li>高于同行标准表示健康</li><li>低于标准需改进</li></ul></li></ul></li></ul><h3 id="研发投入rd-investment"><strong>研发投入（R&amp;D Investment）</strong></h3><ul><li><p><strong>研发费用占比（R&amp;D Expense Ratio）</strong>:</p><ul><li><p><strong>评分标准</strong>:</p><ul><li><blockquote><p>20% 表示高投入</p></blockquote></li><li><p>10-20% 为合理</p></li><li><p>&lt; 10% 需分析原因</p></li></ul></li></ul></li></ul><h3 id="客户和市场customer-and-market"><strong>客户和市场（Customer and Market）</strong></h3><ul><li><p><strong>客户留存率（Customer Retention Rate）</strong>:</p><ul><li><p><strong>评分标准</strong>:</p><ul><li><blockquote><p>90% 表示高留存</p></blockquote></li><li><p>80-90% 良好</p></li><li><p>&lt; 80% 需改进</p></li></ul></li></ul></li><li><p><strong>市场份额（Market Share）</strong>:</p><ul><li><strong>评分标准</strong>:<ul><li>增长表示业务拓展成功</li><li>稳定表示市场定位正确</li><li>下降需分析原因</li></ul></li></ul></li></ul><h3 id="示例计算和评分">示例：计算和评分</h3><p>以下是如何使用这些指标来为软件公司评分的一个简单示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设有一个软件公司的财务数据  </span></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&#x27;指标&#x27;</span>: [<span class="string">&#x27;年度收入增长率&#x27;</span>, <span class="string">&#x27;毛利率&#x27;</span>, <span class="string">&#x27;净利率&#x27;</span>, <span class="string">&#x27;经营活动现金流&#x27;</span>, <span class="string">&#x27;自由现金流&#x27;</span>, <span class="string">&#x27;客户获取成本&#x27;</span>, <span class="string">&#x27;客户终身价值&#x27;</span>, <span class="string">&#x27;研发费用占比&#x27;</span>, <span class="string">&#x27;客户留存率&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;值&#x27;</span>: [<span class="number">35</span>, <span class="number">72</span>, <span class="number">18</span>, <span class="number">1000000</span>, <span class="number">500000</span>, <span class="number">300</span>, <span class="number">1500</span>, <span class="number">25</span>, <span class="number">85</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 DataFrame  </span></span><br><span class="line">df = pd.DataFrame(data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 评分标准（示例）</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">score</span>(<span class="params">row</span>):</span><br><span class="line">    <span class="keyword">if</span> row[<span class="string">&#x27;指标&#x27;</span>] == <span class="string">&#x27;年度收入增长率&#x27;</span>:</span><br><span class="line">        <span class="keyword">if</span> row[<span class="string">&#x27;值&#x27;</span>] &gt; <span class="number">30</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">&#x27;高增长&#x27;</span></span><br><span class="line">        <span class="keyword">elif</span> row[<span class="string">&#x27;值&#x27;</span>] &gt; <span class="number">15</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">&#x27;稳健增长&#x27;</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">&#x27;增长缓慢&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> row[<span class="string">&#x27;指标&#x27;</span>] == <span class="string">&#x27;毛利率&#x27;</span>:</span><br><span class="line">        <span class="keyword">if</span> row[<span class="string">&#x27;值&#x27;</span>] &gt; <span class="number">70</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">&#x27;高盈利&#x27;</span></span><br><span class="line">        <span class="keyword">elif</span> row[<span class="string">&#x27;值&#x27;</span>] &gt; <span class="number">50</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">&#x27;良好&#x27;</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">&#x27;需提高&#x27;</span></span><br><span class="line">    <span class="comment"># 添加其他指标的评分标准</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;评分未定义&#x27;</span></span><br><span class="line"></span><br><span class="line">df[<span class="string">&#x27;评分&#x27;</span>] = df.apply(score, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(df)</span><br></pre></td></tr></table></figure><h3 id="总结-8">总结</h3><ul><li><strong>收入和增长性</strong>：反映公司的业务扩展和收入稳定性。</li><li><strong>盈利能力</strong>：衡量公司的利润水平。</li><li><strong>现金流</strong>：评估公司产生现金的能力。</li><li><strong>效率指标</strong>：反映公司运营的有效性。</li><li><strong>资本结构</strong>：了解公司的财务稳健性。</li><li><strong>研发投入</strong>：表明公司对未来发展的投入。</li><li><strong>客户和市场</strong>：评估客户关系和市场地位。</li></ul><p>这些指标可以根据公司实际情况和行业标准进行调整和定制。</p><h2 id="盈利能力的综合评估">盈利能力的综合评估</h2><p>盈利能力的综合评估通常结合毛利率、净利率和资产回报率三者来衡量。每个指标衡量公司的不同方面，因此可以从以下几个方面综合考虑：</p><h3 id="毛利率gross-profit-margin"><strong>毛利率（Gross Profit Margin）</strong>：</h3><ul><li><strong>衡量</strong>：公司在扣除生产成本后的销售收入，反映了公司成本控制的效率。毛利率高说明公司产品或服务的生产成本相对较低。</li><li><strong>计算公式</strong>：<br /><span class="math display">\[\text{毛利率} = \frac{\text{销售收入} - \text{销售成本}}{\text{销售收入}} \times 100\%\]</span></li></ul><h3 id="净利率net-profit-margin"><strong>净利率（Net Profit Margin）</strong>：</h3><ul><li><strong>衡量</strong>：公司在扣除所有费用、利息和税费后的实际盈利能力。净利率是公司整体盈利能力的更综合反映。</li><li><strong>计算公式</strong>：<br /><span class="math display">\[\text{净利率} = \frac{\text{净利润}}{\text{销售收入}} \times 100\%\]</span></li></ul><h3 id="资产回报率roa-return-on-assets"><strong>资产回报率（ROA, Return on Assets）</strong>：</h3><ul><li><strong>衡量</strong>：公司利用其资产产生利润的能力，反映了公司资产的使用效率。</li><li><strong>计算公式</strong>：<br /><span class="math display">\[\text{资产回报率 (ROA)} = \frac{\text{净利润}}{\text{总资产}} \times 100\%\]</span></li></ul><h3 id="盈利能力的综合计算思路">盈利能力的综合计算思路：</h3><p>为了评估公司的整体盈利能力，可以从以下角度综合考虑毛利率、净利率和资产回报率：</p><ol type="1"><li><p><strong>权重法</strong>：</p><ul><li>给每个指标分配权重，然后计算综合得分。通常，毛利率、净利率和资产回报率可以根据其重要性被赋予不同的权重。一个常见的方式是： <span class="math display">\[\text{盈利能力综合得分} = w_1 \times \text{毛利率} + w_2 \times \text{净利率} + w_3 \times \text{ROA}\]</span> 其中 <span class="math inline">\(w_1, w_2, w_3\)</span> 为各指标的权重，总和为1。根据不同的行业或公司的具体情况，权重可以调整。</li></ul></li><li><p><strong>简单平均法</strong>：</p><ul><li>直接对毛利率、净利率和ROA取平均值，衡量整体盈利能力： <span class="math display">\[\text{盈利能力} = \frac{\text{毛利率} + \text{净利率} + \text{ROA}}{3}\]</span> 这种方法适用于各指标重要性差别不大时。</li></ul></li><li><p><strong>行业对比法</strong>：</p><ul><li>将公司的各项盈利指标与同行业的平均水平进行比较，来评估整体盈利能力。如果某一项指标显著高于行业平均水平，可以认为公司在该方面具有竞争优势。</li></ul></li></ol><h3 id="综合分析建议">综合分析建议：</h3><ul><li>毛利率衡量的是成本控制和产品溢价能力，净利率反映了运营效率和最终盈利水平，而ROA则考察资产的使用效率。三者结合可以给出更全面的盈利能力评估。</li><li>如果某个公司的毛利率高，但净利率较低，可能意味着其运营成本或其他非直接成本较高。</li><li>ROA低则可能表明公司对资产的利用效率不高，即使净利率或毛利率表现不错，整体盈利能力可能也受限。</li></ul><p>通过权重法或平均法，可以从这三项指标得出综合的盈利能力评分，帮助决策者更好地评估公司的整体财务健康状况。</p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 金融 </tag>
            
            <tag> 徐高 </tag>
            
            <tag> 视频 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>统计学的学习笔记</title>
      <link href="/2025/02/01/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E7%9A%84%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
      <url>/2025/02/01/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E7%9A%84%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<p>更具体地说，如果我们没有为参数选择一个共轭先验（见 6.6.1 节），那么 (8.22) 和 (8.23) 中的积分通常在解析上是不可解的，我们就无法以封闭形式计算后验、预测或边际似然。在这种情况下，我们必须借助近似方法。例如，可以采用 <strong>随机近似方法</strong>，如 <strong>马尔可夫链蒙特卡罗（MCMC）</strong>（Gilks 等，1996）</p><ul><li>机器学习《Python深度学习(Deep Learning with Python, 2/e)》知其然，《机器学习的数学基础（MATHEMATICS FOR MACHINE LEARNING）》知其所以然</li></ul><p>推断统计学（Inferential Statistics）是统计学的一个核心分支，它的目标是：基于样本数据对总体（population）作出合理推断或决策。 描述统计对已知样本进行总结与展示，包括均值、方差、图表等 包括估计、检验、模型等，不涉及概率推断。</p><h4 id="重点记录">重点记录</h4><ul><li>oracle不等式<br /></li><li>各种残差的概念的理解：方差 偏差 残差 标准误差 标准差 残差偏差（<em>residual deviance</em>）<br /></li><li>残差在模型诊断中的重要性<br /></li><li>小波基底 压缩感知 傅⾥叶滤波<br /></li><li>统计学复杂的地方：不同假设对应不同估计和方案选择，所以结果出来需要对假设做检验<br />偏差-方差权衡（<em>bias-variance trade off</em>）<br /></li><li>投资风险就是方差<br /><span id="more"></span></li></ul><h4 id="数据的选取">数据的选取</h4><ul><li>医学数据<br /><strong>earinf</strong>: 夏季耳部感染数据<br /></li><li>金融数据<br />例如股票的数据</li></ul><h4 id="知识点">知识点</h4><ul><li><p>逆高斯分布<br /><strong>逆高斯分布有一个有趣的解释，与布朗运动相关。布朗运动是指粒子随时间的随机运动。</strong>对于具有正漂移（倾向于从当前位置移动）的布朗运动的粒子，逆高斯分布描述了粒子到达某个固定正距离 $$所需时间的分布。而正态分布（也称为高斯分布）描述了在固定时间点粒子从原点的距离分布。逆高斯分布因与正态分布的这种关系而得名。当 <span class="math inline">\(\phi = 1\)</span> 时，泊松分布是其特例。<br /></p></li><li><p>响应变量的对数转换的最主要目的，是让残差常数化<br /></p></li><li><p>模型建模三部曲</p><ul><li>假设<br /></li><li>估计或拟合、推断(一种系数显著性检验，例如t检验)<br /></li><li>诊断假设合理性（各种检验技术，例如F检验）<br /></li></ul></li><li><p>⼴义线性模型（GLM，McCullagh 和 Nelder 1989） 这些模型使⽤指数族分布的成员（如伯努利、泊松和⾼斯分布等）来描述响应变量。<br /></p></li><li><p>不同统计量的定义和背后的本质以及其对应的某一种的情况。<br /></p></li><li><p>等高线是降维作用<br /></p></li><li><p>求切线，先求法线，通过升维函数的梯度求。<br /></p></li><li><p>统计学书籍介绍：<br />https://martechcareer.com/advice/stats-data-analyst-should-know 数据科学家也需要补上这堂统计课<br /></p></li><li><p>Cauchy-Schwarz 不等式说明两个向量的内积的绝对值小于等于它们长度的乘积。只有当两个向量共线时（即线性相关），等号成立。<br /></p></li><li><p>曼哈顿距离就是<span class="math inline">\(\mathcal l_1\)</span>范数<br /></p></li><li><p>概率和统计：概率是统计的工具，统计是分析数据的方法。<br /></p></li><li><p>上采样、下采样；过采样、欠采样</p><ul><li>上采样&amp;下采样<br />对于CV领域来说，可以理解为放大图片和缩小图片，将原始图片放大就是上采样，将多个像素点合成一个从而缩小图片就是下采样，所以池化操作就可以理解为一种下采样。如果想进一步了解图像领域的上采样和下采样，可以参照：图像的上采样（upsampling）与下采样（subsampled）<br />对于非cv领域能，我觉得这个词用得少一点，如果是特征增多，一般称为特征构造，如果是选择部分特征，一般说特征选择。<br /></li><li>过采样&amp;欠采样<br />这主要是针对不平衡的数据集做的一些操作。<br />欠采样（undersampling）：当数据不平衡的时，比如对于一个只用0和1的二分类问题，样本标签1有10000个数据，样本标签0有6000个数据时，为了保持样本数目的平衡，可以选择减少标签1的数据量，这个过程就叫做欠采样。<br />过采样（oversampling）减少数据量固然可以达到以上效果，并且在一定程度上防止过拟合，但是这也牺牲了数据，因此存在另一种增加样本的采样方法，也就是增加标签0的样本数。<br /></li></ul></li><li><p>统计的流派<br />对于人类来讲，一个能把英语翻译成汉语的人，必定能很好理解这两种语言。这就是直觉的作用。在人工智能领域，包括自然语言处理领域，后来把这样的方法论称作“鸟飞派”，也就是看看鸟是怎么飞的，就能模仿鸟造出飞机，而不需要了解空气动力学。事实上我们知道，怀特兄弟发明飞机靠的是空气动力学而不是仿生学。<br />自然语言的处理从基于规则方法的传统（语言和这个世界一样，不规则、不确定性占了很大比例），现在转入了统计的语言处理方法。<br />这里有一个很好的例子，来自于腾讯搜索部门。最早的语言模型是使用《人民日报》的语料训练的，因为开发者认为这些语料干净、无噪音。但是实际的效果就比较差，经常出现搜索串和网页不匹配的例子。后来改用网页的数据，尽管他们有很多的噪音，但是因为训练数据和应用一致，搜索质量反而好。<br />分词的二义性是语言歧义性的一部分，1990年前后，当时清华大学电子工程系工作的郭进博士用统计语言模型成功解决了分词二义性的问题，将汉语分词的错误率降低了一个数量级。<br />就像飞机不需要拍动翅膀。<br />今天几乎所有的科学家都不坚持『机器要像人一样思考才能获得智能』，但是很多门外汉在谈到人工智能时依然想象着『机器在像我们那样思考。』...机器智能最重要的是能够解决人脑所能解决的问题，而不是在于是否需要采用和人一样的方法。<br /></p></li><li><p>微积分<br />求导是切线斜率；求积分是求面积<br />微积分的本质是研究关于连续变化的问题<br />数学：普及，平民化，现在又抽象化了，走向精英阶层<br /></p></li><li><p><strong>世界的不确定性</strong><br />科学家们开始改变思维：能找到现象背后的因果模型更好，因为这毕竟是一劳永逸的事情，很好的体现了上帝的意志。<br />但是，在这个每个突破的都需要漫长的时间里，我们可以通过大数据思维，在基于不确定的前提下，借助数据量的突破性增长、借助计算机能力的突破性发展，以及人类在人工智能方面的飞跃（Alphago战胜了李世石），通过数据的相关性来掌握事物的规律，然后基于机器学习，不断优化模型。这就是大数据思维的核心。<br />用不确定的眼光看待世界，再用信息来消除这种不确定性，是大数据解决智能问题的本质。（从确定性到相关性）<br />世界的不确定性，通过统计、大数据来模拟近似。<br />几千年来，我们人类的只是都是建立在归纳法之上，归纳法隐含的假设就是『未来将继续和过去一样』，换句话说应该叫连续性假设。<br />与机械思维是建立在一种确定性的基础上所截然不同的是，信息论完全是建立在不确定性基础上<br />知识就像圆，你懂得越多，无知就越多。 因为这个世界是不确定性为主导的。（针对人的有限性）<br />吾生也有涯，而知也无涯，以有涯随无涯，殆已。<br />肖申克的救赎，工作和监狱，确定性和自由的不确定性。 （出狱又回来）<br /></p></li><li><p>风险模型<br />统计模型：目前主要使用，根据历史交易中的欺诈交易信息训练分类算法，然后将经过采集加工后的交易信息输入分类算法，即可得到交易风险分值。<strong>由于统计模型采用模糊识别，并不精确匹配欺诈类型规则，因为对新出现的交易欺诈有一定的预测性。</strong><br /></p></li><li><p><strong>统计就是数学物理化</strong><br />重视经验和数据，而非严格逻辑推理<br /></p></li><li><p>量子物理<br />什么是量子物理？物理学家们所测量的能量、电荷、角动量等都是量子化的。它们并非无限可分的；能量、电荷等等全部存在一个最小单位。<br />所以说，量子物理和布尔运算一样，都是离散的思想。<br /></p></li><li><p>数据、信息、知识<br />数据（原料）-&gt;信息-&gt;知识<br /></p></li><li><p>科学的分类<br />科学就是在表面的变化中找不变；<br />硬科学就是找因果关系，包括公式、规律、原理、模式等；<br />软科学就是找相关性，例如统计学、大数据科学、AI等；<br /></p></li><li><p><strong>迭代和递归的区别</strong><br />迭代的含义<br />从“编程之美”的角度看，可以借用一句非常经典的话：“迭代是人，递归是神！”来从宏观上对二者进行把握。<br /><strong>迭代是将输出做为输入,再次进行处理。</strong>比如将摄像头对着显示器；比如镜子对着镜子；比如KTV中将麦克对着音箱；比如机关枪扣动扳机发射子弹后，利用后座力继续扣动扳机。<br />用程序表述就是：for (int i=0; i &lt; 100; i++) n = f(n);<br />再给迭代举个通俗点的例子：假如你有一条哈士奇和一条中华田园犬，怎么让它们串出比较纯正的哈士奇呢？先让哈士奇与中华田园犬配对，生下小狗。再让哈士奇与小狗配对，当然要等小狗长大后。就这样一直让哈士奇与新生的小狗配对，一代一代地迭，最终你能得到比较纯正的哈士奇。如果你纠结猫三狗四，猪五羊六，牛七马八这样的自然规律，不妨把两条狗改为老鼠与宠物仓鼠，他们一个月就能迭代一次。<br /><strong>从计算机角度讲，递归是迭代的特例。</strong>https://www.zhihu.com/question/20278387<br />a.递归中一定有迭代，但是迭代中不一定有递归；大部分可以相互转换。<br />b.相对来说，能用迭代不用递归（因为递归不断调用函数，浪费空间，容易造成堆栈溢出）</p></li><li><p>CNN 卷积核 完全由训练过程自动学习，不需要人工设计</p></li><li><p><strong>抽样分布 关注统计量的抽样分布，而不是原数据的分布（数据分布） 自助法</strong></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 统计 </tag>
            
            <tag> 整理 </tag>
            
            <tag> 知识 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《随机漫步的傻瓜》读书心得</title>
      <link href="/2024/03/02/%E3%80%8A%E9%9A%8F%E6%9C%BA%E6%BC%AB%E6%AD%A5%E7%9A%84%E5%82%BB%E7%93%9C%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2024/03/02/%E3%80%8A%E9%9A%8F%E6%9C%BA%E6%BC%AB%E6%AD%A5%E7%9A%84%E5%82%BB%E7%93%9C%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>之前买错了《漫步华尔街》，现在补买回来这本书。本书有点哲学书的味道，讲了很多的人生，可以称之为轻哲学。</p><p><strong>本书的核心思想是概率论，而概率论的核心是随机性、非对称、非线形，在生活和社会经济领域皆如此。</strong></p><p><strong>点题：</strong> <img src="/img2/随机漫步的傻瓜/随机漫步的傻瓜40.pic.jpg" /> <img src="/img2/随机漫步的傻瓜/随机漫步的傻瓜41.pic.jpg" /> <img src="/img2/随机漫步的傻瓜/随机漫步的傻瓜42.pic.jpg" /></p><p><strong>学习方法</strong></p><ul><li>阅读前人的事迹，向过去学习；</li><li>利用蒙特卡罗随机序列，向未来学习。</li></ul><span id="more"></span><h1 id="未然历史">未然历史</h1><ul><li>未然历史<br />除了考虑可能观察得到的可能性结果，我们也应考虑观察不到的可能性结果。<br /></li><li>结果导向<br />不管白猫黑猫，抓到老鼠就是好猫。<br />不能以结果论英雄，但这似乎只是失败者的自我安慰，成功者总是把成功归功于自己的英明决策。</li></ul><h1 id="概率论">概率论</h1><p>我们习惯于低估随机性的力量，成功者可能只是幸运的傻瓜。俗话说的这种人凭运气赚的钱会凭实力亏回去。所谓的谋事在人，成事在天，所谓的时也，命也，运也，非吾之所能也。</p><h2 id="随机过程">随机过程</h2><p>随机过程是指随着时间的流逝，各种事件纷纷出现的动态过程。概率论这一分支的研究对象是<strong>连续性随机事件</strong>的演变过程，我们可以称之为<strong>历史的数学</strong>。一个过程的关键，在于它含有时间因素。</p><p><strong>概率论影响着多种科学，因为不确定性是这个世界的最重要法则。</strong></p><p>在我们的随机世界里，数学只是一种思考方式，除此别无他用。</p><p>不要把正确性和可理解性混为一谈，但因为本性的原因，人往往会被蒙蔽。 <img src="/img2/随机漫步的傻瓜/随机漫步的傻瓜6.pic.jpg" /></p><p><strong>进化被随机性愚弄：事件的遍历性，该来的总会来。</strong> <img src="/img2/随机漫步的傻瓜/随机漫步的傻瓜14.pic.jpg" /> <img src="/img2/随机漫步的傻瓜/随机漫步的傻瓜15.pic.jpg" /></p><h2 id="随机序列">随机序列</h2><p>蒙特卡罗发生器可以模拟产生随机序列。</p><p>每次我想探索某个观念时，总情不自禁的去创造随机序列，其实是分析各种可能性，然后结合概率和期望值来决策的方法。<br />真正的老板是生意头脑（直觉）+数学分析（工具），两者结合。</p><h2 id="统计学">统计学</h2><p><strong>统计学：你得到的信息越多，对结果就会越有把握（可信赖度）。但这是有问题的。某一方面的信息多少并不能给你带来更多的把握。</strong> 科学只是系统性的推测而已。统计学是一把双刃剑，类似于盲人摸象，可能只是看到了某个局部的片面的信息，可以认为是天真的经验论者。<br />所以说，数学是用以思考而不是计算的工具。</p><p><img src="/img2/随机漫步的傻瓜/随机漫步的傻瓜21.pic.jpg" /></p><h1 id="非对称性">非对称性</h1><h2 id="非对称陈述">非对称陈述</h2><p><img src="/img2/随机漫步的傻瓜/随机漫步的傻瓜22.pic.jpg" /> <img src="/img2/随机漫步的傻瓜/随机漫步的傻瓜23.pic.jpg" /></p><h2 id="样本数的意义条件概率">样本数的意义（条件概率）</h2><p><img src="/img2/随机漫步的傻瓜/随机漫步的傻瓜25.pic.jpg" /></p><h2 id="非对称性现象">非对称性现象</h2><p><img src="/img2/随机漫步的傻瓜/随机漫步的傻瓜16.pic.jpg" /></p><h2 id="噪声">噪声</h2><p><strong>每天轰炸我们的紧急新闻中，除了噪声，几乎空无一物。</strong> 大量的信息在统计上并不重要，其不足以推演出有意义的结论。 噪声和信息的区别。</p><p>工作伦理会使人只留意噪声，而忽略了有意义的信号。<br />每个人都会向长期的性质靠拢。所谓的均值回归、本性难移也是这个意思。所以在判断历史事件时，时间的尺度很重要。所谓的日久见人心，路遥知马力。所以人必须具备<strong>长期主义</strong>思考问题的认知。 <img src="/img2/随机漫步的傻瓜/随机漫步的傻瓜10.pic.jpg" /> <img src="/img2/随机漫步的傻瓜/随机漫步的傻瓜11.pic.jpg" /> <img src="/img2/随机漫步的傻瓜/随机漫步的傻瓜12.pic.jpg" /></p><h2 id="借助非对称性赚钱靠稀有事件赚钱">借助非对称性赚钱（靠稀有事件赚钱）</h2><p>如果某一件事情的失败代价过于沉重、难以承受，那么其成功的概率有多高根本无关紧要。其实也是一种底线思维，例如犯罪就是这样的例子。</p><p><img src="/img2/随机漫步的傻瓜/随机漫步的傻瓜17.pic.jpg" /> <img src="/img2/随机漫步的傻瓜/随机漫步的傻瓜18.pic.jpg" /> <img src="/img2/随机漫步的傻瓜/随机漫步的傻瓜19.pic.jpg" /> <img src="/img2/随机漫步的傻瓜/随机漫步的傻瓜20.pic.jpg" /></p><h2 id="金融从业者">金融从业者</h2><p><img src="/img2/随机漫步的傻瓜/随机漫步的傻瓜7.pic.jpg" /> 类似中国人讲话的模棱两可，打太极。</p><p>事实上，交易员的心理结构应该引导他敢于去做其他人不做的事情。 <img src="/img2/随机漫步的傻瓜/随机漫步的傻瓜24.pic.jpg" /> <img src="/img2/随机漫步的傻瓜/随机漫步的傻瓜13.pic.jpg" /></p><h1 id="随机现象的几个有名特征">随机现象的几个有名特征</h1><p>由观察者对随机现象的重要性认知错误产生，其实一个随机序列总会呈现某种可觉察的形态，而人类却天真的以为自己发现了其中的奥秘。</p><ul><li>幸存者偏差</li><li>数据挖掘（data mining）</li><li>数据探索</li><li>过度适配</li><li>回归平均值</li></ul><p>随机现象看起来不随机：（内曼·皮尔逊鉴定法 Neyman-pearson） <img src="/img2/随机漫步的傻瓜/随机漫步的傻瓜27.pic.jpg" /></p><h1 id="非线性">非线性</h1><p>差之毫厘，谬以千里。马太效应、赢者通吃的现象。 <img src="/img2/随机漫步的傻瓜/随机漫步的傻瓜28.pic.jpg" /> <img src="/img2/随机漫步的傻瓜/随机漫步的傻瓜29.pic.jpg" /></p><h1 id="路径依赖">路径依赖</h1><p><img src="/img2/随机漫步的傻瓜/随机漫步的傻瓜30.pic.jpg" /> <img src="/img2/随机漫步的傻瓜/随机漫步的傻瓜31.pic.jpg" /></p><p>比方说：借“丢硬币”来打破生活中的一些小小的僵局，也就是让随机性帮助你做决定，让命运女神做主，你主要欣然接受即可。（AI中碰到一些死循环，或者无路可走的时候，也采用随机性的选择；还有在爬山搜索中，也采用随机的方式优化局部最优的问题。）</p><h1 id="人性弱点">人性弱点</h1><ul><li>人的非理性：（人天生很难保持理性）<br />我们知道自己的确会受制于情绪，大多数的精力和能量也源于情绪，因此解决方法不是驾驭我们的心。我们必须接受自己只是动物的事实，我们需要一些低等形式的窍门帮助自己，而不是讲大道理。例如出门到咖啡店学习，而不是呆家里。<br />传统经济学认为人是理性的人，因为这么做，对经济学家来说是最好处理的。（数学上所谓的最优化，模型好建立）。<br />人只能说是相对理性，而实证科学与之相反，以实际观察人的行为为基础。（行为经济学）<br />心理学家卡尼曼和特韦尔斯基专长是发掘人类在哪些地方欠缺理性思考的能力。<br /><img src="/img2/随机漫步的傻瓜/随机漫步的傻瓜37.pic.jpg" /></li><li>我们的本性并不善于了解概率，大部分概率的结果完全违背直觉。<br /></li><li>批判性心态<br /><strong>索罗斯：保持批判性的开放心态，并且不以改变看法为耻。 （反知识分子心态、百无一用是书生。）</strong><br />索罗斯这种真正的投机者和别人不一样的地方，在于他们缺乏路径依赖，他完全不受过去的行为束缚，每一天都是一张白纸。<br />我们要求自相矛盾的权利。因为人在变，时势也在变。这也是一种自我批判的精神。这样，自己的观念就不会受路径依赖的观念的束缚。（贬义来说就是善变）<br /><strong>所以有时我们看当老板的老是变来变去，这就是老板的不同常人的地方。</strong><br />这种特质在人类中是少数的，因为我们的文化认为这是叛变、是变节、是善变。 <img src="/img2/随机漫步的傻瓜/随机漫步的傻瓜38.pic.jpg" /></li></ul><h1 id="期权">期权</h1><p>期权的定价其实就是期望值的应用。</p><p><strong>价外期权：</strong>指当标的资产价格低于于执行价（看涨期权中）或者高于执行价（看跌期权中），如果执行期权的话将会有损失产生。<br />平价期权：当标的资产价格等于执行价，或者非常接近执行价时</p><p><img src="/img2/随机漫步的傻瓜/随机漫步的傻瓜33.pic.jpg" /> <img src="/img2/随机漫步的傻瓜/随机漫步的傻瓜32.pic.jpg" /> <img src="/img2/随机漫步的傻瓜/随机漫步的傻瓜34.pic.jpg" /></p><h1 id="文章摘要">文章摘要</h1><ul><li>《伊利亚特》：英雄之所以是英雄，是因为他们的行为非常英勇，而不是战场上的胜利。</li><li>需要做好应对黑天鹅事件的准备，因为命运女神唯一不能控制的是你的行为。</li><li>只有所说的话比保持沉默更有价值时，我才说出来。 <strong>沉默是金，言多必失。</strong></li><li>人们常说，哲学养不活哲学家。</li><li>知识分子对于暴发户，理性上的轻蔑，不能掩饰心底的艳羡。</li><li>营销即一切，有些人根本不懂得如何推销自己。</li><li>常识不过是18岁前学得到的一大堆错误看法。（政府的愚民政策）</li><li>一项陈述只可能归属于两个范畴：演绎法或归纳法。</li><li>我认为真正的交易员通常穿着邋遢，仪表糟糕，并且对于垃圾桶内废纸上内容的求知欲，远远高于墙上挂的塞尚的画作。</li><li>股市的关键在于基本面。</li><li><strong>曾子曰：“吾日三省吾身：为人谋而不忠乎？与朋友交而不信乎？传不习乎？”</strong></li><li>迷信能给人的日常生活注入一些诗意。</li><li>一将功成万骨枯。</li><li>计量经济学： 将统计学应用到不同时期（称为时间序列）选出的样本上，它的作用是研究经济变量、资料和其它事物的时间序列。</li></ul><p><img src="/img2/随机漫步的傻瓜/随机漫步的傻瓜1.jpg" /> <img src="/img2/随机漫步的傻瓜/随机漫步的傻瓜2.jpg" /> <img src="/img2/随机漫步的傻瓜/随机漫步的傻瓜3.pic.jpg" /> <img src="/img2/随机漫步的傻瓜/随机漫步的傻瓜4.pic.jpg" /></p><p><img src="/img2/随机漫步的傻瓜/随机漫步的傻瓜5.pic.jpg" /></p><p><img src="/img2/随机漫步的傻瓜/随机漫步的傻瓜8.pic.jpg" /> <img src="/img2/随机漫步的傻瓜/随机漫步的傻瓜9.pic.jpg" /></p><p><img src="/img2/随机漫步的傻瓜/随机漫步的傻瓜26.pic.jpg" /></p><p><img src="/img2/随机漫步的傻瓜/随机漫步的傻瓜35.pic.jpg" /> <img src="/img2/随机漫步的傻瓜/随机漫步的傻瓜36.pic.jpg" /></p><p><img src="/img2/随机漫步的傻瓜/随机漫步的傻瓜39.pic.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 投资 </tag>
            
            <tag> 股票 </tag>
            
            <tag> 随机 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《漫步华尔街》读书心得</title>
      <link href="/2023/12/30/%E3%80%8A%E6%BC%AB%E6%AD%A5%E5%8D%8E%E5%B0%94%E8%A1%97%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2023/12/30/%E3%80%8A%E6%BC%AB%E6%AD%A5%E5%8D%8E%E5%B0%94%E8%A1%97%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>本来是要买塔勒布的《随机漫步的傻瓜》，错买成了这本书。不过作为一部不错的股市、债券、期货、期权入门的书倒还不错。</p><h1 id="主要思想">主要思想</h1><p><strong>一定要明白，所谓原则、道理其实都是统计学意义上的合理性，真正的个体并不一定。</strong></p><ul><li>随机漫步（Random walk）<br /></li><li>均值回归<br />金融万有引力定律<br /></li><li>风险管理<br />不同年龄的风险承受能力不同，适配不同的投资组合。<br />杠杆的风险<br /></li><li>指数投资<br />作者重点推荐的投资思想，其实就是个人不可能持续战胜市场的明智选择。</li></ul><h1 id="价格-or-价值">价格 or 价值</h1><p>股票无非是关注价值还是价格，而价值又可以细分为关注当前价值还是未来价值（增长、成长性），价格关注的是人的心理（普通大众的心理预期），关键是否有人愿意出更高的价格。价值的难点在于需要很高的专业知识和洞察力。</p><h1 id="泡沫-萧条">泡沫 萧条</h1><p>经济发展-&gt;民众手中有钱-&gt;投资渠道少-&gt;从众-&gt;泡沫形成<br />恐慌情绪就像病毒，或者说开闸的洪水，很难软着陆。<br />就像气球，吹大的时候有个过程，当泄气是一瞬间的事情。<strong>上升像爬楼梯，下降像坐电梯。</strong><br /><span id="more"></span></p><h1 id="并购的股市获利套路">并购的股市获利套路</h1><p>高市盈率的公司收购低市盈率的公司。</p><p><img src="/img2/漫步华尔街/漫步华尔街8.jpg" /></p><h1 id="互联网估值">互联网估值</h1><p><img src="/img2/漫步华尔街/漫步华尔街9.jpg" /><br />开始用户数是核心，但产品创新、相对竞品的优势才是长期的价值所在。<br />短期看其实是一种博傻行为。或者说是一种赌博行为。</p><h1 id="比特币">比特币</h1><p>区块链技术、去中心化、分布式记账、匿名，极限2100万枚。通过挖矿（计算机暴力计算）获得。<br />风险：官方不认可，法律风险。</p><h1 id="市盈率和增长率">市盈率和增长率</h1><p><strong>正是市盈率而非股价道出了市场如何对股票进行估值。</strong>但也要考虑未来的增长和成长性考虑进来，所以市盈率往往也包括了未来的价值部分。<br /><img src="/img2/漫步华尔街/漫步华尔街14.jpg" /></p><p>事实上，股价始终是错误的，有效市场假说表明，没有人确切知道股价是过高还是过低。有效市场假说也<strong>不认为</strong>，股价变动漫无目的且反复无常。</p><h1 id="投资最重要的事">投资最重要的事</h1><p>跟趋势（获得收益、确定性）、考虑黑天鹅事件（底线思维、随机性）。所以犹太人的3个三分之一理财策略很有用。</p><h1 id="投资风险的定义">投资风险的定义</h1><p><img src="/img2/漫步华尔街/漫步华尔街20.jpg" /><br /><img src="/img2/漫步华尔街/漫步华尔街21.jpg" /></p><h1 id="投资组合理论">投资组合理论</h1><p>目的：降低风险。<br />马科维茨，Harry Markowitz。<br /><strong>现代投资组合理论（MPT）：可将具有风险（波动性）的股票放在投资组合当中，使得投资组合的整体风险小于其中所有单只股票的风险。</strong></p><p><img src="/img2/漫步华尔街/漫步华尔街22.jpg" /><br /><img src="/img2/漫步华尔街/漫步华尔街23.jpg" /><br /><img src="/img2/漫步华尔街/漫步华尔街24.jpg" /><br /><img src="/img2/漫步华尔街/漫步华尔街25.jpg" /></p><p>投资组合风险<br /><img src="/img2/漫步华尔街/漫步华尔街26.jpg" /><br /><img src="/img2/漫步华尔街/漫步华尔街27.jpg" /><br /><img src="/img2/漫步华尔街/漫步华尔街28.jpg" /><br /><img src="/img2/漫步华尔街/漫步华尔街29.jpg" /><br /><img src="/img2/漫步华尔街/漫步华尔街30.jpg" /></p><p>量化分析师<br /><img src="/img2/漫步华尔街/漫步华尔街31.jpg" /><br /><img src="/img2/漫步华尔街/漫步华尔街32.jpg" /></p><h1 id="行为金融学">行为金融学</h1><p>人的非理性行为：</p><ul><li>过度自信<br /></li><li>判断偏差<br /><img src="/img2/漫步华尔街/漫步华尔街33.jpg" /><br /></li><li>羊群效应<br /></li><li>风险厌恶<br /><img src="/img2/漫步华尔街/漫步华尔街34.jpg" /><br />非理性的行为造成的偏差往往不会相互抵消，反而经常相互强化。甚至机构还利用这种非理性制造泡沫来获利。<br /><img src="/img2/漫步华尔街/漫步华尔街35.jpg" /></li></ul><p><strong>所以，我们不能完全依赖套利交易来消除市场价格与基础价值之间的偏差。不过长期来说，周期会引导均值回归。</strong></p><h1 id="投资新方法">投资新方法</h1><ul><li>聪明的β<br /></li><li>风险平价</li></ul><h2 id="聪明的β">聪明的β</h2><p>夏普比率:<br /><img src="/img2/漫步华尔街/漫步华尔街38.jpg" /></p><p>定义：<br />考虑某一因素或组合因素：价值胜出、小既是好、市场中存在的一些趋势、低β值股票与高β值股票收益相同。<br />通过承担额外风险而增加收益的投资技术。<br /><img src="/img2/漫步华尔街/漫步华尔街37.jpg" /></p><p>还要考虑各因素之间的相关系数：<br /><img src="/img2/漫步华尔街/漫步华尔街39.jpg" /></p><p>两种投资价值观：</p><ul><li>任何超额收益都代表了对投资组合额外风险的适当补偿；<br /></li><li>任何超额利益之所以产生，是因为存在市场无效性的情况，而不是一味存在风险。</li></ul><h2 id="风险平价">风险平价</h2><p>给低风险资产加杠杆来提升收益。<br /><img src="/img2/漫步华尔街/漫步华尔街40.jpg" /><br /><img src="/img2/漫步华尔街/漫步华尔街41.jpg" /><br /><img src="/img2/漫步华尔街/漫步华尔街42.jpg" /></p><h2 id="风险平价投资组合与传统6040投资组合之比较">风险平价投资组合与传统60/40投资组合之比较</h2><p><img src="/img2/漫步华尔街/漫步华尔街43.jpg" /><br /><img src="/img2/漫步华尔街/漫步华尔街44.jpg" /><br /><img src="/img2/漫步华尔街/漫步华尔街45.jpg" /><br /><img src="/img2/漫步华尔街/漫步华尔街46.jpg" /><br /><img src="/img2/漫步华尔街/漫步华尔街47.jpg" /></p><h1 id="投资手册">投资手册</h1><p><img src="/img2/漫步华尔街/漫步华尔街48.jpg" /><br /><img src="/img2/漫步华尔街/漫步华尔街49.jpg" /></p><p><img src="/img2/漫步华尔街/漫步华尔街50.jpg" /><br /><img src="/img2/漫步华尔街/漫步华尔街51.jpg" /><br /><img src="/img2/漫步华尔街/漫步华尔街52.jpg" /><br /><img src="/img2/漫步华尔街/漫步华尔街53.jpg" /></p><h1 id="生命周期投资指南">生命周期投资指南</h1><p>一个人必须根据其生命周期的不同阶段来制定不同的投资策略。<br />你必须将你对风险持有的态度和你承担风险的能力区分开来。你承担得起的风险取决于你的整体财务状况，包括你的收入类型和收入来源，但不包括投资性收入。<br /><img src="/img2/漫步华尔街/漫步华尔街58.jpg" /><br /><img src="/img2/漫步华尔街/漫步华尔街59.jpg" /></p><p>4%解决方案：你每年的花费的钱不应该超过你储蓄的4%（投资的平均预期收益）。</p><h1 id="选股规则">选股规则</h1><ul><li>所购股票限于盈利增长看起来能够连续五年超平均水平的公司。<br /></li><li>绝不能为一只股票支付超过其坚实基础价值所能理解的价格。（需考虑未来盈利的增长调整目前市盈率）<br /></li><li>买入投资者可以在其预期增长故事之上建立空中楼阁的股票会有帮助。<br /></li><li>尽可能少交易。（持有赚钱的股票，抛掉赔钱的股票）</li></ul><h1 id="衍生品">衍生品</h1><p>衍生品就是一种金融工具，其价值决定于（或衍生于）某个作为基础的标的的资产的价格，譬如股票、债券、货物或商品的价格。<br />金融衍生品：期货合约、期权合约。</p><p>期货、期权最大的问题是杠杆率的问题。<br /><img src="/img2/漫步华尔街/漫步华尔街60.jpg" /><br /><img src="/img2/漫步华尔街/漫步华尔街61.jpg" /><br /><img src="/img2/漫步华尔街/漫步华尔街62.jpg" /><br /><img src="/img2/漫步华尔街/漫步华尔街63.jpg" /><br /><img src="/img2/漫步华尔街/漫步华尔街64.jpg" /><br /><img src="/img2/漫步华尔街/漫步华尔街65.jpg" /></p><p>期货、期权定价策略<br /><img src="/img2/漫步华尔街/漫步华尔街66.jpg" /><br /><img src="/img2/漫步华尔街/漫步华尔街67.jpg" /></p><h1 id="文章摘要">文章摘要</h1><ul><li>博傻理论：greater fool theory<br /></li><li>行为学理论：强调群体心理。<br />行为金融学并非传统经融学的一个分支，它在更好的描述人性方面取代了传统的主流经融学。（人非完全理性）<br /></li><li><strong>一物的价值仅等于他人愿意支付的价格。</strong><br /></li><li>概念股：能讲出令人信服的好故事。<br /></li><li><strong>股市就是城里最大的游戏。</strong><br /></li><li>股票市场只有10%可以从逻辑的角度思考，剩下的90%应从心理方面去分析。<br />从长远来说，未来盈利必定会影响现值，而在短期，占主导地位的因素则是大众情绪。<br /></li><li>复利:72法则。有时也简化成70法则。<br /></li><li><strong>正是市盈率而非股价道出了市场如何对股票进行估值。</strong>但也要考虑未来的增长和成长性考虑进来，所以市盈率往往也包括了未来的价值部分。<br /></li><li>事物的本质很少呈现于表象之上。<br /></li><li>人类天生难以接受随机性。人有追求确定性的偏好。<br /></li><li>对方说的话，需要从对方的利益动机进行分析，不能全信。<br /></li><li><strong>跟趋势（获得收益、确定性）、考虑黑天鹅事件（底线思维、随机性）。所以犹太人的3个三分之一理财策略很有用。</strong><br /></li><li>股票代表对真实资产的索取权。<br /></li><li>在固定账户中坚持不懈的储蓄，无论数目大小，必有好结果。</li></ul><p><img src="/img2/漫步华尔街/漫步华尔街1.jpg" /><br /><img src="/img2/漫步华尔街/漫步华尔街2.jpg" /><br /><img src="/img2/漫步华尔街/漫步华尔街3.jpg" /><br /><img src="/img2/漫步华尔街/漫步华尔街4.jpg" /><br /><img src="/img2/漫步华尔街/漫步华尔街5.jpg" /><br /><img src="/img2/漫步华尔街/漫步华尔街6.jpg" /><br /><img src="/img2/漫步华尔街/漫步华尔街7.jpg" /></p><p><img src="/img2/漫步华尔街/漫步华尔街10.jpg" /><br /><img src="/img2/漫步华尔街/漫步华尔街11.jpg" /><br /><img src="/img2/漫步华尔街/漫步华尔街12.jpg" /><br /><img src="/img2/漫步华尔街/漫步华尔街13.jpg" /></p><p><img src="/img2/漫步华尔街/漫步华尔街15.jpg" /><br /><img src="/img2/漫步华尔街/漫步华尔街16.jpg" /><br /><img src="/img2/漫步华尔街/漫步华尔街17.jpg" /><br /><img src="/img2/漫步华尔街/漫步华尔街18.jpg" /><br /><img src="/img2/漫步华尔街/漫步华尔街19.jpg" /></p><p><img src="/img2/漫步华尔街/漫步华尔街36.jpg" /></p><p><img src="/img2/漫步华尔街/漫步华尔街54.jpg" /><br /><img src="/img2/漫步华尔街/漫步华尔街55.jpg" /><br /><img src="/img2/漫步华尔街/漫步华尔街56.jpg" /><br /><img src="/img2/漫步华尔街/漫步华尔街57.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 原则 </tag>
            
            <tag> 投资 </tag>
            
            <tag> 股票 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《投资最重要的事》读书心得</title>
      <link href="/2023/12/29/%E3%80%8A%E6%8A%95%E8%B5%84%E6%9C%80%E9%87%8D%E8%A6%81%E7%9A%84%E4%BA%8B%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2023/12/29/%E3%80%8A%E6%8A%95%E8%B5%84%E6%9C%80%E9%87%8D%E8%A6%81%E7%9A%84%E4%BA%8B%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<h1 id="如何才能战胜市场">如何才能战胜市场？</h1><ol type="1"><li>稳妥的思考框架。<br />原则，方法论。<br /></li><li>情绪控制<br />必须自己做到。压力应对。</li></ol><p>面对未来的时机选择，风险管理。</p><p>巴菲特：价值投资，在别人恐惧时贪婪，在别人贪婪时恐惧。</p><h1 id="降龙十八掌">降龙十八掌</h1><p>投资需要平衡很多基本问题，面面俱到，又有机统一。</p><h2 id="学习第二层次思维">学习第二层次思维</h2><ul><li>环境的不可控+人心里的波动</li><li>第二层次思维<br /><strong>在大众思维的基础上进行思考，反过来想。</strong><br /><img src="/img2/投资最重要的事/投资者最重要的事6.jpg" /><br /></li><li><strong>第二层思维需要考虑的东西</strong><br /><img src="/img2/投资最重要的事/投资者最重要的事7.jpg" /></li></ul><span id="more"></span><h2 id="理解市场有效性及局限性">理解市场有效性及局限性</h2><p>超过别人，要么有信息优势，要么有分析优势。<br />市场的局限性，无效性。<br /><img src="/img2/投资最重要的事/投资者最重要的事11.jpg" /></p><h2 id="准确估计价值">准确估计价值</h2><ul><li><p>两种投资方法<br /><img src="/img2/投资最重要的事/投资者最重要的事12.jpg" /><br /></p></li><li><p>基本面分析的两种方法：价值投资和成长型投资。<br />成长型投资介于枯燥乏味的价值投资和冲动刺激的动量投资之间。<br />成长型投资关注未来，而价值投资强调当前，但不可避免的要面对未来。<br /><img src="/img2/投资最重要的事/投资者最重要的事13.jpg" /><br /><strong>在投资领域中，正确并不等于正确性能够被立即证实。（长期思维，意志力，独立思考等）</strong><br />巴菲特：只有当潮水退出的时候，你才会发现谁在裸泳。<br /><img src="/img2/投资最重要的事/投资者最重要的事14.jpg" /></p><p>关键是要是正确的，而这只有上帝知道。<br /><img src="/img2/投资最重要的事/投资者最重要的事15.jpg" /></p></li></ul><h2 id="价格与价值的关系">价格与价值的关系</h2><p><img src="/img2/投资最重要的事/投资者最重要的事16.jpg" /><br /><img src="/img2/投资最重要的事/投资者最重要的事17.jpg" /><br />心理因素对价格的影响</p><p><img src="/img2/投资最重要的事/投资者最重要的事18.jpg" /><br /><img src="/img2/投资最重要的事/投资者最重要的事19.jpg" /></p><p><img src="/img2/投资最重要的事/投资者最重要的事20.jpg" /><br /><img src="/img2/投资最重要的事/投资者最重要的事21.jpg" /></p><p>低于价值买进并非万无一失，但它是我们最好的机会。</p><h2 id="风险管理">风险管理</h2><p>理解风险、识别风险、控制风险;</p><h3 id="理解风险">理解风险</h3><p>投资只关乎一件事：应对未来。<br />没有人能确切的预知未来，所以风险是不可避免的。因此，应对风险是投资中一个必不可少的要素。<br />收益和风险是硬币的两面。<br />风险补偿：为了吸引投资，风险更高的投资必须提供更好的收益前景、更高的预期收益。<br />风险收益曲线：<br /><img src="/img2/投资最重要的事/投资者最重要的事23.jpg" /><br /><img src="/img2/投资最重要的事/投资者最重要的事22.jpg" /><br /><strong>在考虑投资组合时要牢记，极低概率事件会影响到你的大部分投资。</strong><br />未来有很多种可能，而结果却只有一个。期望值的管理。</p><h3 id="识别风险">识别风险</h3><p>杰出的投资者需要创造收益和控制风险并重。<br />风险意味着即将发生的结果的不确定性，以及不利结果发生事损失的不确定性。<br />普通相信没有风险本身就是最大的风险，因为至于投资者适当规避风险时，预期收益中才会包含风险溢价。<br /><strong>无忧无虑才是真正最危险的事。生于忧患死于安乐的道理。</strong><br />关于风险控制的神话是很少成真的，风险是不能消除的，只能被转移和分散。<br />企业也一样。<br /><img src="/img2/投资最重要的事/投资者最重要的事24.jpg" /><br /><img src="/img2/投资最重要的事/投资者最重要的事25.jpg" /><br /><img src="/img2/投资最重要的事/投资者最重要的事26.jpg" /><br />上两图的曲线斜率代表了风险的溢价量。</p><h3 id="控制风险">控制风险</h3><p><img src="/img2/投资最重要的事/投资者最重要的事27.jpg" /><br /><img src="/img2/投资最重要的事/投资者最重要的事28.jpg" /><br /><img src="/img2/投资最重要的事/投资者最重要的事29.jpg" /><br />有时，你必须爬上枝头，因为果实就在那。</p><h2 id="关注周期">关注周期</h2><p>万物皆有周期，周期永远胜在最后。<br />你不能预测，但可以准备。<br />信贷周期，周期具有自我修正能力，而信贷周期是驱动经济周期波动的因素之一。<br /><img src="/img2/投资最重要的事/投资者最重要的事30.jpg" /></p><h2 id="钟摆意识">钟摆意识</h2><p><img src="/img2/投资最重要的事/投资者最重要的事31.jpg" /><br />学者们认为投资者对待风险态度是一成不变的，但实际上它的波动非常大。<br />亏损的风险和错失机会的风险。</p><h2 id="抵御消极影响">抵御消极影响</h2><p>人性的弱点</p><p><img src="/img2/投资最重要的事/投资者最重要的事32.jpg" /></p><p>博傻：参与+选择时机及时退出。 关键是退出的时机太难选择了，掺杂了太多的人性弱点。</p><p>比较心理，将自己与他人进行过多比较是一种侵蚀意志的过程，虽然这是天性使然，而且，这会给你施加过多的压力。</p><p><img src="/img2/投资最重要的事/投资者最重要的事33.jpg" /></p><p>理性者有可能屈服于情感的破坏性力量。</p><h2 id="逆向投资">逆向投资</h2><p>不合群，独树一帜，不随潮流，标新立异，这种行为是孤独并且令人不安的。</p><h2 id="寻找便宜货">寻找便宜货</h2><p>均值回归。</p><h2 id="耐心等待机会">耐心等待机会</h2><p><img src="/img2/投资最重要的事/投资者最重要的事34.jpg" /></p><p>进行有效客户管理的关键总是在于----降低他们的期望。</p><h2 id="认识预测的局限性">认识预测的局限性</h2><h2 id="正确认识自身">正确认识自身</h2><p>市场温度的测量<br /><img src="/img2/投资最重要的事/投资者最重要的事35.jpg" /><br /><img src="/img2/投资最重要的事/投资者最重要的事36.jpg" /><br /><img src="/img2/投资最重要的事/投资者最重要的事37.jpg" /><br /><img src="/img2/投资最重要的事/投资者最重要的事38.jpg" /><br /><strong>做个表？？？</strong></p><h2 id="重视运气">重视运气</h2><p>运气的重要性：实力让你进入决赛，但运气决定了谁是冠军。<br />谋事在人成事在天。认命。</p><p><img src="/img2/投资最重要的事/投资者最重要的事39.jpg" /></p><p><img src="/img2/投资最重要的事/投资者最重要的事40.jpg" /></p><p>未然历史</p><p>防御性投资</p><p><img src="/img2/投资最重要的事/投资者最重要的事41.jpg" /></p><h2 id="多元化投资">多元化投资</h2><p><img src="/img2/投资最重要的事/投资者最重要的事42.jpg" /><br /><img src="/img2/投资最重要的事/投资者最重要的事43.jpg" /></p><p>因此，防守----重点在于规避错误----是每一场伟大的投资游戏的重要组成部分。因为市场的不可控性，类似业余比赛。</p><p><img src="/img2/投资最重要的事/投资者最重要的事44.jpg" /><br /><img src="/img2/投资最重要的事/投资者最重要的事45.jpg" /></p><h2 id="避免错误">避免错误</h2><p>金融危机：黑天鹅事件+高杠杆</p><p><img src="/img2/投资最重要的事/投资者最重要的事46.jpg" /></p><p>考虑极端事件的连锁效应</p><p><img src="/img2/投资最重要的事/投资者最重要的事47.jpg" /></p><p>模型无法预测变化，无法预测异常时期。（模型就是随机性的方面，模型忽略了很多要素和细节，所以在随机事件面前，模型是无能为力的。）</p><h2 id="增值的意义">增值的意义</h2><p>投资组合公式：<br /><img src="/img2/投资最重要的事/投资者最重要的事48.jpg" /></p><h2 id="合理预期">合理预期</h2><p>伏尔泰：完美是优秀的敌人。</p><h1 id="最重要的事">最重要的事</h1><h1 id="文章摘要">文章摘要</h1><ul><li>在你得不到想要的东西时，你得到的是经验；<br />唯一不变的收获是问心无愧。<br /></li><li>最有价值的经验是在困难时期学到的。<br />逆境才会让人进步。<br /></li><li>所有的投资者都战胜市场是不可能的，因为他们本身就是市场。<br /></li><li>万物皆有周期，人生也一样。</li></ul><p><img src="/img2/投资最重要的事/投资者最重要的事1.jpg" /><br /><img src="/img2/投资最重要的事/投资者最重要的事2.jpg" /><br /><img src="/img2/投资最重要的事/投资者最重要的事3.jpg" /><br /><img src="/img2/投资最重要的事/投资者最重要的事4.jpg" /><br /><img src="/img2/投资最重要的事/投资者最重要的事5.jpg" /></p><p><img src="/img2/投资最重要的事/投资者最重要的事8.jpg" /><br /><img src="/img2/投资最重要的事/投资者最重要的事9.jpg" /></p><p><img src="/img2/投资最重要的事/投资者最重要的事10.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 原则 </tag>
            
            <tag> 投资 </tag>
            
            <tag> 股票 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《几何学的力量》读书心得</title>
      <link href="/2023/12/16/%E3%80%8A%E5%87%A0%E4%BD%95%E5%AD%A6%E7%9A%84%E5%8A%9B%E9%87%8F%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2023/12/16/%E3%80%8A%E5%87%A0%E4%BD%95%E5%AD%A6%E7%9A%84%E5%8A%9B%E9%87%8F%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>几何学最早是丈量世界而诞生，从而抽象了现实的世界。<br />一个人想要谈论某个事物，前提是他必须弄清楚该事物的定义。定义和概念是很重要的，于数学及现实皆然。<br />学几何学的是其中的逻辑思想，特别是欧几里得几何的研究问题的方法。整个体系的建设，从几个不证自明的公理出发，通过推理、演绎，建立相关的定理、命题，搭建了整个壮丽的几何学大厦。基于不同的公理可以构建不同的几何体系，例如欧氏几何和非欧几何。<br />数学有很大一部分作用在于，厘清哪些事物是我们暂时或者永远都不用关心的，这种选择性注意是人类理性的基本组成部分。这其实也是抽象的过程，抽象就是省略细节，关注你研究重点的过程，例如算术关注的是数量，而事物的本身是什么并不重要；而拓扑学（庞加莱创立，研究洞的问题）关注的是形状，大小、距离反而不关心；而物理关心的是位置和速度；欧氏几何研究的是长度、面积等不变量。</p><h1 id="给不同的事物赋予相同的名称">给不同的事物赋予相同的名称</h1><p>其实就是考虑研究的特征，把形同特征的事物归类为同一事物，也是一种抽象的过程。<br />庞加莱：数学是一门给不同事物赋予相同的名称的艺术。<br /><span id="more"></span></p><h2 id="变换的几种形式">变换的几种形式</h2><p><img src="/img2/几何学的力量/几何学的力量10.jpg" /><br /><img src="/img2/几何学的力量/几何学的力量11.jpg" /><br />在不同变换选择，认为该对称变换后图形相同（为真），对应不同的数学研究方向体系。研究的是变换后，不会导致哪些重要的量发生变化，简而言之，就是对称变换和守恒定律之间的基本关系。找出你感兴趣的对称变换的守恒量，每一种对称性都有一个相关的守恒定律。<br />研究对称性变换的不变量是一个方向的基础。</p><h1 id="随机游走">随机游走</h1><p>马尔可夫链，类似大数定律，但是与大数定律不同的是，马尔可夫链研究的不要求独立性。<br /><img src="/img2/几何学的力量/几何学的力量12.jpg" /><br /><img src="/img2/几何学的力量/几何学的力量13.jpg" /><br /><img src="/img2/几何学的力量/几何学的力量14.jpg" /><br /><img src="/img2/几何学的力量/几何学的力量15.jpg" /><br />香农最先意识到，马尔可夫链不仅可以用于分析文本，还可以用于生成文本。（ChatGPT的核心思想）</p><p>网球比赛的随机游走分析。<br /><img src="/img2/几何学的力量/几何学的力量17.jpg" /></p><h1 id="公钥密码体制">公钥密码体制</h1><p>公钥秘钥体制在很大程度上取决于你能想出两个大的素数作为私钥，这里大是指300位数或者更大的数。但你去哪里找到他们呢？<br />费马小定律。</p><h1 id="机器学习">机器学习</h1><h2 id="梯度下降法">梯度下降法</h2><p>AI，不同策略下损失函数梯度下降。<br /><img src="/img2/几何学的力量/几何学的力量18.jpg" /></p><p>其实是神经网络中不同参数w下，对应策略函数结果导致损失函数的下降幅度最大的w选择。</p><h2 id="强化随机选择探索">强化+随机选择（探索）</h2><p>线性回归是一种比较合理的策略+神经网络（非线性）<br />神经网络图<br /><img src="/img2/几何学的力量/几何学的力量19.jpg" /></p><h1 id="距离的概念">距离的概念</h1><p>等距、等时、层级等等。无论在多么抽象的环境里，我们都能创造出距离的概念，还有几何图形的概念。<br />人格特质地图<br /><img src="/img2/几何学的力量/几何学的力量20.jpg" /><br />google 研发的数学模型 Word2vec，号称“所有英语单词的地图”</p><h1 id="的定义">0/0的定义</h1><p><img src="/img2/几何学的力量/几何学的力量21.jpg" /></p><h1 id="几何平均数">几何平均数</h1><p><img src="/img2/几何学的力量/几何学的力量22.jpg" /></p><p>群体免疫的原理<br /><img src="/img2/几何学的力量/几何学的力量23.jpg" /></p><h1 id="传染病的sir模型">传染病的SIR模型</h1><p><img src="/img2/几何学的力量/几何学的力量24.jpg" /><br /><img src="/img2/几何学的力量/几何学的力量25.jpg" /></p><p><strong>看图，上升像坐电梯，下降像走楼梯。</strong><br /><img src="/img2/几何学的力量/几何学的力量26.jpg" /></p><h1 id="斐波那契数列和黄金比例">斐波那契数列和黄金比例</h1><p><img src="/img2/几何学的力量/几何学的力量28.jpg" /></p><h1 id="移动运算和俯冲运算本征数列">移动运算和俯冲运算本征数列</h1><p><img src="/img2/几何学的力量/几何学的力量29.jpg" /><br /><img src="/img2/几何学的力量/几何学的力量30.jpg" /><br /><img src="/img2/几何学的力量/几何学的力量31.jpg" /><br /><img src="/img2/几何学的力量/几何学的力量32.jpg" /><br /><img src="/img2/几何学的力量/几何学的力量33.jpg" /><br /><img src="/img2/几何学的力量/几何学的力量34.jpg" /></p><h1 id="投影">投影</h1><p><img src="/img2/几何学的力量/几何学的力量37.jpg" /><br /><img src="/img2/几何学的力量/几何学的力量38.jpg" /><br /><img src="/img2/几何学的力量/几何学的力量39.jpg" /></p><h1 id="文章摘要">文章摘要</h1><ul><li>就好比你向一位盲人解释什么事色彩。<br /></li><li>化圆为方，于1882年被证明是不可能的。<br /></li><li>敢于放手尝试的雄心和敢于接受失败的谦逊。<br /></li><li>只有自我教育才是真正的教育。<br /></li><li>直觉和逻辑是数学思想不可或缺的两大支柱。<br /></li><li>菲欧几何是巨大的数学领域的基础，包括描述我们所在物理空间的数学。<br /></li><li>莫比乌斯带只有一个面。<br /></li><li>纯粹数学就是逻辑思想的诗篇。<br /></li><li>数学家渴望用符号而不是文字来开展研究工作。<br /></li><li>随机游走理论变成金融数学的标准工具。<br /></li><li>从数学的角度讲，投机者的预期收益是零。 （所以学数学的人都特别理性）<br /></li><li>google = 10^100<br /></li><li>所有模型都是错的，但其中有些是有用的。</li></ul><p>PS： 随机游走的书！<br />公钥私钥的原理<br />活在当下，允许一切发生，一切都是最好的安排，除了生死都是小事。</p><p><img src="/img2/几何学的力量/几何学的力量1.jpg" /><br /><img src="/img2/几何学的力量/几何学的力量2.jpg" /><br /><img src="/img2/几何学的力量/几何学的力量3.jpg" /><br /><img src="/img2/几何学的力量/几何学的力量4.jpg" /><br /><img src="/img2/几何学的力量/几何学的力量5.jpg" /><br /><img src="/img2/几何学的力量/几何学的力量6.jpg" /><br /><img src="/img2/几何学的力量/几何学的力量7.jpg" /><br /><img src="/img2/几何学的力量/几何学的力量8.jpg" /><br /><img src="/img2/几何学的力量/几何学的力量9.jpg" /></p><p><img src="/img2/几何学的力量/几何学的力量16.jpg" /></p><p><img src="/img2/几何学的力量/几何学的力量27.jpg" /></p><p><img src="/img2/几何学的力量/几何学的力量35.jpg" /><br /><img src="/img2/几何学的力量/几何学的力量36.jpg" /></p><p><img src="/img2/几何学的力量/几何学的力量40.jpg" /><br /><img src="/img2/几何学的力量/几何学的力量41.jpg" /><br /><img src="/img2/几何学的力量/几何学的力量42.jpg" /><br /><img src="/img2/几何学的力量/几何学的力量43.jpg" /><br /><img src="/img2/几何学的力量/几何学的力量44.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数学 </tag>
            
            <tag> 几何 </tag>
            
            <tag> 科普 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《格局》读书心得</title>
      <link href="/2023/12/07/%E3%80%8A%E6%A0%BC%E5%B1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2023/12/07/%E3%80%8A%E6%A0%BC%E5%B1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>很早就看完的一本书，吴军写的，放车上，一直没有做笔记，书套都找不到了 :），今天在咖啡厅就顺便做了，不过可能经济不景气的原因，咖啡厅的人倒是很少。<br />本书副标题“世界永远不缺聪明人”，一本心灵鸡汤汇编而成的书，有点像《论语》，主要是教做人的道理。想起赵普的半部论语治天下，这本书倒也可以放车上，偶尔看看。</p><h1 id="看行业看产业">看行业、看产业</h1><p>例如现在的新能源？<br /><img src="/img2/格局/格局4.jpg" /><br /><img src="/img2/格局/格局5.jpg" /></p><span id="more"></span><h1 id="方法论">方法论</h1><p><img src="/img2/格局/格局3.jpg" /><br /><img src="/img2/格局/格局6.jpg" /><br /><img src="/img2/格局/格局7.jpg" /></p><h1 id="向上管理主动性">向上管理、主动性</h1><p><strong>向上管理是战略问头，勤恳工作是战术问题。不能用战术上的勤奋掩盖战略上的懒惰。</strong><br />在工作中，主动性不仅体现在像老黄牛一样把本职工作做好，还要主动和领导沟通，承担更多、更重要任务。通常情况下，经理最了解和自己走得比较近、经常主动谈工作的下属的情况。<br />主动做事的收益或许不会在一两天内体现出来，但是长期坚持下来，主动做事的人就能和其他人拉开距离。到了关键时刻，只有主动做事，奇迹才会发生。</p><h1 id="辩证法">辩证法</h1><p>全面看问题，反过来想，考虑反对意见、不好的情况后再开始行动。</p><h1 id="分享利益">分享利益</h1><p>分享利益是我们将来作为职业人士必须具备的基本素质。</p><h1 id="感恩">感恩</h1><p>勿因小恶而忘大善。<br /><img src="/img2/格局/格局9.jpg" /></p><h1 id="尽人事听天命">尽人事、听天命</h1><p><strong>问心无愧是我们唯一稳得的报酬。</strong><br /><img src="/img2/格局/格局11.jpg" /></p><h1 id="跳出思维定式">跳出思维定式</h1><p>换个环境的作用。<br /><img src="/img2/格局/格局14.jpg" /><br /><img src="/img2/格局/格局15.jpg" /><br /><img src="/img2/格局/格局17.jpg" /></p><h1 id="超越免费的5个法则">超越免费的5个法则</h1><p>时效性、个性化、可用性（易理解性）、可靠性、黏性。</p><ul><li>时效性能够超越免费<br /><img src="/img2/格局/格局32.jpg" /><br /></li><li>突出个性化<br /></li><li>提供具有可用性的产品和易理解性的服务<br />在任何时代，把事情解释清楚这个本领都可以变成一个很赚钱的生意。<br /></li><li>提供可靠而易得的服务<br /></li><li>打造具有数据黏性的服务<br /><img src="/img2/格局/格局33.jpg" /></li></ul><h1 id="李嘉图定律">李嘉图定律</h1><p>在信息时代，李嘉图定律带来的势差放大效应。赢者通吃效应。<br /><img src="/img2/格局/格局34.jpg" /><br /><img src="/img2/格局/格局35.jpg" /><br /><img src="/img2/格局/格局36.jpg" /><br /><img src="/img2/格局/格局37.jpg" /></p><h1 id="文章摘要">文章摘要</h1><ul><li>态度：attitude，格局：altitude，差一个字母。<br /></li><li><strong>王永庆：人有多大的气度，就有多大的生意。<br />有饭吃，有地方住，就要有精气神。例如有社保就OK？:)<br />在诸多目标中，终极目标当属生活本身。</strong><br /></li><li>核心公司核心业务<br /></li><li>民主只在公权领域，私企肯定是老板专权的，这样才能达到最优和高效率。独立决断是一个领导者负责任的体现。<br /></li><li>就事论事;就是先把当下事情解决了，其它事情以后说。<br /></li><li>运气守恒定律。<br /></li><li><strong>至少要在心里否认自己一次，这是很多人不愿意去做的。 确实，人否认自己太难了。</strong><br /></li><li>教育的目的是培养既能适应社会，也能愉悦自己的人。<br /></li><li>做大事、做困难的事，价值高的事情总是困难的。<br /></li><li>只有在金钱和地位面前丢掉奴性，保持自由人心态，才能赢得对方的尊重。<br /></li><li>尼采说过，除了神灵、野兽和哲学家，人都忍受不了孤独。<br /></li><li><strong>真正安全的，恰恰是拥抱风险。</strong> 保持自己进步的能力，而不是幻想找到一个稳定的行业和工作。</li></ul><p><img src="/img2/格局/格局1.jpg" /><br /><img src="/img2/格局/格局2.jpg" /></p><p><img src="/img2/格局/格局8.jpg" /></p><p><img src="/img2/格局/格局10.jpg" /></p><p><img src="/img2/格局/格局12.jpg" /><br /><img src="/img2/格局/格局13.jpg" /></p><p><img src="/img2/格局/格局16.jpg" /></p><p><img src="/img2/格局/格局18.jpg" /><br /><img src="/img2/格局/格局19.jpg" /><br /><img src="/img2/格局/格局20.jpg" /><br /><img src="/img2/格局/格局21.jpg" /><br /><img src="/img2/格局/格局22.jpg" /><br /><img src="/img2/格局/格局23.jpg" /><br /><img src="/img2/格局/格局24.jpg" /><br /><img src="/img2/格局/格局25.jpg" /><br /><img src="/img2/格局/格局26.jpg" /><br /><img src="/img2/格局/格局27.jpg" /><br /><img src="/img2/格局/格局28.jpg" /><br /><img src="/img2/格局/格局29.jpg" /></p><p><img src="/img2/格局/格局30.jpg" /><br /><img src="/img2/格局/格局31.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人生道理 </tag>
            
            <tag> 鸡汤 </tag>
            
            <tag> 做人 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《魔鬼经济学》读书心得</title>
      <link href="/2023/12/03/%E3%80%8A%E9%AD%94%E9%AC%BC%E7%BB%8F%E6%B5%8E%E5%AD%A6%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2023/12/03/%E3%80%8A%E9%AD%94%E9%AC%BC%E7%BB%8F%E6%B5%8E%E5%AD%A6%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>一本比较别具一格的经济类的书，<strong>一共四本（佩服作者这么多口水）</strong>，以案例为出发点，引出经济学的知识。很多观点还是别具一格，令人耳目一新，从常人想不到的角度提出一些新的、还挺有道理的观点。不过因为是以案例为主，整书知识显得有点凌乱，不够系统。<br />茶余饭后消遣看看可以，猎奇一下。想真正的系统的了解学习经济学就帮不上忙了。<br /><strong>道德代表了人类希望这个世界应该如何运转，而经济学则代表着其实际的运转方式。</strong><br />经济学，从根本而言，是一门研究动机的学科：人如何得偿所愿或满足所需，尤其是其他人欲求相同的情况下。<br />人的动机分三大类:总体来说是利己、趋利避害。因为价值观不同，在面临三者的取舍时，不同人会做出不同的选择，例如海瑞会选择道德一样，所以说传统经济学中提出的经济人是理性的（经济动机），是不完全的。</p><ul><li>经济动机<br /></li><li>社会动机： 名声、从众心理、别人看法。<br /></li><li>道德动机： 情绪价值、名声、负罪感等，自身的价值观。人心中的道德指南针，类似古话说的“心中的一杆秤”</li></ul><h1 id="利用信息差获利">利用信息差获利</h1><p>信息就是权力、就是资源。互联网的作用本质上是消除了信息差。有句话说，领导的主要权力就是信息，领导往往比下面人的信息更多，更充分。<br />有时面对专家的咨询反而加剧了信息的不对称现象，因为专家会利用手中的信息资源优势来获取利益。或者利用信息差来制造恐惧心理，创造需求。<br />不说假话，但真话不说全，也是一种利用信息不对称获得优势地位的手段。<br /><span id="more"></span><br /><img src="/img2/魔鬼经济学/魔鬼经济学3.jpg" /></p><h1 id="可控制的风险和不可控制的风险">可控制的风险和不可控制的风险</h1><p><img src="/img2/魔鬼经济学/魔鬼经济学4.jpg" /><br />风险=危害+愤怒<br /><img src="/img2/魔鬼经济学/魔鬼经济学5.jpg" /></p><h1 id="非预期后果法则">非预期后果法则</h1><p><img src="/img2/魔鬼经济学/魔鬼经济学29.jpg" /></p><h1 id="公司的问题">公司的问题</h1><p>谁做皇帝新装的那个小孩？<br /><img src="/img2/魔鬼经济学/魔鬼经济学35.jpg" /></p><h1 id="小问题入手">小问题入手</h1><p><img src="/img2/魔鬼经济学/魔鬼经济学36.jpg" /><br /><img src="/img2/魔鬼经济学/魔鬼经济学37.jpg" /></p><p><img src="/img2/魔鬼经济学/魔鬼经济学38.jpg" /><br />孩子不怕大声说出自己喜欢的东西，但人长大之后，这个特质就神奇的消失了。<br />寻找乐趣、浅显思考、不惧惯俗，这就是小孩思考的特征。成年人最大的危险是虚伪。</p><h1 id="人会对诱因做出反应">人会对诱因做出反应</h1><p>例如绩效考核，这当中会出现非预期后果，以及委托人的问题。</p><p><img src="/img2/魔鬼经济学/魔鬼经济学39.jpg" /><br /><img src="/img2/魔鬼经济学/魔鬼经济学40.jpg" /><br /><img src="/img2/魔鬼经济学/魔鬼经济学41.jpg" /><br /><img src="/img2/魔鬼经济学/魔鬼经济学42.jpg" /><br /><img src="/img2/魔鬼经济学/魔鬼经济学43.jpg" /><br /><img src="/img2/魔鬼经济学/魔鬼经济学44.jpg" /></p><h1 id="如何说服别人">如何说服别人</h1><p><img src="/img2/魔鬼经济学/魔鬼经济学50.jpg" /><br /><img src="/img2/魔鬼经济学/魔鬼经济学51.jpg" /><br /><img src="/img2/魔鬼经济学/魔鬼经济学52.jpg" /><br /><img src="/img2/魔鬼经济学/魔鬼经济学53.jpg" /><br /><img src="/img2/魔鬼经济学/魔鬼经济学54.jpg" /></p><h1 id="讲故事的能力">讲故事的能力</h1><p><img src="/img2/魔鬼经济学/魔鬼经济学55.jpg" /><br /><img src="/img2/魔鬼经济学/魔鬼经济学56.jpg" /><br /><img src="/img2/魔鬼经济学/魔鬼经济学57.jpg" /><br /><img src="/img2/魔鬼经济学/魔鬼经济学58.jpg" /></p><h1 id="学会放弃">学会放弃</h1><p><strong>放弃是魔鬼式思考的核心。</strong><br /><img src="/img2/魔鬼经济学/魔鬼经济学59.jpg" /><br /><img src="/img2/魔鬼经济学/魔鬼经济学60.jpg" /><br /><img src="/img2/魔鬼经济学/魔鬼经济学61.jpg" /><br /><img src="/img2/魔鬼经济学/魔鬼经济学62.jpg" /><br /><img src="/img2/魔鬼经济学/魔鬼经济学63.jpg" /><br /><img src="/img2/魔鬼经济学/魔鬼经济学64.jpg" /><br /><img src="/img2/魔鬼经济学/魔鬼经济学65.jpg" /></p><h1 id="文章摘要">文章摘要</h1><ul><li>多数暴力组织的一个核心信条就是：多数武力威胁从来不会超出威胁这一步。<br /></li><li>保持神秘性才更有吸引力。<br /></li><li><strong>委托代理问题：在某项任务中，委托方和代理方似乎有着相同的目标，但事实上其动机各不相同。例如老板和职业经理人的关系就是这样。</strong><br /></li><li><strong>有时候在人生中，径直走向中心是最英勇的举动。</strong><br /></li><li>政客通常会回避有争议的话题。<br /></li><li>国家医疗服务体系，是英国人拥有的最接近宗教的事物。<br /></li><li><strong>如果不懂装懂会带来破坏性的后果，那为何人们还在继续不懂装懂呢？这很简单，说“我不知道”的代价通常比出错的代价还要高，至少对说话者本身来说如此。</strong><br /></li><li>美国空乘人员不像其它行业一样，他们是不收小费的。<br /></li><li>地位越往上走，就越是利益的深水区，利益会更错综复杂。<br /></li><li>心理账户：等值金额在不同场景带来的心理愉悦感是不一样的。<br /></li><li>我不希望你们十全十美，但我希望你们精益求精。<br /></li><li><strong>偏见是一种文化局限性的一种内生体现。</strong><br /></li><li><strong>利益最大化要求将某项行为的边际成本设定等同于其边际效益。</strong><br /></li><li>凡事反过来想，其实就是辩证法全面看问题的观点。正面思考就是你常规能想到的，所以需要反过来思考。<br /></li><li>在处理人性问题时，需要的是智慧。</li></ul><p><img src="/img2/魔鬼经济学/魔鬼经济学1.jpg" /><br /><img src="/img2/魔鬼经济学/魔鬼经济学30.jpg" /></p><p><img src="/img2/魔鬼经济学/魔鬼经济学2.jpg" /></p><p><img src="/img2/魔鬼经济学/魔鬼经济学6.jpg" /><br /><img src="/img2/魔鬼经济学/魔鬼经济学7.jpg" /><br /><img src="/img2/魔鬼经济学/魔鬼经济学8.jpg" /><br /><img src="/img2/魔鬼经济学/魔鬼经济学9.jpg" /></p><p><img src="/img2/魔鬼经济学/魔鬼经济学10.jpg" /><br /><img src="/img2/魔鬼经济学/魔鬼经济学11.jpg" /><br /><img src="/img2/魔鬼经济学/魔鬼经济学12.jpg" /><br /><img src="/img2/魔鬼经济学/魔鬼经济学13.jpg" /><br /><img src="/img2/魔鬼经济学/魔鬼经济学14.jpg" /><br /><img src="/img2/魔鬼经济学/魔鬼经济学15.jpg" /><br /><img src="/img2/魔鬼经济学/魔鬼经济学16.jpg" /><br /><img src="/img2/魔鬼经济学/魔鬼经济学17.jpg" /><br /><img src="/img2/魔鬼经济学/魔鬼经济学18.jpg" /><br /><img src="/img2/魔鬼经济学/魔鬼经济学19.jpg" /></p><p><img src="/img2/魔鬼经济学/魔鬼经济学20.jpg" /><br /><img src="/img2/魔鬼经济学/魔鬼经济学21.jpg" /><br /><img src="/img2/魔鬼经济学/魔鬼经济学22.jpg" /><br /><img src="/img2/魔鬼经济学/魔鬼经济学23.jpg" /><br /><img src="/img2/魔鬼经济学/魔鬼经济学24.jpg" /><br /><img src="/img2/魔鬼经济学/魔鬼经济学25.jpg" /><br /><img src="/img2/魔鬼经济学/魔鬼经济学26.jpg" /><br /><img src="/img2/魔鬼经济学/魔鬼经济学27.jpg" /><br /><img src="/img2/魔鬼经济学/魔鬼经济学28.jpg" /></p><p><img src="/img2/魔鬼经济学/魔鬼经济学31.jpg" /><br /><img src="/img2/魔鬼经济学/魔鬼经济学32.jpg" /><br /><img src="/img2/魔鬼经济学/魔鬼经济学33.jpg" /><br /><img src="/img2/魔鬼经济学/魔鬼经济学34.jpg" /></p><p><img src="/img2/魔鬼经济学/魔鬼经济学45.jpg" /><br /><img src="/img2/魔鬼经济学/魔鬼经济学46.jpg" /><br /><img src="/img2/魔鬼经济学/魔鬼经济学47.jpg" /><br /><img src="/img2/魔鬼经济学/魔鬼经济学48.jpg" /><br /><img src="/img2/魔鬼经济学/魔鬼经济学49.jpg" /></p><p><img src="/img2/魔鬼经济学/魔鬼经济学66.jpg" /><br /><img src="/img2/魔鬼经济学/魔鬼经济学67.jpg" /><br /><img src="/img2/魔鬼经济学/魔鬼经济学68.jpg" /><br /><img src="/img2/魔鬼经济学/魔鬼经济学69.jpg" /></p><p><img src="/img2/魔鬼经济学/魔鬼经济学70.jpg" /><br /><img src="/img2/魔鬼经济学/魔鬼经济学71.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 经济学 </tag>
            
            <tag> 案例 </tag>
            
            <tag> 另类 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《债务危机》读书心得</title>
      <link href="/2023/11/04/%E3%80%8A%E5%80%BA%E5%8A%A1%E5%8D%B1%E6%9C%BA%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2023/11/04/%E3%80%8A%E5%80%BA%E5%8A%A1%E5%8D%B1%E6%9C%BA%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>2022年9月份看的一本书，现在补上读书心得。<br />思考，个人如何面对经济危机？国家、企业呢？不同层级应该有不同的对策。</p><h1 id="文章摘要">文章摘要</h1><ul><li>由于信贷同时创造了购买力和债务，因此增加信贷的好坏取决于能否把借款用于生产性目的，从而创造出足够多的收入来还本付息。<br />PS：目前很多是借新债还旧债。<br /></li><li><strong>大量举债的风险主要是决策者能否有意愿和能力将坏账分摊到多年</strong>，取决于两个因素：<br />短期债务长期化，让时间来解决问题的思想。<ol type="1"><li>债务是否以决策者能够控制的货币计价；<br /></li><li>决策者能否对债权人和债务人施加影响。<br /></li></ol></li><li>化繁就简地讲，你一旦借钱，就会创造出一个周期，买一件你目前买不起的东西，你的消费必然会超出你的收入，借款时，你不仅是向贷款人借钱，你实际上是向未来的自己借钱，在未来的某个时刻，你必须降低消费水平，以偿还债务。<br />PS: 但借款用于生产就不一样。<br />PS：所以借款必须考虑未来的收入预期。如果因为太乐观，导致未来某个时刻的收入不能覆盖相关债务，就发生经济危机。对一个国家、经济体和个人都一样。<br /></li><li>决策者可以采取以下四种措施，以降低<strong>债务与收入之间的比率</strong>和<strong>偿债总额与用于偿债的现金流之间的比率</strong>。<ul><li>财政紧缩（即减少收入）<br /></li><li>债务违约/重组<br /></li><li>央行印钞，购买资产（或提供担保）<br /></li><li>将资金和信贷从充足的领域转向不足的领域。</li></ul></li><li>和谐的去杠杆化<br />降低债务和收入比率，同时保持适当的经济增长率和通胀率。<br />但实际上很难，物极必反，往往都是快速下降的硬着陆方式（也就是说的自我强化），而不是缓缓下降的软着陆。就像地产。不过话说，挤掉泡沫，来一次置之死地而后生。<br />例如房地产的三条红线，导致地产的偿债增加；同时由于地产房价的泡沫，导致需求减少，减低了收入，故出现地产的债务危机。<br /><span id="more"></span></li><li>反向的财务效应<br />当一个人的财务总量下降时，就会出现方向的“财富效应”，导致贷款和支出减少，借款人一方面存在当心的负面心理，另一方面，财务状况变差，抵押品减少，因此贷款水平会下降。<br /></li><li>大部分被称为资金的东西其实就是信贷，而信贷这东西，会在经济繁荣的时候凭空出现，也会在经济不振的时候凭空消失。<br />也就是说，去杠杆化进程基本上就是，人们发现自己所谓的财富其实大部分不过是他人的付款承诺（因为是信贷，或者抵押物减值，或者作假等），一旦对方不遵守诺言（还不起），自己的财务就不复存在了。<br /></li><li>一个国家之所以发生债务危机，是因为债务和偿债成本的增速高于偿债所需收入的增速，最后不得不去杠杆化。<br /></li><li>金钱有两个功能，交换媒介和财务储藏手段。<br /></li><li>政府必须化解信贷危机，刺激整体经济，政府可能难以通过税收和借款筹集资金，因此央行被迫增加印钞量，提供更多的资金，用于购买国债，如果不这么做，政府就必须跟私营部门争夺有限的资金，导致资金进一步紧张。<br />债务货币化。<br /></li><li>地产和汇率只能救一个。<br />救地产，必须防水，提供资金流；这样会导致汇率下降。<br /></li><li>信贷就是资金，如果在抵制信贷收缩和积极刺激经济之间取得平衡，印钞就不会导致通胀发生。<br />经济不好就导致信贷紧缩。<br />和谐的去杠杆化的关键在于平衡通胀性力量和通缩性力量。<br /></li><li>人们对货币购买力损失率（即通胀率，或者汇率下降）和损失补偿率（即利率）的相对变化非常敏感。<br /></li><li><strong>国家的经济增长越来越多的依赖于债务而不是生产力的进步来拉动。</strong><br /></li><li>由于一个人的消费是另一个人的收入，因此消费下降会波及整个经济，造成失业率上升，消费进一步萎缩。<br /></li><li>实物资产来抵抗恶性通货膨胀。<br /></li><li>失业率上升以及可能造成潜在的社会动荡，被认为比物价重新上涨更具威胁性。<br /></li><li>危机 就是物极必反的规律。<br /></li><li>资产价格下降，导致负面财富效应，在金融市场上形成恶性循环，促使支出和收入的下降，反过来影响实体经济，泡沫倒转，演变为崩溃。<br /></li><li>像往常一样，政治家和商业领袖继续夸大经济形势，但统计数据显示经济疲软。<br /></li><li>滞胀：经济增长停滞，但物价增长。<br /></li><li>由于银行的负债（即短期存款）和资产（即流动性低的贷款和证券）之间存在流动性错配，银行从结构上很容易搜到挤兑的冲击。<br /></li><li>套利交易。<br /></li><li>购买政府债券就是量化宽松。 后期需要考虑缩减量化宽松政策，出售央行积累的债务。央行其实是最后贷款人。<br /></li><li>通缩是比通胀更危险的经济策略。<strong>关键是保证流动性</strong><br /></li><li>沉默是金。<br /></li><li>信贷支撑的购房活动推动了房价上涨，创造了自我强化的预期，吸引了不想错过时机的新借款人/贷款人。这是泡沫时期的典型现象。<br /></li><li>经济萎缩 经济萧条（去杠杆化）<br /></li><li><strong>应对危机好比战地救护，这些工作不可能做得十全十美。</strong><br /><strong>PS:中国现在的问题是地产失速，市场萎缩。而地产及关联产业之前创造的信贷占比很大，如何填上这个缺口是关键，对经济、就业、消费都是如此。</strong><br /></li><li>失业率一个关键的指标是：首次申领失业救济的人数。<br /></li><li><strong>经济危机的表现就是信贷危机，资产价格和经济增长率自我强化式下降，导致流动性危机，所以这时候关键是货币的扩张而不是收缩。</strong><br /></li><li>金融危机基本都要10年左右的时间，所以经常说失去的十年。<br /></li><li>宏观审慎政策是管理经济的重要工具。<br /></li><li>GDP：商品+服务的总值。<br />储备：外汇+黄金<br />长期利率：10年期政府债券收益率；<br />短期利率：3个月内贷款利率；<br /></li><li>宏观审慎政策是管理经济的重要工具。</li></ul><p><img src="/img2/债务危机/债务危机32.jpg" /></p><p><img src="/img2/债务危机/债务危机33.jpg" /><br /><img src="/img2/债务危机/债务危机7.jpg" /><br /><img src="/img2/债务危机/债务危机8.jpg" /><br /><img src="/img2/债务危机/债务危机9.jpg" /><br /><img src="/img2/债务危机/债务危机10.jpg" /><br /><img src="/img2/债务危机/债务危机11.jpg" /><br /><img src="/img2/债务危机/债务危机12.jpg" /><br /><img src="/img2/债务危机/债务危机13.jpg" /><br /><img src="/img2/债务危机/债务危机14.jpg" /><br /><img src="/img2/债务危机/债务危机15.jpg" /><br /><img src="/img2/债务危机/债务危机16.jpg" /><br /><img src="/img2/债务危机/债务危机17.jpg" /><br /><img src="/img2/债务危机/债务危机18.jpg" /><br /><img src="/img2/债务危机/债务危机19.jpg" /><br /><img src="/img2/债务危机/债务危机20.jpg" /><br /><img src="/img2/债务危机/债务危机21.jpg" /><br /><img src="/img2/债务危机/债务危机22.jpg" /><br /><img src="/img2/债务危机/债务危机23.jpg" /><br /><img src="/img2/债务危机/债务危机24.jpg" /><br /><img src="/img2/债务危机/债务危机25.jpg" /><br /><img src="/img2/债务危机/债务危机26.jpg" /><br /><img src="/img2/债务危机/债务危机27.jpg" /><br /><img src="/img2/债务危机/债务危机28.jpg" /><br /><img src="/img2/债务危机/债务危机29.jpg" /><br /><img src="/img2/债务危机/债务危机30.jpg" /><br /><img src="/img2/债务危机/债务危机31.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 债务 </tag>
            
            <tag> 金融危机 </tag>
            
            <tag> 经济 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《赤裸裸的统计学》读书心得</title>
      <link href="/2023/10/15/%E3%80%8A%E8%B5%A4%E8%A3%B8%E8%A3%B8%E7%9A%84%E7%BB%9F%E8%AE%A1%E5%AD%A6%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2023/10/15/%E3%80%8A%E8%B5%A4%E8%A3%B8%E8%A3%B8%E7%9A%84%E7%BB%9F%E8%AE%A1%E5%AD%A6%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>一本关于统计学的普及类书籍，从宏观上介绍了统计学的主要概念和关键的原理。写得通俗易懂，作为一本入门的了解统计学的书籍来说还是不错的。<br />本书的主要目的是需要明白数学在生活中的意义，所以很多的例子是结合生活中的实际场景来展开的，使统计学的概念变得更加直观和便于理解。确实是，学习一门课程，考虑生活意义，从生活意义出发是让学生提高兴趣的最好的方法，避免一开始就进入枯燥的定理和公式，只会劝退大部分的学生。<br />统计学中重要的区分因果关系和相关性（具有统计学意义）。</p><h1 id="统计学是大数据时代最炙手可热的学问">统计学是大数据时代最炙手可热的学问</h1><p>从抽样数据来解决大问题。统计学的一个核心功能就是使用手中已有的数据进行合理推测，以回答那些我们还未掌握所有信息的“大”问题。简而言之，我们能够使用“已知世界”的数据来对“未知世界”进行推断。</p><ul><li>将一序列复杂数据浓缩成单一数字的应用。例如足球中会把一堆数据浓缩成一个单独的评分。<br /></li><li>描述统计学存在的意义就是简化，因此不可避免的丢失一些内容和细节，方便对比和评价。（<strong>简化才能适应人的大脑，才容易传播，特别是面对公共领域的时候。</strong>），浓缩的技术、取舍权重不同导致不同的结果。<strong>人们喜欢看到简单的答案！</strong><br /></li><li>任何一个风险评估模型都必须以概率为基础。（<strong>概率的不确定性会导致黑天鹅事件的颠覆效应。</strong>）<br /></li><li>统计学总是想告诉我们一些信息，但是面对这些信息，聪明而又诚实的人经常有不同的看法。</li></ul><h1 id="描述统计学">描述统计学</h1><p>平均数、中位数、四分位数等。</p><ul><li>25百分位和75百分位，通常作为中产阶级中的高收入和低收入人群。</li></ul><p>标准差也是一个能让我们在一堆杂乱无章的数字中发现真理的统计数值。我们用它来衡量数据相对于平均值的分散程度，根据标准差，我们可以知道所有观察数值的分散情况。<br />标准差和方差。<br /><img src="/img2/赤裸裸的统计学/赤裸裸的经济学5.jpg" /></p><span id="more"></span><h1 id="统计数字会说谎">统计数字会说谎</h1><p><strong>判断比数学更重要。</strong>每个从统计学出发的管理需要考虑的是人性而非仅仅是科学。</p><ul><li>马克·吐温：“谎言有三种，谎言、该死的谎言，以及统计学。”（<strong>就像历史是任人打扮的小姑娘一样</strong>）</li></ul><p>从数字出发，组织数据的方式，截取不同的部分，从不同的视角，不同的变量出发，得出不同的有利于自己的结论，是统计学谎言的常用套路。例如百分率和百分差。数据仅仅是工具而已。</p><h1 id="相关性和相关系数">相关性和相关系数</h1><p>相关性作为一个统计工具的魅力就在于将两个变量的关联精炼成一个描述性数据：相关系数。并且相关系数不受两个变量单位的限制。（因为计算的时候会将变量统一转变为标准差。）<br /><img src="/img2/赤裸裸的统计学/赤裸裸的经济学11.jpg" /></p><h1 id="概率和期望值">概率和期望值</h1><p>概率学是一门研究不确定实践和结果的学问。这个世界的因果关系我们人类知之甚少。<br /><img src="/img2/赤裸裸的统计学/赤裸裸的经济学14.jpg" /><br />大数定律：随着实验次数增多，结果的平均值会越来越接近期望值。<br />大数定律及概率密度函数:<br /><img src="/img2/赤裸裸的统计学/赤裸裸的经济学15.jpg" /><br /><img src="/img2/赤裸裸的统计学/赤裸裸的经济学16.jpg" /><br />保险的精算师其实就是关于概率和期望值的游戏。<br /><img src="/img2/赤裸裸的统计学/赤裸裸的经济学17.jpg" /><br /><img src="/img2/赤裸裸的统计学/赤裸裸的经济学18.jpg" /><br /><img src="/img2/赤裸裸的统计学/赤裸裸的经济学19.jpg" /><br /><img src="/img2/赤裸裸的统计学/赤裸裸的经济学20.jpg" /><br /><img src="/img2/赤裸裸的统计学/赤裸裸的经济学21.jpg" /></p><h1 id="黑天鹅事件">黑天鹅事件</h1><p>VaR模型</p><p><img src="/img2/赤裸裸的统计学/赤裸裸的经济学22.jpg" /><br /><img src="/img2/赤裸裸的统计学/赤裸裸的经济学23.jpg" /><br /><img src="/img2/赤裸裸的统计学/赤裸裸的经济学24.jpg" /><br /><img src="/img2/赤裸裸的统计学/赤裸裸的经济学25.jpg" /></p><h2 id="一些常见的与概率有关的错误误解和道德困境">一些常见的与概率有关的错误、误解和道德困境</h2><ul><li>想当然的认为事件之间不存在联系。<br />例如两个发动机故障可能是相关的，同批次、同年限。<br /></li><li>对两个事件的统计独立一无所知。<br />掷硬币。<br /></li><li>成群病例的发生。<br />真的是特例和巧合，例如中彩票一样。<br /></li><li>检方谬误<br /></li><li>回归平均数（或趋均数回归）<br /><img src="/img2/赤裸裸的统计学/赤裸裸的经济学26.jpg" /><br /></li><li>统计性歧视</li></ul><h1 id="数据与偏见">数据与偏见</h1><p><img src="/img2/赤裸裸的统计学/赤裸裸的经济学27.jpg" /></p><h1 id="中心极限定律">中心极限定律</h1><p>大数定理：单个实验，多次，达到平均值。<br />中心极限定理：小样本与大样本相似，小样本平均值符合正态分布。<br /><img src="/img2/赤裸裸的统计学/赤裸裸的经济学29.jpg" /><br /><img src="/img2/赤裸裸的统计学/赤裸裸的经济学30.jpg" /><br /><img src="/img2/赤裸裸的统计学/赤裸裸的经济学31.jpg" /><br />标准差和标准误差的区别：<br /><strong>标准误差就是所有样本平均值的标准差。</strong><br />样本平均值的聚集程度会随着样本数量的增多而上升。<br /><img src="/img2/赤裸裸的统计学/赤裸裸的经济学32.jpg" /><br /><img src="/img2/赤裸裸的统计学/赤裸裸的经济学33.jpg" /><br /><img src="/img2/赤裸裸的统计学/赤裸裸的经济学34.jpg" /></p><h1 id="统计推断与假设检验">统计推断与假设检验</h1><p><img src="/img2/赤裸裸的统计学/赤裸裸的经济学35.jpg" /><br /><img src="/img2/赤裸裸的统计学/赤裸裸的经济学36.jpg" /><br /><img src="/img2/赤裸裸的统计学/赤裸裸的经济学37.jpg" /><br /><img src="/img2/赤裸裸的统计学/赤裸裸的经济学38.jpg" /></p><h1 id="民意测验和误差幅度">民意测验和误差幅度</h1><p><img src="/img2/赤裸裸的统计学/赤裸裸的经济学39.jpg" /><br /><img src="/img2/赤裸裸的统计学/赤裸裸的经济学40.jpg" /><br /><img src="/img2/赤裸裸的统计学/赤裸裸的经济学41.jpg" /></p><h1 id="回归分析与线性关系">回归分析与线性关系</h1><p>具体来说，回归分析能够在控制其它因素的前提下，对某个具体变量与某个特定结果之间的关系进行量化。回归分析寻找的是两个变量之间的最佳拟合曲线关系。<br />最小二乘法（OLS)<br /><img src="/img2/赤裸裸的统计学/赤裸裸的经济学42.jpg" /><br /><img src="/img2/赤裸裸的统计学/赤裸裸的经济学43.jpg" /></p><p>回归系数的正态分布<br /><img src="/img2/赤裸裸的统计学/赤裸裸的经济学44.jpg" /></p><p>当回归系数至少是标准误差的两倍或以上的时候，该系数极有可能具有统计学意义。</p><p>R平方 = 1 -（残差平方和/总平方和） 拟合程度（0，1），越大拟合程度越好,1代表都落在曲线上。</p><p>多元线性回归<br /><img src="/img2/赤裸裸的统计学/赤裸裸的经济学46.jpg" /></p><h1 id="致命的回归错误七宗罪">致命的回归错误（七宗罪）</h1><ul><li>用回归方程式来分析非线性关系；<br /></li><li>相关关系并不等同于因果关系；<br /></li><li>因果倒置；<br />关键在于我们不应该使用那些受结果影响的解释变量，不然的话，因和果将会永无休止的纠缠下去。<br />我们应该确保解释变量会影响因变量，而不是相反。<br /></li><li>变量遗漏偏差；<br />例如打高尔夫的人得心脏病比常人多，缺少了年龄这个变量。（打高尔夫的人都年龄偏大）<br /></li><li>高度相关的解释变量（多元共线性）<br />当两个解释变量高度相关时，研究人员将会在回归方程中只采用其中一个，或创造一个新的综合变量。<br /></li><li>隔离数据进行推断<br />样本相似性的场景才有效。<br /></li><li>数据矿（变量过多）<br /></li><li>差分类查分实验<br /></li><li>不连续分析实验 （分界线两端的相近值分入不同组做实验）</li></ul><p>总结：<br /><img src="/img2/赤裸裸的统计学/赤裸裸的经济学47.jpg" /><br /><img src="/img2/赤裸裸的统计学/赤裸裸的经济学48.jpg" /></p><h1 id="项目评估与反现实">项目评估与反现实</h1><p><img src="/img2/赤裸裸的统计学/赤裸裸的经济学50.jpg" /><br />常用实验方法</p><ul><li>随机控制实验<br /></li><li>自然实验<br /></li><li>非对等对照实验</li></ul><h1 id="文章摘要">文章摘要</h1><ul><li>用数据说谎容易，但是用数据说出真相却很难。<br /></li><li>数据只不过是知识的原材料。<br /></li><li>中位数：偶数个取中间两个的平均值。<br /></li><li>你无法管理你无法衡量的东西。<br /></li><li>大数定理：单个实验，多次，达到平均值。<br />中心极限定理：小样本与大样本相似，小样本平均值符合正态分布。<br /></li><li>怀孕的女性是养成消费习惯的最佳人群。</li></ul><p><img src="/img2/赤裸裸的统计学/赤裸裸的经济学1.jpg" /><br /><img src="/img2/赤裸裸的统计学/赤裸裸的经济学2.jpg" /><br /><img src="/img2/赤裸裸的统计学/赤裸裸的经济学3.jpg" /><br /><img src="/img2/赤裸裸的统计学/赤裸裸的经济学4.jpg" /></p><p><img src="/img2/赤裸裸的统计学/赤裸裸的经济学6.jpg" /><br /><img src="/img2/赤裸裸的统计学/赤裸裸的经济学7.jpg" /><br /><img src="/img2/赤裸裸的统计学/赤裸裸的经济学8.jpg" /><br /><img src="/img2/赤裸裸的统计学/赤裸裸的经济学9.jpg" /><br /><img src="/img2/赤裸裸的统计学/赤裸裸的经济学10.jpg" /></p><p><img src="/img2/赤裸裸的统计学/赤裸裸的经济学12.jpg" /><br /><img src="/img2/赤裸裸的统计学/赤裸裸的经济学13.jpg" /></p><p><img src="/img2/赤裸裸的统计学/赤裸裸的经济学28.jpg" /></p><p><img src="/img2/赤裸裸的统计学/赤裸裸的经济学45.jpg" /></p><p><img src="/img2/赤裸裸的统计学/赤裸裸的经济学49.jpg" /></p><p><img src="/img2/赤裸裸的统计学/赤裸裸的经济学51.jpg" /></p><p>查：</p><p>爱好数学的CIO<br />金融行业 风险价值（VaR）模型。</p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 统计学 </tag>
            
            <tag> 普及 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《王阳明大传》读书心得</title>
      <link href="/2023/10/01/%E3%80%8A%E7%8E%8B%E9%98%B3%E6%98%8E%E5%A4%A7%E4%BC%A0%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2023/10/01/%E3%80%8A%E7%8E%8B%E9%98%B3%E6%98%8E%E5%A4%A7%E4%BC%A0%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>这几年的王阳明热，带来的结果就是市面上关于王阳明的书籍鱼目混珠。之前也看过一些这类型的书，看完后觉得写得最好的还是这一套书。<br />《王阳明大传》分上、中、下三卷，共三本。<br />此传记不同普通传记以故事驱动的做法，而是以王阳明思想的发展为主线，同步对比分析各种流派（儒、释、道）和人物（陆九渊、朱熹等）。是结合王阳明生涯及心学思想的一本宋明哲学史、思想史。<br />通过王阳明的诗词、文章、奏章，和亲朋好友的书信来往，以及门人关于王阳明的记录，分析了王阳明的人生轨迹及思想形成历程。本书更像一本严格考证的书，从中也可看出作者（日本人）严谨的治学态度。本书是从日语翻译过来的，译者的水平也很高，看起来就像一本中文大师级的水准，完全没有翻译的痕迹。</p><p>近年，王阳明的思想进入了主流的宣传，这也是政治的需要，王阳明的“致良知、知行合一”，确实是为官、为民的楷模。<br />不过哲学层面这些形而上的东西，有时候看起来特别绕，像是一种文字游戏。特别是在讲述朱熹的格物致知、知行二分和王阳明的知行合一的讨论，感觉怎么讲都对，就像一个硬币的两面，只是大家看待问题的角度不同罢了。</p><h1 id="生平">生平</h1><p>王阳明，浙江余姚（属于绍兴）人，名守仁。他老爸王华，又称“龙山先生”，状元。<br />被贬贵州龙场（龙场悟道），在那里悟出了“心则理”的思想，并提出“知行合一”说。致良知是在晚年才提出，但致良知是阳明心学的根基。良知就是己所不欲，勿施于人。<br />王阳明是大圣人、大儒家、大豪杰，道德层面无出其右者，文学水平也高，同时，在军事层面也在文人中无可比拟。其三征的功绩在历史上都是赫赫之战功。<br />不过因奸臣诋毁，在政治上没有达到他对应的能力和功劳。</p><span id="more"></span><h1 id="知行合一">知行合一</h1><p>知行合一必须要在事上磨炼，所谓说，读万卷书，不如行万里路。知行合一讲究实践。<br /><img src="/img2/王阳明大传/王阳明大传48.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传49.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传50.jpg" /></p><h1 id="儒家佛教道教对比">儒家、佛教、道教对比</h1><p>王阳明因为也经历过沉溺与道教、佛教，也比较了道教、佛教和儒家的不同。<br />主要是人的本性问题，特别是人伦方面的孝道；所以说儒家的思想是一种入世的思想，而非逃避的思想，修身齐家治国平天下的思想。<br />爹娘便是灵山佛，不敬爹娘敬甚人？<br /><img src="/img2/王阳明大传/王阳明大传15.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传16.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传17.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传18.jpg" /></p><p><strong>不过话说，从佛教的角度看，一切都是虚幻的，是空的，也不存在所谓的私心了。所谓的人伦、孝道，君君臣臣都是空的。</strong><br /><img src="/img2/王阳明大传/王阳明大传19.jpg" /></p><p><img src="/img2/王阳明大传/王阳明大传20.jpg" /></p><h1 id="启示">启示</h1><p>王阳明身体不好,体弱多病，波折很多，例如被贬贵州龙场，三征立功，但不受重用，被小人诋毁。子孙也不是很贤良。所以对比自己的一切，也不算什么了。</p><p>不过有个问题是，俗话说，善战者无赫赫之战功，因为王阳明太牛，三征都是因为准备充分，短时间平定，所以也给奸臣留下了诋毁的空间，有时候的养寇自重也不一定是坏事，在权谋的角度来说，这样可能更容易获得权力。</p><h1 id="文章摘要">文章摘要</h1><ul><li>中国儒家思想的核心是克己，而国外是个体的解放和自由。<br /></li><li>中国诗歌和绘画讲究一个“藏”字，刻意避免过于外露。讲究意境。<br /></li><li>朱熹在对经典做注释时，掺杂了太多的佛家和老庄的思想。<br /></li><li>君子喻于义，小人喻于利。<br /></li><li>曾子曰：“君子以文会友，以友辅仁”<br /></li><li>《大明王朝1566》的1566是嘉靖驾崩的年份，是嘉靖45年。王阳明是嘉靖7年去世。<br /></li><li>先表扬后提出改正意见，先抑后扬的手法。<br /></li><li>主忧臣辱，主辱臣死。<br /></li><li>兵无常势，水无常形。<br /></li><li>文过饰非：用漂亮的言辞掩饰自己的错误和过失。<br /></li><li>一直到五岁，还不会说话。（真是贵人语迟）</li></ul><p><img src="/img2/王阳明大传/王阳明大传00.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传1.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传2.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传3.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传4.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传5.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传6.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传7.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传8.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传9.jpg" /></p><p><img src="/img2/王阳明大传/王阳明大传10.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传11.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传12.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传13.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传14.jpg" /></p><p><img src="/img2/王阳明大传/王阳明大传21.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传22.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传23.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传24.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传25.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传26.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传27.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传28.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传29.jpg" /></p><p><img src="/img2/王阳明大传/王阳明大传30.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传31.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传32.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传33.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传34.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传35.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传36.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传37.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传38.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传39.jpg" /></p><p><img src="/img2/王阳明大传/王阳明大传40.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传41.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传42.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传43.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传44.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传45.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传46.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传47.jpg" /></p><p><img src="/img2/王阳明大传/王阳明大传51.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传52.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传53.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传54.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传55.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传56.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传57.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传58.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传59.jpg" /></p><p><img src="/img2/王阳明大传/王阳明大传60.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传61.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传62.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传63.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传64.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传65.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传66.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传67.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传68.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传69.jpg" /></p><p><img src="/img2/王阳明大传/王阳明大传70.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传71.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传72.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传73.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传74.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传75.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传76.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传77.jpg" /><br /><img src="/img2/王阳明大传/王阳明大传78.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 王阳明 </tag>
            
            <tag> 心学 </tag>
            
            <tag> 致良知 </tag>
            
            <tag> 知行合一 </tag>
            
            <tag> 三征 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>知行合一，致良知《王阳明大传》读书笔记</title>
      <link href="/2023/10/01/%E3%80%8A%E7%8E%8B%E9%98%B3%E6%98%8E%E5%A4%A7%E4%BC%A0%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
      <url>/2023/10/01/%E3%80%8A%E7%8E%8B%E9%98%B3%E6%98%8E%E5%A4%A7%E4%BC%A0%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<p>众所周知，王阳明因知行合一、致良知的思想而名闻天下。但就像一个人的成长一样，王阳明的思想也是在人生的不断磨砺中逐渐形成。</p><p>自古以来，江浙地区名人辈出，孕育出众多英雄豪杰、文人骚客。<br />王阳明，浙江余姚人（属绍兴），名守仁。他老爸王华，又称“龙山先生”，明朝成化十七年状元。</p><p>王阳明早年沉溺于道教、佛教，后期悟出了它们的不足，进而笃信儒学。<br />而导致王阳明思想发生巨大转变的核心也是关于对孝道的理解。所谓百善孝为先，这是儒学的根本，也是不同于道教、佛教的一种入世思想。俗话说：爹娘便是灵山佛，不敬爹娘敬甚人？</p><p>欲戴皇冠，必承其重。<br />从人物传记中收获的最大价值就是让自己明白，自身所谓的各种不如意，和那些名人经历的苦难相比，只能说是微不足道。</p><p>王阳明，被人们称作中国历史上两个半完人之一，一个是孔子，另一个就是王阳明，半个曾国藩。</p><p>王阳明是大圣人、大儒家、大豪杰。道德层面无出其右者，文学造诣也很高，在军事层面更是文人中的佼佼者，其军事的三征功绩在历史上都是赫赫之战功。</p><p>但即使是这么一位圣人，也因奸臣的诋毁，在政治上屡屡受到打压和迫害。</p><p>就像司马迁《报任少卿书》说的“盖西伯拘而演《周易》；仲尼厄而作《春秋》；屈原放逐，乃赋《离骚》”。</p><p>被贬到贵州龙场的王阳明，龙场悟道，在那里悟出了“心则理”的思想和“知行合一”的学说，并在晚年提出了“致良知”。形成了“知行合一、致良知”的思想体系。</p><p>“致良知”是阳明心学的根基，良知就是己所不欲，勿施于人。而“知行合一”就必须要在事上磨炼。读万卷书不如行万里路，知行合一讲究的是实践。</p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 王阳明 </tag>
            
            <tag> 心学 </tag>
            
            <tag> 致良知 </tag>
            
            <tag> 知行合一 </tag>
            
            <tag> 三征 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《中央银行与货币供给》读书心得</title>
      <link href="/2023/09/30/%E3%80%8A%E4%B8%AD%E5%A4%AE%E9%93%B6%E8%A1%8C%E4%B8%8E%E8%B4%A7%E5%B8%81%E4%BE%9B%E7%BB%99%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2023/09/30/%E3%80%8A%E4%B8%AD%E5%A4%AE%E9%93%B6%E8%A1%8C%E4%B8%8E%E8%B4%A7%E5%B8%81%E4%BE%9B%E7%BB%99%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>本书的例子，很适合做统计分析论文的参考。其分析的方法和思想很规范。</p><h1 id="m0m1m2">M0，M1，M2</h1><p>各国央行都以流动性为依据对货币进行层次划分。<br /><img src="/img2/中央银行与货币供给/中央银行与货币供给2.jpg" /><br /><img src="/img2/中央银行与货币供给/中央银行与货币供给3.jpg" /></p><p>M1 流动性，目的是交易<br />M2 M1+储蓄 <strong>（现在主要是看M2的情况。）</strong></p><p><img src="/img2/中央银行与货币供给/中央银行与货币供给14.jpg" /><br /><span id="more"></span></p><h1 id="高能货币">高能货币</h1><p>H=C+R<br />M=C+D<br />H表示高能货币（货币基数,基础货币），C表示非银行公众所持有的通货；R表示商业银行的存款准备金。D表示商业银行存款。<br />C/M表示通货比率；R/D表示准备金比率。<br />高能货币的任何增长都将导致货币存量的同比率增长。<br /><img src="/img2/中央银行与货币供给/中央银行与货币供给4.jpg" /></p><h1 id="货币本质">货币本质</h1><p>货币的本质是负债<br /><img src="/img2/中央银行与货币供给/中央银行与货币供给5.jpg" /><br /><img src="/img2/中央银行与货币供给/中央银行与货币供给6.jpg" /></p><h1 id="货币政策">货币政策</h1><p>央行改变基础货币的三大政策：<br />公开市场业务、向商业银行贷款、改变法定准备金率。<br /><img src="/img2/中央银行与货币供给/中央银行与货币供给8.jpg" /></p><p><strong>经济的混沌性：多主体、多变量的交叉影响；人的非理性成分；经济的蝴蝶效应。</strong></p><h1 id="金融创新">金融创新</h1><p>货币市场基金（余额宝）<br /><img src="/img2/中央银行与货币供给/中央银行与货币供给11.jpg" /><br /><img src="/img2/中央银行与货币供给/中央银行与货币供给12.jpg" /></p><h1 id="流动性">流动性</h1><p><img src="/img2/中央银行与货币供给/中央银行与货币供给17.jpg" /></p><h1 id="货币的外生性和内生性">货币的外生性和内生性</h1><p><img src="/img2/中央银行与货币供给/中央银行与货币供给20.jpg" /></p><h1 id="存款准备金制度">存款准备金制度</h1><p><img src="/img2/中央银行与货币供给/中央银行与货币供给21.jpg" /></p><h1 id="公开市场操作">公开市场操作</h1><p>弗里曼认为，公开市场操作实际上是唯一有效的货币政策工具。在他看来，其它货币政策工具所能做到的，公开市场操作也能做到。<br />公开市场操作在美联储货币政策操作中的作用越来越重要，成为美联储主要的货币政策工具。</p><h2 id="公告效应"><strong>公告效应：</strong></h2><p><img src="/img2/中央银行与货币供给/中央银行与货币供给22.jpg" /></p><h2 id="扭曲操作">扭曲操作</h2><p><img src="/img2/中央银行与货币供给/中央银行与货币供给23.jpg" /></p><h2 id="人民银行公开市场操作">人民银行公开市场操作</h2><p><img src="/img2/中央银行与货币供给/中央银行与货币供给24.jpg" /><br /><img src="/img2/中央银行与货币供给/中央银行与货币供给25.jpg" /></p><h1 id="货币政策的中间目标">货币政策的中间目标</h1><p><img src="/img2/中央银行与货币供给/中央银行与货币供给26.jpg" /><br /><img src="/img2/中央银行与货币供给/中央银行与货币供给27.jpg" /></p><h1 id="量化宽松-流动性陷阱">量化宽松 流动性陷阱</h1><p><img src="/img2/中央银行与货币供给/中央银行与货币供给28.jpg" /><br /><img src="/img2/中央银行与货币供给/中央银行与货币供给29.jpg" /><br /><img src="/img2/中央银行与货币供给/中央银行与货币供给30.jpg" /></p><h1 id="表外业务">表外业务</h1><p><img src="/img2/中央银行与货币供给/中央银行与货币供给31.jpg" /></p><h1 id="社会融资规模">社会融资规模</h1><p><img src="/img2/中央银行与货币供给/中央银行与货币供给32.jpg" /></p><h1 id="数字货币">数字货币</h1><p>数字货币与比特币的区别<br /><img src="/img2/中央银行与货币供给/中央银行与货币供给35.jpg" /></p><p>数字货币不单纯是技术问题，需要考虑对整个经济体系的影响，对整个经济政策的影响。<br /><img src="/img2/中央银行与货币供给/中央银行与货币供给37.jpg" /><br /><img src="/img2/中央银行与货币供给/中央银行与货币供给38.jpg" /><br /><img src="/img2/中央银行与货币供给/中央银行与货币供给39.jpg" /></p><h1 id="文章摘要">文章摘要</h1><ul><li>货币的两种本质观：商品交换媒介；价值存储手段。<br /></li><li>货币供应量是基础货币与货币乘数之积。其中，基础货币受中央银行直接控制，而货币乘数受商业银行与社会公众之行为影响。<br /></li><li>股票是直接连接货币政策与社会经济活动的纽带。</li></ul><p><img src="/img2/中央银行与货币供给/中央银行与货币供给1.jpg" /></p><p><img src="/img2/中央银行与货币供给/中央银行与货币供给7.jpg" /></p><p><img src="/img2/中央银行与货币供给/中央银行与货币供给9.jpg" /></p><p><img src="/img2/中央银行与货币供给/中央银行与货币供给10.jpg" /></p><p><img src="/img2/中央银行与货币供给/中央银行与货币供给13.jpg" /></p><p><img src="/img2/中央银行与货币供给/中央银行与货币供给15.jpg" /><br /><img src="/img2/中央银行与货币供给/中央银行与货币供给16.jpg" /></p><p><img src="/img2/中央银行与货币供给/中央银行与货币供给18.jpg" /><br /><img src="/img2/中央银行与货币供给/中央银行与货币供给19.jpg" /></p><p><img src="/img2/中央银行与货币供给/中央银行与货币供给33.jpg" /><br /><img src="/img2/中央银行与货币供给/中央银行与货币供给34.jpg" /></p><p><img src="/img2/中央银行与货币供给/中央银行与货币供给36.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 央行 </tag>
            
            <tag> 货币 </tag>
            
            <tag> 人民银行 </tag>
            
            <tag> 数字货币 </tag>
            
            <tag> 货币政策 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《未来大历史》读书心得</title>
      <link href="/2023/09/10/%E3%80%8A%E6%9C%AA%E6%9D%A5%E5%A4%A7%E5%8E%86%E5%8F%B2%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2023/09/10/%E3%80%8A%E6%9C%AA%E6%9D%A5%E5%A4%A7%E5%8E%86%E5%8F%B2%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>本书的写作目的之一是在于构建一个过去思维（也就是‘历史’）与未来思维的连接案例，这样我们就能更好的用过去取照亮可能的未来。<br />作者通过人类对时间的理解，讲述了A、B两种时间序列的理解。然后分析生物通过神经系统的机制，如何应对未来进行说明。特别是人类有意识管理未来的方面，作者也做了分析和预测。总体来说是一个关于未来不错的一本书，也类似于人工智能的剖析，把动植物、人类的对未来的处理机制进行了较好的阐述。<br /><span id="more"></span></p><h1 id="未来的思维">未来的思维</h1><p>或许我们大部分的思维关乎的都是可能的未来。有时候是下意识的，有时候却是有意识的。<br /><img src="/img2/未来大历史/未来大历史2.jpg" /></p><p>生活中的戏剧性和兴奋感大多来自对未来的无知，而这也给了我们选择的自由。<br /><img src="/img2/未来大历史/未来大历史3.jpg" /></p><h1 id="时间的a序列和b序列">时间的A序列和B序列</h1><p><img src="/img2/未来大历史/未来大历史7.jpg" /></p><h2 id="时间a序列">时间A序列</h2><p><img src="/img2/未来大历史/未来大历史8.jpg" /></p><h2 id="时间b序列">时间B序列</h2><p><img src="/img2/未来大历史/未来大历史9.jpg" /><br /><img src="/img2/未来大历史/未来大历史10.jpg" /><br /><img src="/img2/未来大历史/未来大历史11.jpg" /><br /><img src="/img2/未来大历史/未来大历史12.jpg" /></p><h1 id="决定论因果关系和时间之矢">决定论，因果关系和时间之矢</h1><p><img src="/img2/未来大历史/未来大历史13.jpg" /><br /><img src="/img2/未来大历史/未来大历史14.jpg" /><br /><img src="/img2/未来大历史/未来大历史15.jpg" /><br /><img src="/img2/未来大历史/未来大历史16.jpg" /><br /><img src="/img2/未来大历史/未来大历史17.jpg" /><br /><img src="/img2/未来大历史/未来大历史18.jpg" /></p><h1 id="生命的复杂性和目的性">生命的复杂性和目的性</h1><p><img src="/img2/未来大历史/未来大历史20.jpg" /><br /><img src="/img2/未来大历史/未来大历史21.jpg" /></p><h1 id="预见未来管理未来的总体原则">预见未来、管理未来的总体原则</h1><ul><li>我们没有来自未来的证据。<br /></li><li>关于未来的可能性，仅有的证据来自过去。<br />当人们说自己知晓未来的时候，他看见的并非还不存在的事件，但他们看见了已经存在的原因与征兆。<br /></li><li>我们对未来的观念会左右未来的样子。<br /></li><li>尽管没有来自未来的证据，我们仍可以在过去找到关于未来的强力暗示。</li></ul><h1 id="趋势捕捉的四种方法">趋势捕捉的四种方法</h1><p><img src="/img2/未来大历史/未来大历史22.jpg" /><br /><img src="/img2/未来大历史/未来大历史23.jpg" /></p><h2 id="一致性原则">一致性原则</h2><p><img src="/img2/未来大历史/未来大历史24.jpg" /></p><h2 id="混沌性">混沌性</h2><p><img src="/img2/未来大历史/未来大历史25.jpg" /></p><h1 id="焦虑区域">焦虑区域</h1><p><img src="/img2/未来大历史/未来大历史26.jpg" /></p><h1 id="管理未来的工具包">管理未来的工具包</h1><p><img src="/img2/未来大历史/未来大历史28.jpg" /></p><h2 id="贝叶斯分析法">贝叶斯分析法</h2><p><img src="/img2/未来大历史/未来大历史29.jpg" /></p><h1 id="神经系统">神经系统</h1><p><img src="/img2/未来大历史/未来大历史32.jpg" /><br /><img src="/img2/未来大历史/未来大历史33.jpg" /><br /><img src="/img2/未来大历史/未来大历史34.jpg" /></p><h1 id="自然时间心理时间和社会时间">自然时间、心理时间和社会时间</h1><p>社会时间基于他人的节奏而存在（日程安排）。</p><h1 id="占卜的存在意义">占卜的存在意义</h1><p><img src="/img2/未来大历史/未来大历史37.jpg" /><br /><img src="/img2/未来大历史/未来大历史38.jpg" /><br /><img src="/img2/未来大历史/未来大历史39.jpg" /><br /><img src="/img2/未来大历史/未来大历史40.jpg" /><br /><img src="/img2/未来大历史/未来大历史41.jpg" /><br /><img src="/img2/未来大历史/未来大历史42.jpg" /><br /><img src="/img2/未来大历史/未来大历史43.jpg" /></p><h1 id="概率论">概率论</h1><p><img src="/img2/未来大历史/未来大历史50.jpg" /></p><h1 id="宇宙背景辐射">宇宙背景辐射</h1><p><img src="/img2/未来大历史/未来大历史62.jpg" /></p><h1 id="文章摘要">文章摘要</h1><ul><li>大历史会用所有可能的维度和不同的学科视角去看待过去。<br /></li><li>未来，是我们所有人余生都会奔赴的地方。<br /></li><li>如果要理解时间，我们就需要理解未来。<br /></li><li>能把万事万物简化为基本法则，并不意味着能用这些法则重建整个宇宙。<br /></li><li>哲学家只是用不同的方式解释世界，而问题在于改变世界。<br /></li><li>时间有它的意义：它意味着变化.<br /></li><li>无论是对于飞鱼还是对于鸟儿来说，每一方的所有成功都是暂时的，然而，暂时的成功便是全部的意义。<br /></li><li>预测是基于统计学意义上的，而不是针对某个个体。<br /></li><li>从人类社会来类比，DNA类似于政府的体制。<br /></li><li>超有机体指分工协作的社会性生物群体，如蚂蚁、蜜蜂等。<br /></li><li><strong>干细胞具备变化成为各种不同细胞的潜力。</strong><br /></li><li><strong>专业分工解释了为什么大多数宏观生物的细胞只会用到DNA中不到一半的基因，只有维持基本运作所需的基因才会被所有细胞使用。</strong><br />PS：类似人类的分工。<br /></li><li>正是集体学习才让人类与地球上所有其它物种如此迥异，而它的时间进度比自然选择要快的多。<br /></li><li>如果你做的预测足够多，有时未来就会被你说中。</li></ul><p><img src="/img2/未来大历史/未来大历史1.jpg" /></p><p><img src="/img2/未来大历史/未来大历史4.jpg" /><br /><img src="/img2/未来大历史/未来大历史5.jpg" /><br /><img src="/img2/未来大历史/未来大历史6.jpg" /></p><p><img src="/img2/未来大历史/未来大历史19.jpg" /></p><p><img src="/img2/未来大历史/未来大历史27.jpg" /></p><p><img src="/img2/未来大历史/未来大历史30.jpg" /><br /><img src="/img2/未来大历史/未来大历史31.jpg" /></p><p><img src="/img2/未来大历史/未来大历史35.jpg" /><br /><img src="/img2/未来大历史/未来大历史36.jpg" /></p><p><img src="/img2/未来大历史/未来大历史44.jpg" /><br /><img src="/img2/未来大历史/未来大历史45.jpg" /><br /><img src="/img2/未来大历史/未来大历史46.jpg" /><br /><img src="/img2/未来大历史/未来大历史47.jpg" /><br /><img src="/img2/未来大历史/未来大历史48.jpg" /><br /><img src="/img2/未来大历史/未来大历史49.jpg" /></p><p><img src="/img2/未来大历史/未来大历史51.jpg" /><br /><img src="/img2/未来大历史/未来大历史52.jpg" /><br /><img src="/img2/未来大历史/未来大历史53.jpg" /><br /><img src="/img2/未来大历史/未来大历史54.jpg" /><br /><img src="/img2/未来大历史/未来大历史55.jpg" /><br /><img src="/img2/未来大历史/未来大历史56.jpg" /><br /><img src="/img2/未来大历史/未来大历史57.jpg" /><br /><img src="/img2/未来大历史/未来大历史58.jpg" /><br /><img src="/img2/未来大历史/未来大历史59.jpg" /><br /><img src="/img2/未来大历史/未来大历史60.jpg" /><br /><img src="/img2/未来大历史/未来大历史61.jpg" /></p><p><img src="/img2/未来大历史/未来大历史63.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 未来 </tag>
            
            <tag> 历史 </tag>
            
            <tag> 时间 </tag>
            
            <tag> 宇宙 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《深度思考》读书心得</title>
      <link href="/2023/08/27/%E3%80%8A%E6%B7%B1%E5%BA%A6%E6%80%9D%E8%80%83%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2023/08/27/%E3%80%8A%E6%B7%B1%E5%BA%A6%E6%80%9D%E8%80%83%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>本书的书名有误导性的意图，其实是一本关于职业女性成长经历的书（在欧莱雅、GAP、香奈儿），而不是介绍关于思维方面的。因为是时尚行业的内容，本书更多的强调了作者在创意和直觉方面在她职业生涯中起到的重大的作用。作者是一位偏内向的女生，她也阐述了内向的人也照样可以在职业上达到和外向人的高度。</p><h1 id="解构主义">解构主义</h1><p><img src="/img2/深度思考/深度思考4.jpg" /><br /><img src="/img2/深度思考/深度思考5.jpg" /><br /><span id="more"></span></p><h1 id="工作心态">工作心态</h1><p><img src="/img2/深度思考/深度思考8.jpg" /><br /><img src="/img2/深度思考/深度思考9.jpg" /><br /><img src="/img2/深度思考/深度思考10.jpg" /><br /><img src="/img2/深度思考/深度思考11.jpg" /><br /><img src="/img2/深度思考/深度思考12.jpg" /></p><h1 id="关于内向">关于内向</h1><p>内向的人在独处中获得能量；外向的人在社交中获得能量。<br /><img src="/img2/深度思考/深度思考14.jpg" /></p><h1 id="管理冲突">管理冲突</h1><p><img src="/img2/深度思考/深度思考18.jpg" /></p><h1 id="关于创意">关于创意</h1><p><img src="/img2/深度思考/深度思考19.jpg" /></p><h1 id="文章摘要">文章摘要</h1><ul><li>你如何定义自己，这完全取决于你自己。<br /></li><li>当你承认自己的脆弱时，你反而可以变得更强大。<br />PS：自信其实就是接受自己的不足。<br /></li><li>图书馆发霉的空气中飘荡着最伟大的思想家、学者、发明家、诗人和政治家的思想。<br />是收藏人间智慧的圣所。<br /></li><li>投资银行和管理咨询是可靠而实在的工作，可我没有学过经济学或数学。<br /></li><li>有时候，你需要在毫无计划的情况下迈出第一步，跟随直觉，顺从时势。<br /></li><li>不完美和缺陷可视为美。<br /></li><li><strong>即使你在同一个公司长期担任一个职务，你也需要自我更新，用新的眼光看待问题，打破常规思维，寻找更多的可能性。</strong><br /></li><li>重要的不是决定，而是决定背后的动机。<br /></li><li>逆境和不安通常是我们的优秀老师，他们让我们回归到最在乎的事物。<br /></li><li>人是利益驱动的，话听一半，考虑讲话者背后的动机，人都是挑对自己有利的部分来讲。所谓的公说公有理婆说婆有理。<br /></li><li><strong>有些问题永远找不到答案，但仍值得去思考。</strong><br /></li><li>你可以同时遵守规则和打破规则。<br /></li><li>我不将自己和竞争对手比，这样会让竞争对手牵着鼻子走。<br /></li><li>我们决定将它称为“旅程”而不是“项目”，一切旅程都需要时间、耐心和坚持。<br /></li><li>借用香奈儿的有名的说法，她的设计展示的是穿着裙子的女人，而不是裙子本身。</li></ul><p><img src="/img2/深度思考/深度思考1.jpg" /><br /><img src="/img2/深度思考/深度思考2.jpg" /><br /><img src="/img2/深度思考/深度思考3.jpg" /></p><p><img src="/img2/深度思考/深度思考6.jpg" /><br /><img src="/img2/深度思考/深度思考7.jpg" /></p><p><img src="/img2/深度思考/深度思考13.jpg" /></p><p><img src="/img2/深度思考/深度思考15.jpg" /><br /><img src="/img2/深度思考/深度思考16.jpg" /><br /><img src="/img2/深度思考/深度思考17.jpg" /></p><p><img src="/img2/深度思考/深度思考20.jpg" /><br /><img src="/img2/深度思考/深度思考21.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 香奈儿 </tag>
            
            <tag> 女总裁 </tag>
            
            <tag> 职业经理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《亚马逊逆向工作法》读书心得</title>
      <link href="/2023/07/22/%E3%80%8A%E4%BA%9A%E9%A9%AC%E9%80%8A%E9%80%86%E5%90%91%E5%B7%A5%E4%BD%9C%E6%B3%95%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2023/07/22/%E3%80%8A%E4%BA%9A%E9%A9%AC%E9%80%8A%E9%80%86%E5%90%91%E5%B7%A5%E4%BD%9C%E6%B3%95%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<h1 id="逆向工作法">逆向工作法</h1><p>本质是从客户的角度出发。<br />开发之前，我们要写新闻稿(PR)来阐明这种新想法或新产品将如何惠及客户，还要撰写常见问题 (FAQ) 清单，预先解答棘手的问题。我们会仔细地、批判性地研究和修改这两份文档，感到满意之后才会正式开发</p><h1 id="关键观点">关键观点</h1><ul><li>禁用PPT，采用"六页纸备忘录";<br />我们的信条：最重要的的是想法，而不是陈述者<br /></li><li>树立“抬杠者招聘流程”，抬杠：抬高标准杠；STAR法，situation，task，action，result；<br /><img src="/img2/亚马逊逆向工作法/亚马逊逆向工作法20.jpg" /><br /><img src="/img2/亚马逊逆向工作法/亚马逊逆向工作法21.jpg" /><br /><img src="/img2/亚马逊逆向工作法/亚马逊逆向工作法22.jpg" /><br /></li><li>关注可控的投入类指标，而非产出类指标（财务数据，股市等）；<br />关注因而不是果。<br /><img src="/img2/亚马逊逆向工作法/亚马逊逆向工作法23.jpg" /><br /></li><li>调整组织结构，建立具有单线程领导者的自主团队，减少依赖（技术依赖、组织依赖）；<br />技术依赖--API、微服务；类似菜单。<br />组织依赖--多功能团队；<br />前端与客户打交道，需要快速决策，适合多功能团队；中后端是共享服务，以专业性为主，需要知识的沉淀和专业化发展，适合职能性的团队；另外，IT中的项目团队这种有限期的项目适合矩阵型团队。<br /><strong>协调的目的就是为了获取资源；因为资源有限，依赖导致结构性的阻力。</strong><br /></li><li>改革领导者的薪酬结构，以便鼓励长期承诺和长期思维，薪酬激励以几年内获得的股票为主；<br /><img src="/img2/亚马逊逆向工作法/亚马逊逆向工作法4.jpg" /><br /><img src="/img2/亚马逊逆向工作法/亚马逊逆向工作法5.jpg" /><br /></li><li>明确表述公司文化的核心要素，例如亚马逊的长期思维、客户至上、渴望创新、运营卓越；<br />股东的长期利益和客户利益是完全一致的；客户至上，不太关注竞争对手。<br /></li><li>确定一套领导力准则，必须有大家的参与与贡献；<br />高标准。开放的心态很重要，避免防御者心态。有则改之无则加勉。<br />领导者不要迷恋自己或团队身上的香水味，要以最优标准要求自己和团队。<br /></li><li><strong>画出你的“飞轮”，明确什么是公司成长的驱动因素。</strong><br /><img src="/img2/亚马逊逆向工作法/亚马逊逆向工作法8.jpg" /></li></ul><span id="more"></span><h1 id="文章摘要">文章摘要</h1><ul><li>要永远少承诺、多交付，以确保超过客户的期望。<br /></li><li>只有良好的意愿没有用，建立机制才有用。<br />意愿-准则-机制-标准<br /></li><li>两个披萨团队，少于10个人；<br /></li><li>大部分的决策，在拥有70%的信息就可以做出了，等到90%再做决策，说明你的决策就慢了，容易错失机会。<br /></li><li>事实证明，提供有价值的反馈和洞见，其难度并不亚于叙述体备忘录写作本身。我职业生涯中收到的最珍责的礼物，是我参与阅读并评论的备忘录的陈述者所送的两支钢笔。（会议结束后，我常会把写有我的书面评注的纸质备忘录交还给陈述者）。两位陈述者都告诉我，我的评注对他们的业务成功起到了重要作用。<br />PS：送礼送钢笔也挺好。<br /></li><li>层级最高者需要最后发言，以避免影响其他人。<br /></li><li>客户故事和异常报告被加入数据包。<br /></li><li>与会者也可包括更多的人。如果让公司的中层人员参加业务回顾周会，让他们观摩经验更丰富的领导者何讨论和思考，可以提高他们的业务参与感，促进他们的成长和发展。<br /></li><li>失败和创新是连体双胞胎。<br /></li><li>我和我的团队很快就发现，创新道路比跟随战略更具挑战性。跟随战略的路线图是相对清晰的--研究竞争对手的产品，然后加以模仿。创新没有任何路线图。<br /></li><li>侧向加载<br /></li><li>决策-&gt;承当责任、风险-&gt;成功或失败-&gt;复盘（从成功或失败中学习）<br /></li><li>目标 如何达成目标（方案） 需要资源 时间<br />只有3/4的目标能达成，如果目标都达成，说明目标的门槛过低；</li></ul><p><img src="/img2/亚马逊逆向工作法/亚马逊逆向工作法1.jpg" /><br /><img src="/img2/亚马逊逆向工作法/亚马逊逆向工作法2.jpg" /><br /><img src="/img2/亚马逊逆向工作法/亚马逊逆向工作法3.jpg" /></p><p><img src="/img2/亚马逊逆向工作法/亚马逊逆向工作法6.jpg" /><br /><img src="/img2/亚马逊逆向工作法/亚马逊逆向工作法7.jpg" /></p><p><img src="/img2/亚马逊逆向工作法/亚马逊逆向工作法9.jpg" /><br /><img src="/img2/亚马逊逆向工作法/亚马逊逆向工作法10.jpg" /><br /><img src="/img2/亚马逊逆向工作法/亚马逊逆向工作法11.jpg" /><br /><img src="/img2/亚马逊逆向工作法/亚马逊逆向工作法12.jpg" /><br /><img src="/img2/亚马逊逆向工作法/亚马逊逆向工作法13.jpg" /><br /><img src="/img2/亚马逊逆向工作法/亚马逊逆向工作法14.jpg" /><br /><img src="/img2/亚马逊逆向工作法/亚马逊逆向工作法15.jpg" /><br /><img src="/img2/亚马逊逆向工作法/亚马逊逆向工作法16.jpg" /><br /><img src="/img2/亚马逊逆向工作法/亚马逊逆向工作法17.jpg" /><br /><img src="/img2/亚马逊逆向工作法/亚马逊逆向工作法18.jpg" /><br /><img src="/img2/亚马逊逆向工作法/亚马逊逆向工作法19.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 亚马逊 </tag>
            
            <tag> 工作 </tag>
            
            <tag> AWS </tag>
            
            <tag> 逆向 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《数字化转型路线图》读书心得</title>
      <link href="/2023/07/22/%E3%80%8A%E6%95%B0%E5%AD%97%E5%8C%96%E8%BD%AC%E5%9E%8B%E8%B7%AF%E7%BA%BF%E5%9B%BE%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2023/07/22/%E3%80%8A%E6%95%B0%E5%AD%97%E5%8C%96%E8%BD%AC%E5%9E%8B%E8%B7%AF%E7%BA%BF%E5%9B%BE%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>作者以宝洁全球共享服务部门为例，讲述了宝洁如何实现数字化转型的故事。</p><h1 id="名词理解">名词理解</h1><h2 id="共享服务的理解">共享服务的理解</h2><p>共享服务部门通常采取的策略是，将企业内部各个业务部门相似的业务流程集中到一起，然后将这些业务流程简化、标准化、集中化、自动化。然后，最终实现，共享服务部门不仅仅是向业务部门提供低成本服务的'服务提供者'，也是推进企业进行转型的重要部门。转型包括承担更多的治理职能（例如为业务部门制定流程标准），提供引导业务部门转型的服务（例如协助其他业务部门进行数字化），以及提供实现价值最大化的服务（例如使用分析方法来提高销售额）等。</p><h2 id="指数型组织">指数型组织</h2><p>指数型组织是数字化的，是以信息为基础的机构。传统企业的发展动力来自其所控制的稀缺资源。</p><h2 id="数字原生企业">数字原生企业</h2><p>例如京东。</p><h2 id="数字孪生">数字孪生</h2><p>例如智慧城市、导航、生活服务</p><h2 id="四次工业革命">四次工业革命</h2><p>第一次工业革命：蒸汽机技术<br />第二次工业革命：电气技术<br />第三次工业革命：信息技术等<br />第四次工业革命：AI<br />第四次工业革命带来的指数型数字技术。</p><h2 id="it能力成熟度">IT能力成熟度</h2><p><img src="/img2/数字化转型路线图/数字化转型路线图32.jpg" /></p><span id="more"></span><h2 id="商业模式画布">商业模式画布：</h2><p><img src="/img2/数字化转型路线图/数字化转型路线图20.jpg" /></p><h1 id="数字化转型的5个阶段">数字化转型的5个阶段</h1><p>阶段： 流程自动化（手工到线上）-- 单点突破（商业模式变更，例如直接面对消费者，而不是经过供应商）-- 局部同步 -- 全面同步 -- 活力DNA<br /><img src="/img2/数字化转型路线图/数字化转型路线图4.jpg" /><br /><img src="/img2/数字化转型路线图/数字化转型路线图5.jpg" /></p><h2 id="第一阶段-夯实基础">第一阶段 夯实基础</h2><p><img src="/img2/数字化转型路线图/数字化转型路线图9.jpg" /><br /><img src="/img2/数字化转型路线图/数字化转型路线图10.jpg" /><br /><img src="/img2/数字化转型路线图/数字化转型路线图13.jpg" /></p><h2 id="第二阶段-单点突破">第二阶段 单点突破</h2><p><img src="/img2/数字化转型路线图/数字化转型路线图14.jpg" /><br /><img src="/img2/数字化转型路线图/数字化转型路线图18.jpg" /><br /><img src="/img2/数字化转型路线图/数字化转型路线图24.jpg" /></p><h3 id="数字化场景">数字化场景</h3><p>高铁的购票、验票、临时身份证的应用就是典型的数字化场景。</p><h3 id="组织免疫系统的排斥效应">组织免疫系统的排斥效应</h3><p>变革会出现类似身体免疫系统的排斥效应。<br />- 免疫系统不一定是个坏东西。要预测并对免疫系统的反应做好准备；<br />- 免疫系统的反应可以在组织的所有层级产生，但最棘手的问题发生在中间管理层；<br />- 变革越大，免疫系统的反应越难应对。</p><h3 id="找到数字杠杆点">找到数字杠杆点</h3><p>数字杠杆点就是可以最大限度应用数字技术的领域。察觉未来趋势、使用敏捷文化和采用前沿技术平台。找到适合自己企业本身的杠杆点，需要深入理解企业面临的机遇和战略选择。<br />将战略机遇和痛点转化为数字化创意。<br /><img src="/img2/数字化转型路线图/数字化转型路线图23.jpg" /></p><h3 id="时机的选择">时机的选择</h3><p>创新、战略就是站在未来看现在。<br /><img src="/img2/数字化转型路线图/数字化转型路线图21.jpg" /><br /><img src="/img2/数字化转型路线图/数字化转型路线图22.jpg" /></p><h3 id="方法论">方法论</h3><p>组织：领导重视（领导小组）、资源保障（项目人员）、解决方案、组织架构；</p><h2 id="第三阶段-局部同步">第三阶段 局部同步</h2><p><img src="/img2/数字化转型路线图/数字化转型路线图25.jpg" /><br /><img src="/img2/数字化转型路线图/数字化转型路线图28.jpg" /><br /><img src="/img2/数字化转型路线图/数字化转型路线图29.jpg" /></p><h3 id="变革的造势">变革的造势</h3><p><img src="/img2/数字化转型路线图/数字化转型路线图26.jpg" /></p><h2 id="第四阶段-全面同步">第四阶段 全面同步</h2><p><img src="/img2/数字化转型路线图/数字化转型路线图30.jpg" /><br /><img src="/img2/数字化转型路线图/数字化转型路线图33.jpg" /><br /><img src="/img2/数字化转型路线图/数字化转型路线图36.jpg" /></p><h3 id="数字化再造">数字化再造</h3><p><img src="/img2/数字化转型路线图/数字化转型路线图31.jpg" /></p><h3 id="保持前沿性思考">保持前沿性思考</h3><p><img src="/img2/数字化转型路线图/数字化转型路线图34.jpg" /></p><h3 id="与风险投资家和初创企业合作">与风险投资家和初创企业合作</h3><p>成熟的应用，传统的应用和业务和大企业合作；<br />新技术的应用，创新的场景可以考虑和初创企业合作。<br /><img src="/img2/数字化转型路线图/数字化转型路线图35.jpg" /></p><h2 id="第五阶段-活力dna">第五阶段 活力DNA</h2><p><img src="/img2/数字化转型路线图/数字化转型路线图37.jpg" /><br /><img src="/img2/数字化转型路线图/数字化转型路线图38.jpg" /><br /><img src="/img2/数字化转型路线图/数字化转型路线图43.jpg" /></p><h3 id="持续的变革">持续的变革</h3><p><img src="/img2/数字化转型路线图/数字化转型路线图39.jpg" /><br /><img src="/img2/数字化转型路线图/数字化转型路线图40.jpg" /></p><h3 id="来自客户的预警信号">来自客户的预警信号</h3><p>对于大多数企业来说，问题不在于没有意识到当前商业模式所面临的威胁，而是低估了这种威胁的紧迫性。<br />自鸣得意的认为当前的战略在过去行之有效造成的惰性、对数字化颠覆潜在的影响的误判和对组织承受新竞争压力的乐观预测。<br /><img src="/img2/数字化转型路线图/数字化转型路线图41.jpg" /></p><h3 id="指数型思维">指数型思维</h3><p>人类的思维更容易理解线性增量，而不太容易判断指数增量。<br /><img src="/img2/数字化转型路线图/数字化转型路线图42.jpg" /></p><h1 id="转型案例">转型案例</h1><p>御讯的思考、<strong>Gertner的趋势分析</strong><br /><img src="/img2/数字化转型路线图/数字化转型路线图44.jpg" /><br /><img src="/img2/数字化转型路线图/数字化转型路线图45.jpg" /><br /><img src="/img2/数字化转型路线图/数字化转型路线图46.jpg" /><br /><img src="/img2/数字化转型路线图/数字化转型路线图47.jpg" /><br /><img src="/img2/数字化转型路线图/数字化转型路线图48.jpg" /></p><h1 id="文章摘要">文章摘要</h1><ul><li><p>要促进新一代技术与实体经济的融合，提升实体经济的生产效率，推动产业变革。<br />只有在技术创新和突破导致生产方式的革命性变化，以及推进产业体系出现重大变化的情况下，才会引发产业革命，不断催生新模式、新业态。<br /></p></li><li><p>宝洁每台电脑的桌面上都有一个定制的工具栏，每个人都可以实时查询自己的工作指标。<br />PS：有点像老朱说的个人工作平台。<br /></p></li><li><p>航空业常用的检查清单方法。<br />一份清单能使复杂的行动取得重复性的成功。<br />能自动化的自动化，不能自动化的标准化（SOP）<br /></p></li><li><p>每次变革都是机遇。<br /></p></li><li><p>企业应该如何面对生命周期中最为重要的变革管理问题:实现核心业务完全数字化转型。<br /></p></li><li><p>在工业革命中，企业要么转型，要么破产；<br />超过70%的数字化转型失败了。<br />变革正以指数级的速度发生。<br /></p></li><li><p>工业革命期间的转型需要完全不同的商业策略，而不是基于现有商业模式的渐进创新。<br /></p></li><li><p>真正的转型必须建立长期领先竞争对手的核心能力。<br /></p></li><li><p>作为领导者，我在工作中学会的第一个教训就是，永远不要试图把企业的问题外包出去，领导者可以委派职责，但不能委派责任。<br /></p></li><li><p>数字化颠覆战略（为整体业务创造新的商业模式）与企业的IT策略（运营和生产自动化）完全不同。<br /></p></li><li><p>卓越的数字化转型领导者同时具备三个独特的要素：足够的知识储备、充足的投入时间以及在数字化转型过程中不断突破种种障碍的能力。<br /></p></li><li><p>大多数企业转型过程中出现的“拔河比赛”、“事后诸葛亮”。缺乏一个共同的目标会使大家难以团结一致。<br /></p></li><li><p>修复千年虫的行动可能是我们见过的在信息技术问题上最成功的全球合作案例。<br /></p></li><li><p>成本协同效益、收益协同效应。<br /></p></li><li><p>增量思维和颠覆性思维的区别。<br /></p></li><li><p>允许团队做些“文化防火墙”之外的操作。<br /></p></li><li><p>勤奋是好运之母。<br /></p></li><li><p>未来已经在这里了，只是分配得不是很均匀。<br /></p></li><li><p>数字化最终会创造出公平的竞争环境。例如现在的教育，因为在线课程，地域的师资不公平得到一定的缓解。<br /></p></li><li><p>以客户为中心：<br />Zappos公司：“我们是一家服务公司，只是碰巧卖鞋而已。”<br /></p></li><li><p>书--Google--ChatGPT<br />但人对原理的掌握是核心，这样才能分辨对错并进行决策。</p><p><img src="/img2/数字化转型路线图/数字化转型路线图1.jpg" /><br /><img src="/img2/数字化转型路线图/数字化转型路线图2.jpg" /><br /><img src="/img2/数字化转型路线图/数字化转型路线图3.jpg" /></p><p><img src="/img2/数字化转型路线图/数字化转型路线图6.jpg" /><br /><img src="/img2/数字化转型路线图/数字化转型路线图7.jpg" /><br /><img src="/img2/数字化转型路线图/数字化转型路线图8.jpg" /></p><p><img src="/img2/数字化转型路线图/数字化转型路线图11.jpg" /><br /><img src="/img2/数字化转型路线图/数字化转型路线图12.jpg" /></p><p><img src="/img2/数字化转型路线图/数字化转型路线图15.jpg" /><br /><img src="/img2/数字化转型路线图/数字化转型路线图16.jpg" /><br /><img src="/img2/数字化转型路线图/数字化转型路线图17.jpg" /></p><p><img src="/img2/数字化转型路线图/数字化转型路线图19.jpg" /></p><p><img src="/img2/数字化转型路线图/数字化转型路线图27.jpg" /></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> IT </tag>
            
            <tag> 宝洁 </tag>
            
            <tag> 信息化 </tag>
            
            <tag> 数字化转型 </tag>
            
            <tag> 策略 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《微积分的人生哲学》读书心得</title>
      <link href="/2023/07/16/%E3%80%8A%E5%BE%AE%E7%A7%AF%E5%88%86%E7%9A%84%E4%BA%BA%E7%94%9F%E5%93%B2%E5%AD%A6%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2023/07/16/%E3%80%8A%E5%BE%AE%E7%A7%AF%E5%88%86%E7%9A%84%E4%BA%BA%E7%94%9F%E5%93%B2%E5%AD%A6%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>花了两天时间就读完的一本书，一方面是字数不多，另一方面，里面涉及太多的数学公式，并且还是高等数学的公式，看不懂了，也只好直接跳过去。<br />本书是一个大学数学教授和高中数学老师的针对数学问题进行书信来往讨论的一本书，间或结合讨论的问题，引出了一些人生哲学。总体来说，这本书一般般，茶余饭后看看可以，没有太大的价值。<br /><span id="more"></span></p><h1 id="不可预计的随机选择">不可预计的随机选择</h1><p>有些人对蒙提打开有羊之门之后，所剩下的两扇门的明显不对称性感到困惑。但请记住：蒙提所做的事情是非随机的，他给你看的总是山羊。这就是对称被打破的原因。<br /><img src="/img1/微积分的人生哲学/微积分的哲学人生7.jpg" /><br /><img src="/img1/微积分的人生哲学/微积分的哲学人生8.jpg" /><br /><img src="/img1/微积分的人生哲学/微积分的哲学人生9.jpg" /></p><h1 id="混沌系统">混沌系统</h1><p>把婚姻比喻成混沌系统，倒是挺有意思。很多时候都是一些微不足道的扰动导致吵架、离婚。<br /><img src="/img1/微积分的人生哲学/微积分的哲学人生12.jpg" /><br /><img src="/img1/微积分的人生哲学/微积分的哲学人生13.jpg" /></p><h1 id="最速曲线等时曲线">最速曲线、等时曲线</h1><p>直线的路径不一定是最快的。<br /><img src="/img1/微积分的人生哲学/微积分的哲学人生14.jpg" /><br /><img src="/img1/微积分的人生哲学/微积分的哲学人生15.jpg" /><br /><img src="/img1/微积分的人生哲学/微积分的哲学人生16.jpg" /></p><h1 id="有序变化无理性变化突变">有序变化、无理性变化、突变</h1><p><img src="/img1/微积分的人生哲学/微积分的哲学人生17.jpg" /><br /><img src="/img1/微积分的人生哲学/微积分的哲学人生18.jpg" /></p><h1 id="文章摘要">文章摘要</h1><ul><li>这些公式在视觉上非常美，他们就像艺术。<br /></li><li>微积分是关于变化的数学研究。<br /></li><li>微积分是以连续性为基础的学科。其核心假设就是：事物都在平缓的发生变化，任何事物都与其此前的一刻有着无限小的差异。<br /></li><li>有些问题数学是无解的，不可能获得明确的答案，正如我们的人生一样。<br /></li><li><strong>这种把世界上任何事物都看成由无限小的变化累加而成的观点，是微积分最具革命性的见解。</strong>想出如何把这种思想变成可运算的数学方法是一个突破，这直接导致了17世纪微积分的创立。<br /></li><li>大一时，老师让我们去图书馆选一本书，我每次都选同一本《原理与方法：原子能的奥秘》。<br /></li><li>生活不是线性的，生活更像划皮划艇航程中无法预测的激流。<br /></li><li><strong>虽然从卢米斯退休了，但还是念念不忘，我总是做上课的铃声响了，却还在找教室的梦。<br />PS：我自己也是经常做考试开始了，还在不断地找考试的教室。</strong><br /></li><li><strong>上面有几十个鸟笼，都标记了号码，如<span class="math inline">\(\sqrt{e}\)</span> ， <span class="math inline">\(\pi\)</span>，还有其他有意思的常量。</strong><br /></li><li>时间总是推着你，这就是人生。</li></ul><p><img src="/img1/微积分的人生哲学/微积分的哲学人生1.jpg" /><br /><img src="/img1/微积分的人生哲学/微积分的哲学人生2.jpg" /><br /><img src="/img1/微积分的人生哲学/微积分的哲学人生3.jpg" /><br /><img src="/img1/微积分的人生哲学/微积分的哲学人生4.jpg" /><br /><img src="/img1/微积分的人生哲学/微积分的哲学人生5.jpg" /><br /><img src="/img1/微积分的人生哲学/微积分的哲学人生6.jpg" /></p><p><img src="/img1/微积分的人生哲学/微积分的哲学人生10.jpg" /><br /><img src="/img1/微积分的人生哲学/微积分的哲学人生11.jpg" /></p><p><img src="/img1/微积分的人生哲学/微积分的哲学人生19.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 微积分 </tag>
            
            <tag> 书信 </tag>
            
            <tag> 人生 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《中国人史纲》读书心得</title>
      <link href="/2023/07/08/%E3%80%8A%E4%B8%AD%E5%9B%BD%E4%BA%BA%E5%8F%B2%E7%BA%B2%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2023/07/08/%E3%80%8A%E4%B8%AD%E5%9B%BD%E4%BA%BA%E5%8F%B2%E7%BA%B2%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>“中国人”这个书名就表明了这是一部平民史观的历史书，有别于传统历史书为王侯将相立传。<br />历史是不断地重复着，一个强盛的王朝，经过传承，昏君、暴君的出现，加上外戚、宦官、权臣的作乱，贪污腐败横行，民不聊生，饿殍遍野，官逼民反，然后某人当上皇帝，又开始重复着过往的历史。<br />看历史书需要跳出精彩的故事框架，从人性、利益的角度去看历史，而不是仁义道德。凡是政治都是现实的、肮脏的，而政治是超出任何领域的一门智慧学。<br />本书狠狠地批评了儒家观念、作为。儒家守旧，不思进取，存在的目的就是取悦君王，采取愚民政策来加强帝王的统治。儒家表面的仁义道德，背后的男盗女娼，从而引出了王阳明的致良知、知行合一。<br />几千年来，儒家对秦始皇不惜各种抹黑，本书也给秦始皇做了平反。在秦始皇实行车同轨，书同文，中央集权、设置郡县，两千多年的封建历史，基本在这个框架内修修补补。<br />中国的改革往往都以失败告终。商鞅的变法，被车裂，不过变法倒是保留下来；王安石的变法基本全盘失败；王莽的理想社会给全盘否定；其它如张居正的变法也无疾而终，百日维新只留下了壮烈的戊戌六君子。而日本的大化改新（全盘学唐朝）、明治维新（全盘西化）却意外的成功，书中给的见解确实有道理，一方面，大化改新的时候，中国的科举没有传到日本，这样就没有产生一群士大夫群体，改革就少了一大阻力。另一方面，在日本，天皇是万世一系的，并且权力基本是在大将军、幕府的手中，天皇更多是象征的意义，所以天皇对改革是没有利益的损失。<br />正如作者所说，中国像一个巨大的立方体，在排山倒海的浪潮中，它会倾倒。但在浪潮退去后，仍昂然地矗立在那里，以另一面正视世界，永不消失、永不沉没。就二十世纪，使人沮丧的大黑暗时代结束，五千年专制帝王制度结束悠久的但已不能适应时代的生活方式和意识形态，也被逐渐抛弃。奄奄息的华夏人返老还童，英姿焕发，创造出中国第四个黄金时代，在全世界万邦之中，充当忠实的和强大光荣的角色，而且成为最重要的主角之一。</p><p>扩展的主题：“河流与文明的关系”，一方水土养一方人。</p><span id="more"></span><h1 id="文章摘要">文章摘要</h1><ul><li>贩夫走卒<br /></li><li>中国科举制度有它的功能和贡献，但它的副产品之一是，培养出来个中国所特有的“官场”社会阶层，在这个社会阶层中， 阿谀帝王成为一项主要的课题。<br /></li><li>泾渭二河在西安的东北合流，合流之后，产生一种奇异的现象，即河水中分，泾河的速度较急，它的水沿着北岸奔驰而下，骤然进入平原反而较为清澈，而含沙量同样多的渭河，河水沿着南岸，因速度较缓，却较为浑浊。中国有一句谚语说“泾渭分明”，即借此比喻两个截然不同的事物，虽混杂在一起，而仍各保持特质。<br /></li><li>钱塘江 全长四百九十四公里，它的入海处呈现一个庞大的喇叭口形状，以“钱塘潮”闻名于世。江水和因潮汐而倒灌入江口的海水，互相搏击，加上其他迄今仍弄不清楚的奇异原因，使江潮浪头高耸天际，发出天崩地裂的巨声。尤以阴历八月十八左右，最为壮观，常吸引数十万观潮的群众，在岸上惊心动魄。<br /></li><li>成都以芙蓉花闻名，所以也称锦城或称蓉城。<br /></li><li>山东淄博。充当战国时代齐王国国都一百三十九年。是公元前四世纪时中国两大超级巨城之一 (另一是秦王国国都咸阳)。史籍上对它人口的稠密形容为:“吐气成云，挥汗如雨。<br /></li><li>有一些历史学家非常瞧不起神话在历史中的实质地位，但神话是一个民族的灵魂，一个民族的历史如果没有神话部分，这个民族不过是一群木偶而已。从神话的内容，我们可据以了解初民的生活背景和人文反映。所有的神话都是矛盾百出，有时候简直不知所云。中国的神话也是如但这更证实它是初民的产物。<br /></li><li>伊尹[ yī yǐn ]商初的贤相。相传汤伐桀，灭夏，遂王天下。汤崩，其孙太甲无道，伊尹放诸桐宫，俟其悔过，再迎之复位。卒时，帝沃丁葬以天子之礼。<br /></li><li>周王朝所属的每一个封国都有自己完整的本国史，但只有鲁国史留传下来。鲁国史称为“春秋”。留传下来的部分，起于本世纪(前八)公元前722年。史学家就从这时候起，直到公元前五世纪前481年，共二百四十二年间，称为“春秋时代”。这是一个人工的划分--犹如“世纪”也是一个人工的划分一样。事实上整个社会剧烈的变动，应起自周政府东迁，但中国历史学者在二十世纪前，全部属于儒家学派，他们一直使用这个称谓，在没有发现这种划分有重大害处之前，我们仍顺应这个习惯。<br /></li><li>姜小白的霸业即管仲的霸业，姜小白只是躯壳，管仲才是灵魂。但姜小白更为伟大，因为他能任用管仲。<br /></li><li>霸权只是超级强国维护自己利益的工具，不再含有初起时那种以保护弱者自居的骑士的意义。<br /></li><li>成大事者，不谋于众。<br /></li><li>历史不是简单的重复：<br />问题是，社会科学与自然科学不同，历史发展与化学方程式不同，同一刺激，因人、因时代和因环境的不同，反应也异。所以人们在觅取历史启示或教训时，必须特别小心。<br /></li><li>世界上有两种东西能摧毁人性和人伦，那就是权力和金钱。<br /></li><li>中国诗是世界上唯一不能翻译的文学作品。<br /></li><li>饿殍（piǎo）遍野<br /></li><li>中国古代习惯使用单音节。<br /></li><li>血流有声、血流成河。<br /></li><li>判断，是人类最高智慧的表现。判断如果错误，就必须付出判断错误的代价，小焉者是个人的失败，大焉者是国家受到伤害，甚至灭亡。对同一现象，竟产生两种完全不同的判断 (事实上有时候还产生两种以上完全不同的判断)，跟当事人的智慧见解、生活体验，以及心理背景，有密切关系。<br /></li><li>清政府的政策是，用科举控制汉人，用婚姻控制蒙古人，结果证明完全成功。<br /></li><li>华人的特性就是高度的忍辱负重。<br /></li><li>一个厉害的人非常少见了，一个厉害的人也往往刚愎自用，像李世民自己厉害之外，还能从善如流，真的是千古一人。向理性屈服不是一件容易的事情。李世民大帝个人的优秀是最主要的因素，他严厉地控制自己不去触及无限权力的毒牙，并且鼓励和接受最难堪的逆耳之言。</li></ul><p><img src="/img1/中国人史纲/《中国人史纲》读书心得1.jpg" /><br /><img src="/img1/中国人史纲/《中国人史纲》读书心得2.jpg" /><br /><img src="/img1/中国人史纲/《中国人史纲》读书心得3.jpg" /><br /><img src="/img1/中国人史纲/《中国人史纲》读书心得4.jpg" /><br /><img src="/img1/中国人史纲/《中国人史纲》读书心得5.jpg" /><br /><img src="/img1/中国人史纲/《中国人史纲》读书心得6.jpg" /><br /><img src="/img1/中国人史纲/《中国人史纲》读书心得7.jpg" /><br /><img src="/img1/中国人史纲/《中国人史纲》读书心得8.jpg" /><br /><img src="/img1/中国人史纲/《中国人史纲》读书心得9.jpg" /><br /><img src="/img1/中国人史纲/《中国人史纲》读书心得10.jpg" /><br /><img src="/img1/中国人史纲/《中国人史纲》读书心得11.jpg" /><br /><img src="/img1/中国人史纲/《中国人史纲》读书心得12.jpg" /><br /><img src="/img1/中国人史纲/《中国人史纲》读书心得13.jpg" /><br /><img src="/img1/中国人史纲/《中国人史纲》读书心得14.jpg" /><br /><img src="/img1/中国人史纲/《中国人史纲》读书心得15.jpg" /><br /><img src="/img1/中国人史纲/《中国人史纲》读书心得16.jpg" /><br /><img src="/img1/中国人史纲/《中国人史纲》读书心得17.jpg" /><br /><img src="/img1/中国人史纲/《中国人史纲》读书心得18.jpg" /><br /><img src="/img1/中国人史纲/《中国人史纲》读书心得19.jpg" /><br /><img src="/img1/中国人史纲/《中国人史纲》读书心得20.jpg" /><br /><img src="/img1/中国人史纲/《中国人史纲》读书心得21.jpg" /><br /><img src="/img1/中国人史纲/《中国人史纲》读书心得22.jpg" /><br /><img src="/img1/中国人史纲/《中国人史纲》读书心得23.jpg" /><br /><img src="/img1/中国人史纲/《中国人史纲》读书心得24.jpg" /><br /><img src="/img1/中国人史纲/《中国人史纲》读书心得25.jpg" /><br /><img src="/img1/中国人史纲/《中国人史纲》读书心得26.jpg" /><br /><img src="/img1/中国人史纲/《中国人史纲》读书心得27.jpg" /><br /><img src="/img1/中国人史纲/《中国人史纲》读书心得28.jpg" /><br /><img src="/img1/中国人史纲/《中国人史纲》读书心得29.jpg" /><br /><img src="/img1/中国人史纲/《中国人史纲》读书心得30.jpg" /><br /><img src="/img1/中国人史纲/《中国人史纲》读书心得31.jpg" /><br /><img src="/img1/中国人史纲/《中国人史纲》读书心得32.jpg" /><br /><img src="/img1/中国人史纲/《中国人史纲》读书心得33.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 历史 </tag>
            
            <tag> 中国史 </tag>
            
            <tag> 平民 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《思考快与慢》读书心得</title>
      <link href="/2021/10/24/%E3%80%8A%E6%80%9D%E8%80%83%E5%BF%AB%E4%B8%8E%E6%85%A2%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2021/10/24/%E3%80%8A%E6%80%9D%E8%80%83%E5%BF%AB%E4%B8%8E%E6%85%A2%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<h1 id="快思考慢思考">快思考、慢思考</h1><p>系统1--快思考--直觉思维--高效、代价小；但存在偏见--第六感<br />系统2--慢思考--理性思维--动脑筋，分析性思维，监控、检验系统1，但本身存在惰性，容易给系统1误导。<br /><strong>所以说，需要三思而后行，说话前，先数5下，再决定是否说。</strong></p><h1 id="偏好的四重模式">偏好的"四重模式"</h1><p><img src="/img1/《思考快与慢》读书心得9.jpg" /></p><p>首行：所得面前选择规避；在损失面前选择冒险；<br />底行：为收益愿冒风险；对损失保持谨慎。<br /><span id="more"></span><br />例子<br /><img src="/img1/《思考快与慢》读书心得10.jpg" /><br /><img src="/img1/《思考快与慢》读书心得11.jpg" /></p><h1 id="沉没成本">沉没成本</h1><p><img src="/img1/《思考快与慢》读书心得12.jpg" /><br /><img src="/img1/《思考快与慢》读书心得13.jpg" /></p><h1 id="框架思维">框架思维</h1><p><img src="/img1/《思考快与慢》读书心得14.jpg" /><br /><img src="/img1/《思考快与慢》读书心得15.jpg" /></p><h1 id="过程忽视峰终定律">过程忽视、峰终定律</h1><p><img src="/img1/《思考快与慢》读书心得16.jpg" /></p><h1 id="时间的奥秘">时间的奥秘</h1><p>拉长时间的尺度看，以前让你很难受的事情，现在来看也没任何感觉了。想想再过10年，你现在觉得迈不过的坎，也不过尔尔。一点痕迹都没有了。<br /><img src="/img1/《思考快与慢》读书心得18.jpg" /></p><h1 id="文章摘要">文章摘要</h1><ul><li>桃色新闻在政客中更多，是因为权力的催情效果。<br /></li><li>幸运在每个成功案例中都扮演着重要的角色。<br />所以说成功是不可以复制的，因为运气是不可复制。但话说，成功是努力+运气，虽然运气不可复制，但努力是基础，如果不努力，那成功的基础都失去了。<br /></li><li>当人们太过专注于某事时，大脑就会屏蔽掉其它事情。（心流：能让人忘却时间的概念，忘掉自己，也忘掉自身的问题。）<br />案例《看不见的大猩猩》<br /></li><li>切换任务是需要付出努力和能量的。<br /></li><li>联想的3大原则：相似性、时空相接、因果关系。<br /></li><li>放松是事情进展顺利的标志；紧张则说明存在某种问题，需要不断调动系统2参与其中。<br /></li><li><strong>人们很难对熟悉感和真相加以区别。所以谎言传了千遍就成了真相。</strong><br /></li><li><strong>如果你很在意自己在别人眼里是否值得信赖、是否聪明睿智，那么说话时就言简意赅吧。</strong><br /></li><li>人喜欢一致性。<br /></li><li>心理学，其实就是洞察人性的一门学科。<br /></li><li><strong>锚定效应：心理暗示，对比。谈判的时候可以利用这个策略。取上得中，取中得下。一种博弈的手段。</strong><br />讨论中，不要在对方提出的问题框架内讨论，要学会找到对方的弱点，然后抛出问题，然后锚定这些问题进行讨论。<br />心理上要肯定自己。<br /></li><li>人的利己性，所以需要站在对方的角度思考，对方的利益是什么，对方害怕什么。<br /></li><li>象统计学家那样思考：使用基础比率来进行预测，而不是根据典型性来预测。<br /></li><li>皱眉通常增强系统2的警觉性。 俗话说，眉头一皱计上心来。<br /></li><li>一个股票的价值包括了关于公司价值和对股票前景的最佳预测的所有信息。最难的判断是，是否这些信息都包含在股价里了。<br /></li><li>直觉只不过是人们的认知而已。预认知模式。<br />在环境缺乏牢靠的规律时，不要相信直觉。例如政治和股票。<br /></li><li>乐观偏见是认知偏见中最重要的一种，可能有益，也可能带来风险。<br /></li><li>损失厌恶这一概念绝对是心理学对行为经济学最重要的贡献，所以大范围的改革通常失败，变革的阻力。<br /><strong>变革的阻力是因为人类损失厌恶的心理作用。</strong><br /></li><li>我们生活中做出的每个重要的选择都会带有一定的不确定性。但人性中又存在对确定性的偏好。<br /></li><li><strong>后悔是一种情绪，也是一种自我惩罚。</strong><br /></li><li>人的思维，往往是以结果来评判，而不是享受过程。以成败论英雄。<br /></li><li>增加幸福感的最简单的方法就是分配好你的时间，你能抽出更多的时间做自己喜欢的事情吗？<br /></li><li>判断某人是否理性的唯一标准并非是看这个人的信念或偏好是否合理，而是看他们是否一致。<br /></li><li><strong>所以说，成功都是反人性的。</strong><br /></li><li><strong>当人们忙于认知活动时，更有可能做出自私的选择。又累又饿的保释官更可能否定保释申请。 所以找老板决策的时候，最好等老板心情好的时候。</strong><br />熟悉了，就会喜欢。人对熟悉的东西、熟悉的环境更放松，所以可以在老板熟悉的环境下，多给老板汇报，增加曝光度。<br />好心情使系统2放松对行为的控制：当人们心情好时，直接和创造力会增强，但也会放松警惕，易犯逻辑性错误。<br />有证据显示，当人们劳累或是精力耗尽时，更容易受那些空洞却有说服力的信息影响，例如广告。（但不是决策！只是受影响。）<br /></li><li><strong>人们偏好一致性，一个好故事最重要的是信息的前后一致性，而不是其完整性。所以说，自信、坚持自己观点的重要性。</strong><br /></li><li>在看到一个判断的时候，先思考决策的样本的大小，样本大小对于结果的影响是致命的。<br />人们对样本的大小没有足够的敏感性，对事物的信任多于质疑。<br /></li><li><strong>要思考宏观的、高大上的、形而上的东西。跳出做事的思维。找个助手去管具体的事情。</strong><br /></li><li>启发性手段其实就是所谓的经验法则。包括经验，包括先入为主等偏见。 可得性法则也类似，会根据记忆中获取的难易度来评估事情，所谓新闻媒体也在利用这方面给人们洗脑，所谓的公知就是这个套路。</li></ul><p><img src="/img1/《思考快与慢》读书心得1.jpg" /><br /><img src="/img1/《思考快与慢》读书心得2.jpg" /><br /><img src="/img1/《思考快与慢》读书心得3.jpg" /><br /><img src="/img1/《思考快与慢》读书心得4.jpg" /><br /><img src="/img1/《思考快与慢》读书心得5.jpg" /><br /><img src="/img1/《思考快与慢》读书心得6.jpg" /><br /><img src="/img1/《思考快与慢》读书心得7.jpg" /><br /><img src="/img1/《思考快与慢》读书心得8.jpg" /><br /><img src="/img1/《思考快与慢》读书心得17.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 有限理性 </tag>
            
            <tag> 思考 </tag>
            
            <tag> 直觉 </tag>
            
            <tag> 理性 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《蓝血十杰》读书心得</title>
      <link href="/2021/10/07/%E3%80%8A%E8%93%9D%E8%A1%80%E5%8D%81%E6%9D%B0%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2021/10/07/%E3%80%8A%E8%93%9D%E8%A1%80%E5%8D%81%E6%9D%B0%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>以桑顿为代表的十名军方统计控制处的精英，二战后集体加入福特汽车公司，从而改写了他们的人生，他们的人生或成功或失败，但对理性的狂热是他们共有的标签：强调数据分析、强调管理控制。他们是一群血管里流动着都是数字的人，结果让人变成数字奴隶。</p><p>成也萧何败也萧何，因为理性，因为相信数据、事实、分析、冷静、控制，摒弃传统的经验、直觉、判断力、热情、想象力，让他们获得了巨大的成功。但是物极必反，也正是因为他们这种超乎寻常的理性，也为他们的最终失败埋下了伏笔。<br />大数据不能代替企业家，企业家在面对复杂多变的经济环境、竞争态势时，他们的意志、关系处理能力、应变能力，甚至天生的直觉和想象力，这些远远不是冷冰冰的数据所能代替，这些需要的是面对混沌时的一种天生的领导力。<br />变化才是永恒不变的真理，所有的成功都是阶段性的。俗话说，面多加水，水多加面，理性也需要和感性相结合，两者不是互相替代的关系！要把握一个度。<br />正如《追求卓越》的作者所说，不能因为理性而扼杀了创新，降低了对客户对产品的关注。<br /><span id="more"></span><br /><img src="/img1/蓝血十杰19.jpeg" /><br /><img src="/img1/蓝血十杰20.jpeg" /></p><h1 id="所有明智的决定都基于三个因素">所有明智的决定，都基于三个因素</h1><ol type="1"><li>聪明、有经验又有扎实教育背景的决策人才；<br /></li><li>常识和判断力；<br /></li><li>事实。</li></ol><h1 id="数字权力">数字权力</h1><p>让数字说话；分析报告是很好的载体，阐述数字的人的天然权力。</p><h1 id="米尔斯忍辱负重">米尔斯：忍辱负重</h1><p>毅力、决心。人生是一场修行之路。<br /><img src="/img1/蓝血十杰11.jpeg" /><br /><img src="/img1/蓝血十杰12.jpeg" /></p><h1 id="坚定和自信">坚定和自信</h1><p><img src="/img1/蓝血十杰31.jpeg" /></p><h1 id="面对不同的观点">面对不同的观点</h1><p><img src="/img1/蓝血十杰32.jpeg" /></p><h1 id="文章摘要">文章摘要</h1><ul><li>在不理性的世界里寻找理性的答案。<br /></li><li>底特律:汽车; 曼彻斯特:工业; 硅谷:IT。<br /></li><li>战争是面对面的残酷和勇气的较量，也是一场调动资源的竞赛。（兵马未动，粮草先行）<br /></li><li>膨胀起来的大型企业需要更多的规范与控制以免被自身重量压垮。<br /></li><li>人类比自己想象的更愚蠢，任何问题，我们只能找到暂时的答案。变化才是永恒不变的真理。<br /></li><li><strong>教导经理人，只相信数字，不相信人。把人变成数字的奴隶。把系统变成宗教，变成信仰。</strong><br /></li><li><strong>信息情报就是力量</strong><br /></li><li>把组织的力量误认为自己的力量。<br /></li><li>企业不是科学，甚至还算不上是艺术。（因为企业背后是人，而人性又是最不可捉摸的。）<br /></li><li>以思考之力量，克服习惯之惰性。<br /></li><li><strong>亲人让你热爱这个世界；敌人让你成长；（但是也只有敢于斗争，善于斗争才能成长，一味的退却不会让你成长）；斗争的艺术，锻炼自己成长的机会。</strong><br />是不是在做正确的事，正确的路往往更难走；老毛：与天斗、与地斗、与人斗，其乐无穷。<br />站着死。<br />塞翁失马焉知非福；<br /></li><li><strong>领导力：梦想、画饼的能力。激励大家的能力。</strong><br />把重点都集中在未来，让人跳开日常的世俗琐事，强迫别人展望未来的远景。<br /></li><li><strong>天真的作用，只是让人毫无防御能力。（害人之心不可有，防人之心不可无）</strong><br /></li><li><strong>教父：想想你身边的人怎么想的。（对方的利益所在，对方怕什么。），对人性的理解。</strong><br /></li><li>最初的6个月我们可能会失望，幻想会破灭，我们必须克服它，脑中切记着未来美好前景，继续工作下去。<br /></li><li>他们所受的军事训练已使他们对无孔不入的侮辱习以为常。<br />这个团体的容忍和克制，在一群知识分子中通常是极其罕见的。<br />工作中肯定有不开心的时刻。<br /></li><li>让手下互相为敌，避免某人权力过大。（帝王心术）<br /></li><li>炫目者未必是金。<br /></li><li>模仿是无法走向成功的，只有创新的做法才能为成功铺路。<br /></li><li>华盛顿这个除了谣言，什么都无法制造的城市。<br /></li><li>大公司要有耐心来推动事情的发展，不可能一蹴而就。要有耐心。<br /></li><li>组织的核心是解决授权问题，操作上分权，管理上集权。<br />管理人员：计划、决策和控制。<br /></li><li>诚实是最好的策略。<br /></li><li>信心：锻炼的好机会，成功让你身价百倍，失败也拥有了经验。<br /></li><li>风险和收益成正比；但大多数人都想以最低的风险寻求最大的利润。<br /></li><li><strong>好，我会重新组织，再想想不同的战斗计划。（不达目标，永不投降）</strong><br /></li><li>不过拖延几乎就跟否决一样糟糕，这是企业里封杀任何事情的好方法，把它退回去做进一步研究，然后就不了了之了。<br /></li><li>失败往往是成功的黎明前短暂的黑暗。（失败是成功之母）<br />如果一个人没有失败过，那只能说明他人生定的目标太低，没有挑战过困难的事情。<br /></li><li><strong>大公司内部为争取控制权而发生一些争吵是很正常的事。<br />其间的过程充满了怀疑和马后炮式的批评，不过一个优秀的斗士总能熬过这段政治长路，不过也少不了许多失眠的夜晚。</strong><br /></li><li>不能让老板觉得处于寡不敌众的态势。<br /></li><li>因为老板讲个笑话，每个人会笑得更用力。<br /></li><li>传给高层的消息必须是好消息！<br /></li><li>形容词不能代替数字。<br /></li><li>有太多我们所谓的管理意在使大家难以工作。---- 彼得·德鲁克<br /></li><li>福特是由数豆子的人，而不是由爱车的人控制。<br /></li><li>留在总部，事后批评别人是一回事，到前线必须做困难的决策，比较容易遇到危险，那就是另一回事了。<br /></li><li>事情不扰人，扰人的是观点。</li></ul><p><img src="/img1/蓝血十杰01.jpeg" /><br /><img src="/img1/蓝血十杰02.jpeg" /><br /><img src="/img1/蓝血十杰03.jpeg" /><br /><img src="/img1/蓝血十杰04.jpeg" /><br /><img src="/img1/蓝血十杰05.jpeg" /><br /><img src="/img1/蓝血十杰06.jpeg" /><br /><img src="/img1/蓝血十杰07.jpeg" /><br /><img src="/img1/蓝血十杰08.jpeg" /><br /><img src="/img1/蓝血十杰09.jpeg" /><br /><img src="/img1/蓝血十杰10.jpeg" /><br /><img src="/img1/蓝血十杰13.jpeg" /><br /><img src="/img1/蓝血十杰14.jpeg" /><br /><img src="/img1/蓝血十杰15.jpeg" /><br /><img src="/img1/蓝血十杰16.jpeg" /><br /><img src="/img1/蓝血十杰17.jpeg" /><br /><img src="/img1/蓝血十杰18.jpeg" /><br /><img src="/img1/蓝血十杰21.jpeg" /><br /><img src="/img1/蓝血十杰22.jpeg" /><br /><img src="/img1/蓝血十杰23.jpeg" /><br /><img src="/img1/蓝血十杰33.jpeg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 管理 </tag>
            
            <tag> 理性 </tag>
            
            <tag> 数字 </tag>
            
            <tag> 福特 </tag>
            
            <tag> 事实 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《目标感》读书心得</title>
      <link href="/2021/09/20/%E3%80%8A%E7%9B%AE%E6%A0%87%E6%84%9F%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2021/09/20/%E3%80%8A%E7%9B%AE%E6%A0%87%E6%84%9F%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>很久没有写读书笔记了，前段时间忙着儿子的高考，忙着换工作，现在终于稳定下来，也有时间看书和写读书笔记了。<br />目标，就是人生的灯塔，指引着前进的道路。首先，讨论到目标，必须明确目标的定义，在本书中，对<strong>目标的定义如下：是指为了完成对自我有意义，同时对自我之外的世界也有意义的事情时，产生稳定且可概括的意图。</strong>目标的本质是确保一个人至少在足够长的时间里，在其行动上做出承诺，并且通常在对它的实现上还要有所进展。<br />一个崇高的目标，不一定非得是英雄式的，或是需要非常大胆的冒着生命危险的行为，关键是为他人提供价值。例如对我们普通人来说，<strong>可以从工作中找到目标感，将工作视为奉献社会和承担家庭责任的一种方式。</strong><br /><span id="more"></span><br />在历史中，有各种各样的人生目标，有远大的目标，也有阶段性的目标。这些决定了人生的道路。例如周总理的为中华之崛起而读书，教父为了家族的未来而忍辱负重，这些都决定了他们后半生的人生道路。<br />人生目标就是终极关切。人生目标是短期目标和驱使大多数日常行为的动机背后所隐藏的深层根源。所以对年轻人来说，我们的教育就是让学生构建人生的目标感，点燃自己的人生。同时，年轻人在实现目标的时候，需要从家庭获得帮助，也需要良师益友的帮助。</p><p>鼓励创业精神是年轻人实现目标，锻炼自己的最好的方式：<br /><img src="/img1/目标感7.jpg" /><br /><img src="/img1/目标感8.jpg" /></p><h1 id="文章摘要">文章摘要</h1><ul><li>那些追逐崇高目标的人总是心怀喜悦，即便是要付出持续的牺牲，他们也感觉有使命在驱使自己这样做。<br /></li><li>一个人缺乏动机，背后的原因是因为缺乏目标感。<br /></li><li>责任、应对挑战，游戏心态；与天斗、与地斗、与人斗，其乐无穷。<br /></li><li>人类最真实也最深切的渴望，就是普遍存在对有意义生活的渴望。<br /></li><li>追寻崇高的目标，一方面意味着献身于某件值得做的事情，另一方面也意味着以高尚的方式去做。<br /></li><li>浅尝辄止<br /></li><li><strong>因焦虑而想出来的短期策略，是无法孕育想象力和勇气的。只有把眼光放长远，以充满活力的目标驱动，才能培养和建立未来所需要的各项技能。</strong><br /></li><li>互联网的原罪:碎片化；利用人的弱点获利。<br /></li><li><strong>父母应该和小孩分享自己的目标和目标感；但很多时候家长连自己的目标都没有找到，有如何引导小孩来树立自己的目标呢？</strong><br /></li><li>每个人都有自己的角色。<br /></li><li><strong>从父母口中说出最有价值的四个字“你能做到。”</strong><br /></li><li>最好的防守就是进攻。</li></ul><p><img src="/img1/目标感1.jpg" /><br /><img src="/img1/目标感2.jpg" /><br /><img src="/img1/目标感3.jpg" /><br /><img src="/img1/目标感4.jpg" /><br /><img src="/img1/目标感5.jpg" /><br /><img src="/img1/目标感6.jpg" /><br /><img src="/img1/目标感9.jpg" /><br /><img src="/img1/目标感10.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人生 </tag>
            
            <tag> 目标 </tag>
            
            <tag> 教育 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《天路历程，从今生到永世》读书心得</title>
      <link href="/2021/02/15/%E3%80%8A%E5%A4%A9%E8%B7%AF%E5%8E%86%E7%A8%8B%EF%BC%8C%E4%BB%8E%E4%BB%8A%E7%94%9F%E5%88%B0%E6%B0%B8%E4%B8%96%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2021/02/15/%E3%80%8A%E5%A4%A9%E8%B7%AF%E5%8E%86%E7%A8%8B%EF%BC%8C%E4%BB%8E%E4%BB%8A%E7%94%9F%E5%88%B0%E6%B0%B8%E4%B8%96%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p><strong>基督教的逻辑是：</strong><br />1、前提：人都有原罪，会受到末日审判。<br />2、问题：如何得救？<br />3、方案：信耶稣得永生，并且总要信。<br /><img src="/img1/天路历程2.jpg" /><br />《天国历程》类似《西游记》，讲述了基督徒（天路客）为了获得救赎，为了到达天上耶稣撒冷的锡安山，进入那荣耀之门，得到永生的艰苦历程。过程有犹豫、有险阻、不过在福音师的指引下，在忠信、盼望等教友的支持下，最终还是达到天国，得到永生。<br />整部书是通过梦境寓言的方式推进，书中引用了很多圣经的话语，整本书就是班扬提供的一卷《圣经》的经文注释。换句话说，这就是一本传道书，通过寓言，教诲人们要遵从上帝的旨意，沿着上帝的教诲之道，达到赎罪、进入天国。<br /><span id="more"></span></p><h1 id="文章摘要">文章摘要</h1><ul><li>基督徒，天路客，是一趟孤独而寂寞之旅，寻求生命终极依归的孤独者。<br />在这灵魂深处的经历中，人是不可能有同伴的。<br /></li><li>他们的旅程和通往荣耀之路的故事。<br /></li><li>意志薄弱的时候，听从误导，误入歧途。<br /></li><li>不要疑惑、总要信。 中国话，心诚则灵。<br /></li><li>没有一个活着的人可以摆脱他们的重担。 （人生的本质是苦难的。）<br /></li><li>锡安山，上帝居住的地方，天上的耶路撒冷。<br /></li><li>投机：宗教只是用来达到他们目的的托词而已。为这世界的缘故而信神的，也将为这世界的缘故将神丢弃。<br /></li><li>坚守正直、诚实的品质。<br /></li><li>殉道是通往天国的最短的路径。<br /></li><li><strong>突然发现自己是个普通人，为各种软弱所困。</strong><br /></li><li>温柔的良心是懦弱的表现。<br /></li><li>油画家的画远看顶好，凑近瞧可就难看了。</li></ul><p><img src="/img1/天路历程1.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 基督教 </tag>
            
            <tag> 天路客 </tag>
            
            <tag> 基督徒 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《搞定》读书心得</title>
      <link href="/2021/02/14/%E3%80%8A%E6%90%9E%E5%AE%9A%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2021/02/14/%E3%80%8A%E6%90%9E%E5%AE%9A%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>GTD：Getting Things Done<br />关注主要目标和价值观能为我们提供一个必要的选择标准。<br />2分钟的思考训练：碰到事情，进行两分钟的思考，构建以结果和行动为导向的思维方式（谋定而后动）,然后记录下来（清理大脑中的全部事务）。<br /><img src="/img1/搞定1.jpg" /></p><span id="more"></span><h1 id="横向管理工作流程的5个步骤">横向管理工作流程的5个步骤</h1><ol type="1"><li>（收集）收集引起我们注意的事务和信息；<br /></li><li>（理清）理清每个项目的意义和相关措施；<br /></li><li>（整理）组织整理结果，提出选项；<br /></li><li>（回顾）进行思考问题；（每周回顾）<br /></li><li>（行动）选择行动。</li></ol><p><img src="/img1/搞定2.jpg" /><br /><img src="/img1/搞定8.jpg" /></p><h1 id="纵向管理项目的5个阶段">纵向管理项目的5个阶段</h1><p>当你着手处理平凡琐事的时候，必须着眼于大局，这样一来，所有的繁琐的小事才能够沿着正确的方向发展。<br />1、定义目标和原则（SMART：具体的、可衡量的、可达成的、相关的、有时限的）<br />   <strong>问问为什么？</strong></p><ol type="1"><li>界定成功<br /></li><li>建立决策标准<br /></li><li>调配资源<br /></li><li>阐明重点<br /></li><li>拓宽选择</li></ol><p>2、展望结果<br />3、头脑风暴/集思广益（不判断、不质疑、不评估、不批判）<br />4、组织整理<br />5、明确下一步的行动方案。</p><p><strong>有疑虑的时候，尝试向上思考，回归上一个层面去思考。</strong></p><h1 id="总体检视工作的6层次法">总体检视工作的6层次法</h1><p><img src="/img1/搞定7.jpg" /></p><h1 id="摆脱烦恼">摆脱烦恼</h1><p><strong>切换到不同层面、不同视角去看待问题。活在当下。</strong><br />世上有许多事情你视而不见，只是因为你的态度和高度所限。<br />情绪低落的时候，不妨拿出自己收集的别人对自己的肯定的看法通读一遍，这样可以增强自己的自信心、振作精神。<br />如果某件事情引起你的注意，让你烦恼，你需要多长时间才能摆脱这些感觉。在内心做好准备，同时从全新的视角稳妥的投入到必须面对的新工作。<br />采取新的视角看问题，从新的视角带来新的行动。<br /><strong>为了迅速提升到更高的层面，你能放弃自己对某个层面的控制吗？有舍才有得！</strong></p><h1 id="大目标">大目标</h1><p>从战略的角度思考，梦想；幻想者模式。<br /><strong>不要制定小目标，它们没有让人热血沸腾的魔力，本身也很可能不能实现。要制作大计划。目标要高远</strong>，工作时要牢记，崇高而富有逻辑的计划一旦制定将永远有效。</p><h1 id="工具">工具</h1><p>苹果日历程序、Trello看板、印象笔记。</p><h1 id="邮件处理的技巧">邮件处理的技巧</h1><p><img src="/img1/搞定9.jpg" /><br /><img src="/img1/搞定10.jpg" /></p><h1 id="记日志的习惯">记日志的习惯</h1><p><img src="/img1/搞定12.jpg" /></p><h1 id="文章摘要">文章摘要</h1><ul><li>知识工作者的定义：不确定性<br /><img src="/img1/搞定3.jpg" /><br /></li><li>移动、互联、始终在线<br /></li><li><strong>放松精神的技巧，远离烦恼与忧虑的能力，很可能就是伟人们成功的秘诀之一。</strong>（举重若轻）<br /><strong>你发挥能量的能力与你放松的能力成正比。</strong><br /></li><li>形形色色一心往上爬的人想拼命挤进贵族阶层，然而，他们挖空心思却只能证明自己并不身属此类。 （类似了不起的盖茨比）<br /></li><li><strong>产生心理压力的一个重要原因是，别人的行为超出了你设立的标准，或者别人容忍此类行为的存在。</strong><br /></li><li>你到底需要多详实的计划？需要详细到什么程度呢？一个简单的答案就是，只要你能够让你的大脑摆脱事务的纠缠就足够了。<br /></li><li><strong>跑步最困难的时刻是穿起跑鞋的那瞬间。</strong><br /></li><li><strong>不论我们感觉前途多么黯淡，上帝永远不会放弃我们。（生命总会找到出口）</strong><br /></li><li>列出行动和项目清单不只是为了完成工作，然后无所事事。处理自己关注的事情的目的，在于你可以做自己真正喜欢的事情。虽然这些事情不在清单里面，但是因为你头脑中放下了其他，就可以全身心投入到你喜欢的事情上面。<br /></li><li>并非所有重要的东西都能被计算出来，而所有能够被计算出来的东西未必都是重要的。<br /></li><li>团队原则<br /><img src="/img1/搞定11.jpg" /><br /></li><li>只有在面对如何分配有限资源这种做出艰难选择之际，你才真正需要明确目标、提高效率。<br /></li><li>无论是个人竞争力还是组织的竞争力，取决于应对突发事件的能力，只有意外变化带来的压力向系统和行为发起挑战时，卓越才能真正得到呈现。<br /></li><li><strong>如果一切尽在掌握，就会进展缓慢。（拥抱不确定性的道理）</strong><br />知识工作者的特征就是不确定性，需要人的创造性思维。<br /></li><li>做最坏的打算、设想最好的情况，放弃中庸之道。<br /></li><li>开车和修车的技术是不一样的。 甲方是有开车技术，乙方才有修车的技术。</li></ul><p><img src="/img1/搞定4.jpg" /><br /><img src="/img1/搞定5.jpg" /><br /><img src="/img1/搞定6.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 工作 </tag>
            
            <tag> 效率 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Markdown使用示范</title>
      <link href="/2021/01/01/Markdown%E4%BD%BF%E7%94%A8%E7%A4%BA%E8%8C%83/"/>
      <url>/2021/01/01/Markdown%E4%BD%BF%E7%94%A8%E7%A4%BA%E8%8C%83/</url>
      
        <content type="html"><![CDATA[<p>本文是MD编辑中常用的一些命令及格式。<br /><span id="more"></span><br /><img src="/img/markdown.png" /></p><p>破折号 —— 共2 个 — ，占据两个全角字符位置</p><h1 id="手工数学公式生成器">手工数学公式生成器</h1><p>https://webdemo.myscript.com/</p><p>公式：当你需要在编辑器中插入数学公式时，可以使用两个美元符 $$ 包裹 TeX 或 LaTeX 格式的数学公式来实现。<br />Latex数学公式中的空格：</p><p>wiki中copy过来的代码中，公式的前后分别增加$$，同时，如果有"",例如</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;\displaystyle \scriptstyle &#123;\frac &#123;\mathrm &#123;d&#125; &#125;&#123;\mathrm &#123;d&#125; x&#125;&#125;\log _&#123;a&#125;(x)=&#123;\frac &#123;1&#125;&#123;x\ln(a)&#125;&#125;&#125;  \scriptstyle \frac&#123;\mathrm&#123;d&#125;&#125;&#123;\mathrm&#123;d&#125;x&#125;\log_a(x) = \frac&#123;1&#125;&#123;x\ln(a)&#125;  </span><br><span class="line">要把第一个\scriptstyle去掉，同时，去掉第二个\scriptstyle及后面的公式。</span><br><span class="line">变成：</span><br><span class="line">$$&#123;\scriptstyle &#123;\frac &#123;\mathrm &#123;d&#125; &#125;&#123;\mathrm &#123;d&#125; x&#125;&#125;\log _&#123;a&#125;(x)=&#123;\frac &#123;1&#125;&#123;x\ln(a)&#125;&#125;&#125;$$</span><br></pre></td></tr></table></figure><p>显示结果如下：</p><p><span class="math display">\[{\scriptstyle {\frac {\mathrm {d} }{\mathrm {d} x}}\log _{a}(x)={\frac {1}{x\ln(a)}}}\]</span></p><p>数学公式的语法：<br />http://kuing.orzweb.net/viewthread.php?tid=5</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">两个quad空格：a \qquad b  </span><br><span class="line">1个quad空格：a \quad b   </span><br><span class="line">大空格： a\ b  </span><br><span class="line">中等空格：a\;b  </span><br><span class="line">小空格：a\,b   </span><br><span class="line">没有空格：ab  </span><br><span class="line">紧贴：a\!b</span><br><span class="line"></span><br><span class="line">半角空格: &amp;ensp;或 &amp;#8194;</span><br><span class="line">全角空格: &amp;emsp;或 &amp;#8195;</span><br><span class="line">不换行空格: &amp;nbsp;或 &amp;#160;</span><br></pre></td></tr></table></figure><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">公式对齐  </span><br><span class="line">$$  </span><br><span class="line">\begin&#123;align*&#125;  </span><br><span class="line"> f(x) &amp;= (x+a)(x+b) \\  </span><br><span class="line"> &amp;= x^2 + (a+b)x + ab  </span><br><span class="line">\end&#123;align*&#125;  </span><br><span class="line">$$  </span><br><span class="line">它采用“&amp;”分割各个对齐单元，使用“\\”换行。</span><br></pre></td></tr></table></figure><h1 id="链接">链接</h1><h2 id="带标题的图片链接">带标题的图片链接</h2><figure><img src="/img/markdown.png" title="图片 Title" alt="图片alt" /><figcaption aria-hidden="true">图片alt</figcaption></figure><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">插入图片Markdown语法代码：  </span><br><span class="line">![图片alt](图片链接 &quot;图片title&quot;)。</span><br><span class="line">图片Alt 就是显示在图片下方的图片说明</span><br></pre></td></tr></table></figure><p><strong>注意：图片前后要空一行才能显示alt标识。</strong></p><p>在HTML中，image标签用于嵌入图像到网页中。除了指定图像路径之外，图像元素还有两个常用的属性：alt和title。虽然这两个属性都对图像的访问和可访问性具有重要作用，但它们的用途不完全相同：</p><p>alt属性：<br />alt代表“替代文本”，可用于描述无法显示图像的原因或代替图像。如果图像无法显示，这个属性将替换成描述图像的文字。对于视力障碍者和使用屏幕阅读器的人来说，这是非常重要的，因为他们无法获得图像的视觉信息，而是通过阅读屏幕阅读器中的文本来理解页面。此外，alt属性也有助于SEO优化，因为它提供了更多的关键字和描述信息。 title属性：<br />title代表“标题”，可用于添加有关图像的附加信息，例如图像的作者，照片的地点等。当用户将鼠标悬停在图像上时，将显示一个小文本框，其中包含该属性的值。此属性对于提供有关图像的附加信息和上下文非常有用，并为用户提供更多详细信息。但请注意，这个属性不应视为必要的可访问性功能。</p><p>综上所述，alt属性用于提供与图像相关的重要信息，而title属性用于为图像提供更多的背景和描述信息，使用户更好地了解图像的上下文和含义。</p><h2 id="文件链接">文件链接</h2><p><a href="/img/Oracle主数据-zhs.pdf">Oracle主数据</a></p><h2 id="网页链接">网页链接</h2><p><a href="https://www.zhihu.com/question/20409634">Markdown使用说明</a></p><h1 id="格式">格式</h1><h2 id="给字体加颜色">给字体加颜色</h2><p>需要用到Html。（！好像不能放到一段的最前面！需要在前面增加： ）</p><div class="sourceCode" id="cb1"><pre class="sourceCode tex"><code class="sourceCode latex"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>蓝色文字   </span><span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>&amp;nbsp;&lt;font color=&quot;#4590a3&quot; size = &quot;3px&quot;&gt;&lt;/font&gt;  </span><span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span><span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>午夜蓝：</span><span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>&amp;nbsp;&lt;font color=&quot;MidnightBlue&quot; size = &quot;3px&quot;&gt;&lt;/font&gt;  </span><span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span><span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>红色文字  </span><span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>&amp;nbsp;&lt;font color=&quot;#FF0000&quot; size = &quot;3px&quot;&gt;&lt;/font&gt;  </span></code></pre></div><h2 id="粗体字和斜体字">粗体字和斜体字</h2><p><strong>粗体字</strong>,<em>斜体字</em></p><h2 id="左缩进">左缩进</h2><h3 id="方法1">方法1：</h3><p>          缩进测试缩缩进测试缩进测试缩进测试缩进测试缩进测试缩进测试缩进测试缩进测试缩进测试缩进测试缩进测试缩进测试缩进测试进测试缩进测试缩进测试缩进测试缩进<br />### 方法2<br />　　（前面加两个全角空格）缩进测试缩缩进测试缩进测试缩进测试缩进测试缩进测试缩进测试缩进测试缩进测试缩进测试缩进测试缩进测试缩进测试缩进测试进测试缩进测试缩进测试缩进测试缩进</p><h2 id="标题的使用方法">标题的使用方法</h2><div class="sourceCode" id="cb2"><pre class="sourceCode tex"><code class="sourceCode latex"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a># 一级标题  </span><span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>## 二级标题  </span><span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>### 三级标题  </span><span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>#### 四级标题  </span><span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>##### 五级标题  </span><span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>###### 六级标题</span></code></pre></div><h2 id="无序列表">无序列表</h2><div class="sourceCode" id="cb3"><pre class="sourceCode tex"><code class="sourceCode latex"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>- 文本1  </span><span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>- 文本2  </span><span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>- 文本3</span></code></pre></div><h2 id="有序列表">有序列表</h2><div class="sourceCode" id="cb4"><pre class="sourceCode tex"><code class="sourceCode latex"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>1. 文本1  </span><span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>2. 文本2  </span><span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>3. 文本3</span></code></pre></div><h2 id="多级列表">多级列表</h2><p>多级列表只要4个空格或者一个Tab的缩进即可。</p><h2 id="引用别人的文字">引用别人的文字</h2><blockquote><p>下面的文字是引用自网络查找“......”</p></blockquote><blockquote><p>下面文字引用自“。。。。。。”</p></blockquote><div class="sourceCode" id="cb5"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>## 使用```表示代码块  </span><span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>支持的语言：actionscript, apache, bash, clojure, cmake, coffeescript, cpp, cs, css, d, delphi, django, erlang, go, haskell, html, http, ini, java, javascript, json, lisp, lua, markdown, matlab, nginx, objectivec, perl, php, python, r, ruby, scala, smalltalk, sql, tex, vbscript, xml</span></code></pre></div><p><code>javascript （支持的语言，不显示在页面的）   var canvas = document.getElementById("canvas");   var context = canvas.getContext("2d");</code></p><h1 id="特殊">特殊</h1><h2 id="符号转义">符号转义</h2><p>如果你的描述中需要用到 markdown 的符号，比如 _ # * 等，但又不想它被转义，这时候可以在这些符号前加反斜杠，如 _ # * 进行避免。</p><h2 id="特殊字符自动转换">特殊字符自动转换</h2><p>在 HTML 文件中，有两个字符需要特殊处理： &lt; 和 &amp; 。 &lt; 符号用于起始标签，&amp; 符号则用于标记 HTML 实体，如果你只是想要显示这些字符的原型，你必须要使用实体的形式，像是 &lt; 和 &amp;。</p><h1 id="如何站内链接文件">如何站内链接文件</h1><p>例子如下，站内有一个文件名“文章发表时间显示了两遍.md”的文件，链接的语法如下，注意不需要后缀名。</p><div class="sourceCode" id="cb6"><pre class="sourceCode tex"><code class="sourceCode latex"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>&#123;% post_link 文章发表时间显示了两遍 %&#125;</span></code></pre></div><p>表格语法</p><div class="sourceCode" id="cb7"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>&lt;style&gt;  </span><span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>table th:first-of-type &#123;  </span><span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    width: 100px;  </span><span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>&#125;  </span><span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>  </span><span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>table th:nth-of-type(2) &#123;  </span><span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    width: 200px;  </span><span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>&#125;  </span><span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>table th:nth-of-type(3) &#123;  </span><span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    width: 800px;  </span><span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>&#125;  </span><span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>&lt;/style&gt;  </span><span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>   </span><span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>换行用&lt;br&gt; or  &lt;br /&gt;  </span><span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>  </span><span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>| 水果        | 价格    |  数量  |  </span><span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>    | --------   | -----:   | :----: |  </span><span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>    | 香蕉        | $1      |   5    |  </span><span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>    | 苹果        | $1      |   6    |  </span><span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>    | 草莓        | $1      |   7  &lt;br&gt; 8  |  </span><span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>  </span><span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>  </span><span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>    | 左对齐标题 | 右对齐标题 | 居中对齐标题 |  </span><span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>| :------| ------: | :------: |  </span><span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>默认标题栏居中对齐，内容居左对齐。  </span></code></pre></div><p>markdown里如何显示HTML标签和转义字符？<br />https://blog.csdn.net/microcosmv/article/details/51868284</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>《宏观经济学》读书心得</title>
      <link href="/2020/09/20/%E3%80%8A%E5%AE%8F%E8%A7%82%E7%BB%8F%E6%B5%8E%E5%AD%A6%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2020/09/20/%E3%80%8A%E5%AE%8F%E8%A7%82%E7%BB%8F%E6%B5%8E%E5%AD%A6%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<h1 id="一国的收入衡量">一国的收入衡量</h1><p>供给与需求这种基本工具既是微观经济分析的中心，又是宏观经济分析的中心。<br />GDP 同时衡量两件事：经济中所有人的总收入和用于经济中物品和服务产出的总支出。<br />国内生产总值：Gross domestic product<br />GDP是某一既定时期一个国家内生产的所有最终物品和服务的市场价值。</p><figure><img src="/img1/宏观经济学2.jpg" title="循环流量图" alt="循环流量图" /><figcaption aria-hidden="true">循环流量图</figcaption></figure><span id="more"></span><p>折旧被称为规定资本的消费。<br />可支配收入：家庭和非公司制企业完成他们对政府的义务之后剩下的收入。</p><blockquote><p>几个恒等式<br />GDP = C + I + G + NX<br />GDP = 消费 + 投资 + 政府购买 + 净出口<br />考虑在一个简化后的封闭经济体（没有进出口）,GDP 用Y表示<br />Y - C - G = I<br />左边是用于消费和政府购买后剩下的一个经济中的总收入，这个量称为国民储蓄。 用 S 代替Y - C - G，得到<br />S = I<br />这个等式说明，储蓄等于投资。<br />用T表示政府以税收的形式从家庭得到的数量减去以转移支付形式返还给家庭的数量，得：<br />S = (Y - T - C) + (T - G)<br />这个等式把国民储蓄分为:私人储蓄（ Y - T - C ） 和公共储蓄 (T - G)</p></blockquote><p>消费：家庭除购买新住房之外用于物品和服务的支出。<br />投资：用于资本设备、存货和建筑物的支出，包括家庭用于购买新住房的支出。<br />政府购买：地方、州和联邦政府用于物品和服务的支出。包括政府员工的工资和用于公务的支出。（转移支付不计入政府购买）</p><p>名义GDP：按现期价格评价物品与服务的生产；<br />真实GDP：按不变价格评价物品与服务的生产。<br />当经济学家谈到经济GDP时，他们通常是指真实的GDP。<br />GDP平减指数：用名义GDP与真实GDP的比率乘以100计算的物价水平衡量指标。</p><p>经济衰退的经验判断是真实GDP连续两个季度下降。<br />GDP是一个好的经济衡量指标，但GDP不能衡量幸福，没有包括环境质量，也没有涉及到收入分配。<br />亚当·斯密 的看不见的手理论，即人们追求自己的私利讲给整个社会带来利益。<br /><strong>用GDP这样的统计数字把经济状况量化是发展宏观经济学这门学科的第一步。 </strong></p><h1 id="生活费用的衡量">生活费用的衡量</h1><p>CPI 消费物价指数 consumer price index 普通消费者所购买的物品与服务的总费用的衡量指标。<br />消费者物价指数 = （ 当年一篮子物品与服务的价格 / 基年一篮子的价格 ） * 100<br />通货膨胀率 = ( 第二年CPI - 第一年 CPI) / 第一年CPI * 100%<br />从前一个时期以来物价指数变动的百分比。<br />生产物价指数 Producer price index PPI 企业购买的一篮子物品与服务的费用衡量指标。<br />消费者物价指数的目的是衡量生活费用的变动。存在3个问题：<br />1. 替代偏向<br />2. 新物品的引进<br />3. 无法衡量的质量变动</p><figure><img src="/img1/宏观经济学3.jpg" title="通货膨胀的两个衡量指标" alt="通货膨胀的两个衡量指标" /><figcaption aria-hidden="true">通货膨胀的两个衡量指标</figcaption></figure><p>名义利率：通常公布的、未根据通货膨胀的影响校正的利率；<br />真实利率：根据通货膨胀的影响校正过的利率。<br />真实利率 = 名义利率 - 通货膨胀率</p><h1 id="生产和增长">生产和增长</h1><p>学术：从简单的现象入手，再类比推开，例如从麻雀开始解剖。<br />生产率的决定因素：物质资本、人力资本、自然资源、技术知识。<br />物质资本：用于生产物品与服务的设备和建筑物存量。<br />人力资本：工人通过教育、培训和经验而获得的知识和技能。<br />自然资源：由自然界提供的用于生产物品与服务的投入，如土地、河流和矿藏。<br />技术知识：社会对产生物品与服务的最好方法的了解。</p><p>技术知识和人力资本的区别：知识是社会教科书的质量，而人力资本是人们用于阅读这本教科书的时间量。<br />储蓄和投资：由于资本是生产出来的生产要素。提高未来生产率的一种方法是把更多现期资源投资于资本的生产。</p><figure><img src="/img1/宏观经济学4.jpg" title="生产函数图示" alt="生产函数图示" /><figcaption aria-hidden="true">生产函数图示</figcaption></figure><p>如上图所示，有追赶效应，开始时贫穷的国家倾向于比开始时富裕的国家增长更快的特征。<br />人力资本的投资和物质资本的投资一样，也有机会成本。<br />在某些方面看，贸易是一种技术。通过进口某物品，相当于获得了生产该产品的技术。<br />解决了激励就解决了贫穷。</p><h1 id="储蓄投资和金融体系">储蓄、投资和金融体系</h1><p>金融体系：经济中促使一个人的储蓄与另一个人的投资相匹配的一组机构。<br />出售股票来筹集资金称为权益融资，而出售债券筹集资金称为债务融资。<br />挤出：政府借款所引起的投资减少。<br />当政府通过预算赤字减少了国民储蓄时，利率就会上升，且投资减少。<br />金融市场与大多数市场不同，起着联系现在与未来的重要作用。</p><h1 id="金融学基本工具">金融学基本工具</h1><p>金融学：研究人们如何在某一时期内做出关于配置资源和应对风险的决策的学科。</p><p>现值：用现行利率产生一定量未来货币所需要的现在货币量。<br />终值：在现行利率既定时，现在货币量将带来的未来的货币量。<br />寻找一定量未来货币现值的过程称为贴现。</p><blockquote><p>复利计算的魔力与70规则<br />如果某个变量每年按x%增长，那么大约在70/x年之后，该变量翻一番。</p></blockquote><figure><img src="/img1/宏观经济学5.jpg" title="效用函数" alt="效用函数" /><figcaption aria-hidden="true">效用函数</figcaption></figure><p>大多数人是风险厌恶者，人们更害怕失去。</p><p>股票基本面分析：为决定一家公司的价值而对其会计报表和未来的前景进行研究。</p><figure><img src="/img1/宏观经济学10.jpg" title="有效市场假说" alt="有效市场假说" /><figcaption aria-hidden="true">有效市场假说</figcaption></figure><figure><img src="/img1/宏观经济学11.jpg" title="有效市场假说" alt="有效市场假说" /><figcaption aria-hidden="true">有效市场假说</figcaption></figure><p>有效市场假说认为，胜过市场是不可能的。关于金融市场的许多研究证实了，胜过市场是极为困难的。<br />有价证券的价格反映了影响其价值的所有已知信息！<br />所以，如果信息造假，那市场反映的也是虚假的市场价格。<br />任何一家公司的股票价格应该反映其被预期的未来盈利性。</p><h1 id="失业">失业</h1><p>失业者：包括能工作且在之前四周内努力找工作但没有找到工作的人。<br />家庭劳动者不算（例如家庭主妇，或者在家带小孩的男人）<br />丧失信心的工人：想工作但已经放弃找工作的人，这个也不算在失业率里面。</p><p>大多数失业是短期的，而在任何一个既定时间段所观察到的大多数失业者又是长期的。</p><p><strong>摩擦性失业</strong>：由于工人寻找最适合自己爱好和技能的工作需要时间而引起的失业；<br />结构性失业：于某些劳动市场上可提供的工作岗位数量不足以为每个想工作的人提供工作而引起的失业。</p><h1 id="货币制度">货币制度</h1><p>你的现金或支票代表了对未来物品与服务的索取权。<br />货币：经济中人们经常用于向其他人购买物品与服务的一组资产。<br />商品货币：以有内在价值的商品为形式的货币。 例如黄金。<br />法定货币：没有内在价值，由政府法令确定作为通货使用的货币。 例如纸币。<br />通货：公众手中持有的纸币钞票和铸币。<br />活期存款：储户可以通过开支票而临时支取的银行账户余额。</p><figure><img src="/img1/宏观经济学12.jpg" title="美国经济中货币存量的两种衡量指标" alt="美国经济中货币存量的两种衡量指标" /><figcaption aria-hidden="true">美国经济中货币存量的两种衡量指标</figcaption></figure><p>货币政策：中央银行的决策者对货币供给的安排。<br />美联储改变货币供给的主要工具是通过公开市场操作，买卖美国政府的债券。<br />实际上，美联储主席被称为美国第二有影响的人物。</p><h2 id="银行与货币供给">银行与货币供给</h2><p>准备金：银行得到但没有贷出去的存款。</p><figure><img src="/img1/宏观经济学13.jpg" title="T型账" alt="T型账户" /><figcaption aria-hidden="true">T型账户</figcaption></figure><p>当银行只把部分存款作为准备金时，银行创造了货币。</p><figure><img src="/img1/宏观经济学14.jpg" title="银行创造了货币" alt="银行创造了货币" /><figcaption aria-hidden="true">银行创造了货币</figcaption></figure><figure><img src="/img1/宏观经济学15.jpg" title="币乘数" alt="币乘数" /><figcaption aria-hidden="true">币乘数</figcaption></figure><p>杠杆：将借到的货币追加到用于投资的现有资金上。<br />杠杆率：资产与银行资本的比率。</p><figure><img src="/img1/宏观经济学16.jpg" title="资不抵债" alt="资不抵债" /><figcaption aria-hidden="true">资不抵债</figcaption></figure><figure><img src="/img1/宏观经济学17.jpg" title="资不抵债" alt="资不抵债" /><figcaption aria-hidden="true">资不抵债</figcaption></figure><p>贴现率：美联储向银行发放贷款的利率。<br />联邦基金利率：银行向另外一家银行进行隔夜贷款的利率。</p><h1 id="货币增长与通货膨胀">货币增长与通货膨胀</h1><p>温和的通货膨胀的成本并不像公众认为的那么大。<br />是什么因素决定货币的价值，是供给与需求。<br />货币需求反映了人们想以流通性形式持有的财务量。物价水平上升（货币价值下降）增加了货币需求。<br />货币的注入都增加了人们对物品与服务的需求，因此，这种对物品与服务的需求的增加引起了物价的上升。<br />货币流通速度：货币易手的速度。 类似一美元钞票每年有多少次要用于支付新生产的物品与服务。</p><blockquote><p>数量方程式：方程式 M * V = P * Y，它把货币量、货币流通速度和经济中物品与服务产出的美元价值联系在一起。<br />M: 货币量 V： 货币流通速度 P：物价水平（GDP 平减指数） Y：表示产量。<br />数量方程式说明，经济中货币量的增加必然反映在其它三个变量中的一个上面：物价水平必然上升，产量必然上升，或货币流通速度必然下降。<br />在许多情况下，货币的流通速度是较为稳定的。</p></blockquote><figure><img src="/img1/宏观经济学18.jpg" title="名义GDP 货币量 与货币流通速度" alt="名义GDP 货币量 与货币流通速" /><figcaption aria-hidden="true">名义GDP 货币量 与货币流通速</figcaption></figure><p>货币通胀税：货币通胀税就像是一种向每个持有货币的人征收的税。<br /><strong>通货膨胀不好，但通货紧缩可能更坏。引起财富向有利于债权人而不利与债务人的再分配。</strong><br />尽管在长期中或货币政策是中性的，但在短期中它对真实变量具有重要影响。</p><h1 id="开放经济的宏观经济学基本概念">开放经济的宏观经济学：基本概念</h1><p>净出口与资本净流出相等： NCO = NX</p><p><strong>资本净流出（NCO）：本国居民购买的外国资产减外国人购买本国人的资产。</strong><br />（货币也是一种资产）</p><blockquote><p>S = I + NCO<br />储蓄 = 国内投资 + 资金净流出<br />也就是说，一国的储蓄必定等于其国内投资加资本净流出。</p></blockquote><p>当经济学家谈论美元的升值或贬值时，他们通常是指一种考虑到许多单个汇率的汇率指数。<br />购买力平价：一种认为任何一单位通货应该能在所有国家买到等量物品的汇率理论。</p><h1 id="开放经济的宏观经济理论">开放经济的宏观经济理论</h1><p>两个市场：<br />1） 可贷资金市场，它协调经济中的储蓄与投资，以及国外可贷资金的流动（所谓资本净流出）。<br />2） 外汇市场，它协调那些想用国内通货交换其它国家通货的人。</p><figure><img src="/img1/宏观经济学19.jpg" title="外汇市场可贷资金市场" alt="外汇市场可贷资金市场" /><figcaption aria-hidden="true">外汇市场可贷资金市场</figcaption></figure><figure><img src="/img1/宏观经济学20.jpg" title="外汇市场" alt="外汇市场" /><figcaption aria-hidden="true">外汇市场</figcaption></figure><figure><img src="/img1/宏观经济学21.jpg" title="资本净流出如何取决于真实利率" alt="资本净流出如何取决于真实利率" /><figcaption aria-hidden="true">资本净流出如何取决于真实利率</figcaption></figure><figure><img src="/img1/宏观经济学22.jpg" title="开放经济的实际均衡" alt="开放经济的实际均衡" /><figcaption aria-hidden="true">开放经济的实际均衡</figcaption></figure><p>S = I + NCO<br />NCO = NX</p><p>资本净流出是联系两个市场的变量，在可贷资金市场，资本净流出是需求的一部分，那些想购买国外资产的人必须通过在可贷资金市场上或得资源为这种购买筹资；在外汇市场上，资本净流出是供给的来源，那些想购买外国资产的人必须供给美元，以便用美元兑换那个国家的通货。</p><blockquote><p>个人理解<br />是不是可以理解，出口之后获得美金，如果不动，或全部换成人民币，放银行就当时居民储蓄； 这些钱可以用于国内投资或者国外投资，所以 S = I + NCO；<br />根据本国真实利率和国外利率情况，决定拿多少人民币去购买多少国外资产，例如美国债券。这个多少的量决定了外，如果人民币利率较高，人民币情愿存在国内，导致外汇市场上的人民币供应量减少，引起汇率的变动。<br />预算赤字和贸易赤字是孪生赤字。</p></blockquote><h1 id="总需求与总供给">总需求与总供给</h1><p>研究经济的主要变量:GDP、失业、利率、物价水平、政府支出（财政政策）、税收、货币供给。</p><h2 id="关于经济波动的三个关键事实">关于经济波动的三个关键事实</h2><p>1） 经济波动是无规律的且不可预测的<br />2） 大多数宏观经济变量同时波动<br />尽管平均而言投资只占GDP的七分之一左右，但在衰退期间GDP减少的的三分之二左右是由投资减少导致的。<br />所以，衰退期间刺激投资是首要的。<br />3) 随着产量减少，失业增加。</p><p>古典经济学：<br />1） 古典二分法：把变量分为真实变量和名义变量。<br />2） 货币中性：货币供给的变化影响名义变量，而不影响真实变量</p><p>短期波动的现实性:<br />大多数经济学家认为，古典理论描述了长期世界，但没有描述短期世界。<br />在短期中，真实变量和名义变量是高度相关的，而且货币供给的变动可以暂时地使真实GDP背离其长期趋势。<br /><strong>人们往往对极为艰辛的时期有乐观地回忆，这种时期包括极端贫穷和战争。（忆苦思甜）</strong></p><figure><img src="/img1/宏观经济学23.jpg" title="总需求曲线" alt="总需求曲线" /><figcaption aria-hidden="true">总需求曲线</figcaption></figure><figure><img src="/img1/宏观经济学24.jpg" title="总需求曲线" alt="总需求曲线" /><figcaption aria-hidden="true">总需求曲线</figcaption></figure><figure><img src="/img1/宏观经济学25.jpg" title="长期总供给曲线" alt="长期总供给曲线" /><figcaption aria-hidden="true">长期总供给曲线</figcaption></figure><figure><img src="/img1/宏观经济学26.jpg" title="总需求与总供给模型中的长期增长与通货膨胀" alt="总需求与总供给模型中的长期增长与通货膨胀" /><figcaption aria-hidden="true">总需求与总供给模型中的长期增长与通货膨胀</figcaption></figure><p><strong>黏性工资理论：名义工资对经济状况变动的调整缓慢。换句话说，工资在短期中是黏性的。</strong><br />黏性价格理论<br />当实际物价水平背离人们预期的物价水平时，预期产量就背离自然产出水平。<br />产量的供给量 = 自然产出水平 + α（实际物价水平 - 预期的物件水平）</p><p><strong>所以预期会影响宏观经济，也就是现代经济中，信心的重要性。</strong></p><figure><img src="/img1/宏观经济学27.jpg" title="短期总供给曲线总结" alt="短期总供给曲线总结" /><figcaption aria-hidden="true">短期总供给曲线总结</figcaption></figure><figure><img src="/img1/宏观经济学28.jpg" title="分析宏观经济波动中的四个步骤" alt="分析宏观经济波动中的四个步骤" /><figcaption aria-hidden="true">分析宏观经济波动中的四个步骤</figcaption></figure><p><strong>金融体系就是一序列的套利方程式。但是在金融体系中，有无数扭曲和小震动相互叠加。当小震动和扭曲足够多时，事情就会变得非常糟糕。</strong><br />审慎的宏观工具：允许央行放松某些部门的借贷，而不必提高整个经济的利率(例如中国的定向宽松政策)。利率基本就像个没有精度的子弹。<br />滞胀：产量减少而物价上升的时期。</p><h1 id="货币政策和财政政策对总需求的影响">货币政策和财政政策对总需求的影响</h1><p>凯恩斯的主要观点是：衰退和萧条之所以会发生，是因为对物品和服务的总需求不足。<br />货币政策：央行的货币供给<br />财政政策：政府支出和税收水平。<br />流动性偏好理论：凯恩斯的理论，认为利率的调整使货币的供给与货币需求平衡。</p><figure><img src="/img1/宏观经济学30.jpg" title="货币注入" alt="货币注入" /><figcaption aria-hidden="true">货币注入</figcaption></figure><p>增加货币的供给是总需求向右移动，也就是说增加了总需求。<br />流动性陷阱：利率已经降到0<br />即使名义利率不能再下降，较高的预期通货膨胀也可以使真实利率降为负数，这就能刺激投资支出。<br />量化宽松：央行可以运用多于正常使用的多种金融工具来进行扩张性公开市场操作。例如，它可以购买抵押贷款和公司债券，从而降低这些类型贷款的利率。 这种非常规的货币政策被称为量化宽松，因为它增加了银行准备金的数量。<br />股市预测了过去五次衰退中的九次。 ；）<br />美联储的目标之一是稳定总需求。<br />政府购买对总需求有一种乘数效应。<br />乘数效应:当扩张财政政策增加了收入，从而增加了消费支出时引起的总需求的额外变动。<br />边际消费倾向 MPC：家庭额外收入中用于消费不用于储蓄的比例。 乘数效应 = 1/(1-MPC)<br />挤出效应：当扩张性财政政策引起利率上升，从而减少投资支出时所引起的总需求减少。<br />乘数效应 与 挤出效应的联合作用，决定了政府的扩张性财政政策对总需求的影响。<br />在经济出现波动的时候，政府的主要政策工具是稳定总需求，从而稳定生产和就业。</p><p>凯恩斯认为，总需求的波动主要是因为非理性的悲观主义和乐观主义。他用动物本能这个词来指代态度任意变动。这种态度的变化在某种程度上是自我实现。<br />从原则上说，政府可以调整货币政策和财政政策以对这些乐观主义和悲观主义情绪做出反应。<br />美联储的工作就是在宴会开始时把酒杯拿走。如今的刺激计划更侧重于基础设施建设。</p><h1 id="通货膨胀与失业之间的短期权衡取舍">通货膨胀与失业之间的短期权衡取舍</h1><p>痛苦指数：通货膨胀率与失业率</p><figure><img src="/img1/宏观经济学32.jpg" title="菲利普斯曲线(Phillips curve)" alt="菲利普斯曲线(Phillips curve)" /><figcaption aria-hidden="true">菲利普斯曲线(Phillips curve)</figcaption></figure><p>推理依据：低失业总是与高需求相关，而很高的总需求会给整个经济带来工资与物价上升的压力。<br />短期菲利普斯曲线：<br />失业率 = 自然失业率 - α（实际通货膨胀 - 预期通货膨胀）<br />所以政府管理预期的政策很重要，例如减低人们的预期通货膨胀。</p><figure><img src="/img1/宏观经济学33.jpg" title="对总供给的不利影响" alt="对总供给的不利影响" /><figcaption aria-hidden="true">对总供给的不利影响</figcaption></figure><p>在遇到不利的总供给变动时，决策者面临着反通胀和反失业的艰难选择。<br />温和的通货膨胀。</p><h1 id="最后的思考">最后的思考</h1><p>宏观经济学的六个争论问题</p><ol type="1"><li>货币政策与财政政策决策者应该试图稳定经济吗？<br /></li><li>政府反衰退应该增加支出还是减税<br /></li><li>货币政策应该按规则制定还是相机抉择<br /></li><li>中央银行应该把零通货膨胀作为目标吗？<br /></li><li>政府应该平衡其预算吗？<br /></li><li>应该为了鼓励储蓄而修改税法吗？</li></ol><blockquote><p> <font color="MidnightBlue" size = "3px">重难点<br />难点是跨市场的关联，例如货币市场和货物市场；利率市场和外汇市场。<br />经济学是软科学，从不同角度看问题，会得出不同的结论；另外，经济学的各种要素是你影响我，然后我反过来影响你，螺旋上升；还有，其中的因果关系也不是一概不变的，看你看问题的角度和切入点，决定了谁是因谁是果。</font></p></blockquote><p><img src="/img1/宏观经济学1.jpg" /></p><p><img src="/img1/宏观经济学6.jpg" /><br /><img src="/img1/宏观经济学7.jpg" /><br /><img src="/img1/宏观经济学8.jpg" /><br /><img src="/img1/宏观经济学9.jpg" /><br /><img src="/img1/宏观经济学29.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 宏观经济学 </tag>
            
            <tag> 曼昆 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《经济学原理之微观经济学》读书心得</title>
      <link href="/2020/09/15/%E3%80%8A%E7%BB%8F%E6%B5%8E%E5%AD%A6%E5%8E%9F%E7%90%86%E4%B9%8B%E5%BE%AE%E8%A7%82%E7%BB%8F%E6%B5%8E%E5%AD%A6%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2020/09/15/%E3%80%8A%E7%BB%8F%E6%B5%8E%E5%AD%A6%E5%8E%9F%E7%90%86%E4%B9%8B%E5%BE%AE%E8%A7%82%E7%BB%8F%E6%B5%8E%E5%AD%A6%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<h1 id="一经济学和经济学家">一、经济学和经济学家</h1><p>经济学：研究社会如何管理自己的稀缺资源。<br />微观经济学：研究家庭和企业如何做出决策，以及他们如何在市场上相互交易的学科。<br />宏观经济学：研究整体经济现象，包括通货膨胀、失业和经济增长的学科。</p><p>经济学家的身份：当经济学家试图去解释世界时，他们是科学家；当经济学家去帮助改善世界时，他们是政策顾问。</p><p>关于世界的表述有两种类型，第一种是实证表述(positive statements)，试图描述世界是什么样子的观点；另一种是规范表述（normative statements),试图描述世界应该是什么样子的观点，这些受价值观的影响，涉及伦理、宗教和政治哲学的看法。</p><p>萧伯纳：如果让所有经济学家围坐在一起，他们不会达成任何一个共识。</p><ol type="1"><li>科学判断的不同<br />类似物理学地心说、日心说的分歧，气象学存在是否全球变暖的分析，经济学作为一门年轻的学科，在随着不断的深入，存在分歧不足为奇。特别是对一些经济变量的重要参数大小有不同的直觉。<br /></li><li>价值观不同<br /></li><li>感觉和现实<br />经济学家的共识程度远远超过人们认为的那样。</li></ol><p>图形的作用：图形提供了一个可以根据大量树木而辨认森林的透镜。<br />经济学家通常更喜欢在其他条件不变的情况下，观察一个变量对另一个变量的影响。<br /><strong>在经济学中，区分沿着一条曲线的变动与曲线的移动是很重要的。可以用一种简单的方法来判断什么时候必须移动曲线：当一个未用任何坐标轴表示的变量发生变动时，曲线就会移动。</strong><br />因果关系：当用现实世界的数据来画图时，要确定一种变量如何影响另一种变量往往是极为困难的。<br /><strong>忽略的变量：当你看到一幅图被用以支持一种关于原因和结果的观点时，应当问一下，有没有一种被忽略的变量变动能解释你观察到的结果，这一点很重要。</strong></p><p>数学公式是一种模型和逻辑表达，需要考虑背后的本质描述！<br /><span id="more"></span></p><h1 id="二经济学十大原理">二、经济学十大原理</h1><p><img src="/img1/经济学十大原理.png" /></p><h2 id="人们如何做出决策">1、人们如何做出决策</h2><h3 id="人们面临权衡取舍">1）人们面临权衡取舍</h3><p>社会面临着在效率和平等之间的取舍。</p><h3 id="某种东西的成本是为了得到它所放弃的东西">2）某种东西的成本是为了得到它所放弃的东西</h3><p><strong>机会成本：为了得到某种东西所必须放弃的东西。</strong></p><h3 id="理性人考虑边际量">3）理性人考虑边际量</h3><p><strong>边际收益和边际成本</strong><br />要习惯于边际思考的逻辑<br />当且仅当一种行为的边际收益大于边际成本时，一个理性决策者才会采取这种行为。</p><h3 id="人们会对激励做出反应">4）人们会对激励做出反应</h3><p><strong>在经济学研究中，激励起着中心作用。可以说，整个经济学的内容可以简单的概括为：“人们会对激励做出反应。其余内容都是对此的解释。”</strong></p><h2 id="人们如何相互影响">2、人们如何相互影响</h2><h3 id="贸易可以让每个人的状况变得更好">5）贸易可以让每个人的状况变得更好</h3><p>贸易可以让每个人都从事自己最擅长的活动。</p><h3 id="市场通常是组织经济活动的一种好方法">6）市场通常是组织经济活动的一种好方法</h3><p>市场经济：当很多企业和家庭在物品与服务市场上相互交易时，通过他们分散决策配置资源的经济。<br />市场中存在的是分散的决策和利己的决策者。<br />价格就是看不见的手用来指引经济活动的工具。</p><h3 id="政府有时可以改善市场结果">7）政府有时可以改善市场结果</h3><p>只有在政府实施规则并维持对市场经济至关重要的制度时，看不见的手才能施展其魔力。<br />市场会存在失灵的情况。</p><h2 id="整体经济如何运行">3、整体经济如何运行</h2><h3 id="一国的生活水平取决于它生产物品与服务的能力">8）一国的生活水平取决于它生产物品与服务的能力</h3><h3 id="当政府发行了过多货币时物价上升">9）当政府发行了过多货币时，物价上升</h3><h3 id="社会面临通货膨胀与失业之间的短期权衡取舍">10）社会面临通货膨胀与失业之间的短期权衡取舍</h3><p>通货膨胀指数和失业指数号称痛苦指数。</p><h1 id="三像经济学家一样思考">三、像经济学家一样思考</h1><p>每个研究领域都有自己的语言和思考方式。<br />1、科学方法：观察、理论和进一步观察<br />在经济学研究中，进行实验往往是不可能的。经济学家像天文学家、进化生物学家一样，通常不得不使用这个世界向他们提供的数据。为了寻找实验室的替代品，经济学家十分关注历史所提供的自然实验。<br />（可以通过网络的虚拟世界作为类似经济学的实验室进行研究）</p><p>2、假设的作用<br />科学思考的艺术：无论在物理学、生物学还是经济学中，就是决定作出什么假设。通过不同的假设来回答不同的问题。假设能使问题简化，而对答案又没实质的影响。</p><p>3、经济模型<br />例如生物用人体模型来讲授解剖学。虽然人体模型略去了很多细节，尽管它缺乏真实性，但研究这些模型对了解人体如何运作是有帮助的。<br />经济学也用模型来了解世界，但不是塑料模型，而通常是由图形和方程组成的模型。经济模型也忽略了许多细节，以便我们了解真正重要的东西。正如生物教师的模型并不包括人体所有的肌肉和毛细血管一样，经济学的模型也不包括经济的每一个特征。<br />所有模型，物理学的、生物学的、经济学的，都为了加深我们对现实的理解而简化了现实。</p><p>大多数经济模型都是用数学工具来构建的。</p><h1 id="四相互依存和贸易的好处">四、相互依存和贸易的好处</h1><p>1、机会成本和比较优势<br /><strong>机会成本：为了得到某种东西所必须放弃的东西。机会成本衡量了每个生产者所面临的两种物品之间的权衡取舍。</strong><br /><strong>比较优势：一个生产者以低于另一个生产者的机会成本生产某种物品的能力。</strong><br />尽管某一个人有可能在两种物品的生产上都具有绝对优势，但一个人却不可能在两种物品的生产上都具有比较优势。因为一种物品的机会成本是另一种物品机会成本的倒数。比较优势反映了相对的机会成本。<strong>专业化和贸易的好处不是基于绝对优势，而是基于比较优势。</strong>当每个人专门生产自己有比较优势的物品时，经济的总量就增加。</p><h1 id="五供给与需求的市场力量">五、供给与需求的市场力量</h1><p>供给与需求是使市场运行的力量，他们决定了每种物品的产量以及出售的价格，<strong>如果你想知道任何一种事件或政策如何影响经济，你就应该先考虑它将如何影响供给与需求。</strong></p><p>市场：是由某种物品或服务的买者和卖者组成的一个群体。<br />竞争市场：有许多买者和卖者，以至于每个人对市场价格的影响都微乎其微的市场。换句话说，每个人都是价格的接受者。</p><p>需求定理：认为在其它条件不变时，一种物品的价格上升，对该物品的需求量减少的观点。<br />供给定理：认为在其它条件不变时，一种物品的价格上升，该物品的供给量增加的观点。<br />均衡：市场价格达到使供给量与需求量相等的水平时的状态。<br />供求定理：认为任何物品的价格都会自发调整，使该物品的供给与需求达到平衡的观点。<br />配置资源:在市场经济中，价格是配置稀缺资源的机制。</p><p><strong>均衡变动</strong></p><figure><img src="/img1/微观经济学6.jpg" title="分析均衡变动的三个步骤" alt="分析均衡变动的三个步骤" /><figcaption aria-hidden="true">分析均衡变动的三个步骤</figcaption></figure><figure><img src="/img1/微观经济学7.jpg" title="需求增加如何影响均衡" alt="需求增加如何影响均衡" /><figcaption aria-hidden="true">需求增加如何影响均衡</figcaption></figure><blockquote><p> <font color="MidnightBlue" size = "3px"><strong>重点和难点：曲线的移动与沿着曲线的变动；或者说要分清是供给的增加还是供给量的增加。</strong></font></p></blockquote><h1 id="六弹性及其应用">六、弹性及其应用</h1><p>弹性是衡量买者与卖者对市场条件变化的反应程度。（定性到定量的研究）<br />需求价格弹性：衡量一种物品需求量对其价格变动的反应程度的指标，用需求量变动百分比除以价格变动百分比来计算。</p><p>只要学会说“供给与需求”，甚至连一只鹦鹉都可以成为一个经济学家。</p><h1 id="七供给需求与政府政策">七、供给、需求与政府政策</h1><p>对买者征税和对卖者征税是相同的。在这种情况下，税收都是在买者支付的价格和卖者得到的价格之间打入了一个楔子。在新均衡时，买者和卖者分摊了税收负担。对买者征税和对卖者征税的唯一区别是谁来把钱交给政府。<br />税收负担更多的落在缺乏弹性的市场一方身上。例如劳动的供给远比劳动的需求缺乏弹性，这就意味着，是工人而不是企业承当了工薪税的负担。</p><figure><img src="/img1/微观经济学8.jpg" title="向卖者征税" alt="向卖者征税" /><figcaption aria-hidden="true">向卖者征税</figcaption></figure><figure><img src="/img1/微观经济学9.jpg" title="向买者征税" alt="向买者征税" /><figcaption aria-hidden="true">向买者征税</figcaption></figure><figure><img src="/img1/微观经济学10.jpg" title="税收负担如何分摊" alt="税收负担如何分摊" /><figcaption aria-hidden="true">税收负担如何分摊</figcaption></figure><p>经济受两种规则体系支配：供求规律和政府制定的法规。</p><h1 id="八消费者生产者与市场效率">八、消费者、生产者与市场效率</h1><p>市场上的供求均衡可以最大化买者和卖者得到的总收益。<br />消费者剩余：买者愿意为一种物品支付的量减去其为此实际支付的量。<br />需求曲线以下和价格以上的面积是衡量一个市场上的消费者剩余。</p><figure><img src="/img1/微观经济学11.jpg" title="价格如何影响消费者剩余" alt="价格如何影响消费者剩余" /><figcaption aria-hidden="true">价格如何影响消费者剩余</figcaption></figure><p>生产者剩余：卖者出售一种物品得到的量减去其生产成本。</p><figure><img src="/img1/微观经济学12.jpg" title="价格如何影响生产者剩余" alt="价格如何影响生产者剩余" /><figcaption aria-hidden="true">价格如何影响生产者剩余</figcaption></figure><p>市场效率：消费者剩余和生产者剩余是经济学家用来研究市场中买者和卖者福利的基本工具。</p><p>消费者剩余 = 买家的评价 - 买者支付的量<br />生产者剩余 = 卖家得到的量 - 卖家的成本<br />总剩余 = 买家的评价 - 买者支付的量 + 卖家得到的量 - 卖家的成本 = 买家的评价 - 买家的成本</p><p>如果资源配置使总剩余最大化，我们可以说，这种配置是有效率的。</p><figure><img src="/img1/微观经济学13.jpg" title="市场均衡时的消费者剩余和生产者剩余" alt="市场均衡时的消费者剩余和生产者剩余" /><figcaption aria-hidden="true">市场均衡时的消费者剩余和生产者剩余</figcaption></figure><h1 id="九赋税的代价">九、赋税的代价</h1><p>只有死神和赋税是不可避免的。<br />税收是我们为文明社会所付出的代价。</p><p>无谓损失：市场扭曲（例如税收）引起的总剩余减少，<br />由于税收扭曲了激励，就引起了市场配置时的无效率。供给和需求的弹性越大，税收的无谓损失也就越大。（大家用脚投票）。</p><figure><img src="/img1/微观经济学14.jpg" title="无谓损失和税收收入如何随税收规模而变动。" alt="无谓损失和税收收入如何随税收规模而变动。" /><figcaption aria-hidden="true">无谓损失和税收收入如何随税收规模而变动。</figcaption></figure><p>微观经济学家研究如何最好的设计税制，包括如何达到平等与效率之间的适当平衡；宏观经济学家研究税收如何影响整个经济，以及决策者可以如何运用税制来稳定经济活动，并实现更快的经济增长。</p><h1 id="十应用国际贸易">十、应用：国际贸易</h1><h1 id="十一外部性">十一、外部性</h1><p>清新的空气和清洁的水肯定是有价值的，但是，必须把他们的价值与其机会成本进行权衡取舍，也就是说，与为了得到他们而必须放弃的东西相比较。</p><h1 id="十二公共物品和公共资源">十二、公共物品和公共资源</h1><p><strong>生活中最美好的东西都是免费的。</strong></p><figure><img src="/img1/微观经济学15.jpg" title="四种类型物品" alt="四种类型物品" /><figcaption aria-hidden="true">四种类型物品</figcaption></figure><p><strong>一个数学家不能为一项定理申请专利。所以这些定理是公共物品，没有排他性和竞争性。所以，基础研究基本是政府资助的项目。</strong></p><p>成本--收益分析：比较提供一种公共物品的社会成本与社会收益的研究。</p><p>公共资源有竞争性，所以会出现公地悲剧的问题。<br /><strong>公地悲剧：一个说明从整个社会的角度看，为什么公共资源的使用大于合意水平的寓言。</strong></p><h1 id="十三税制的设计">十三、税制的设计</h1><p>税制未来变动的方向是对消费而不是收入征税。从促进经济增长的角度看，消费税是最好的，因为消费税会鼓励储蓄和资本的形成。<br />平均税率：支付的总税收除以总收入。<br />边际税率：增加1美元收入所支付的额外税收。<br />定额税：对每个人等量征收的税收。</p><p>收益原则：认为人们应该根据他们从政府服务中得到的利益来纳税的思想。<br />支付能力原则：认为应该根据一个人可以承受的负担来对这个人征税的思想。</p><p>比例税：高收入纳税人和低收入纳税人缴纳收入中相同比例的税收；<br />累进税：高收入纳税人缴纳的税收在收入中的比例高于低收入纳税人的这一比例。</p><p>对慈善进行税收扣除，从而鼓励了私人而不是政府解决社会问题。</p><p><strong>平等和效率两个目标往往是冲突的，对税收政策的分歧，往往是因为对这两个目标的侧重不同。</strong></p><h1 id="十四生产成本">十四、生产成本</h1><p><strong>企业的目标是利润最大化。</strong><br />显性成本：需要企业支出货币的投入成本；<br />隐形成本：不需要企业支出货币的投入成本。</p><p>经济学家考虑隐形成本，会计师只考虑显性成本。</p><p>例如作为一种机会成本的资本成本，就是隐形成本。</p><figure><img src="/img1/微观经济学16.jpg" title="经济学家与会计师" alt="经济学家与会计师" /><figcaption aria-hidden="true">经济学家与会计师</figcaption></figure><p>固定成本：不随着产量变动而变动的成本；<br />可变成本：随着产量变动而变动的成本。</p><p>平均总成本 ATC<br />平均固定成本：AFC （Average Fixed Cost）<br />平均可变成本：AVC<br />边际成本：MC （Marginal Cost）<br />有效规模：使平均总成本最小的产量。</p><figure><img src="/img1/微观经济学17.jpg" title="一个典型企业的成本曲线" alt="一个典型企业的成本曲线" /><figcaption aria-hidden="true">一个典型企业的成本曲线</figcaption></figure><p>专业化-&gt;规模经济-&gt;沟通成本-&gt;规模不经济。</p><figure><img src="/img1/微观经济学18.jpg" title="成本的诸多类型的总结" alt="成本的诸多类型的总结" /><figcaption aria-hidden="true">成本的诸多类型的总结</figcaption></figure><h1 id="十五竞争市场上的企业">十五、竞争市场上的企业</h1><p>边际收益（ MR ）：增加一单位销售量引起的总收益的变动。<br />平均收益： AR 总收益除以销售量。 平均收益等于物品的价格。</p><figure><img src="/img1/微观经济学19.jpg" title="一个竞争企业的利润最大化" alt="一个竞争企业的利润最大化" /><figcaption aria-hidden="true">一个竞争企业的利润最大化</figcaption></figure><blockquote><p>在本质上，由于企业的边际成本曲线决定了企业在任何一种价格时愿意供给的物品的数量，因此，边际成本曲线也是竞争企业的供给曲线。<br />同时，需求曲线上的某一点，也代表了买者在这个价格下的购买意愿。也就是买者的评价。</p></blockquote><figure><img src="/img1/微观经济学20.jpg" title="竞争企业的短期供给曲线" alt="竞争企业的短期供给曲线" /><figcaption aria-hidden="true">竞争企业的短期供给曲线</figcaption></figure><p>如果物品的价格低于生产的平均可变成本，企业选择停止营业。</p><p><strong>沉没成本：已经发生而且无法收回的成本。 </strong><br /><strong>因为沉没成本无法收回，所以，当你做出包括经营战略在内的各种生活决策时，可以不考虑沉没成本。</strong><br /><strong>在短期中固定成本是沉没成本，企业在决定生产多少产品时可以不予以考虑。</strong>企业的短期供给曲线是边际成本曲线在平均可变成本之上的那一部分，而且，固定成本的大小对供给决策无关紧要。</p><figure><img src="/img1/微观经济学21.jpg" title="短期和长期内的需求增加" alt="短期和长期内的需求增加" /><figcaption aria-hidden="true">短期和长期内的需求增加</figcaption></figure><h1 id="十六垄断">十六、垄断</h1><figure><img src="/img1/微观经济学22.jpg" title="竞争企业与垄断企业的需求曲线" alt="竞争企业与垄断企业的需求曲线" /><figcaption aria-hidden="true">竞争企业与垄断企业的需求曲线</figcaption></figure><p>竞争企业是价格接受者，而垄断企业是价格决定者。<br />与竞争企业一样，垄断企业的目标也是利润最大化。<br />实际上，垄断者类似于一个私人收税者。</p><p>寓言：故事典型化，结论一般化。</p><p><strong>套利：指在一个市场上以低价购买一种物品，而在另一个市场上以高价出售，以便从价格差中获利的过程。</strong></p><h1 id="十七垄断竞争">十七、垄断竞争</h1><p>小说市场既不适用于竞争模式，也不适用于垄断模式，垄断竞争是对它的最好描述。</p><p>寡头：只有少数几个提供相似或相同产品的买者的市场结构。<br />垄断竞争：存在许多出售相似但不相同产品的企业的市场结构。</p><figure><img src="/img1/微观经济学23.jpg" title="市场结构的四种类型" alt="市场结构的四种类型" /><figcaption aria-hidden="true">市场结构的四种类型</figcaption></figure><figure><img src="/img1/微观经济学24.jpg" title="垄断竞争与完全竞争" alt="垄断竞争与完全竞争" /><figcaption aria-hidden="true">垄断竞争与完全竞争</figcaption></figure><p>在很大程度上，广告提供了信息，建立了具有可靠质量的品牌，促进了竞争。</p><h1 id="十八寡头">十八、寡头</h1><p>寡头的关键特征是合作与利己之间的冲突。<br /><strong>囚徒困境这个博弈说明了为什么合作是困难的，即使在合作使所有人状况变得更好时，人们在生活中也往往不能相互合作。</strong></p><figure><img src="/img1/微观经济学25.jpg" title="纳什均衡" alt="纳什均衡" /><figcaption aria-hidden="true">纳什均衡</figcaption></figure><figure><img src="/img1/微观经济学26.jpg" title="囚徒困境" alt="囚徒困境" /><figcaption aria-hidden="true">囚徒困境</figcaption></figure><p><strong>占优策略：无论其他参与者选择什么策略，对一个参与者都为最优的策略。<br />这是个纳什均衡：在其他人的策略为既定的情况下，每个罪犯都选择了可能的最优策略。但从他们的角度来看，这是个糟糕的结果。由于各自追求自己的利益，两者囚徒共同达到了使每个人状况变坏的结果。（利己的激励作用）</strong></p><p>纳什均衡，其实就是<strong>从别人的策略出发</strong>考虑自己的策略，就是别人选择某种策略的情况下，自己不同策略的利益情况。</p><h1 id="十九生产要素市场">十九、生产要素市场</h1><p>生产要素：用于生产物品与服务的投入。包括：劳动、土地、资本三种最重要的生产要素。<br />资本要素：包括办公楼和电脑设备等；<br />土地要素：办公室所处的物理空间。<br />生产要素的需求是派生需求。</p><p>生产函数;用于生产一种物品的投入量与该物品产量之间的关系。<br />边际产量值:一种投入的边际产量乘以该产品的价格。</p><p>任何一种生产要素的供给发生变化，不仅影响该种要素的收益，而且影响所有要素的收益。</p><h1 id="二十收入与歧视">二十、收入与歧视</h1><p>收入的差异受两种类型的影响：<br />1、补偿性工资差别<br />为抵消不同工作的非货币特征而产生的工资差别。 例如工地工人比一些普通文员工资高，夜班比白班高等；<br />2、人力资本<br />对人的投资的积累，例如教育和在职培训。（ 也可以看做是一种对教育成本的补偿性工资差别）</p><p>工资受能力、努力和机遇的影响！<br />效率工资：企业为了提高工人的生产率而支付高于均衡工资的工资。</p><h1 id="二十一收入不平等与贫困">二十一、收入不平等与贫困</h1><p>基尼系数（Gini coefficient) 分配差异的指数。<br /><strong>越平等地分割蛋糕，蛋糕就会变小，这是几乎每一个人都同意的有关收入分配的一个结论。</strong></p><h1 id="二十二消费者选择理论">二十二、消费者选择理论</h1><figure><img src="/img1/微观经济学27.jpg" title="消费者偏好" alt="消费者偏好" /><figcaption aria-hidden="true">消费者偏好</figcaption></figure><p>由于人们更愿意放弃他们已经拥有的数量较多的物品，而不愿意放弃他们不多的物品，因此，无差异曲线凸向原点。</p><p><strong>工资增加需要综合考虑收入效应和替代效应！两者的叠加才能决定最终的行为。</strong></p><figure><img src="/img1/微观经济学28.jpg" title="工资增加" alt="工资增加" /><figcaption aria-hidden="true">工资增加</figcaption></figure><h1 id="二十三微观经济学前沿">二十三、微观经济学前沿</h1><p>1、不对称信息<br />市场成功和市场失灵之间的紧张关系是微观经济学的中心。<br />关于不对称信息的研究给了我们一个警惕市场的新理由。当一些人知道的比另外一些人多时，市场也不能使资源得到最好的利用。<br />2、政治经济学<br />政治经济学：用经济学的分析方法研究政府。<br />阿罗不可能性定理：一个数学结论，他表明在某些假设条件下，没有一种方案能把个人偏好加总为一组正当的社会偏好。<br />没有一种投票制度能满足完美的投票制度（确定性、传递性、不相关选择的独立性、没有独裁者）<br />3、行为经济学<br />经济学中将心理学的观点考虑进来的分支学科。<br />人们并不总是理性的。</p><h1 id="文章摘要">文章摘要</h1><p>1、亚当·斯密 看不见的手！<br />2、李嘉图：比较优势<br />3、竭泽而渔<br />4、平等和美丽一样，是“情人眼里出西施。”<br />5、投票会引起多数人的专制。</p><p><img src="/img1/微观经济学1.jpg" /><br /><img src="/img1/微观经济学3.jpg" /><br /><img src="/img1/微观经济学4.jpg" /><br /><img src="/img1/微观经济学5.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 曼昆 </tag>
            
            <tag> 经济学原理 </tag>
            
            <tag> 微观经济学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《影响力》读书心得</title>
      <link href="/2020/08/09/%E3%80%8A%E5%BD%B1%E5%93%8D%E5%8A%9B%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2020/08/09/%E3%80%8A%E5%BD%B1%E5%93%8D%E5%8A%9B%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p><a href="/img1/影响力.xmind">影响力读书笔记脑图</a></p><h1 id="诉诸于利益">诉诸于利益</h1><p>你要说服一个人之前，要想想如何才能达到目的。<br />说服一个人，你要诉诸于利益，而不是诉诸于道理。<br />（成人的世界没有对错，只有利益）<br />不同的人有不同的利益，其实这也是一种换位思考的能力。<br />利益的考虑需要利用对比心理，利用人类失去厌恶的心理（对失去比获得更敏感和在意），所以要强调对方可能的损失，不同文化的人对利益的看待也不一样的，例如中国人就很在意没面子的事情。<br /><span id="more"></span></p><h1 id="顺从专家">顺从专家</h1><p>销售员、筹款家、广告商</p><h1 id="文明的进步就是人们不假思索中可以做的事情越来越多">文明的进步，就是人们不假思索中可以做的事情越来越多。</h1><p>总结出更多的思考范式，提高决策的速度，虽然这些范式不是完美的，但只要达到一定的成功率，对人们来说还是利大于弊。（信息爆炸，人类脑力不够用的需要）<br />例如优惠券的用途：1、优惠 2、避免去哪家的思考决策过程。<br /><strong>利益是所有人行为模式的触发器！</strong></p><h1 id="对比原理">对比原理</h1><p><img src="/img1/影响力2.jpg" /><br /><img src="/img1/影响力3.jpg" /><br /><img src="/img1/影响力4.jpg" /><br /><img src="/img1/影响力5.jpg" /></p><p>对比心理（卖东西）</p><ul><li>先坏后好<br /></li><li>先贵后便宜<br /></li><li>先提价后打折<br /></li><li>先大件后配件</li></ul><h1 id="拒绝--后撤术">拒绝--后撤术</h1><p>先提大要求，后提小要求。<br />先提出一个大一些的要求，对方拒绝后，再提出你真正需求。（互惠原理+知觉对比原理）<br />并能增强让别人的责任感和满意度。</p><h1 id="文章摘要">文章摘要</h1><ul><li><strong>缓慢的语速，代表着权威、自信、从容；缓慢的语速更容易让人信服。</strong><br /></li><li>抬手不打笑脸人，抬口不骂送礼人。<br /></li><li>开弓没有回头箭<br /></li><li>波斯帝国信使<br />人总是自然而然地讨厌带来坏消息的人，哪怕报信人跟坏消息一点关系也没有。光是两者之间存在联系，就足以引发我们的厌恶。<br />类似人们会因为坏天气而责怪天气预报员。<br />同样也是，人们会喜欢带来好消息的人，哪怕好消息和他一点关系都没有。<br /></li><li>午宴术：人们对在就餐期间接触到的人或事物更为喜爱。<br /></li><li>我们的研究发现，在权威的命令下，成年人几乎愿意干任何事情。<br /></li><li><strong>学生应试培养的是一种竞争意识；素质教育应该培养的是一种合作的意识。</strong></li></ul><p><img src="/img1/影响力1.jpg" /></p><p><img src="/img1/影响力6.jpg" /><br /><img src="/img1/影响力7.jpg" /><br /><img src="/img1/影响力8.jpg" /><br /><img src="/img1/影响力9.jpg" /><br /><img src="/img1/影响力10.jpg" /><br /><img src="/img1/影响力11.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 心理学 </tag>
            
            <tag> 说服力 </tag>
            
            <tag> 顺从 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《穷查理宝典》读书心得</title>
      <link href="/2020/08/01/%E3%80%8A%E7%A9%B7%E6%9F%A5%E7%90%86%E5%AE%9D%E5%85%B8%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2020/08/01/%E3%80%8A%E7%A9%B7%E6%9F%A5%E7%90%86%E5%AE%9D%E5%85%B8%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p><a href="/img1/穷查理宝典-芒格.xmind">穷查理宝典-芒格</a><br /><a href="/img1/人类误判心理学（25种）.xmind">人类误判心理学（25种）</a><br /><img src="/img1/穷查理宝典34.jpg" /><br /><img src="/img1/穷查理宝典35.jpg" /><br /><span id="more"></span></p><h1 id="文章摘要">文章摘要</h1><ul><li>逆向思维<br />反过来想<br />遇事要报怀疑态度，甚至逆其道而行.<br /></li><li>找出你最擅长的事，然后持之以恒、乐此不彼的把它做好。<br /><strong>思考自己最擅长的事？</strong><br /></li><li>你要是借了别人的车，别忘了加满油再还给人家。<br /></li><li>在商业世界里，我们往往会发现，取胜的系统在最大化或最小化一个或几个变量上走到近乎荒谬的极端。<br />类似学习，各科平均，某个特长科目突出，往往的排名就比较高。<br /></li><li>生活简单化，但是工作敢于挑战复杂度。<br /></li><li><strong>经济系统太过复杂，超出了人类的认知能力。<br />因为经济系统渗有人类的活动，人的世界比物理的世界更复杂、更不可预测。<br />最终，投资也变成一种直觉、一种哲学的评估，而非数学。</strong><br /></li><li>花在学习和思考上的时间比花在行动上的时间要多。<br />谋定而后动<br /></li><li>诚实是最好的策略。<br /></li><li>避免邪恶之人，尤其是那些性感诱人的异性。<br />色字头上一把刀<br /></li><li><strong>我总是把天堂想象成一个图书馆。</strong><br /></li><li>我们可以提供建议，但无法提供行为。<br />师傅领进门，修行靠个人。<br /></li><li>承担工作的责任，做可靠的人。<br /></li><li>如果你能与凡人交谈，且彬彬有礼，或与国王同行，而不奴颜婢膝。<br /></li><li>多元化就像航空母舰需要很多驱逐舰等一起的作战体系。<br /></li><li>大企业病：如果你的头脑有了新的想法，这种系统会反对你。<br /></li><li><strong>如果你想改变行为，就必须先改变动机。</strong><br /></li><li><strong>巴浦洛夫联想。当人们听到坏消息，他们会讨厌带来坏消息的人。</strong><br /></li><li>认知受激励机制引起的偏见。<br /></li><li>操作性条件反射原理：人们会重复他们上一次成功的活动。<br /></li><li>人类的大脑需要理由才能更好的理解事情。<br /></li><li><strong>学习一门知识的捷径就是写一本书。</strong><br /></li><li>科学无法解决终极的自然之谜，那是因为归根到底，我们本身正是要解决谜团的一部分。<br /></li><li>人类在某种情况下会出现集体非理性的倾向。<br /></li><li><strong>如果你想说服别人，要诉诸于利益，而非诉诸于理性。</strong><br /></li><li>宗教的负罪感极大的推动了诚信精神的发展。<br /></li><li>要想得到你想要的东西，最好的方法是让你自己配得起它。</li></ul><p><img src="/img1/穷查理宝典1.jpg" /><br /><img src="/img1/穷查理宝典2.jpg" /><br /><img src="/img1/穷查理宝典3.jpg" /><br /><img src="/img1/穷查理宝典4.jpg" /><br /><img src="/img1/穷查理宝典5.jpg" /><br /><img src="/img1/穷查理宝典6.jpg" /><br /><img src="/img1/穷查理宝典7.jpg" /><br /><img src="/img1/穷查理宝典8.jpg" /><br /><img src="/img1/穷查理宝典9.jpg" /><br /><img src="/img1/穷查理宝典10.jpg" /><br /><img src="/img1/穷查理宝典11.jpg" /><br /><img src="/img1/穷查理宝典12.jpg" /><br /><img src="/img1/穷查理宝典13.jpg" /><br /><img src="/img1/穷查理宝典14.jpg" /><br /><img src="/img1/穷查理宝典15.jpg" /><br /><img src="/img1/穷查理宝典16.jpg" /><br /><img src="/img1/穷查理宝典17.jpg" /><br /><img src="/img1/穷查理宝典18.jpg" /><br /><img src="/img1/穷查理宝典19.jpg" /><br /><img src="/img1/穷查理宝典20.jpg" /><br /><img src="/img1/穷查理宝典21.jpg" /><br /><img src="/img1/穷查理宝典22.jpg" /><br /><img src="/img1/穷查理宝典23.jpg" /><br /><img src="/img1/穷查理宝典24.jpg" /><br /><img src="/img1/穷查理宝典25.jpg" /><br /><img src="/img1/穷查理宝典26.jpg" /><br /><img src="/img1/穷查理宝典27.jpg" /><br /><img src="/img1/穷查理宝典28.jpg" /><br /><img src="/img1/穷查理宝典29.jpg" /><br /><img src="/img1/穷查理宝典30.jpg" /><br /><img src="/img1/穷查理宝典31.jpg" /><br /><img src="/img1/穷查理宝典32.jpg" /><br /><img src="/img1/穷查理宝典33.jpg" /></p><p><img src="/img1/穷查理宝典36.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 芒格 </tag>
            
            <tag> 伯克希尔 </tag>
            
            <tag> 巴菲特 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《金刚经说什么》读书心得</title>
      <link href="/2020/07/26/%E3%80%8A%E9%87%91%E5%88%9A%E7%BB%8F%E8%AF%B4%E4%BB%80%E4%B9%88%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2020/07/26/%E3%80%8A%E9%87%91%E5%88%9A%E7%BB%8F%E8%AF%B4%E4%BB%80%E4%B9%88%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p><img src="/img/学佛.png" /><br /><a href="/img/金刚经.xmind">读书心得脑图</a></p><h1 id="文章摘要">文章摘要</h1><ul><li>《金刚经》全称是《金刚般若波罗蜜经》<br />金刚：气体最坚、其用最利、其相最明般若：佛教用语，智慧的意思。 波罗蜜：到达彼岸的意思<br /></li><li>弥勒佛：大肚能容容天下难容之事，开口便笑笑世间可笑之人。<br /></li><li>文章本天成，妙手偶得之；<br />书到今生读已迟。<br /></li><li>人生不如意事常八九，可以语人者无二三；<br /></li><li>三分人事七分天<br /></li><li>佛教在南北朝时期鼎盛<br />南朝四百八十寺，多少楼台烟雨中。<br /></li><li>一念之差：念：一呼一吸之间谓之念。<br /></li><li>南无阿弥陀佛：南无是皈依的意思。<br /></li><li>淡泊以明志，宁静以致远。<br /></li><li>摩诃：大的意思<br /></li><li>相：四相：我相、人相、众生相、寿者相；相就是现象、观念。<br /></li><li>事如春梦了无痕 放下<br /></li><li>李商隐：此情可待成追忆，只是当时已惘然。<br />这里其实有两个望文生义的点，古代词语用法和现在不一样。<br />1.可待：何必等到。 李商隐在《牡丹》中也用到过可待，在那里的意思是何必等到。<br />2.只是：就在。<br />在唐代，“只”这个字的用法是表示限定范围，不是转折。所以这句诗的意思其实是：这些感受何必等到事后成为追忆时才觉得它很珍贵，就在当时就知道它很珍贵而有一种惘然感觉了。<br />那些美好的事和年代，只能留在回忆之中了。而在当时那些人看来那些事都只是平常罢了，却并不知珍惜。<br /></li><li>卦者，变也。<br /><span id="more"></span></li></ul><p><img src="/img/金刚经1.jpg" /><br /><img src="/img/金刚经2.jpg" /><br /><img src="/img/金刚经3.jpg" /><br /><img src="/img/金刚经4.jpg" /><br /><img src="/img/金刚经5.jpg" /><br /><img src="/img/金刚经6.jpg" /><br /><img src="/img/金刚经7.jpg" /><br /><img src="/img/金刚经8.jpg" /><br /><img src="/img/金刚经9.jpg" /><br /><img src="/img/金刚经10.jpg" /><br /><img src="/img/金刚经11.jpg" /><br /><img src="/img/金刚经12.jpg" /><br /><img src="/img/金刚经13.jpg" /><br /><img src="/img/金刚经14.jpg" /><br /><img src="/img/金刚经15.jpg" /><br /><img src="/img/金刚经16.jpg" /><br /><img src="/img/金刚经17.jpg" /><br /><img src="/img/金刚经18.jpg" /><br /><img src="/img/金刚经19.jpg" /><br /><img src="/img/金刚经20.jpg" /><br /><img src="/img/金刚经21.jpg" /><br /><img src="/img/金刚经22.jpg" /><br /><img src="/img/金刚经23.jpg" /><br /><img src="/img/金刚经24.jpg" /><br /><img src="/img/金刚经25.jpg" /><br /><img src="/img/金刚经26.jpg" /><br /><img src="/img/金刚经27.jpg" /><br /><img src="/img/金刚经28.jpg" /><br /><img src="/img/金刚经29.jpg" /><br /><img src="/img/金刚经30.jpg" /><br /><img src="/img/金刚经31.jpg" /><br /><img src="/img/金刚经32.jpg" /><br /><img src="/img/金刚经33.jpg" /><br /><img src="/img/金刚经34.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 南怀瑾 </tag>
            
            <tag> 金刚经 </tag>
            
            <tag> 佛教 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《认同》读书心得</title>
      <link href="/2020/06/01/%E3%80%8A%E8%AE%A4%E5%90%8C%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2020/06/01/%E3%80%8A%E8%AE%A4%E5%90%8C%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p><a href="/img/认同.xmind">读书笔记脑图</a> <img src="/img/认同13.jpg" /> <span id="more"></span></p><h1 id="文章摘要">文章摘要</h1><ul><li>观察所有在场的人（不只是向你“开火”的人） 一直和现场支持者们保持目光接触，会让你避免犯下危险的错误：把焦点仅仅放在令你大为光火的捣乱者身上</li><li>发言要像聊天一样，像面试一样。</li><li><strong>不能陷入细节的讨论中，这样会偏离主题，并且细节的讨论会给你带来更多不可控的受攻击点。</strong></li><li><strong>王安石：天变不足畏、祖宗不足法、人言不足恤；</strong></li><li>避免讨论偏离正轨</li><li>在遭到抨击时，我们所有人都会表现出以下三种固有的倾向，即反击、逃避或自我辩解。</li><li><strong>回答的原则就是：</strong><br />别人说：不是 　　　　回答：是<br />别人说：不行 　　　　回答：行<br />别人说：没考虑到 　　回答：考虑到了<br /></li><li>滑坡谬误（Slippery slope）<br />是一种逻辑谬论，即不合理地使用连串的因果关系，将“可能性”转化为“必然性”，以达到某种意欲之结论。<br />但其实每个的推断还有很多不同的可能性，却武断地将某个可能性引伸成为必然性，然后串联这些不合理的因果关系，推断成一件毫无关联的结果，这就是滑坡谬误。文革时期曾经出现的上纲上线，亦是滑坡谬误的典型例子。<br />滑坡谬误的典型形式为“如果发生A，接着就会发生B，接着就会发生C，接着就会发生D，……，接着就会发生Z”，而后通常会明示或暗示地推论“Z不应该发生，因此我们不应允许A发生”。A至B、B至C、C至D、……等因果关系好似一个个“坡”，从A推论至Z的过程就像一个滑坡。<br />滑坡谬误的问题在于，每个“坡”的因果强度不一，有些因果关系只是可能、而非必然，有些因果关系相当微弱，有些因果关系甚至是未知或缺乏证据的，因而即使A发生，也无法一路滑到Z，Z并非必然（或极可能）发生。<br />相对地，若有充足证据显示每个“坡”都有合理、强烈的因果连结，即不构成滑坡谬误。<br />例如：1.小红认为，屠户当前杀猪，今后就会杀人。小红犯了滑坡谬论。<br />例如：2.小时候偷针、大了窃国。</li></ul><p><img src="/img/认同1.jpg" /></p><p><img src="/img/认同3.jpg" /> <img src="/img/认同4.jpg" /> <img src="/img/认同5.jpg" /> <img src="/img/认同6.jpg" /> <img src="/img/认同7.jpg" /> <img src="/img/认同8.jpg" /> <img src="/img/认同9.jpg" /> <img src="/img/认同10.jpg" /> <img src="/img/认同11.jpg" /> <img src="/img/认同12.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 沟通 </tag>
            
            <tag> 认同 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《幸福资本论》读书心得</title>
      <link href="/2020/05/24/%E3%80%8A%E5%B9%B8%E7%A6%8F%E8%B5%84%E6%9C%AC%E8%AE%BA%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2020/05/24/%E3%80%8A%E5%B9%B8%E7%A6%8F%E8%B5%84%E6%9C%AC%E8%AE%BA%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>很普通的一本书！没什么养分。</p><h1 id="文章摘要">文章摘要</h1><ul><li>经济活动中有两个杠杆，即OPT（other people‘s time）和OPM（other people’s money）。前者指利用别人的时间，后者指花费别人的金钱。用别人的时间与金钱终极目的是为了成就我们的事业。所以，在我看来，所有商务技能中，唯有组织力学和行为心理学有必要认真学习。</li><li>消费就是“现在心情”买单，投资就是为“将来的发展”花钱。</li><li>我们常说的投资其实就是成功者向下一个有可能成功的人递交接力棒的行为。</li><li>企业价值评估体系DCF（Discounting Cash Flow)估值法。 大家如果做价值投资，肯定听过巴菲特一句很经典的名言。一个企业的价值等于这个企业在剩余寿命下创造的自由现金流折现值。</li><li>因此，创业不是有计划的按部就班，而是在某种冲动的作用下发生的。</li><li>《老子》五十六章：「和其光，同其尘。」意思是含敛光耀，混同尘世。后用「和光同尘」比喻随波逐流，不露锋芒的处世态度。</li><li>德鲁克说过事业就是创造客户。</li><li>信用 = （ 专业度 + 可靠度 + 亲密度 ） / 利己心</li><li>金钱具有超强的媒介性质。</li><li>嘤其鸣矣，求其友声 嘤：鸟叫的声音。这首诗的意思是：从深谷中出来的鸟，飞到高树上，那嘤嘤的叫声，是想寻求伙伴的啊！“嘤其鸣矣，求其友声”这两句诗用来比喻寻求志同道合的朋友。</li></ul><p><img src="/img/幸福资本论.jpeg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 梵高 </tag>
            
            <tag> 毕加索 </tag>
            
            <tag> 金钱 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《靠谱》读书心得</title>
      <link href="/2020/05/05/%E3%80%8A%E9%9D%A0%E8%B0%B1%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2020/05/05/%E3%80%8A%E9%9D%A0%E8%B0%B1%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<h1 id="一沟通技巧">一、沟通技巧</h1><h2 id="先讲结论">01 先讲结论</h2><p><strong>不光是咨询报告，还有日常的邮件、笔记、与上司的沟通等，一切都贯彻着“先讲结论”的原则。这样做的好处是能够简洁明了地讲清问题。在短时间内将必要的信息传达给对方。</strong></p><h3 id="遵循pert模式">遵循PERT模式</h3><p>POINT 结论<br />REASON 依据<br />EAMPLE 具体事例<br />POINT 重申结论结束<br /><span id="more"></span></p><h3 id="如何回答问题">如何回答问题</h3><p><strong>平时说话要改掉想到什么就说什么的毛病。</strong><br />现在脑中整理好表达问题的PERT模式，然后从结论说起。<br />人很容易焦急，认为被提问时必须马上回答，于是便会不假思索地说出临时想到的东西。<br />我也曾经有这个毛病。面对提问，心中会产生一种恐惧感，如果张口结舌、哑口无言的话会被他人笑话脑子愚笨。为了消除尴尬，姑且就先说点什么应付一下。<br />但后来我明白了，这种“应付”在商务场合上是行不通的。这一点在我第一年工作时就被人指出来了。<br />“大石，你回答我的问题的时候，用不着说一些应付的话。“<br />我突然意识到，说一些应付的话反而更让人觉得你脑子不灵光。<br /><strong>当经理对我说：“考虑5分钟，请好好整理一下思路后再回答一遍。”</strong> 我心中立刻通透起来。之前我很在意回答问题的速度，现在明白，原来整理好思路再答也可以。经理一句话，点醒梦中人。<br /><strong>从那时起，面对不能立刻回答的问题时，我先请对方给我一两分钟的时间，自己在脑子中默默地整理好思路，然后从结论开始回答对方的问题。</strong></p><h2 id="直入主题">02 直入主题</h2><ul><li>不找借口，坦率的回答是或者不是<br />其实经理想要知道的无非是工作是否已经完成，没完成的原因是什么，不过如此。他不想听属下的借口，工作没完成也不能强求，只不过需要想一下完成工作的方法。<br />即使上司发怒，也不是针对属下。现在我知道，那不过是上司想要你尽快完成工作罢了。<br /></li><li>只有明确为什么，才能了解问题所在<br />通过明确回答是YES还是NO，接下来进行附加的说明，回答对方的问题。<br /></li><li>遇到问题时，提出解决方案<br />当上司让你去做做很难完成的任务时，坦率的回答YES or NO，同样适用。<br />一般来说，回答可以，不过......,来提出资源的要求。<br />因为上司的目的只是想顺利的推进工作。</li></ul><h2 id="用数据事实说话">03 用数据、事实说话</h2><p><strong>假如“世界通用语”存在的话，它应该不是英语而是数据。</strong><br />将一些凭感觉想到的问题落实在“数据”上，变成清晰明确的“证据”，更容易让人理解和信服。</p><h2 id="用数据逻辑说话">04 用数据、逻辑说话</h2><p>将问题点汇总成逻辑树将其结构化，利用公司和市场的数据进行分析。<br /><strong>世界最通用的语言是逻辑和数据</strong></p><h2 id="逻辑先行">05 逻辑先行</h2><p>职位越高就越是重视用数据来看问题，做出理性判断。<br />管理层的人一定比任何人都重视业绩。因此，即使讨论很困难，只要合乎逻辑、能和业绩挂钩的方案，他们都会认真倾听。</p><h2 id="让对方听得懂">06 让对方听得懂</h2><p><strong>“能让对方明确理解的说话技巧”放在技能的首位</strong></p><ul><li>以对方“毫不知情”为前提，构建逻辑，组织语言。<br />向没有背景知识的人说明，试着让他们理解。<br />因为对方是对内容完全不了解的外行，所以也就不会在细节处纠缠。我们需要他们帮忙看的不是细节，而是整体的流程是否容易被人理解，逻辑是否通畅。<br />向外行人试着说明时，我们发现，有时那些对于我们来说是常识的东西，在别人看来却并非如此。<br /></li><li>边说边揣摩对方的理解程度<br />如果在讲话中，发现自己的想法对方没有理解，那就当场补充，充分做出解释。<br />不提出问题不代表完全理解，而是预示着不理解。<br /></li><li>观察对方的动作，揣测对方的理解程度。<br />不理解的信号：<br />1、自己开始讲下一页的资料时，对方却在观看这一页；<br />2、对方不看向自己，而是看向旁人；<br />3、含糊地反馈自己”大致理解了“”大概听懂了“等。</li></ul><h2 id="配合对方的步调">07 配合对方的步调</h2><p><strong>文件要遵循客户惯用的格式编写。</strong></p><h2 id="充分理解对方的期望值">08 充分理解对方的期望值</h2><p>在商务场合上最重要的是什么？不断的超越对方的期望值。</p><ul><li>首先要正确地理解对方真正想看到什么。（了解对方的期望值）<br /><strong>不能体现对方期望值的事情不要做，避免理想主义、完美主义。</strong><br /></li><li>经常给出超越对方期望值的成果。<br />把握对方的期望值水平，绝不偏离这个期望值。并且拿出120%的结果。<br /><strong>很多咨询师的商业秘诀就是预测客户的期望值，在最关键的地方给出超越客户期待的成果。</strong><br /></li><li>不能满足对方的期望就不要轻易许诺。<br />有时需要降低对方的期望，对自身要求作出调整。<br /><strong>商务合作的基础就是拿出超越对方的期望的成绩，所以当预先知道无论怎么努力都无法做出超值成绩的时候，就不能承诺了。<br />而在这种情况下，就要通过事先沟通，让对方降低对</strong>次要<strong>部分的要求，这就是对期望值的管理。</strong></li></ul><h2 id="超越上司的期待">09 超越上司的期待</h2><p>无论是发出指示的上司，还是接受安排的属下，都要明确以下四个要点：<br />1、工作的背景和目的<br />2、具体的工作成果<br />3、质量要求<br />4、优先顺序和紧急程度</p><p><strong>对于模糊不清的部分，从自己的角度出发做出假设，与上司沟通。</strong></p><h1 id="二逻辑思考技巧">二、逻辑思考技巧</h1><h2 id="思考方式">10 思考方式</h2><p>工作不能一下子就盲目开始，要先考虑思考方式。考虑思考方式就是思考用什么思路才能得到结果。<br />要在方法和步骤获得同意后再行动。<br />做出大致的计划，就工作流程达成共识。</p><h2 id="熟练运用逻辑树">11 熟练运用逻辑树</h2><p>进入咨询公司后，首先要学习的技能就是逻辑树、结构化思维（MECE）、问题解决法等一序列逻辑思维或问题解决的顺序方法。<br /><strong>其实结构化思维就是运用逻辑树进行思考的方式，就是利用逻辑树将问题结构化</strong>，，学会逻辑树，可以在脑海中清楚地看出问题的各个部分在全局中所属的位置。这样就能发现什么重要，什么不重要，就可以从全局上判断什么是最关键的。逻辑树的每一根分支并不同等重要，有的分支权重占比高，有的低。通过权重的分析，能学会放弃不重要的部分。学会判断重要性，学会放弃，最终能使决策飞速提升。<br />即使是庞大复杂的问题也可以利用逻辑树分解成小问题，可以分别设计每个论点，通过分析每个论点，就能得出整个问题的答案。</p><h2 id="云--雨--伞-提建议的基本原则">12 云--雨--伞 提建议的基本原则</h2><p>天上出现乌云，眼看就要下雨，带上伞比较好。<br />这其实是对事实、分析和行动三者的比喻。<br /><strong>在提方案、建议的时候，加入”事实“、”分析“、”推荐行动方案“。预算、计划等应该是放在行动计划里面。</strong></p><h2 id="假设性思考">13 假设性思考</h2><p>”先做假设“是咨询式思维方式中最重要的特征之一。事先对问题做假设，锁定调查的关键点，才能做高效率的调查分析。让假设、检验、反馈做快速运转。<br />一旦掌握了假设性思考，做决定的速度就会飞速提升。假设性思考也可以提前思考，到了真正面对的时候就更方便快速决策了。</p><h2 id="有主见的汲取信息">14 有主见的汲取信息</h2><p>动脑，简而言之就是有自己的想法。带着自己的主张去接触信息。<br /><strong>有主见的思考方式：就是在看答案、看具体你内容前，留给自己一分钟思考的时间。</strong>带着思考的答案去看文章，比较自己思考的内容和文章内容的比较。想法有错误没关系。持有自己的想法，本来就是为了认识自身错误，为了意识到自己和他人想法不同。<br />事前有自己的想法才会有新的发现。提高思考能力并没有什么捷径，但是置身咨询公司这个必须强制自己思考的环境中，思考方式自然得到了锻炼。</p><h2 id="探求问题本质的思考方式">15 探求问题本质的思考方式</h2><p>收购一家企业，关注的两个本质：1、这个企业运作的核心是什么？2、如果收购的话，相应的企业的价值是多少？<br />收集信息的过程并不等于思考，只有在看清问题的本质后，信息才能体现真正的价值。<br />提升思考力，不是大量地收集信息，而是挖掘出一到两个本质并细致加工。</p><h1 id="三资料制作技巧">三、资料制作技巧</h1><h2 id="文书写作的基础----会议记录法">16 文书写作的基础----会议记录法</h2><p>会议纪要就是将会议内容结构化汇总。<br />会议记录：<br />1、记录决策事项、确认事项、需要向相关人员确认决策的内容；<br />2、记录决策事项，作为日后的依据。<br />格式：<br />日期时间<br />地点<br />参加人员<br />本日内容安排（论点·议题）<br />已决定事项<br />未决定事项（需要下次决定的问题）<br />需要确认的事项<br />下次会议之前的准备（负责人和截止日期）</p><p>因为会议记录有证据的作用，因此有时候也应该有关键人员提出的保留意见。例如”虽然决策是这样，但是某某说了那样的意见“，或者”有这样的反对意见，但是决定是这样“。针对决策，添加上某人的意见或发言作为你参考，特备是会议上关键人员的意见非常有效。<br />例如：<br />关于什么的意见，最终全部通过。<br />虽然某某问题，但是不影响（部长意见）</p><h2 id="最强的ppt制作方法">17 最强的PPT制作方法</h2><p>1、简洁之上<br />一页一个问题<br />2、基本结构<br />数据或事实+分析和意见<br />（一页最多两个图表、一页最多一个主张、可靠的资料标明出处）<br /><img src="/img/靠谱2.jpg" /></p><h2 id="excelppt速度定输赢">18 Excel、PPT：速度定输赢</h2><p>咨询公司一天做四五十页的PPT很常见。<br />使用快捷键能有效提高效率。</p><h2 id="从预设结果推算出工作计划">19 从预设结果推算出工作计划</h2><p>”做空包“是一种制定工作计划的方法。<br />简单的说，就是在着手工作时已经有了最终成果获产品的框架结构。<br />具体的说，就是利用PPT先写出各个标题，做出资料的大纲，仅仅有标题没有内容，每页PPT就叫做空包或空页。（其实可以上网找对应的模板，不一定自己从头做）</p><h2 id="检索型阅读法">20 检索型阅读法</h2><p>明确、紧扣阅读目的；<br />像网络检索一样检索目录，选出所需内容，只读重点。<br />尽量多方面、浅层次地接触大量文献。</p><h2 id="抓重点----让工作速度倍增">21 抓重点----让工作速度倍增</h2><p>高效工作的秘诀只此一条：彻底甩掉非必要的工作。<br />80/20原则</p><h2 id="项目管理法----课题管理表">22 项目管理法----课题管理表</h2><h1 id="四专业商务精神">四、专业·商务精神</h1><h2 id="创造价值value">23 创造价值（Value)</h2><p>价值简单地说就是”对他人的贡献“<br />工作不是自己想做什么就做什么，而是要满足对方的需求。将为他人做贡献当做自己的工作的目标。</p><h2 id="不发言勿开会">24 不发言，勿开会</h2><p>不发言的人确实不会创造任何价值。即使意见毫无亮点，开动脑筋在会议上提出自己的想法也比什么都不说要有意义。</p><h2 id="牢记时间就是金钱">25 牢记”时间就是金钱“</h2><p>从客户和老板的角度来看，员工的时间就是金钱。</p><h2 id="速度质量两不误">26 速度质量两不误</h2><p>如果能早点把基础做出来，然后让可以改善问题点的PDCA循环高速运转，也可以在短时间做出高质量的成果出来。<br />与其花时间去追求完美，不如快点做，不美观也没关系。<br />其实把工作从0分做到90分花的时间和从90分做到100分用得时间是一样的。并且，从99分到100分也要花同样的时间。也就是说，越往上走，工作精度的提升越发困难。这就是贝尔实验室提出的90-90法则。<br />因此，就在90分处打住，有时候60分也可以。确实，最后的成果绝不能只有60分。但是决定大致方向的话，只要60分就够了。</p><p><strong>快鱼吃慢鱼、天下武功唯快不破、快速迭代、快速试错、快速反馈</strong></p><h2 id="学会承诺力">27 学会”承诺力“</h2><p>有时候为了承诺的完成，可以求助于他人。这也是一种整合资源的能力。</p><h2 id="拜师学艺">28 拜师学艺</h2><p>对于年轻人来说，重要的不是在哪个公司工作，而是和谁一起工作。因为无论是为人处世还是工作能力，你都会受到影响。工作中只可意会不可言传的部分对专业人员来说才是最重要的。</p><h2 id="发挥追随能力">29 发挥追随能力</h2><p>追随能力就是作为属下可以发挥的领导力。追随能力是作为属下的领导力，优秀的团队必定有优秀的追随者。<br />为了支持上司的提议，要动员自己周围的人。<br />领悟理解上司的提议，想上司所想，思上司所需，积极行动。<br />提出最初方案的是上司的责任，为实现方案设想，率先积极地行动才是追随能力。<br />derek sivers ted talk 2010 德雷克·西弗斯Derek Sivers：如何发起一场运动 （youtube中有视频）<br />2010年，德里克·西弗斯（Derek Sivers）做了一个关于创造力和领导力的TED演讲。在开讲后不久，他放了一个视频，画面中是参加一场户外音乐会的人群。一个光着膀子的年轻人开始独自起舞，坐在旁边的观众好奇地看着他。<br />“一个领导者需要有勇气站出来，甚至面对讥笑。”西弗斯说。但是过了不久，第二个年轻人加入了，也开始舞起来。<br />“现在来了第一个追随者。他起到了关键作用……第一个追随者将一个孤独怪人转变为一个领导者。”随着视频继续播放，有几个人也加入了舞蹈的行列，然后又有几个。视频放到两分钟的时候，跳舞的人已经发展为一大群。</p><h2 id="具备专业精神的团队合作">30 具备专业精神的团队合作</h2><p>具备专业精神的团队就是每一个人都在不同的领域发挥自己特有的价值。</p><h1 id="文章摘要">文章摘要</h1><ul><li>演绎法：因为这样，所以那样。这是按照事务的前因后果说话，其最典型的代表是数学推导。从前提条件开始，按顺序出发，通过逐步推导得出结论，这就是演绎法。<br />与此相反的是归纳法，先讲结论。<br /></li><li>好莱坞电影被认为是低语境。也就是说，好莱坞的电影呈现的内容和意义是全世界都能理解的爱、家庭、正义等。<br />不过，行家喜欢的电影就是高语境（High context)，观看浙西诶电影之前需要了解电影内容相关的背景和文化知识等。</li></ul><p><img src="/img/靠谱1.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 咨询公司 </tag>
            
            <tag> 思维方式 </tag>
            
            <tag> 工作方法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《精准表达》读书心得</title>
      <link href="/2020/05/01/%E3%80%8A%E7%B2%BE%E5%87%86%E8%A1%A8%E8%BE%BE%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2020/05/01/%E3%80%8A%E7%B2%BE%E5%87%86%E8%A1%A8%E8%BE%BE%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<h1 id="序言">序言</h1><h2 id="全书整体结构">全书整体结构</h2><p><img src="/img/精准表达2.jpg" /><br /><strong>假如做到充分理解对方与自己的不同，</strong>以建立清晰的逻辑为基础来思考问题，将思考的结果以简单易懂的方式传达给对方，像这样顺利的提出方案，想必就能解决所有分歧。<br /><span id="more"></span></p><h2 id="方案可能不被通过">方案可能不被通过</h2><p>提交方案就要努力让方案通过：所谓方案，提交出来就是为了让它通过。这句话看似理所当然，可是很多人却没有真正意识到这一点。<br />千万不要误以为方案做出来就该被通过。如果你把方案通过视为理所当然，一旦你的方案未能通过，就会把失败归咎到对方或环境。<br />人们之所以有抱怨，是因为他们在提交方案时从没考虑过被反对的可能性。当你天真的以为自己提交的方案就应该通过时，你就已经把失败归咎到他们身上，这样一来，你会很难得到成长。如果你把责任归咎到自己身上，你对事物的看法也会发生改变。<br />如此这般，在提交方案之前就能预想到不被认可的可能性，就能把注意力集中到自己应从哪些方面做出努力，积极的思考解决问题，让自己更有动力去磨练自己的能力。</p><h2 id="制作书面资料">制作书面资料</h2><p>当你想要把自己想说的东西总结成书面材料时，你可能会突然不知道该怎么去写。或者当你整理会议纪要的时候，你突然发现，听得时候明明感觉自己已经听懂了，可是整理成书面材料的时候却发现自己完全没懂。<br />写出书面说明之前就是一个反复思考、锻炼大脑的过程。</p><p><strong>不善于表达是因为自己原本就不知道应该讲些什么。如果注意到这个问题，就开始致力于把自己想表达的内容总结成书面材料，这样在演讲的时候就能顺畅的表达自己的思想了</strong><br />说明会上流利发言的秘诀并非在于是否善于表达，真正重要的是你事先是否认真做了准备，是否已经理清了大脑中的思绪。<br />但如果这是日常的业务报告或者普通商谈，则无需如此细致，不过原则上还是需要大家制作书面材料，确定自己大致要讲哪些内容和不讲哪些内容。倘若没能理清自己的思路，仅仅简单列个“提纲”，那么这个提纲不过是偷工减料和随口一说罢了。<br />当合理思考及妥善表达时，你的方案才有可能得到通过，两者缺一不可。<br /><img src="/img/精准表达3.jpg" /></p><h2 id="合理思考">合理思考</h2><p><img src="/img/精准表达4.jpg" /></p><h2 id="妥善表达">妥善表达</h2><p><img src="/img/精准表达5.jpg" /></p><h1 id="逻辑思考能力">逻辑思考能力</h1><p>准备对方可能各种视角提出的问题，包括营销、管理、业务、安全等，然后都准备好问题的应答，但不需要在资料中呈现给客户。（特别是需要提前了解参会人员的背景来思考他们可能在他们专业、视角提出的问题）<br />纵向逻辑、横向逻辑</p><p>辩证：全面、动态、关联</p><p>在商务环节，不同企业或部门的人们在价值观及思考方式存在的差异远远超出我们的想象。<strong>例如给非专业领导汇报就是一个非常考水平的。思考乔哈里视窗。</strong>不要奢望所谓的"不言自明","迟早会明白"，如果我们不努力准备，就可能无法获得对方的理解。</p><p><strong>逻辑就是指“把语言合理的组织起来。”</strong><br /><img src="/img/精准表达6.jpg" /></p><p>如果对方听不懂，如果给对方的反应简单归类，你就会发现，对方感到不理解无非有两种反应：</p><ul><li>真是这样吗？<br /></li><li>仅仅如此吗？<br /><img src="/img/精准表达7.jpg" /></li></ul><h2 id="纵向思维逻辑">纵向思维逻辑</h2><p>不同类型的人理解的程度不一样。<br /><img src="/img/精准表达8.jpg" /></p><p>纵向逻辑薄弱的原因</p><ul><li>前提条件不同<br /></li><li>把不同性质的东西混为一谈<br /></li><li>偶然的必然化</li></ul><h3 id="前提条件不同">前提条件不同</h3><p>若要做到有逻辑地讲话，首先我们应从质疑自己的前提条件开始。人们脑海中往往存在各种”隐形前提“，想象一个与自己条件大不相同的人，设想他可能会提出怎样的问题，通过这样的训练，想必你能很快发现隐形前提。<br /><img src="/img/精准表达10.jpg" /></p><h3 id="把不同性质的东西混为一谈">把不同性质的东西混为一谈</h3><p><img src="/img/精准表达11.jpg" /></p><h3 id="偶然的必然化">偶然的必然化</h3><p>要判断自己所说的话中的因果关系是偶然还是必然，应从头到尾地思考哪些因素可能会破坏因果关系。首先请想象一下具体情形，然后按照时间顺序想象最坏的情形，这样你就能轻易发现阻断因果之间联系的主要因素。<br /><img src="/img/精准表达12.jpg" /></p><h2 id="横向思维逻辑">横向思维逻辑</h2><p><img src="/img/精准表达9.jpg" /></p><p>语言中的层次感<br /><img src="/img/精准表达13.jpg" /></p><p>让说话者和听话者站在一致的立场的关键在于，确认对方是以何种立场来思考问题的。<br />让说话者和听话者切入点变得一致的关键在于，确认对方的设想是何种场景。<br /><img src="/img/精准表达14.jpg" /></p><h2 id="逻辑是否合理由对方判断">逻辑是否合理由对方判断</h2><p>需要注意的是，你的逻辑是否合理，是由对方来判断。<br />真正懂逻辑的人可以让任何人都能听懂自己的意思，假如你认为自己很有逻辑，无法和不懂逻辑的人沟通，实际上意味着你自己就缺乏逻辑。</p><h1 id="验证假说能力">验证假说能力</h1><p>假说验证的5个步骤<br /><img src="/img/精准表达15.jpg" /></p><ul><li>要提高自己的提案能力，学习逻辑思考、知道自己的立场是要求对方做出判断，这些固然重要，而强化理解对方需求这样的感性思维，也是提升提案能力的关键。</li></ul><p>如何避免偏离论点<br /><img src="/img/精准表达16.jpg" /><br /><img src="/img/精准表达17.jpg" /></p><p>对上司和顾客说话时，负责思考的人应该是我们，因此我们必须采取假说验证型思考。<br /><img src="/img/精准表达18.jpg" /></p><p>验证假说<br /><img src="/img/精准表达19.jpg" /></p><p>提取启发而不是提供答案<br /><img src="/img/精准表达20.jpg" /></p><p><strong>也就是说，如果我们顺利提取到了启发，那么就无需直接回答论点或者验证假说了。我当年初入社会时非常青涩，常常想要从正面验证对方的假说，或直接回答对方的论点，其结果却是诸事不顺、毫无成效。其实商务世界既不需要斩钉截铁的回答论点，也不需要原原本本地验证对方提供的假说，只要我们聚焦到论点、提取启发、指引方向，那就够了。</strong></p><p>尽管做决策的时候需要某些重要信息，然而现实问题在于我们收集不到太过详细、全面、保密、超前的信息。</p><h1 id="会议设计能力">会议设计能力</h1><p>会议要分析与会者，根据与会者的职位、背景来设计议题。例如，如果对方来的都是管理层，或许我们不该一上来就介绍具体的项目内容，而应该和他们先交流一下管理经验及公司理念等。</p><p><strong>向管理层提交方案时，应当从企业管理的角度思考。</strong></p><h2 id="会议的着陆点">会议的着陆点</h2><p><img src="/img/精准表达21.jpg" /><br /><img src="/img/精准表达22.jpg" /><br /><img src="/img/精准表达23.jpg" /></p><p>会议的定位--&gt;会议的输入（新鲜感）--&gt;会议的输出（前进感、不要急躁）</p><h2 id="会议的着陆形式">会议的着陆形式</h2><p>所谓着陆形式，就是把会议设计成怎样的风格（形式）。顺着对方的理解形式思考模式设计出来的方案更容易获得对方的理解。<br />理解对方风格的三个诀窍:<br />1、是阅读者还是倾听者<br />2、是纵观全局派还是连锁把握派<br /><img src="/img/精准表达24.jpg" /><br />大多数领导是属于纵观全局派。在向纵观全局派提交方案时，我们应当仔细解说”整体“的情况，例如工作的整体情况、课题的整体情况、对策的整体进展等，详细解说纵向逻辑不会引发纵观全局派的共鸣。而逻辑上的略微跳跃些也不会有什么影响，重点是应从全局出发，把握事物的整体，然后分别讲述各个部分的内容。</p><p><img src="/img/精准表达25.jpg" /></p><p>管理层往往属于自上而下的风格。</p><h1 id="制作商务文档的能力">制作商务文档的能力</h1><p>学会使用模板，站在巨人的肩膀上。<br />Simple is best！<br /><img src="/img/精准表达26.jpg" /><br /><img src="/img/精准表达27.jpg" /><br /><img src="/img/精准表达28.jpg" /><br /><img src="/img/精准表达29.jpg" /><br /><img src="/img/精准表达30.jpg" /><br /><img src="/img/精准表达31.jpg" /><br /><img src="/img/精准表达32.jpg" /><br /><img src="/img/精准表达33.jpg" /><br /><img src="/img/精准表达34.jpg" /><br /><img src="/img/精准表达35.jpg" /><br /><img src="/img/精准表达36.jpg" /></p><h1 id="文章摘要">文章摘要</h1><ul><li>在商务场合，没有什么能力比沟通能力更为重要。<br /></li><li>利益出发考虑，一切以开源（拓展新的业务领域）、节流为目标。<br /></li><li>后台系统：在服务业是指不与顾客直接接触的领域。例如，接受订单、发货、库存管理等业务。<br />前台系统：在服务业是指与顾客直接接触的业务领域。例如，接待、应对等与顾客有直接交流的业务。<br /></li><li>思考上位的概念，将自己说过的话翻译成管理学语言，从上位概念的角度进行总结，构筑出一个逻辑相当缜密的理论。<br /></li><li>努力让通不过的方案得到通过，这才是方案的价值。如果随便什么方案都能通过，那么谁都不用辛苦拼搏了。<br /></li><li>对上司和顾客说话时，负责思考的人应该是我们，因此我们必须采取假说验证型思考。<br /></li><li>要时刻以对方否定的方式来思考你的方案。<br /></li><li>大胆假设，小心求证<br /></li><li>世界没那么单纯，不是非此即彼的黑白世界。<br /></li><li>利益分析法，利益驱动。</li></ul><p><img src="/img/精准表达1.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 沟通 </tag>
            
            <tag> 表达 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《沟通的艺术》读书心得</title>
      <link href="/2020/04/12/%E3%80%8A%E6%B2%9F%E9%80%9A%E7%9A%84%E8%89%BA%E6%9C%AF%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2020/04/12/%E3%80%8A%E6%B2%9F%E9%80%9A%E7%9A%84%E8%89%BA%E6%9C%AF%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<h1 id="沟通的本质">沟通的本质</h1><p>沟通的实质：沟通是有关使用信息生成意义的过程。 包括交换信息和创造意义。<br />人际沟通----就像跳舞----是一个经由同伴间的互动而创造出的独特活动。</p><span id="more"></span><h1 id="交流沟通模式">交流沟通模式</h1><p><img src="/img/沟通的艺术3.jpeg" /><br /><img src="/img/沟通的艺术4.jpeg" /><br />沟通是一个交流的过程，其参与者处于不同但又有所重叠的背景下，经由交换信息而建立关系，关系的品质会受到外在的、生理的和心理的噪音干扰。<br />噪音：外在噪音，生理噪音，心理噪音（例如防卫心理）<br />沟通同时具有内容和关系两个向度。<br />沟通是一门艺术而非科学。<br />有效的沟通必须包含：能在大多数情况下维持或增进关系，并借此实现自己的目标。换句话说，能力既要求有效性，又要求适当性。沟通能力是一种既要留心他人又要考虑自己的获取平衡的行为。<br />沟通高手能够从各式各样的沟通行为中选取他们的行动。</p><h1 id="沟通的媒介">沟通的媒介</h1><p><img src="/img/沟通的艺术5.jpeg" /></p><h1 id="自我的塑造和展现">自我的塑造和展现</h1><p>自我应验预言：是指如果个体对事物的发生有所预期，并且他接下来的行为是建立在这些预期上的，那么这件事情的发生比没有预期更可能成真。<br />（自我暗示）<br />自我强加预言：Yes，I can.<br />他人强加预言：Yes，you can.<br /><strong>在仔仔考试的时候，可以通过他人强加预言来激励他。</strong></p><h1 id="知觉看到什么就是什么">知觉：看到什么就是什么</h1><p><img src="/img/沟通的艺术6.jpeg" /><br />知觉检核：</p><ol type="1"><li>描述你注意到的行为<br /></li><li>列出有关此行为至少两种可能的诠释。<br /></li><li>请求对方对行为诠释做澄清。</li></ol><p><img src="/img/沟通的艺术7.jpeg" /></p><p>面对分歧的方法：枕头法；目的是在分析后保持积极的、相互尊重的关系。<br /><img src="/img/沟通的艺术8.jpeg" /></p><h1 id="情绪感觉思考和沟通">情绪：感觉、思考和沟通</h1><p>谈到沟通，不得不承认情绪的重要性。想想看：自信感可以在任何事上帮到你，不管是邀请别人还是要进行一场演讲，而不安则会毁了你本该有的机会。生气或防卫的感觉会毁掉你和对方在一起的时间，而沉着的感觉和表现则有助于你们预防或解决问题。<br /><strong>情商：我们理解和控制自己情绪的能力，以及对他人的感觉保持敏感的能力。</strong><br />管理情绪并不容易，尤其是你在感到害怕、压抑、生气或防卫的时候。<br /><strong>要建设性的表达自己的情绪，适当的表达自己的情绪。总是隐藏或压抑自己的情绪可能导致严重的疾病。要在合适的时机表达自己的情绪，在冲动的时候不要说出冲动的话。如果一些强烈的感情无法说出口，通过写作写出你的感受和想法，也能起到很好的帮助。（就像蒋介石总是写日记一样，通过日记来发泄自己的负面情绪。）</strong><br />表达方式，正面的情绪可以面对面表达，但是，负面的情绪适合通过媒介来表达。<br />我们的情绪更像是我们想法的结果，而非我们所遭遇的事件的结果。ABC理论。<br /><img src="/img/沟通的艺术11.jpeg" /><br /><strong>培养正面情绪和减少负面情绪很重要，习得性乐观。多看到事情的正面。</strong></p><h1 id="语言障碍和桥梁">语言：障碍和桥梁</h1><p>让文字更容易衡量的一个方法就是把他们转变为数字。<br /><strong>采用更强有力的语言。也是自信的一种表现。</strong><br /><img src="/img/沟通的艺术12.jpeg" /><br />在面试时，强有力的语言可以帮助到求职者。雇主认为使用有力的语言风格的应聘者要比语言不那么有力的求职者更有能力、更适合雇用。</p><h1 id="倾听">倾听</h1><p>飞快的思想<br />就生理层面来说，要做到有效的倾听也是一项困难的工作。研究显示，人类有能力在一分钟倾听600个字，但是通常人们在一分钟只能说100-150个字。于是，当别人讲话的时候，我们便有了许多“多余时间”，这会诱使我们去想一些和讲话者内容无关的事，想想我们感兴趣的事，做做白日梦，计划如何反驳对方等。<br /><strong>有效倾听的技巧就是利用“多余时间”来更好的理解说话者的想法，而不是让自己的注意力漫游。（倾听的时候想想对方真正的想法是什么？）</strong><br />借力使力：牵涉到使用沉默和简短的言论来鼓舞对方多说一些话。最好的方法是释义（用自己的措辞重复别人的观点）。</p><h1 id="改善沟通氛围">改善沟通氛围</h1><p><strong>争辩：如何在争论的同时保持一个积极的氛围，关键在于你表达观点的方式。攻击观点而不是攻击人，这一点很重要。此外，如果你能正面陈述一个合理的观点，他被采纳的可能性更高。<br />先肯定对方的观点，再提出自己的意见，采用不防卫的心态很重要。</strong><br /><img src="/img/沟通的艺术13.jpeg" /></p><p>描述式的语言是避免评判别人，而只是把焦点放在说话者的想法和感受上。<br />对批评以不防卫回应。<br />虽然你有良好的意愿，但是当你被别人攻击时很难保持理性。当批评明显不公平时，听者就已经足够难受了，但命中要害的批评常常让人更痛苦，即使批评你的言论是正确的，你也可能倾向于要么以大量的口头攻击回击，要不不声不响的撤出。</p><h1 id="处理人际冲突">处理人际冲突</h1><p><img src="/img/沟通的艺术14.jpeg" /><br /><img src="/img/沟通的艺术15.jpeg" /><br /><img src="/img/沟通的艺术16.jpeg" /><br /><img src="/img/沟通的艺术17.jpeg" /><br /><img src="/img/对待批评.png" /><br /><img src="/img/处理人际冲突.png" /></p><h1 id="文章摘录">文章摘录</h1><ul><li><strong>比起一一味强调技巧，尊重而主动的态度更有利于建立信任关系。</strong><br /></li><li><strong>强调倾听的重要性；倾听可以说是最重要的沟通技能。尽量避免打断别人。</strong><br />站在对方立场去思考、去展开话题，可以问一些开放性的话题。<br /><strong>好的倾听者会使用非语言行为来表现他们的专注，例如保持眼神交流，用适当的面部表情做出反应，而不是用语言打断对方。</strong>在你想说话前数5下再说。<br />避免做自恋的倾听者，例如在回应的时候将对话的焦点从说话者身上转到自己身上。<br /></li><li>向上沟通能力、横向沟通能力：关系、信任、做事。<br /></li><li>媒介沟通会把温暖的面对面互动变成冰冷的电子信息交换。导致没人情味的、以任务为取向的关系。<br /></li><li>评估在一个特定的情景中，坦率和模棱两可如何混合使用最有效。<br /></li><li>如果你是20世纪90年代后才出生的“数字原住民（digital native）”，那么媒介沟通对你来说或许就像呼吸一样自然了。<br />90前就是“数字移民（digital immigrant）”<br /></li><li>如果那看上去太过美好而不像是真的，那八成就不是真的。<br /></li><li>你的人格在你的一生中会趋于稳定，并且随着时间的推移会越来越明显。<br /></li><li>孕育我们长大的文化以不着痕迹的方式塑造着我们对自我的理解。大部分西方文化都是高度个人主义的；反之，其它传统文化，譬如大部分的亚洲人都倾向于集体主义。<br /></li><li>我们评价关系的亲密与否的一个方法就是依据我们和对方分享了多少信息来决定的。<br /></li><li><strong>在工作中袒露个人的看法和感觉是相当冒险的行为。</strong><br /></li><li>当你预测风险的时候，请确保你的预测是现实的。有时候，人们很容易陷进毁灭性的预期里，臆想种种灾难性的后果，而实际上这些可怕的事情并不太可能发生。（但是墨菲定律也是存在的。）<br /></li><li>隐瞒通常被视为比说谎和有目的的欺骗更好的选择。<br /></li><li><strong>同理心教育：如果家长能够向孩子指出，其不当行为会对他人造成苦恼（“看看因为你把某某的玩具拿走，他有多难过。如果现在有人拿走了你的玩具，你不会伤心吗？”），这比简单的把孩子的行为归为不恰当能让孩子获得更多的领悟。</strong><br /></li><li>残疾人是和别人不一样，但不是比别人差。<br /></li><li><strong>用乐观的、抬头挺胸的姿势走路，可以赶走沮丧的情绪。</strong><br /></li><li><strong>说出你心中的感受能有助于有效的管理它们。</strong><br /></li><li><strong>适度的紧张和恐惧有助于提高注意力、提升动力、提高你的表现。例如在演讲和面试的时候。</strong>（适度压力的作用。）<br /></li><li><strong>把自己的难处提出来，而不是指责对方的错。</strong>例如：我们IT又不能关闭他们的麦克风，不让他们讲话。</li></ul><p><img src="/img/沟通的艺术1.jpeg" /><br /><img src="/img/沟通的艺术9.jpeg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 沟通 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《社交天性》读书心得</title>
      <link href="/2020/02/17/%E3%80%8A%E7%A4%BE%E4%BA%A4%E5%A4%A9%E6%80%A7%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2020/02/17/%E3%80%8A%E7%A4%BE%E4%BA%A4%E5%A4%A9%E6%80%A7%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>【中心思想】<br />实验表明社会连接是大脑最重要的需求，在工作社交是一个领导者最重要的能力之一，生活当中增加社交也是提高幸福感的重要手段。<br />本书是一本讲述社会认知神经科学的书，利用诸如功能性核磁共振成像技术的工具，研究大脑神经的工作机制。通过实验，作者提出了我们大脑天生就与他人相互连接，这种能力是人类进化过程当中形成的。<br />人类大脑的连接、心智解读、协调是人类社交的三大驱动力。<br />通过社会连接（social connection），保持稳定的关系，这些会使人感到幸福。研究表明人们在无所事事的时候就会去想他人，说明了么大脑的默认网络和社会认知网络有高度的重合。也正因为如此，社会连接是大脑的做重要的功能。<strong>当我们陷入困境，不得不面对生活的重重压力时，社会支持和社会链接可以发挥极大的缓冲作用。</strong><br /><span id="more"></span></p><h1 id="导演任务">导演任务</h1><p><img src="/img/社交天性9.jpg" /><br /><img src="/img/社交天性10.jpg" /></p><h1 id="自我">自我</h1><p><strong>自我，让你的大脑成为你他人思想的跑马场。</strong><br />很多人，终其一生，在“自我”方面是毫不作为的，充其量，他们只能为自我的幽灵做些事。自我是形成于他们周围人的头脑中，然后再交付给本人的。通常来说，我们的自我意识主要是由我们生活中的其它人构建出来的，它所服务的对象首先是他人，而不是我们自己。<br />我们通常是通过观察别人，而不是审视自己的内心来了解自己的。<br />人类天生就很容易受周围人的影响----会被他们牵着鼻子走。换句话说，我们的耳根子远远比自己想象的软很多。</p><h1 id="社交空间">社交空间</h1><p>一个空间要满足居民的社交需求、公司员工的社交需求。我们应该最优先考虑的问题是，如何在工作场所中创造出一个适当的社交环境。<br />社交空间：<br />给人们预留进行社交活动的物理空间，城市有广场、小区有会所等、楼栋有大堂、楼层也有空间。社交空间需要对应的社交设备和社交活动。<br />社交设备：茶具、大屏电视机、游戏机、桌牌等<br />社交活动： 类似社团，有共同的爱好兴趣</p><p>小时候乡下的大榕树有点类似，一棵大榕树下有一个士多，士多还会摆一些棋牌桌等。</p><h1 id="社会奖赏">社会奖赏</h1><p>社会奖赏，企业激励员工更有效的方法。例如精神激励：地位、荣誉、认可。认可是一种完全免费且可再生的资源。例如团建活动、饭局这些非正式社交。帮助别人也能提高员工的归属感。例如让员工有机会去帮助别人，给予员工帮助别人的机会能够促进他们更加努力的工作。<br />高效领导者需要更强的社交能力。高效领导者给自己定下目标，每年或者每个月认识多少个朋友之类的；给自己定下目标每年要参加多少次活动；给自己定下目标每周要和多少个员工谈话。<br />在管理的时候，既要对事，也要对人。社会化的思考人；非社会化思考事。<br /><strong>当一个项目成员说自己在推进一个项目时碰到了困难，其言下之意往往是，他在与团队的其它成员合作的过程中碰到了困难。如果这个团队的领导者是一个“社会化程度”很高的人，他就会意识到自己团队内部的协作机制需要调整。因此，他不会过分关注团队成员是否需要更多的个人训练。</strong><br />最好的老板了解并关心团队的所有成员的社会动机。老板必须促成自己与团队所有成员之间、团队成员之间、团队与外界之间的社会连接这对团队的成功至关重要。</p><h1 id="高层次的目的">高层次的目的</h1><p><strong>我们都生活在一个由无数充满意义的动作构建而成的世界同一动作既能用低层次的目的来描述，也可以用高层次的目的来描述。</strong><br />例如，砌墙，有些工人考到是砌一堵墙，有些工人看到的是在盖一座宏伟的教堂。<br />三个石匠的故事为管理爱好者熟知，其出处为德鲁克的经典著作《管理的实践》：有人在一个工地看到三个石匠，就问他们在做什么。第一个石匠回答：“我在养家糊口。”第二个石匠边敲边回答：“我在做全国最好的石匠活。”第三个石匠仰望天空，目光炯炯有神，说道：“我在建造一座大教堂。”<br />德鲁克认为，第三个石匠是真正的管理者。第一个石匠已经无可救药，他不是个管理者，也永远不会成为管理者。<br />“最难办的就是第二个石匠”。读《管理的实践》至此，仿佛可以看到德鲁克先生写到这里的时候，也皱了皱眉，轻叹一口气。<br />德鲁克认为，在现实的企业中，大多数的管理者都和第二位石匠一样，只关心自己的专业。因此他以一种嘲讽的口气说道，“很多工匠或专业人士，常常自以为有成就，其实他们只不过在磨亮石头或帮忙打杂罢了。”</p><p>例如，这次新冠疫情，给一线病人运输药品的司机，在回答记者的提问时，说他装运的是病人的生命的希望，而不是局限在药品上。</p><h1 id="辩证法">辩证法</h1><p>按马克思主义哲学的观点，辩证法与形而上学相对立。有些人经常提醒我们说，要学会辩证地看问题。<strong>什么叫辩证地看问题呢？有三个关键词：发展、全面、联系。做到这三点，就算是辩证地看问题。相反，如果你是静止、片面、孤立地看问题，那就叫形而上学。</strong><br />人们对许多科学上令人兴奋的新发现的认识过程都要经历一个黑格尔式的“辩证过程”，这类似于华尔兹三步舞曲：首先，人们希望这个新发现可以用来解释所有无法解释的现象（第一阶段：正题），随之而来的是对上述信念的动摇，认为大多数现象都无法用这个发现来解释（第二阶段：反题），最后才最终认清了现实----这个发现在某些方面或许很有用，但在另一些方面则可能没什么用（第三阶段：合题）。<br />例如，开始是经典牛顿力学，以为都适用所有的运动；后来在宏观层面发现不适用，出来了相对论。以为能涵盖所有，后来发现其实相对论也无法解释微观的世界，出来了量子力学。<br />科学家们一直都在致力于统一的力学定律，但是还没有找到。</p><blockquote><p>形而上学通俗解释 转自知乎https://www.zhihu.com/question/342267841<br />我们现在所说的「形而上学」，可以简单理解成：<br />用理性思维，去研究那些能统一世间一切问题的「大道理」。<br />详细说说吧。<br />我们在学校里学习马哲的时候，课本给我们的解释是：「形而上学就是用孤立、静止、片面的方式看待问题。<br />在课本上，「形而上学」被当成一个贬义词，说谁是形而上学，那一定是在骂他呢。<br />课本上这样讲不够厚道。<br />你听说过哪个学科一开始成立的时候就宣称：我们这个学科就是立志要孤立、片面、僵化地研究问题··那我们不就是吃饱了撑的嘛。<br />课本里给形而上学下定义，就好像学校里调皮的孩子专拿别人的缺点起外号一样。<br />明明人家也是一个健康、正常的孩子，但外号就叫成了「爱哭鬼」「小胖墩儿」。<br />我们课本给形而上学找的这个缺点固然有一定道理，但是这么成天叫人家也不合适呀。<br />西方哲学中形而上学的真正意思是什么呢？<br />亚里士多德是个百科全书式的学者，他写过很多的著作，从哲学到物理学，涉及了很多学科。<br />但是那个时候没有现代学术界「哲学」「物理学」这样的分科。<br />亚里士多德是写痛快了，想研究什么就写什么，可给整理他书籍的后人犯愁了。<br />这么一天堆包罗万象的著作，该怎么分类、命名呢？<br />一个叫安德罗尼柯的人想了一个好办法。<br />他用「研究有形体的事物」和「研究没有形体的事物」，把亚里士多德的著作分成了两大类。<br />前一类著作编在一起，起名叫《物理学》。<br />后一类作品，也就是亚里士多德的哲学作品，也编在一起，放在《物理学》的后面。<br />当时没有合适的名字称呼它们，安德罗尼柯一看怎么办呢，就给起了一个名叫 metaphysics<br />意思是「物理学之后」。<br />安德罗尼柯起这个metaphysics的原本目的，应该是他没有现成的词汇可用，于是就说这部分著作<br />是「编排在《物理学》之后的内容」。<br />但这个词的含义也可以引申成「物理学之后的学问」。<br />也就是说，形而上学研究的是那些高于物理学的、看不见、摸不看的学问。<br />这就是「形而上学」这个词最早的来历。<br />「形而上学」的中文译名也很棒，称得上是中文翻译史上最棒的译名之一。<br />中文典出《易经》：「形而上者谓之道，形而下者谓之器。」<br />《易经》的这句话很精彩，也很好理解。<br />「形」，就是有外形、可以触摸、可以感知的东西。<br />《易经》的这句话，是下了两个定义。<br />第一个定义是说，超过我们感知之外的那些无形的东西，是「道」。<br />「道」，就是「道理」的「道」，指的是「道理」「概念」这些抽象的东西。<br />老子说「大道无形」，就是这个意思。<br />第二个定义是说，我们能感知到的那些有形的东西，是「器」。<br />「器」是「器具」，就是指「东西」「物质」。</p></blockquote><h1 id="文章摘要">文章摘要</h1><ul><li>领导者：压力下的决策能力；<br />军师：无压力状态下的分析能力。</li><li><strong>说服别人关键是利益驱动，例如思考你能给对方带来什么价值，人都是利益驱动的。</strong></li><li>换位思考能力，弄清楚他人的思想和意图。<br /></li><li>社会智能和非社会智能是此消彼长的关系，他们就像跷跷板的两端。 社会智能类似情商<br />抽象推理类似智商<br /></li><li>大脑应对社会痛苦和物理痛苦的反应十分相似。<br /></li><li>我们的大脑天生就容易受人影响，确保我们能获取、接受我们周围的信念和价值观。<br /></li><li>连接主要是我们渴望成为一名社会人的欲望有关，而协调则指一序列神经层面的适应，它们允许群体的信念和价值观影响其中的个体。<br /></li><li>社交天性：以一定的兴趣为基础，每个人都天生与他人相互连接。<br /></li><li><strong>幽默感：运用逆向思维来思考问题。</strong><br /></li><li>罐头笑声：canned laughter 又称背景笑声。<br /></li><li>如果Facebook（10亿人）是一种宗教信仰，那么他就是紧接基督教（21亿人）、伊斯兰教（15亿人）的世界第三大宗教。<br /></li><li><strong>邓巴数（Dunbar's number) 根据某个灵长类物种新大脑皮层比指标估算出该物种的最佳群体规模。<br />对于人类而言，最佳群体规模大概为150，这个数字也是所有灵长类动物中最高的。而人类社会各种“原始”组织的规模一般都在这个数字上下。</strong><br /></li><li>谁写推荐信比写什么内容更重要。<br /></li><li><strong>根据大多数研究，人们最害怕的是在大庭广众之下讲话，相比之下，死亡不过是他们第二畏惧的事情。</strong><br /></li><li>公平是社会连接的一个抽象标志，只是许多人都未能想到这一点，公平非常重要，且大脑的奖赏系统对它十分敏感。<br /></li><li>赞美本身就是意义。<br /></li><li>相互合作激活了人类大脑的奖赏系统，因为合作的本身就是目的。<br /></li><li><strong>进化的动机也许是繁殖，然而我们的心理动机却是快乐。</strong><br /></li><li><strong>利他行为，能激发大脑的奖赏系统。<br />利己分为物质上的利己和精神上的利己。而精神上的利己有可能是物质上的利他。<br />人类兼有自私和无私的动机。</strong><br /></li><li>很多人在一些随机决策的游戏中（例如石头剪刀布），会根据π（3.1415926......）的数字来决定，被3整除出石头，余1出剪刀，余2出布。<br /></li><li><strong>逻辑推理有两种形式，演绎推理和归纳推理。</strong><br /></li><li>很多心理学实验都是以恒河猴作为实验对象。<br />恒河猴主要生活在亚洲，是一种猕猴。它在许多方面与人类十分相似，比如可以在城市生存，能够以花生、冰激凌等多种食物为食，而且喜欢群居生活。<br />基因测序结果表明，人类和恒河猴仍然有93%的遗传基因是相同的。<br />黑猩猩的基因测序，它有98%的遗传基因和人类相同。<br /></li><li>镜像神经元对于心理学的意义，相当于DNA对于生物学的意义。<br /></li><li>辩证法：阴阳、矛盾对立等<br /></li><li>感知、思考、行动<br /></li><li>心智理论：1、逻辑推理；2、想象，推己及人、模拟理论<br /></li><li>学生管理：<br />类似公司的团建，能激发学生的学习热情和动力。<ol type="1"><li>组织班级活动，让某些“差生”担任重要角色，反过来让他们有成就感，融入团队、促进学习热情。<br /></li><li>教别人也是一种最好的学习方法，可以让“差生”讲解一道题（让他提前准备）<br /></li></ol></li><li>共情 feeling into<br /></li><li>哺乳类动物天生就有害怕吵闹与不确定事物的倾向。<br /></li><li>如果说共情是社会心智的顶峰，那自闭症便是社会心智的低谷。<br /></li><li>不要让自己内心深处的声音淹没在别人嘈杂的意见当中，而是要有勇气跟随你的内心和直觉。<br /></li><li>现代科学有关心心智问题的一个基本原理是，心智完全是生物性的，因而是物质实体。然而，大多数人却都固执地相信笛卡尔所描述的那个简单而又令人难以反驳的身心二元论。我们是身体和心灵的结合体，同时它们又是彼此截然不同的两种东西。<br /></li><li><strong>自我控制力强的人不会在思考第一题的答案时，去看第二题的题目----这种做法便是由想早点结束考试的冲动所导致。</strong><br /></li><li>损失厌恶----指在心理学上，我们对“损失”更敏感，因为我们总会设法避免“损失”这种感觉。<br /></li><li>错误共识效应:一种“如果你自己不愿意这么做，那么你会假设大多数人都会拒绝这么做”的倾向。<br /></li><li>一个成功教练的王牌：每个人都会感受到压力的存在，但是成功者永远不会表现出来。 在压力的情况下保持冷静。<br />胸有惊雷而面如平湖者，可拜为大将军。<br />抑制：在心理学上，它通常指的是，一个人很好的控制着自己的面部表情、语调和身体语言，以确保他人无法看出自己的内心世界。<br /></li><li>痛苦是不可避免的，但苦难是可以选择的----村上春树。<br /></li><li><strong>其实年纪大了才能真正看懂阿Q。</strong><br /></li><li>情绪的治疗：<ol type="1"><li>写日记是一种好的情绪发泄方法<br /></li><li>向别人倾诉也是一种好的情绪发泄方法<br /></li></ol></li><li>意志力 willpower，尼采式的概念，说的是通过纯粹个人心灵的力量克服一切障碍的能力。<br /></li><li>有眼睛的海报来增加威慑力。 人们会根据现实环境来调整自己的行为，越有可能被他人观察，他们就越可能遵循规矩。<br />所以说一个人慎独是非常好的一种人格，也是非常难的一种人格。<br /></li><li><strong>曾经帮过你的人，要比受过你恩惠的人更加愿意帮助你。----本杰明·富兰克林</strong><br /></li><li>因为快乐而健康的人更有效率，不容易陷入困境。<br /></li><li>享乐适应：人类有一种倾向，即他们总能适应新的环境（无论是更好的，还是更坏的）<br />生命总能找到出口。<br /></li><li>幸福感是一种心理感受，幸福感很多时候是来自于比较产生。<br /></li><li>良好的人际关系会导致更强的幸福感。<br /></li><li><strong>增加人生中的社会连接很可能是提高我们的幸福感的最有效、最简单的方法。很多人却越来越沉迷于物质主义的价值观。</strong></li></ul><p><img src="/img/社交天性1.jpg" /><br /><img src="/img/社交天性2.jpg" /><br /><img src="/img/社交天性3.jpg" /><br /><img src="/img/社交天性4.jpg" /><br /><img src="/img/社交天性5.jpg" /><br /><img src="/img/社交天性6.jpg" /><br /><img src="/img/社交天性7.jpg" /><br /><img src="/img/社交天性8.jpg" /><br /><img src="/img/社交天性11.jpg" /><br /><img src="/img/社交天性12.jpg" /><br /><img src="/img/社交天性13.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 社交 </tag>
            
            <tag> 神经学 </tag>
            
            <tag> 大脑 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《IT赋能》读书心得</title>
      <link href="/2019/09/15/%E3%80%8AIT%E8%B5%8B%E8%83%BD%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2019/09/15/%E3%80%8AIT%E8%B5%8B%E8%83%BD%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>【<strong>推荐指数（1-5）</strong>】5<br />【<strong>适合人群</strong>】CIO<br />看完本书最大的收获，就是要具备与管理层（非IT人士）的对话能力。为此需要为未来进行思考，为此需要从业务的视角去思考，为此需要能就一些技术趋势、新技术对业务的影响和机会进行思考。并且为了达到这个目的需要平时做好储备，看到一些政策、技术新闻等，需要思考，管理层会怎么问你，你如何回答，所谓有备无患。另外，本书也针对CIO经常需要面临的一些问题进行阐述。是一本转型CIO的不错的书。<br /><span id="more"></span></p><h1 id="第一章-技术展望">第一章 技术展望</h1><p><strong>思考未来！</strong></p><ul><li>对致力于AI技术应用的管理层和团队而言，不可或缺的能力包括嗅探AI技术的应用领域、制定目标的构思能力、勾画支撑工作和业务的系统能力，以及构建系统时的项目集成管理能力。<br /></li><li>管理层确实会关注应用新技术的业务动态，但这并不能说明他们关注技术本身。<br /></li><li>关系到整个公司的重要项目：开发新产品、新服务和新业务。可以说，技术对这些项目的实现将起到关键作用。<br /></li><li>Do the right things; Do things right.<br /></li><li>竞争性IT策略：要考虑对公司而言什么是“正确的事情”，并描绘出相应的信息系统模型，最后制定一个能够开发出该系统的方案。这正是企业管理所要求的能力。<br /></li><li>合理的方案<ol type="1"><li>方案要利于本企业事业的发展，必须符合企业的经营方针和经营计划。当管理者或管理层表达出他们的想法后，CIO必须充分理解这种想法，然后再着手进行企划；<br /></li><li>方案应以社会发展趋势为依据<br /></li><li>方案要“量体裁衣”<br /></li></ol></li><li>若管理者或事业部门没有一个关于系统功能预期的成熟方案，信息部门即便催促他们，也将无功而返。<br /></li><li>有时候，不完善的地方会演变成为一种刺激，从而激发出人们新的设想。<br /></li><li>展望项目前景，即做什么和怎么做。（项目价值、敏捷开发）</li></ul><h2 id="思考三步曲">思考三步曲</h2><ul><li>What 价值 How 方案（如何提供价值） Who 资源（人才及能力）<br /><img src="/img/IT赋能02.jpg" /><br /><img src="/img/IT赋能03.jpg" /></li></ul><h2 id="信息部门的未来">信息部门的未来</h2><ul><li>你怀着怎么样的愿景引领部门发展<br /></li><li>愿景或者说部门中长期的经营方针是怎样的问题<br /></li><li><strong>思考 IT部门的愿景及未来，自己写一下</strong><br /><img src="/img/IT赋能06.jpg" /><br />参考资料：<br />https://www.cnblogs.com/heavyhe/archive/2013/01/07/4547232.html<br />https://zhuanlan.zhihu.com/p/52785817<br />https://blog.csdn.net/pan_tian/article/details/78080902<br />http://www.d1net.com/cio/ciotech/529413.html<br />https://www.traveldaily.cn/article/126822<br />https://ittime.com.cn/news/news_5028.shtml<br /></li><li>核心理念并非是凭空想出来的，而是“经过仔细观察企业内部环境后确定的”<br /><strong>未来前景：显得尤其大胆，能调动团队的积极性。</strong><br /></li><li>2050年，企业的存在形式和业务形式也将发生变化；<br />企业的发展由管理者操持，而其余所有业务都将外包。那些能力出众的人也将在多个企业或团队工作，而不是仅仅供职于一家企业。<br />思考未来10年，IT的形势会怎么样？<br /><img src="/img/IT赋能07.jpg" /><br /></li><li>设计、安装和运行维护将常存<br />在这种情形下，信息部门会如何发展呢？无论技术、业务和企业如何变化，只要企业和企业之间、企业和人之间还存在联系，那么就存在信息传递。因此，如何开展业务、用什么技术传递什么样的信息，这些属于描绘蓝图的“设计师”和“架构师”的工作内容一定不会消失。基于蓝图，开发出信息传递系统，之后的系统运行和维护工作也不可或缺。<br />（设计 运维 运营 大数据）<br />（设计、框架、资源整合）</li></ul><h2 id="不变的是人际关系">不变的是人际关系</h2><ul><li>投资回报率<br />管理者的发问从来都没变过，总而言之，就是下面两个问题：<ol type="1"><li>设立信息系统为什么要耗费这么多资金？<br /></li><li>你说不清楚其作用又是怎么回事？<br /></li></ol></li><li>CIO需要关注的三大问题：及时交付、客户满意度、投资回报率<br />参考资料<br />https://www.chainnews.com/articles/985421842021.htm<br />https://blog.csdn.net/ace_chen0103/article/details/5960984<br />https://blog.51cto.com/johnemoney/32241<br />https://searchcio.techtarget.com.cn/8-15992/<br />https://segmentfault.com/a/1190000004181464</li></ul><h1 id="业务变革">业务变革</h1><ul><li><strong>多数时候，业务未来并非预测技术动向，而是对业务和社会变化趋势进行描述。</strong> （跳出IT的视角）<br />就像淘宝，并非互联网的技术，而是互联网技术引发的商业模式的变更；<br />例如云计算，重点并非关注基础架构的技术，而是在云计算模式上，企业轻资产模式的业务变化、企业业务按需定制的业务模式。<br /></li><li>信息部门若想参与一些关于未来预测和发展前景的讨论，还是应该在平日里多进行一些思考的。<br /></li><li>IT如何驱动业务，从被动到主动！<br /></li><li>试着将预测套用到本企业，<strong>在考虑这些问题的时候，CIO要把自己当成企业的管理层而非IT专家。管理层大致关注以下两点：业务前景和职员的前途。</strong><br /><img src="/img/IT赋能08.jpg" /><br /><img src="/img/IT赋能09.jpg" /></li></ul><h1 id="了解企业经营现状">了解企业经营现状</h1><ul><li><strong>我有能力同管理者对话吗？或者说，我有能力和不熟悉IT的事业部门的人员对话吗？这是IT人员面临最大的问题。</strong><br /></li><li>管理层对IT的期待<ol type="1"><li>提升业务效率/削减开支； 节流<br /></li><li>加强对产品、服务的开发； 开源<br /></li></ol></li><li>在起草经营计划的时候，建议在其中加入有关IT的内容，这需要IT应用部门的协助；<br /></li><li>业务部门单独进行IT投资的案例不断增加，<strong>影子IT，指未经信息部门许可在企业使用的信息系统，的占有率在悄然增加。</strong><br /></li><li>利用自己的专业能力和人际资源，成为个体经营户，同时服务于多家企业。</li></ul><h1 id="关于软件的开发">关于软件的开发</h1><ul><li>利用新技术发展主要业务：技术要思考如何助力业务，而不是陷于技术本身。<br /></li><li>思考未来，思考新技术与业务的关系，IT驱动业务。<br /></li><li>思考软件的弊端：我还是希望各位信息系统主管能够大胆一些，你们只管大胆指出软件的弊端，就算是为了企业。</li></ul><h2 id="软件的定义"><strong>软件的定义</strong></h2><ul><li><strong>业务信息系统由多种软件组成，软件是指数据结构和算法。</strong><br /></li><li>是数据有利于业务发展，并非软件。因为，数据使用人员应提出数据和数据处理的要求。根据数据要求和数据处理要求，由信息部门或IT企业开发软件，由使用人员进行测试和验收，以确定软件是否能按照要求提供数据。<br /></li><li>企业首先要弄清楚业务数据，并整理其结构，然后再着手软件开发。在明确数据结构后，再由技术人员设计能够得到这些数据的算法。<br /></li><li>数据要求包括数据格式和数据结构，使用人员要对其负责。既然是描述业务情况的数据，一般的技术人员并不负有任何责任。但业务人员是不知道如何整理数据结构，因此，技术人员应指导用户如何做，并和她们一起进行数据设计。各部门的使用人员通过一起进行数据整理，就能理解各部门工作间的联系，这样一来自然而然会产生一些关于改变业务模式的想法。<strong>通过利用可见的数据结构描述业务结构，业务的整体结构便会浮现在使用人员的脑海中。</strong><br /><img src="/img/需求定义.png" /><br /><img src="/img/IT赋能3.jpg" /></li></ul><h1 id="协调厉害关系">协调厉害关系</h1><p>干系人管理<br />反对者：</p><ol type="1"><li>不想改变<br /></li><li>想改变但不让步</li></ol><p>处理：</p><ol type="1"><li>沟通<br /></li><li>将反对者拉进项目</li></ol><ul><li>应对反抗者是领导者的职责，但拥有完成该职责的能力且付诸实践并不容易。 努力履行领导的职责，承担风险，并从成功与失败中吸取教训；<br />在职场中经历过笼络和交涉，并从中取得收获的人就会进步。</li></ul><h1 id="信息数据管理">信息（数据）管理</h1><ul><li>公司数据治理方案：（<strong>一定要让管理者看到这些努力</strong>）<br /></li><li><strong>数据能成为企业的资产，而软件却不能。因为是资产，所以需要做管理。</strong><br /><img src="/img/IT赋能5.jpg" /><ol type="1"><li>主数据<br />数据集成对“总体最优”和“集团化经营”必不可少<br />主数据管理：是指通过定义客户、产品、企业（人）、生产环节等基础业务用语，用数据模型对用语进行描述，并通过模型对实际数据进行创建和维护。<strong>主数据管理虽然称为客户或商品主数据管理，但其实际上是一个整合过程。</strong><br /></li><li>数据质量<br />技防：自动对账功能、数据清洗功能<br />人防：数据检查、通报<br /></li><li>数据安全<br />目前的技术具有相当的保护效果，但最后还是要靠人本身；<br />作为应对正当化的措施，需要让“<strong>本企业能锁定一切擅自盗取信息的人”这句话深入人心； （审计系统的功能、也是数据安全的底线）</strong>。<br />任命最可信的人成为管理责任人。<br /></li></ol></li><li>数据也要治理，既然称之为管理，那么承担该工作的中坚部门就不是信息部门，而是使用数据的管理人员以及数据为参照的管理层。由于制定信息安全措施是数据管理的一部分，所以最好由管理层来负责。<br /></li><li>要在把握经营战略对数据的内在需求的基础上制定数据战略。</li></ul><h2 id="数据安全">数据安全</h2><ul><li>信息安全：国家、企业的做法以及采取的措施。<br /></li><li>信息安全属于企业管理的问题，有专门的管理者担任负责人，实际的业务可以交给信息部门。信息安全是全局的管理的问题，不单是技术问题。<br /></li><li>内部舞弊的三种来源：<ol type="1"><li>机会<br />最小权限、分散权限；<br />有制度、对重要业务进行检查；<br />避免人员固化<br /></li><li>动机<br />通过内部的揭发机制，审计来威慑；这不单是信息部门的事情，是整个企业的问题。<br /></li><li>正当化<br />指这个人抱着这种程度的事情别人也会做，我做了也没什么。<br /><img src="/img/IT赋能9.jpg" /><br /><img src="/img/IT赋能10.jpg" /><br /></li></ol></li><li>ERP：企业资源管理系统； 基础业务整合系统，其通过实时监控企业的人、物、资金相关信息，实现企业资源的最合理配置。<br /></li><li><strong>高层去看望一线人员的问题，激励一下一线人员，毕竟，很多企业的管理者从未去过位于郊外的信息系统中心。</strong></li></ul><h1 id="新技术的企划开发及运行">新技术的企划、开发及运行</h1><ul><li>我们企业的业绩还不错，但这只是通货膨胀带来的效果。<br /></li><li>IT部同企业开展新事业和业务等活动紧密联系。<br /></li><li>信息部应该主动提出创新方案，“企业架构”方案便是其中的一种。<br /></li><li>信息系统主管都应对企业和团队的现状有充分认识。<br /></li><li>先了解现状，没有调查就没有发言权。<br /></li><li>思考企业5-10年的发展。<br /></li><li>要考虑引进的技术是否符合企业的实际情况。<br /></li><li>查<strong>artner 发布的最新技术炒作曲线</strong><br /><img src="/img/IT赋能11.jpg" /></li></ul><h2 id="引进新技术的思考四点与用法有关三点与人有关">引进新技术的思考（四点与用法有关，三点与人有关）</h2><ol type="1"><li>是主要技术还是次要技术<br /></li><li>是单项技术还是整个信息系统<br /></li><li>是崭新技术还是成熟技术<br /></li><li>同行业其他企业是否使用<br /></li><li>管理层是否积极<br /></li><li>年轻员工是否有干劲<br /></li><li>自己是否还在任。<br /><img src="/img/IT赋能13.jpg" /><br /><img src="/img/IT赋能14.jpg" /></li></ol><ul><li>将新技术融合到系统里并没有说的那么简单。<br /></li><li>企业如果不根据开放性标准建立平台，就无法达到预期的效果。<br /></li><li>企业管理者经常会把一些词挂在嘴边，如总体最优、综合能力、总体战、协同、横向合作、汇总。虽然每个词语的意思各不相同，但他们都含有这样一层深意：企业并未完全发挥自身实力，其实还可以表现得更出色。说起来容易做起来难。<br /></li><li>和高层吃饭的时候闲聊，但这是同管理者进行沟通交流的宝贵时机，因此也要留心自己的言谈。如果管理者对我正在考虑的事情表示出兴趣，那么在经营会议上，一些大提案就能更加容易过审。</li></ul><h2 id="项目章程">项目章程</h2><p><strong>项目章程的重要性</strong><br /><img src="/img/IT赋能15.jpg" /><br /><img src="/img/IT赋能16.jpg" /></p><ul><li><strong>如果想迅速组建具备信任感和责任感的团队，就要制定章程；</strong><br /></li><li><strong>章程是一份反映权力赋予的文件。通常有三项内容：对所开展业务的范围进行规定、确定成员间的沟通原则、明确成员的职责和责任。</strong><br /></li><li>对信息系统的主管而言，最痛苦的工作是什么？难道不是向企业管理层汇报信息系统研发失败的事情吗？<br />最好在企业进行项目管理时，应在项目经理和管理层间设置一个“项目委员会”。委员会由使用部门的管理人员、项目成员代表、IT企业管理层组成，起一种“指导”作用。值得注意的是，委员会成员并不包括项目经理。项目经理日常给委员会汇报日常的工作。</li></ul><h2 id="项目工期管理">项目工期管理</h2><p>首先，<strong>项目经理根据传统的方法估算每项工作的执行时间，将执行时间砍掉一半。将砍掉的时间称为“缓冲时间”，并对另外进行管理。</strong><br />其次，在同一时间内给每个人仅分配一项工作。<br /><strong>缩短执行时间的目的是为了带来一种积极的紧张感，即让人意识不到最后期限。如果脑海中有最后期限的概念，就会觉得“反正时间还多”，从而无法专注于工作。</strong></p><h2 id="系统整合">系统整合</h2><p><strong>系统整合是最难实现的。</strong></p><ol type="1"><li><strong>信息系统反应的是企业管理模式，因此，其应配合业务实际业务进行系统架构；</strong><br /></li><li><strong>系统系统中存储的数据，不同部门之前的数据内容、存储方式也有很大出入；</strong></li></ol><ul><li>方法：梳理差异，清洗相关的数据。<br /></li><li>技术的个人垄断！贯彻实行系统介绍文件的书面化。备用人员的培养。<br /></li><li>我们可看到系统的小故障确实在增加，但是经过一线人员的努力，这些小故障并未发展成大故障。其实，重要的是让管理者知道信息部门为了维持系统每天的正常运转，付出了多少努力和辛劳。<br /></li><li>系统出故障时难免的。但是，需要让管理者知道自己的努力和付出。<br />再怎么说，软件是人参与的工作，总会有出错的时候。<br /></li><li>与其为了制造出永不出故障的系统而投入大量资金 ，还不如做好一序列解决故障的准备，这样一来，即便系统出故障，也不会阻碍业务进行。<strong>这就是所谓的BCP（业务连续性计划、应急方案）。</strong></li></ul><h1 id="态度和能力">态度和能力</h1><p>接任者：能力、态度<br /><img src="/img/IT赋能17.jpg" /></p><ol type="1"><li>自己肩负的职责，CIO这项职务的意义<br />同时完成新项目以及支撑起运作的信息系统的开发；以建设能够架构这种信息系统的部门为目标，甄选合适的部门人员，估计达成目标需要的经费。<br /></li><li>时刻关注一些新技术的动向</li></ol><h2 id="能力要求">能力要求</h2><p>对未来前景的设想与直面现实时处理繁琐的信息系统业务之间的矛盾；以提高信息部门成员的干劲为目标，对信息系统进行管理、监视；不断设计可以满足企业业务需求的新系统，在严守交货期与成本控制的前提下进行开发；在多次重复使用数据的同时，保证数据安全。</p><h2 id="概念技能">概念技能</h2><p>抽象与具体、大局与分析、扩散与收敛、主观与客观、主观与逻辑、长期与短期的视角反复思考然后行动的能力。<br /><img src="/img/IT赋能18.jpg" /></p><h2 id="管理者三大技能">管理者三大技能</h2><p><img src="/img/IT赋能19.jpg" /></p><ul><li>管理人员不仅要着眼于本部门，还应在把控全局的基础上采取行动。<br /></li><li><strong>利用数据模型从整体上把握业务，这就是在进行一种培养员工整体洞察力的训练。</strong><br /></li><li>如果信息主管能用一张图来描绘下期系统的目标和设计思想、一些大体的系统需求，就能让参与该项目的人员对重建计划和方针形成一个透彻的了解。</li></ul><p><img src="/img/IT赋能1.jpg" /><br /><img src="/img/IT赋能04.jpg" /><br /><img src="/img/IT赋能05.jpg" /><br /><img src="/img/IT赋能2.jpg" /><br /><img src="/img/IT赋能4.jpg" /><br /><img src="/img/IT赋能6.jpg" /><br /><img src="/img/IT赋能7.jpg" /><br /><img src="/img/IT赋能8.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> IT </tag>
            
            <tag> CIO </tag>
            
            <tag> 赋能 </tag>
            
            <tag> 未来 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《以客户为中心：华为公司业务管理纲要》读书心得</title>
      <link href="/2019/09/08/%E3%80%8A%E4%BB%A5%E5%AE%A2%E6%88%B7%E4%B8%BA%E4%B8%AD%E5%BF%83%EF%BC%9A%E5%8D%8E%E4%B8%BA%E5%85%AC%E5%8F%B8%E4%B8%9A%E5%8A%A1%E7%AE%A1%E7%90%86%E7%BA%B2%E8%A6%81%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2019/09/08/%E3%80%8A%E4%BB%A5%E5%AE%A2%E6%88%B7%E4%B8%BA%E4%B8%AD%E5%BF%83%EF%BC%9A%E5%8D%8E%E4%B8%BA%E5%85%AC%E5%8F%B8%E4%B8%9A%E5%8A%A1%E7%AE%A1%E7%90%86%E7%BA%B2%E8%A6%81%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<h1 id="第一篇-以客户为中心">第一篇 以客户为中心</h1><h2 id="为客户服务是华为存在的唯一理由">为客户服务是华为存在的唯一理由</h2><ul><li>未来的竞争是管理的竞争<br /></li><li>问题不可怕，关键是我们面对问题的态度，我们必须要有正确面对问题的态度，必须找到解决问题的正确方法；<br /></li><li>从泥坑中爬出来的人就是圣人。<br /></li><li><strong>市场部在全世界刨那么多坑是好事，我们得赶紧去种树。</strong><br /></li><li><strong>华为公司的特征就是服务文化；下游就是上游的客户；以客户为中心就是帮助客户成功；</strong><br /></li><li>通过序列流程化的组织架构和表格化的操作规范来保证满足客户需求。<br /><span id="more"></span></li></ul><h2 id="华为的价值主张">华为的价值主张</h2><ul><li>所谓商业模式，实际就是实现价值主张并获得相应经济回报的方式<br /></li><li>以客户为中心、以奋斗者为本、长期艰苦奋斗，是公司的核心价值观。坚持自我批判，是自我纠偏的机制。<br /></li><li>千古兴亡多少事，不尽长江滚滚来；<br /></li><li>而创新也是奋斗，是思想上的艰苦奋斗<br /></li><li><strong>什么叫奋斗？为客户创造价值的任何微小活动，以及在劳动的准备过程中，为充实提高自己而做的努力，均叫奋斗，否则，再苦再累也不叫奋斗。</strong><br /></li><li>胜则举杯相庆，败则拼死相救；<br /></li><li><strong>世界上对男人最佳的表达是什么？电影《泰坦尼克号》告诉我们，在生死存亡的时候让女人先走，自己死掉，这就是对男人的最佳表达；</strong><br /></li><li>运营商BG“三朵云”（体验云、知识云和客户方案云）<br /></li><li>一定要在战争中学会战争，一定要在游泳中学会游泳</li></ul><h2 id="质量是华为的生命">质量是华为的生命</h2><ul><li>服务就像黄继光在前面堵枪眼<br /></li><li>品牌就是承诺，品牌的根本是诚信；</li></ul><h2 id="深淘滩低作堰">深淘滩，低作堰</h2><h2 id="客户满意度是衡量一切工作的准绳">客户满意度是衡量一切工作的准绳</h2><ul><li>成就客户就是成就自己<br /></li><li>客户的利益就是我们的利益</li></ul><h1 id="增长">增长</h1><ul><li>持续有效增长，当期看财务指标，中期看财务指标背后的能力提升，长期看格局以及商业生态环境的健康，产业的的可持续发展等。</li></ul><h2 id="追求长期有效增长">追求长期有效增长</h2><ul><li>企业的各种难题，只有在发展中才能得到解决。<br /></li><li>短期导向，会导致杀鸡取卵的取向；<br /></li><li><strong>增长和利润的取舍，在一定的利润基础上，优先考虑增长。</strong><br /></li><li>马太效应，强者恒强；<br /></li><li>没有战略思维是不行的，所以要盯到我们的增长上，盯到我们创造的总效益上，然后再考核人均效益； （人效）</li></ul><h2 id="产品发展的路标是客户需求导向">产品发展的路标是客户需求导向</h2><ul><li><strong>从技术导向转变为客户需求导向</strong><br /></li><li>客户骂你的时候就是客户痛得最厉害的地方，客户的困难就是需求；<br /></li><li><strong>市场营销定位为“两只耳朵，一双眼睛”，一只耳朵倾听客户需求，一只耳朵听行业、技术发展趋势；一双眼睛紧盯竞争对手；</strong><br /></li><li>要向客户提供针对性的综合解决方案，而不是单个产品。</li></ul><h2 id="创新是华为发展的不竭动力">创新是华为发展的不竭动力</h2><ul><li><strong>创新虽然有风险，但不创新才是最大的风险；</strong><br /></li><li>什么事解决方案？解决方案不是以技术为中心，是以需求为中心，这是前端的；后端以技术为中心，是储备性的。<br /></li><li><strong>解决方案应该面向客户的需求和应用场景；</strong><br /></li><li><strong>领先半步是先进，领先三步成先烈；</strong><br /></li><li>心胸有多宽，天下就有多大；<br /></li><li>站在巨人肩膀上前进；<br /></li><li>歪瓜裂枣：枣是裂的最甜，瓜是歪的最甜</li></ul><h2 id="更多地强调机会对公司发展的驱动">更多地强调机会对公司发展的驱动</h2><ul><li>各个大公司的研发经费都在销售额的10%左右。<br /></li><li><strong>对于有志者来说，永远都有机会。任何“时间晚了”的悲叹，都是无为者的自我解嘲。</strong><br /></li><li>目前最重要的不是成本高低的问题，而是能否抓住战略机会的问题。抓住了战略机会，花多少钱都是胜利；抓不住战略机会，不花钱也是死亡。<br /></li><li><strong>务实与务虚相结合。我们企业强调务虚，什么叫务虚呢？就是要利用机会强制地牵引公司前进，寻找企业发展的方向，比如说产品的定位方向、人才管理的方向等各种方向，形成这种方向来牵引公司前进；</strong><br /></li><li>在模糊的情况下必须多线作战，当市场明晰时立即将投资重心转到主线上去。</li></ul><h2 id="聚焦主航道坚持压强原则">聚焦主航道，坚持“压强原则”</h2><ul><li>战略竞争力量不要消耗在非战略机会点上；<br /></li><li>什么叫主航道？世界上每个东西都有正态分布，我们只做正态分布中间的那一段。</li></ul><h2 id="开放竞争合作构建良好的商业生态环境">开放、竞争、合作，构建良好的商业生态环境</h2><ul><li>将来的竞争已经从企业之间的竞争演变为产业链之间的竞争；<br /></li><li><strong>商业生态链的建设，本质上是一个在供应商、合作者和客户之间如何分配利益的问题。</strong><br /></li><li>华为有什么事，捅捅也好，小不震则大震。早些知道什么错了，总比病重了好。</li></ul><h2 id="业务管理的指导原则">业务管理的指导原则</h2><ul><li><strong>工作就是要找准方向，主要矛盾就是工作方向，而方向就是前进的目标，所以能否抓住主要矛盾，关键在于是否有明确的战略目标。</strong><br /></li><li><strong>乱中求治与治中求乱，抓主要矛盾和矛盾的主要方面。</strong><br /></li><li>坚定不移的战略方向，灵活机动的战略战术；<br /></li><li>坚持自己的价值观<br /></li><li>谋定而后动<br /></li><li>纲举目张<br /></li><li>不断激活组织，始终保持它的活力，不使它退化和沉淀；<br /></li><li>坚持以业务为主导，会计做监督的宏观管理方法和体系建设；</li></ul><h1 id="第三篇-效率">第三篇 效率</h1><h2 id="未来的竞争是管理的竞争">未来的竞争是管理的竞争</h2><ul><li>什么东西都是可以买来的，唯有管理是买不来的；<br /></li><li><strong>各级领导干部要从以前的埋头拉车转变到抬头拉车，不要只见树木，不见森林，更多顾及各级管理体系的建设；</strong><br /></li><li><strong>企业管理目标就是流程化的组织建设；</strong><br /></li><li>向管理要效益<br /></li><li>价值-&gt;目标-&gt;策略-&gt;关键成果-&gt;执行计划<br /></li><li>过低的人均效益要求必然带来组织的过度膨胀和机构臃肿；<br /></li><li><strong>提高效率，不是增加劳动强度，而是减少无效工作。减少重复劳动；</strong></li></ul><h2 id="企业管理的目标是流程化组织建设">企业管理的目标是流程化组织建设</h2><ul><li>华为组织改革的方向是有功能型的组织结构，转化为流程型的组织结构，并由IT支持这个组织的运作。基于流程来分配权力、资源和责任的组织，就是流程化组织；<br /></li><li>让听得见炮声的人来决策；<br /></li><li>让听得见炮声的人来呼唤炮火；<br /></li><li>以销售额为中心，是BG（业务集群）的责任；以利润为中心，是区域的责任；<br /></li><li><strong>公司未来的建制，前端是对付不确定的精兵组织，后端是对付确定性的平台和共享组织。</strong><br /></li><li><strong>总部从管控中心向支持、服务、监控中心转变；</strong><br /></li><li><strong>后方部门对于前方部门可以要求功能对齐，但是不要求组织对齐；</strong><br /></li><li>四算：概算、预算、核算、决算<br /></li><li>我们的改革要从以功能为中心转向以项目为中心；<br /></li><li>管理要标准化、简单化；不能过度监控。<br /></li><li>现在公司有些运作环节的信息流还跑不过物流；<br /></li><li>瓶颈管理<br /></li><li><strong>高薪不能养廉，要靠制度养廉；</strong></li></ul><h2 id="从客户中来到客户中去以最简单最有效的方式实现流程贯通">从客户中来，到客户中去，以最简单、最有效的方式实现流程贯通</h2><ul><li><strong>流程是一个团队做事的基本规则。成功的经验总结出来就是流程。什么事流程化建设？就是标准化、程序化、模板化。清晰的流程、重复运行的流程和工作一定要模板化。要把可以规范化的管理都变成扳铁路道岔，使岗位操作标准化、制度化；</strong><br /></li><li><strong>流程要先解决“通”的问题，先打通，再优化。流程通最根本的是数据要通。</strong><br /></li><li>管理就是例外管理，但是必须要把例外管理转成理性管理，不断减少例外管理的数量；<br /></li><li>规范化管理让大家不舒服，但是还是要走规范化管理的道路。<br /></li><li>产品越来越时装化；变化很快<br /></li><li>要一次把事情做好<br /></li><li><strong>抬头看目标、低头思责任</strong><br /></li><li><strong>要实现从刚性预算到弹性预算的转变。业务上去了，预算也相应弹上去，同时要提升机关的服务效率，在标准作业时间内完成审批，保证前线业务增长后就自然、合理、迅速地获得更多资源。</strong></li></ul><h2 id="打造数字化全连接企业">打造数字化全连接企业</h2><ul><li>能买来的系统就买，不要自己做。<br /></li><li><strong>数据是公司的核心资产</strong>，流程通最根本是数据要通；<br /></li><li>要求经营性财务数据要保存15年；<br /></li><li>信息安全关系到公司的生死存亡；<br />但也不要防卫过度，例如不需要每个人都要配备一部防弹车；<br /></li><li>信息安全是公司重大的系统工程，要有架构性思考，从整体看如何构建未来安全环境；<br /></li><li>安全防护系统要敢于投入，采用业界最优秀的产品和技术。</li></ul><h2 id="管理变革的方针">管理变革的方针</h2><ul><li>成功不是走向未来的可靠向导，危机意识才是。<br /></li><li>引进世界领先企业的先进管理体系，要“先僵化，后优化，再固化”<br /></li><li>中国人的劣根性之一就是永远不愿规范；<br /></li><li>要求改良而不是大的改革；<br /></li><li>存在就是相对合理，不要盲目修改，思考清楚后再动。<br /></li><li>英国的光荣革命，实现了君主立宪，比法国的大革命更有合理性；<br /></li><li>价值的产生不完全在于成本降低，提高竞争力和盈利能力才是最主要的目标。<br /></li><li><strong>华为公司最大的浪费是经验的浪费。不断地总结经验，有所发现，有所创造，有所前进。</strong><br /></li><li>善于总结<br />案例有两种，一种是故事化的案例；一种是表格化的案例，可以帮助学员更好地掌握科学的方法，直接用在实际的工作里。<br /></li><li><strong>每个人每个季度都要写案例，不写案例就写心得，多个心得叠加起来，就能写出案例来了。项目做完不输出案例就等于浪费。</strong></li></ul><h1 id="文史数理">文史数理</h1><ul><li><p><strong>阿房宫赋</strong><br />呜呼！灭六国者六国也，非秦也；族秦者秦也，非天下也。嗟夫！使六国各爱其人，则足以拒秦；使秦复爱六国之人，则递三世可至万世而为君，谁得而族灭也？秦人不暇自哀，而后人哀之；后人哀之而不鉴之，亦使后人而复哀后人也。<br /></p></li><li><p><strong>什么是红蓝对抗</strong><br />在军事领域，演习是专指军队进行大规模的实兵演习，演习中通常分为红军、蓝军，演习多以红军守、蓝军进攻为主。类似于军事领域的红蓝军对抗，网络安全中，红蓝军对抗则是一方扮演黑客（蓝军），一方扮演防御者（红军）。在国外的话，进行渗透攻击的团队经常称做红队，在国内称为蓝队实际上应该是比较准确的叫法。<br /></p></li><li><p><strong>六尺巷</strong><br />六尺巷传说是安徽桐城的地方民间传说故事。这是一则发生在清代康熙年间，桐城境内的一桩脍炙人口的民间故事。大学士张英的府邸与吴姓相邻。吴姓盖房欲占张家隙地，双方发生纠纷，告到县衙。因两家都是高官望族，县官欲偏袒相府，但又难以定夺，连称凭相爷作主。相府家人遂驰书京都，张英阅罢，立即批诗寄回，诗曰：<br />千里修书只为墙，让他三尺又何妨。<br />万里长城今犹在，不见当年秦始皇。<br />家人得诗，旋即拆让三尺，吴姓深为感动，也连让出三尺。于是，便形成了一条六尺宽的巷道。<br />张英（1637—1708年），字敦复，又字梦敦，号乐圃，又号倦圃翁，安徽桐城县人。清朝大臣，名相张廷玉之父。<br /></p></li><li><p><strong>什么是蓝血十杰</strong><br />　　蓝血十杰是二次大战结束后，来自美国战时陆军航空队“统计管制处”的十位精英，被刚刚从老亨利·福特手中接过福特汽车公司控制权的亨利二世招致麾下，进入公司计划、财务、事业部、质量等关键业务和管理控制部门。<br />　　　　从此，他们掀起了一场以数据分析、市场导向，以及强调效率和管理控制为特征的管理变革，使得福特公司摆脱了老福特经验管理的禁锢，从低迷不振中重整旗鼓，扭亏为盈，再现当年的辉煌。这十位精英所抱持的对数字和事实的始终不渝的信仰，以及对效率和控制的崇拜，使之获得了“蓝血十杰”的称号，人们将他们尊称为美国现代企业管理的奠基者。<br /></p></li><li><p><strong>酬乐天杨州初逢席上见赠</strong><br />刘禹锡</p><pre><code>         巴山楚水凄凉地， 二十三年弃置身。           怀旧空吟闻笛赋， 到乡翻似烂柯人。           沉舟侧畔千帆过， 病树前头万木春。           今日听君歌一曲， 暂凭杯酒长精神。  </code></pre><p>“<strong>沉舟侧畔千帆过，病树前头万木春</strong>”！是啊，几只船虽然沉没了，但在它的旁边有无数只船顺风满帆，驶向远方。几棵树生病了或腐朽了，越过它们，却是万木争春，无限生机。这一名句寓意深刻，饱和哲理，被后人广泛传诵和引用。不论是漫漫人生路，还是人与人、国与国之间的纷纭关系，不可能一帆风顺，总会遇到各种各样的羁绊和坎坷。但是，一切的困难、挫折、不如意、不愉快都是暂时的、局部的，终将化为尘埃，烟消云散。继之而来的将是阳光灿烂，云白天蓝，绿水青山，百花争妍。满载的航船将顺风快帆，驶向理想的彼岸。</p></li></ul><p><img src="/img/华为1.jpg" /><br /><img src="/img/华为2.jpg" /><br /><img src="/img/华为3.jpg" /><br /><img src="/img/华为4.jpg" /><br /><img src="/img/华为5.jpg" /><br /><img src="/img/华为6.jpg" /><br /><img src="/img/华为7.jpg" /><br /><img src="/img/华为8.jpg" /><br /><img src="/img/华为9.jpg" /><br /><img src="/img/华为10.jpg" /><br /><img src="/img/华为11.jpg" /><br /><img src="/img/华为12.jpg" /><br /><img src="/img/华为13.jpg" /><br /><img src="/img/华为14.jpg" /><br /><img src="/img/华为15.jpg" /><br /><img src="/img/华为16.jpg" /><br /><img src="/img/华为17.jpg" /><br /><img src="/img/华为18.jpg" /><br /><img src="/img/华为19.jpg" /><br /><img src="/img/华为20.jpg" /><br /><img src="/img/华为21.jpg" /><br /><img src="/img/华为22.jpg" /><br /><img src="/img/华为23.jpg" /><br /><img src="/img/华为24.jpg" /><br /><img src="/img/华为25.jpg" /><br /><img src="/img/华为26.jpg" /><br /><img src="/img/华为27.jpg" /><br /><img src="/img/华为28.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 管理 </tag>
            
            <tag> 华为 </tag>
            
            <tag> 客户 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《OKR源于英特尔和谷歌的目标管理利器》读书心得</title>
      <link href="/2019/09/01/%E3%80%8AOKR%E6%BA%90%E4%BA%8E%E8%8B%B1%E7%89%B9%E5%B0%94%E5%92%8C%E8%B0%B7%E6%AD%8C%E7%9A%84%E7%9B%AE%E6%A0%87%E7%AE%A1%E7%90%86%E5%88%A9%E5%99%A8%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2019/09/01/%E3%80%8AOKR%E6%BA%90%E4%BA%8E%E8%8B%B1%E7%89%B9%E5%B0%94%E5%92%8C%E8%B0%B7%E6%AD%8C%E7%9A%84%E7%9B%AE%E6%A0%87%E7%AE%A1%E7%90%86%E5%88%A9%E5%99%A8%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<h1 id="目标管理框架">目标管理框架</h1><p>目标管理：management by objectives MBO<br />OKR：Objective and Key Result<br />OKR是一套严密的思维框架和持续的纪律要求，旨在确保员工紧密协作，把精力聚焦在能促进组织成长的、可衡量的贡献上。<br />目标：Objective 我们想做什么<br />关键结果：Key Result 我们如何知道是否达成了目标<br />oKR原是英特尔公司用以解决目标聚焦与执行效率的工具；<br /><strong>OKR不是，也不应被当成是一张待完成的任务清单。OKR的主要目的是用于识别最关键的业务目标，并通过量化的关键结果去衡量目标达成情况。</strong><br />勇于挑战的精神，OKR上升到了内在动机的层面，寄望的是内驱力驱动。<br />趋利避害，KPI导致压低目标，不敢挑战<br />目标既应关注短期，也应关注长期。因此，目标既应包含有形的经营目标，也应包含像组织发展、员工绩效、劳动态度以及社会责任等无形的目标。<br />变革包括管理变更和组织变革。<br />变化才是永恒不变的真理；敏捷开发就是为了应对需求的变化而诞生的。拥抱变化。</p><span id="more"></span><h1 id="okr评分">OKR评分</h1><p>在开始设置KR时，将成功/失败的概率设成一半一半。请记住，你的1.0指标应当是很有野心的，所以50%的成功几率是合适的。<br /><img src="/img/OKR10.jpg" /></p><h1 id="游戏化的机制">游戏化的机制</h1><p><img src="/img/OKR14.jpg" /></p><h1 id="战略">战略</h1><p>目标市场<br />核心产品<br />价值主张<br /><img src="/img/OKR6.jpg" /></p><h1 id="文章摘要">文章摘要</h1><ul><li>社交属性：具有评论、讨论、at某人的功能!<br /></li><li>项目的定义：有开始的时间、有明确的范围、有结束的时间<br /></li><li>评价<br />自评：虚幻的优越感 我们所有的人都认为自己在平均线以上；<br />评价他人：类己效应 评估中61%的成分是评估者自我的映射，而非被评估人的客观反映。<br /></li><li>优化组织架构和资源配置的方式，提升运作效率和执行力。<br /></li><li>双创：创新、创业<br /></li><li>聪明的人（智慧的人）就是洞察人性的人。<br />利益分析法，人都是趋利避害的，都是利己主义。<br />管理的复杂度就来自于人性的复杂度。<br /></li><li>CEO关注的事项：战略执行力、创新、地缘政治稳定性、总收入增长；<br /></li><li>如果过分强调绩效，对失误零容忍并不惜一切代价地去避免犯错，就会导致大家热衷于掩盖错误和过失，并相互指责，这样会让企业很快在竞争中落伍。<br />要适度冒险，摒弃害怕犯错的文化。<br /></li><li>组织的设计，类似球队的阵型。<br />组织要设计合理、功能定位清晰、权责划分明确、业务与组织方向一致。<br /></li><li>敬业度的定义是，员工对组织及其目标的一种情感承诺；满怀激情地为组织目标而努力。<br /></li><li>有机会真正从事有意义的工作有助于增强敬业度。<br /></li><li>思维方式：固定型思维模式、成长型思维模式：成功是由于努力工作、坚强意志以及坚定的决心所带来的。成长型思维的人，拥抱失败，把这当成是一个简单的过程经历，以及一次学习和成长的机会。成长型思维模式，容忍失败，拥抱快速试错和快速学习的做法。<br /></li><li><strong>说服老板的技巧：把OKR同高管关注的某件事情联系起来；</strong><br /></li><li>大家的共识，要做好思想工作。<br /></li><li>OKR应当源于战略，同时能驱动愿景的达成，并同整体的使命保持一致，这是OKR能否成功的关键保障。<br /></li><li>组织通常都会抵制变革。<br /></li><li>人们会基于自己的经验去理解接收到的信息。<br /></li><li>念念不忘，必有回响； 过度沟通是有用的，特别是对一些重要的事情。<br /></li><li><strong>组织中最大的问题是部门墙。跨部门沟通时，要明确你的目的，以及考虑对方的关注点以及利益。</strong><br /></li><li><strong>所有的事情是思考背后的本质，背后的目的；放大大的语境上去思考。关注价值点。</strong><br /></li><li>鼓励讲出坏消息，但是也要做到，你汇报前如何努力过了，汇报的时候，你有什么好的建议和方案。<br /></li><li>参与者的心里安全感知是团队成功的重要推动力。<br /></li><li><strong>争论的时候，学会把问题抛给对方，找到对方的弱点； 可以说说目的，说说过去做了什么；然后问问对方为这些问题做了哪些推进的工作？</strong><br /></li><li>使命 愿景 战略<br />使命：创造美好生活<br />愿景：成为城市综合服务商<br />战略：为城市居民提供优质住宅，为小康生活提供生活场景；<br /></li><li>愿景：2025愿景，5年后，领先水平，实现自动化运维，达到6个9 的可用率，无感知的客户满意度；<br /></li><li>如何制定目标：当考虑制定目标时，请问你自己一个问题：是什么阻碍了你的战略执行？在制定目标时认真分析那些阻碍你成功执行的问题，将会使一个很好的开始。</li></ul><p><img src="/img/OKR1.jpg" /><br /><img src="/img/OKR2.jpg" /><br /><img src="/img/OKR3.jpg" /><br /><img src="/img/OKR4.jpg" /><br /><img src="/img/OKR5.jpg" /><br /><img src="/img/OKR7.jpg" /><br /><img src="/img/OKR8.jpg" /><br /><img src="/img/OKR9.jpg" /><br /><img src="/img/OKR11.jpg" /><br /><img src="/img/OKR12.jpg" /><br /><img src="/img/OKR13.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> OKR </tag>
            
            <tag> 目标管理 </tag>
            
            <tag> KPI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《高效15法则》读书心得</title>
      <link href="/2019/08/30/%E3%80%8A%E9%AB%98%E6%95%8815%E6%B3%95%E5%88%99%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2019/08/30/%E3%80%8A%E9%AB%98%E6%95%8815%E6%B3%95%E5%88%99%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<h1 id="最大的收获">最大的收获</h1><h2 id="日程表">1、日程表</h2><p>以前自己习惯用任务清单，而不是日程表<br />我发现成功人士并不会按照轻重缓急列一个待办事项清单。事实上，这些很成功的人士完全不会去想着时间这件事。相反，他们会想的是价值、优先权和坚持的习惯。<br />基于日程表而不是任务清单；任务清单很容易让你去做“紧急”的事，而不是最重要的事。不重要的事情放到后面，完不成也没关系，因为不是最重要的，可以拖到后面去。</p><h2 id="要事优先">2、要事优先</h2><p>目标&lt;-最重要的事&lt;-最高效时段<br />你要找到自己的目标，并为此坚持。<br /><span id="more"></span></p><h1 id="如何说不">如何说不</h1><p>你就像是现代社会的“巴浦洛夫的狗”，手机一震，就赶快回应别人的召唤。<br />关键就是说不<br />一般成功的人士和非常成功的人士之间的区别就在于，非常成功人士几乎对所有事情都说不。 巴菲特<br />（其实理解是非常成功人士的邀约太多！）<br />经常对占用他们时间的要求说不的人，幸福感更高，更有活力。<br />允许自己拒绝，不要内疚。不要在意拒绝别人的要求时他们怎么想。 不过也要考虑拒绝的技巧，可以艺术点。<br />如果对某件事情的回答不是“当然是”，那么它的回答就是“不”！</p><h1 id="如何分析自己的任务">如何分析自己的任务：</h1><h2 id="放弃">1、放弃</h2><p>我可以放下什么工作？什么事情我可以完全不做？</p><h2 id="分派">2、分派</h2><p>哪些工作我可以分派给下属？哪些工作可以外包出去？</p><h2 id="重新设计">3、重新设计</h2><p>哪些工作需要我换种节省时间的方式继续做？</p><p>参照以下步骤：<br />1、问自己：<br />这个任务对我或者对公司来说有多重要？如果我完全不做会怎样？<br />价值判断，该不该做<br />2、问自己<br />我是做这项工作的唯一人选吗？公司内外有哪些人可以完成这项工作？<br />谁能做，是否需要自己做<br />3、问自己<br />这个结果怎样才能更快实现？如果我只有一半时间，这项工作怎么完成？<br />怎么做最好。</p><p>别人做得好的事，我是不会做的；不是对我时间利用高效的事，我也不会做。</p><p>底线是你要尽量把所有的事情都外包出去，除非：<br />1、你喜欢做，这是你休息、恢复精力的过程；<br />2、继续完成这件事，是受你的价值观驱使。<br />3、外包出去比你自己做这件事所花的时间更多。</p><h1 id="文章摘要">文章摘要</h1><ul><li>每天只有1440分钟<br /></li><li>事实上，要想提升效率、管理时间，最重要的不是某个技巧，而是需要转变心态。<br /></li><li>先做富有创造力的工作，重复性的工作往后排。<br /></li><li>乔治·布什很重视每周读两本书这件事情，因为这是一种减压的方式，也能使人变得更睿智，或者说只是单纯为了娱乐。他知道学习和再充电是很有价值的任务，他不会让一些所谓的“紧急”的事影响读书这一计划。<br /></li><li><strong>如果你是第一次见一个人，画一张会议桌的表格，写下他们的位置，通过这种方式来帮助你记住他们。记下会议内容并不意味着你要把会上说的每个字都分毫不差地记下来。只要试着记下关键目标、行动、下一步计划，最后进行总结。</strong><br /></li><li>最坏的习惯是，每天第一件事就是检查邮件。这意味着他们的关注点和精力都要被他人支配，而不是做对于他们自己影响最大、最有价值的事情。<br /></li><li>邮件的习惯：<br />FYI：【主题】<br />NRN：【主题】 No Reply Needed 不需要回复邮件<br />【主题】 EOM End of Message 表示收件人不必打开了，因为所有信息都在主题栏了。<br /></li><li><strong>在辅导女儿们准备高中考试时，我现在知道了要先看每章的总结和章节自测。了解教材编撰者认为的重点，然后回课本中寻找答案，比把整篇文章从头读到尾效率更高。</strong><br />可以作为预习的方法。<br /></li><li>寻找捷径<br />把最重要的事情做到极致，其他的差不多就可以了，或者直接就不做。<br /></li><li>积极把工作分派下去的人工作效率和幸福指数更高，也更有激情，他们很少会感觉到“工作过度、不堪重负”<br /></li><li>凡事只做一次的心态；<br /></li><li>每当我有一项小任务需要完成（所需时间少于5分钟）我会立刻完成，而不会把它往后拖。这就确保了每天结束的时候，不会还有一堆工作等着我。<br /></li><li>感恩的态度，增加并扩展了我的幸福感。</li></ul><p><img src="/img/高效15法则1.jpg" /><br /><img src="/img/高效15法则2.jpg" /><br /><img src="/img/高效15法则3.jpg" /><br /><img src="/img/高效15法则4.jpg" /><br /><img src="/img/高效15法则5.jpg" /><br /><img src="/img/高效15法则6.jpg" /><br /><img src="/img/高效15法则7.jpg" /><br /><img src="/img/高效15法则8.jpg" /><br /><img src="/img/高效15法则9.jpg" /><br /><img src="/img/高效15法则10.jpg" /><br /><img src="/img/高效15法则11.jpg" /><br /><img src="/img/高效15法则12.jpg" /><br /><img src="/img/高效15法则13.jpg" /><br /><img src="/img/高效15法则14.jpg" /><br /><img src="/img/高效15法则15.jpg" /><br /><img src="/img/高效15法则16.jpg" /><br /><img src="/img/高效15法则17.jpg" /><br /><img src="/img/高效15法则18.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 高效法则 </tag>
            
            <tag> 15 </tag>
            
            <tag> 时间管理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《让松鼠聚焦》读书心得</title>
      <link href="/2019/08/29/%E3%80%8A%E8%AE%A9%E6%9D%BE%E9%BC%A0%E8%81%9A%E7%84%A6%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2019/08/29/%E3%80%8A%E8%AE%A9%E6%9D%BE%E9%BC%A0%E8%81%9A%E7%84%A6%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>一本很薄的书，姑且称之为一本书吧。<br />本书从“<strong>松鼠对正常事物的关注事件是1秒钟，而对橡果的关注时间长达4分钟</strong>”引出如果说服听众的技巧。其实还是老生常谈，需要找出听众的兴趣点，找到听众的需求，然后加以一些技巧。<br />概括起来就是下图：<br /><img src="/img/让松鼠聚焦4.jpg" /></p><span id="more"></span><h1 id="听众">听众</h1><p>观众想听什么？ 这就是观众的橡果，我们需要找出观众的橡果。<br /><strong>我们想要的结果转换成为对方的想法，找出其中的关联点；从对方的想法引出；这样对方更容易接受。</strong><br />例如：苹果要申请三个账号，可以说是因为领导提出把几个应用分开部署。</p><p>利益分析法；从别人的利益出发；对客户来说，最重要的是“他们能得到什么好处。”<br />戴尔·卡耐基说过：“我喜欢吃布朗尼巧克力，但钓鱼需要用虫子。”<br />所以先要了解你的听众。 站在听众、客户、对方的立场去思考问题。</p><p>观众：听到、理解、感受这句话的含义。</p><h1 id="可信性">可信性</h1><p>在职场中，能力通常被视为确定信誉的方式。<br />如果你没有直接的经验或者知识，你还可以通过使用已经被证明可信的资料来源获得一些为人所信任的信誉。（例如引用第三方的报告等）</p><h1 id="信息的顺序">信息的顺序</h1><p>信息的内容涉及和表达顺序影响着听着的认知过程。<br />信息框架结构（金字塔）<br />必须在信息传递之前，提前给出框架清晰的定义。例如拼图游戏，拼图盒上的完整图案，就是我们开始拼图游戏之前的最好信息。</p><ul><li><strong>说服式结构</strong><br />先讲关键信息，例如结果性的、结论性的信息；<br />先入为主<br /></li><li>叙述结构</li></ul><h1 id="记住我">记住我</h1><p>注意力经济</p><ul><li>使用比喻、类比和记忆工具等能够帮忙听众简历初步的认识。<br /></li><li>使用简单的语言<br /></li><li>分块（分类数据）、排序<br />我们更愿意接受较低的信息熵</li></ul><p>关注听众的需求和反应。</p><h1 id="情感联系的需要">情感联系的需要</h1><p>戴尔·卡耐基说过：“当和人打交道的时候，请记住和你打交道的人，并不是有着逻辑思维的生物，而是具有情感的生物。”<br />一张图片胜过千言万语，如果听众能够直观的看到、想象到，那么他们就能感受到。<br /><strong>使抽象问题具体化</strong> 例如说某个设备的马力多少，听众是没有感觉的，但是说某个设备可以拉动哪一栋大楼，那大家就马上能够理解。</p><p><strong>你可能会忘记我告诉了你什么，也可能会忘记我所展示给你的东西，但你却不会忘记我让你感受到的一切！</strong></p><h1 id="文章摘要">文章摘要</h1><ul><li>澳大利亚是地球上为数不多的没有松鼠的地方</li></ul><p><img src="/img/让松鼠聚焦1.jpg" /><br /><img src="/img/让松鼠聚焦2.jpg" /><br /><img src="/img/让松鼠聚焦3.jpg" /><br /><img src="/img/让松鼠聚焦5.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 说服力 </tag>
            
            <tag> 松鼠 </tag>
            
            <tag> 听众 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《程序员修炼之道》读书心得</title>
      <link href="/2019/08/26/%E3%80%8A%E7%A8%8B%E5%BA%8F%E5%91%98%E4%BF%AE%E7%82%BC%E4%B9%8B%E9%81%93%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2019/08/26/%E3%80%8A%E7%A8%8B%E5%BA%8F%E5%91%98%E4%BF%AE%E7%82%BC%E4%B9%8B%E9%81%93%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>因为看到“采石之人应胸怀大教堂”这句话，决定买这本书看看。<br />其实，开发软件的主要目的就是保持灵活性、稳定性，所以软件开发的主要原则就是解耦；<br />总体上说，一般般的一本书，不过，看完本书，还是有以下三点重要的收获。</p><ul><li>需求文档与政策文档分开<br /></li><li>解开解不开的谜（在盒子外思考）<br /></li><li>创立品牌（标新立异的名字、标识）<br /><span id="more"></span></li></ul><h1 id="要配置不要集成">要配置，不要集成</h1><p>config、Property文件，例如.ini .xml文件<br />与通用的编程的情况相比，可以通过一种大为接近问题领域的方式表示元数据。<br />商业政策和规则更有可能发生变化，通过配置文件维护更为合理。<br />将抽象放进源代码，将细节放进元数据（配置文件）</p><h2 id="需求描述">需求描述</h2><p>两个陈述“<strong>1、只有指定人员才能查看员工档案</strong>”和“<strong>2、只有员工上级和人事部门才可以查看员工的档案</strong>”的区别。<br />陈述2是真的是需求吗？今天也许是，但它在绝对的陈述中嵌入了商业政策。政策会经常改变，所以我们可能并不想把他们硬性地写入我们的需求。<br />我们的建议是，把这些政策文档与需求文档分开，并用超链接把两者链接起来。使需求成为一般描述，并把政策信息作为例子发给开发者----他们需要在实现中支持的事物类型的例子。最后，政策可以成为应用中的元数据。<br />通过陈述1，通过需求文档与政策文档的分离，开发人员就能实现访问控制系统，实现变与不变的分离！</p><p><strong>你必须把底层不变项当做需求进行捕捉，并把具体的或当前的工作实践当做政策计入文档。</strong><br />需求主要是要理解用户背后的目的，而不是用户的陈述！<br />用例：通过角色的视角，通过UML工具，描述用户做什么，怎么做。<br />需求文档通过Web进行管理，我们公司现在是通过Confluence进行管理，类似wiki的功能。</p><h1 id="解开不可能解开的谜">解开不可能解开的谜</h1><p>在盒子外面思考，逆向思维<br />戈尔迪之结<br />立鸡蛋<br />三条线连接，回到起点：<br />。 。</p><p>。 。</p><p>有些约束是绝对的，有些则只是先入为主。绝对约束必须受到尊重，不管它们看上去有多讨厌或多愚蠢。另一方面，有些外表上的约束也许根本不是真正的约束。<br /><strong>在盒子外面思考，鼓励我们找出可能不适用的约束，并忽略他们。</strong><br /><strong>解开谜题的关键是在于确定加给你的各种约束，并确定你确实拥有的自由度！因为在其中你讲找到你的解决方案。这也是有些谜题为何如此有效的原因；你可能会太快就排除了潜在的解决方案。</strong><br />在面对棘手的问题时，列出所有在你面前的可能途径。不要排除任何东西，不管它听起来有多么无用或愚蠢。现在，逐一检查列表中的每一项，并解释为何不能采用某个特定的途径。你确定吗？你能否证明？<br />想一想特洛伊木马，一个棘手问题的新奇解法。按常规思维，走前门，一开始就作为自杀行为而被排除了。<br />对你的各种约束进行分类，并划定优先级。我们想先确定最为严格的约束，然后再在其中考虑其余的约束。<br />你所需要的只是真正的约束、令人误解的约束，还有区分他们的智慧。</p><h2 id="挑战">挑战</h2><ul><li>用心想一想你今天遇到的无论什么难题。你能否解开这个戈尔迪之结？问你自己提出我们在上面列出的那些关键问题，特别是“它必须以这种方式完成吗？”<br /></li><li>当你签约开发你现在的项目时，是否交给了你一组约束？他们是否仍然都适用？对他们的解释是否仍然有效？</li></ul><h1 id="交流">交流</h1><p>有一个简单的营销诀窍，能帮助团队作为整体与外界交流：创立品牌。当你启动项目时，给它取一个名字，最好是不同寻常的某种东西，花30分钟设计一个滑稽的标识，并把它用在你的备忘录和报告上。在与别人交谈时，大方地使用你的团队的名字。这听起来很傻，但它能给你的团队一个用于建设的身份标识，并给外界以某种难忘的、可以与你们的工作想关联的东西。<br />例如阿里的平头哥、达摩院以及每个人的花名。</p><h1 id="了解你的听众">了解你的听众</h1><p>你需要了解你的听众的需要、兴趣、能力。<br />你要在脑海中形成一幅明确的关于你的听众的画面。<br />你需要清除听众的轻重缓急是什么？<br /><img src="/img/听众画像.png" /></p><h1 id="重构">重构</h1><p>重构是一项需要慎重、深思熟虑、小心进行的活动</p><ul><li>不要试图在重构的同时增加功能；<br /></li><li>在开始重构之前，确保你拥有良好的测试。尽可能经常运行这些测试。这样，如果你的改动破坏了任何东西，你就能很快知道。（回归测试）<br /></li><li>采取短小、深思熟虑的步骤。 （小步快跑，及时测试）</li></ul><h1 id="文章摘要">文章摘要</h1><ul><li>参加本地用户组织：不要只是去听讲，而是主动参与。与世隔绝对你的职业生涯来说可能是致命的；打听一下你们公司以外的人都在做什么。<br /></li><li>读书有三上：马上、枕上、厕上<br /></li><li>能不能让那个正确的原则知道正确的行动本身，其实就是区分是否是高手的一个显著标志；<br /></li><li>认知科学认为，频繁的高强度的外部刺激和自主的有意识的反复提醒是加速内化的两个重要的方法。<br /></li><li>理想的阅读状态应该是先大致理解和记住里面的Tip，然后每周争取实践2-3个Tip。<br /></li><li>把问题放到更大的语境中，设法注意更大的图景。<br />留心大图景，要持续不断的观察周围发生的事情，而不只是你自己在做的事情。<br /></li><li>在所有的弱点中，最大的弱点是就是害怕暴露弱点。<br /></li><li>计算机语言会影响你思考问题的方式，以及你看待交流的方式。<br />采用领域语言的语汇、语法、语义，进行实际的编程。<br /></li><li>要崩溃，不要破坏<br />java语言库已经采用了这一哲学。当意料之外的某件事情在runtime系统中发生抛出RuntimeException。如果没有被捕获，这个异常就会渗透到程序的顶部，致使其终止，并显示栈踪迹。<br /></li><li>DRY Don't Repeat Yourself<br />正交性<br /></li><li>软件更像园艺，需要不断的修剪；<br /></li><li>向导式开发，生成骨架代码 skeleton code<br /></li><li>解决需求蔓延的方法最好通过敏捷开发的模式，通过迭代找到最重要的需求。<br /></li><li>在你的作品上签名。<br /></li><li>破窗户理论。<br /></li><li>MVC 视图是对模型的一种解释<br />代码和文档（代码中的注释），可以视为是同一模型的两种视图；</li></ul><p><img src="/img/程序员修炼之道1.jpg" /><br /><img src="/img/程序员修炼之道2.jpg" /><br /><img src="/img/程序员修炼之道3.jpg" /><br /><img src="/img/程序员修炼之道4.jpg" /><br /><img src="/img/程序员修炼之道5.jpg" /><br /><img src="/img/程序员修炼之道6.jpg" /><br /><img src="/img/性能测试.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 原则 </tag>
            
            <tag> 程序员 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《历代经济改革得失》读书心得</title>
      <link href="/2019/08/18/%E3%80%8A%E5%8E%86%E4%BB%A3%E7%BB%8F%E6%B5%8E%E5%8F%98%E9%9D%A9%E5%BE%97%E5%A4%B1%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2019/08/18/%E3%80%8A%E5%8E%86%E4%BB%A3%E7%BB%8F%E6%B5%8E%E5%8F%98%E9%9D%A9%E5%BE%97%E5%A4%B1%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<h1 id="研究方法">研究方法</h1><h2 id="四大利益集团博弈法">四大利益集团博弈法</h2><p><img src="/img/四大集团.jpg" /></p><h2 id="四大基本制度分析法">四大基本制度分析法</h2><p><img src="/img/四大基本制度.jpg" /><br />就四大基本制度的建设而言，实验于商鞅，成型于嬴政，集大成于刘彻。<br /><span id="more"></span></p><h1 id="变革">变革</h1><p>历代经济变革的基本逻辑是：上联：发展是硬道理；下联：稳定压倒一切；<br />经济发展，而又能保持政权的稳定； 经济改革必须在政治体系的边界内进行。</p><p>改革的三个战场：财政、货币和土地。</p><p>变革有时候就像俗话说的，合久必分，分久必合；久之后，会出现弊端，为了平衡，需要走另一个极端来平衡；就像阴阳调和一样。</p><h1 id="王莽改革失败的思考">王莽改革失败的思考</h1><p>时机的问题：根基未稳；太过激进；<br />刘秀本来也想改革，后来推行不下去，留下了：苟非其时，不如息人。<br /><img src="/img/王莽变革失败思考1.jpg" /><br /><img src="/img/王莽变革失败思考2.jpg" /></p><h1 id="儒家">儒家</h1><p>百代都行秦政法<br />儒家在经济治理上是多么无能。翻来覆去的都是“以农为本、轻徭薄税、仁义治国”。儒家“君君臣臣”在政治上对中央集权制度形成了支柱性的作用，可是在经济思想体系上却无法匹配。</p><h1 id="文章摘要">文章摘要</h1><ul><li>统一是中国的一种文化<br />统一是一种宿命般的、带有终极意义的中国文化，是考察所有治理技术的边界。<br /></li><li>晚晴的洋务运动与日本的明治维新几乎同步，却造成了完全不同的国运结局<br /></li><li>观念的优先往往比资源的优先更重要<br /><strong>解放思想</strong><br /></li><li>取之于无形，使人不怒；<br />寓税于价；<br /></li><li>国家的稳定的一个核心是就业<br />消费能促进生产，生产促进就业。 所以，三驾马车有一个就是消费（内需）<br /></li><li>管仲：仓禀实则知礼节，衣食足则知荣辱；<br />有恒产者有恒心<br /></li><li>土地是核心！<br /></li><li>孔子：不患寡而患不均，不患贫而患不安；<br />老子：损有余而补不足；<br /></li><li>桑弘羊： 挑动群众斗群众，人民内部的矛盾。<br /></li><li>魄力 手段<br /></li><li>宁做太平狗，不做乱世人。<br /></li><li>道德经：邻国相望，鸡犬之声相闻，民至老死，不相往来。<br /></li><li>有得必有失，学会取舍。<br /></li><li>纸币的发行也出现在宋代<br />宋代，人口第一次超过了1个亿。<br /></li><li>王安石：天变不足畏，祖宗不足法，人言不足恤。<br />商鞅：治世不一道，便国不法古。<br />“治世不一道”,是指治世的法则不是死板的."便国不法古”,是指使国家安适不效法古人.“便”,读pián,是会意字.金文从人从鞭,会使人服帖之意.篆文从人,从更（指烙饼时不断翻动按压,使饼与鏊子平贴）,会妥帖、安适之意.隶变后楷书写作便.故“便”的本义是指妥帖、安适.<br />“治世不一道,便国不法古.”即无论治理天下还是国家,都不能按照死板的方法去做,要灵活根据实际情况变通.法家提倡效今不法古,是说在旧方法（即古法）已经失效的情况下,就不应该死守了,必须创新.<br /></li><li>在这个世界上，人性的贪婪都是需要制度基础的，好的制度会遏制人的恶，反之则会催化和放大之。在这个意义上，比人的贪婪更可怕的是制度的贪婪。<br /></li><li>11世纪至12世纪（王安石变法时期）常被称为中西方文明的大分流时期，是“世界时间（布罗代尔 法国）”的钟摆从东向西摆动的关键时期。<br /></li><li>在西方史学界，1500年往往被看成是古代与近代的分界线。 （发现新大陆）<br /></li><li>中国的传统不是制造一个更好的捕鼠器，而是从官方取得捕鼠的特权。 （寻租）<br /></li><li><strong>费正清：中国商人最大的成功是，他们的子孙不再是商人！</strong><br /></li><li>生命总会找到出口。<br /></li><li>稳定压倒一切，出自陈云之口，被邓小平多次引用。<br /></li><li>80年代：一切改革都是从违法开始。 原罪。<br /></li><li>李克强：触及利益比触及灵魂还难。<br /></li><li>极左的民粹主义，极右的自由主义。<br /></li><li>财政包干，分灶吃饭 改为分税制。 事权与财权的统一问题。<br /></li><li>历史并不存在单一的真相。</li></ul><p><img src="/img/历代经济改革得失.jpg" /><br /><img src="/img/历代经济变革脑图.png" /><br /><img src="/img/社会形态演进.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 经济改革 </tag>
            
            <tag> 吴晓波 </tag>
            
            <tag> 得失 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《深度工作》读书心得</title>
      <link href="/2019/08/13/%E3%80%8A%E6%B7%B1%E5%BA%A6%E5%B7%A5%E4%BD%9C%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2019/08/13/%E3%80%8A%E6%B7%B1%E5%BA%A6%E5%B7%A5%E4%BD%9C%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>跨出舒适区和恐惧。<br /><img src="/img/深度工作.svg" /><br /><span id="more"></span></p><h1 id="工作的两个趋势">工作的两个趋势</h1><p>机器代替人<br />远程协作代替本地的就业（通讯和协作技术的发展）</p><h1 id="核心能力">核心能力</h1><ul><li>迅速掌握复杂工具的能力；<br /></li><li>在工作质量和速度方面都达到精英层次的能力；</li></ul><h1 id="高质量工作">高质量工作</h1><p>高质量工作产出=时间 * 专注度<br />注意力残留 注意力切换成本</p><h1 id="系统化结构化的思维能力">系统化、结构化的思维能力</h1><p>即兴发言：描述现状、为什么、怎么办；<br />PM的管理领域：范围、计划、成本、质量、人力资源、沟通、风险、干系人、供应商、整合。<br />立体思维：正反、上下左右（干系人）<br />变量：影响问题的变量、确定需要解决的问题、review巩固答案。<br />关注的核心、核心的利益；最核心的要素。<br />利益分析法；<br />思考再回答</p><h1 id="度量黑洞">度量黑洞</h1><p><strong>难以衡量一个知识工作者的贡献，我们称之为度量黑洞；</strong><br />由于知识工作者的工作复杂度比体力劳动者高，所以很难衡量个体努力所带来的价值。<br />所以，知识工作者越来越多表现为可视的忙碌，是因为他们没有更好的方法证明自身的价值。<br />忙碌代表生产能力，在工作中，对于生产能力和价值没有明确的指标时，很多知识工作者都会采用工业时代关于生产能力的指标，以可视的方式完成很多事情。（例如加班也是一种变种），这种思维方式并不一定是非理性的。但从客观角度来说，这种观念已经过时了。知识工作者非生产线，从信息中提取价值的行为往往不忙碌，也并非靠忙碌支撑。<br /><strong>知识工作者给人的工作内容感觉就是：电子邮件、PPT、会议。</strong>只有幻灯片中的图表的不同，才区分了不同的职业。<br />在泰勒时代，衡量生产能力是 单位时间产出产品的数量。</p><h1 id="思考推出去">思考推出去</h1><p>考虑领导的意图，顺着来回答； 不一定是实际的。<br /><strong>汇报的技巧，和领导谈话的技巧 重点</strong></p><h1 id="注意力">注意力</h1><p>我们选择去关注哪些事物，忽略哪些事物，对于我们的生活质量所起的作用。<br />在一段痛苦不安的人生经历后，你选择关注的事物将极大地影响你对未来生活的态度。<br />重置自己的大脑，忽略负面的信息，尽量享受积极的信息；<br />你的世界是你所关注事物的产物。</p><h1 id="肤浅事务">肤浅事务</h1><p>他们不断的查看邮箱的习惯（例如看及时通讯工具也一样），会使这些肤浅的事务占据他们的注意力的中心，这样度日是愚蠢的，因为这样下去你的大脑就会形成固定印象，认为你的工作生活充满了压力、烦扰、沮丧和琐事。换言之，由电邮收件箱代表的世界并不适宜栖息其中。</p><p>我将精心选择目标，然后全情投入。简而言之，我将活出专注的人生，因为这是最好的选择。<br />一个人的身体或头脑在自觉努力完成某项艰难且有价值的工作过程中达到极限 ，往往是最优体验发生的时候。（所以要做困难而有价值的事情）。（深度工作带来的心流经历可以为你带来深度满足感）</p><h1 id="手机公约">手机公约：</h1><p>戒手机，即使是零碎的时间；每天可以集中看半小时； （日常的微信联系不限）<br />有一种见解认为，在网上不断地切换注意力会对大脑产生长久的负面影响，这会对大脑进行重新编排，影响大脑的深度思考能力。<br /><strong>当你被迫等待的时候（例如在商店里排队），执行这些策略会变得尤其困难（不看手机）。在这个时刻，如果你处于离线阶段，那么一定要忍耐这暂时的无聊，凭借大脑的思考度过这一段时间。静静等待并忍受无聊已经成为现代生活的一种新奇体验，从集中注意力训练的角度出发，这具有不可思议的重要价值。</strong></p><h1 id="文章摘要">文章摘要</h1><ul><li>Tim Ferriss: 培养允许坏的小事发生的习惯。否则，你永远也发现不了改变命运的大事。<br /></li><li>人们会很快调整对你的期望，适应你的社交习惯。<br /></li><li>从根本上讲，一名优秀的首席执行官就是一部难以自动化的决策引擎。<br /></li><li><strong>一段简洁明了的代码，一段优美的代码，给人带来的感觉，就像一首诗。</strong><br />在一个项目的整体架构之内，总有空间展示个性和匠心......百年之后，我们的技艺或许如今日的土建工程师看待中世纪大教堂建造者使用的技法一样陈旧，但是我们的匠心却会得到尊重。<br /></li><li>伟大的创造性头脑如艺术家般思考，却如会计般工作。<br /></li><li>开放性办公室：员工的互相协作会产生新的想法。<br /></li><li>对于涉及大量信息和多项模糊不清之处，甚至存在矛盾和约束条件的决策，无意识思维或许更合适。<br /></li><li>如果某一天你没有挑战自己心智的极限，那么就不能说完成了一天的任务。<br /></li><li>有一点需要提醒的是，一定要给自己设定一个几乎不可能的时间期限。你应该总是可以赶在最后期限前完成任务（至少是接近），但是这期间需要你用上吃奶的力气。<br /></li><li>有成果的冥想的目标是：在身体劳作而心智空闲的时候，如走路、慢跑、开车、淋浴，将注意力集中到一件定义明确的专业难题上。<br /></li><li>一个人的日程都是由内在驱动和外在要求这两股力量决定的。<br /></li><li>如何说不，拒绝一些你不想做的事情：<br />1）明确的拒绝，但是理由可以模糊的处理，这样避免给他们机会来否定你拒绝的理由；<br />2）拒绝之后，遏制内心的重读，不需要安抚对方。</li></ul><p><img src="/img/深度工作.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度工作 </tag>
            
            <tag> 目标 </tag>
            
            <tag> 4DX </tag>
            
            <tag> 完工仪式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《菜根谭》读书心得</title>
      <link href="/2019/08/10/%E3%80%8A%E8%8F%9C%E6%A0%B9%E8%B0%AD%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2019/08/10/%E3%80%8A%E8%8F%9C%E6%A0%B9%E8%B0%AD%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>《菜根谭》是一部以处世哲理为核心的格言体小品文集，全书以语录体呈现。作者以“菜根”命名，寓意“人的才智与修养，唯有经受艰辛磨炼方能获得”。</p><p>俗话说：“咬得菜根，百事可做。”明代奇人洪应明正是取其义而写成此书。“谭”与“谈”同义，读音相近，类似《天方夜谭》中“夜谭”的含义。一个人若能克制口腹之欲，忍受物质上的清贫，便能在困境中保持坚韧，从而把握人生的真谛，成就真正的自我。</p><p>《菜根谭》以“站在人生终点回望人生本质”的方式来论述。然而，人生的本质到底是什么？是虚幻无常？是庄周梦蝶般的迷离？还是“平平淡淡才是真”？人生究竟有无意义？如果有意义，那意义又是什么？</p><p>也许答案并不复杂——只要怀有一颗能够感受幸福的平常心，便已身在幸福之中，不必再劳苦奔波去别处寻觅。</p><span id="more"></span><p><img src="/img/修身.png" alt="修身" style="zoom:40%;" /></p><p><img src="/img/做人.png" alt="做人" style="zoom:35%;" /></p><h1 id="文章摘要">文章摘要</h1><ul><li>修身、齐家、治国、平天下</li><li><strong>穷则独善其身，达则兼济天下</strong>，出自《孟子》的《尽心章句上》（第九）。<br />原句为“穷则独善其身，达则兼善天下”，意思是不得志的时候就要管好自己的道德修养，得志的时候就要努力让天下人都能得到好处。<br /></li><li>“居庙堂之高则忧其民，处江湖之远则忧其君”出自范仲淹的《岳阳楼记》的句子。意思是在朝廷里做高官就应当心系百姓；处在僻远的江湖间也不能忘记关注国家安危。<br /></li><li>要奉行恕道，懂得换位思考，能够站到别人的角度想问题；<br /></li><li>修身：静心<br /></li><li><strong>"非淡泊无以明志，非宁静无以致远＂</strong>，出自诸葛亮54岁时写给他8岁儿子诸葛瞻的《诫子书》。意思是不把眼前的名利看得轻淡就不会有明确的志向，不能平静安详全神贯注的学习，就不能实现远大的目标。<br /></li><li>明代诗人杨慎的《临江仙》<br />　　滚滚长江东逝水，浪花淘尽英雄。是非成败转头空，青山依旧在，几度夕阳红。<br />　　白发渔樵江渚上，惯看秋月春风。一壶浊酒喜相逢，古今多少事， 都付笑谈中。<br /></li><li>弃疾《永遇乐·京口北固亭怀古》<br /><strong>千古江山，英雄无觅，孙仲谋处。舞榭歌台，风流总被，雨打风吹去</strong>。斜阳草树，寻常巷陌，人道寄奴曾住。想当年，金戈铁马，气吞万里如虎。<br />元嘉草草，封狼居胥，赢得仓皇北顾。四十三年，望中犹记，烽火扬州路。可堪回首，佛狸祠下，一片神鸦社鼓。凭谁问，廉颇老矣，尚能饭否。<br /></li><li>封狼居胥[fēng láng jū xū]<br />封狼居胥指西汉大将霍去病登<strong>狼居胥山</strong>筑坛祭天以告成功之事，出自于《汉书·霍去病传》，后来封狼居胥成为华夏民族武将的最高荣誉之一。狼居胥，今蒙古国首都乌兰巴托东肯特山。霍去病率大军进行了祭天地的典礼——祭天封礼于狼居胥山举行，祭地禅礼于姑衍山举行。这是一个仪式，也是一种决心，“封狼居胥”的典故成为流传至今佳话。<br /></li><li>不以物喜、不以己悲<br /></li><li>曾子曰：“吾日三省吾身——为人谋而不忠乎？与朋友交而不信乎？传不习乎？”<br /></li><li><strong>无事便思有闲杂念想否。有事便思有粗俗意气否。得意便思有骄矜辞色否。失意便思有怨望情怀否。时时检点，到得从多入少、从有入无处，才是学问的真消息。</strong><br /></li><li>豢养是一个汉语词汇，读音为huàn yǎng，意思是喂养（牲畜）；驯养。养育，供养。比喻收买并利用。<br /></li><li>一切烦恼皆由心生，还自心以本来清净，即为悟道。<br />我们无法改变世界，但是可以改变看世界的眼睛和体悟世界的心灵。<br /><strong>世间一切，皆由心生，皆由心灭；</strong><br /></li><li>有容，德乃大；<br /></li><li>砥柱：比喻能负重任、支危局的人或力量。<br />万里黄河，从源头到入海，就以它那汹涌澎湃的气势而著称。在黄河上，有数不尽的险滩和暗礁，五千年来黄河儿女在这儿繁衍生息，对母亲河的生性和脾气了如指掌。中流砥柱，这樽仅有十多米高的一座山形河石，就被华夏子孙们传颂为英雄石，作为中华民族坚强不屈的象征。<br /></li><li>处事要镇定，待人要真诚；<br /></li><li>悲天悯人<br /></li><li>《庄子集释》卷六下《外篇·秋水》北海若曰：井蛙不可以语于海者，拘于虚也；夏虫不可以语于冰者，笃于时也；<br /></li><li>逍遥游 （《庄子》首篇）北冥有鱼，其名为鲲。鲲之大，不知其几千里也；化而为鸟，其名为鹏。鹏之背，不知其几千里也；怒而飞，其翼若垂天之云。<br /></li><li>身后有余忘缩手，眼前无路想回头；<br /></li><li>家和万事兴<br /></li><li>月下独酌（李白）<br />花间一壶酒，独酌无相亲。<br />举杯邀明月，对影成三人。<br />月既不解饮，影徒随我身。<br />暂伴月将影，行乐须及春。<br />我歌月徘徊，我舞影零乱。<br />醒时同交欢，醉后各分散。<br />永结无情游，相期邈云汉。<br /></li><li><strong>孔子登东山而小鲁，登泰山而小天下；人的视点越高，视野就越宽广。</strong><br /></li><li><strong>东海水曾闻无定波，世事何须扼腕？北邙山未省留闲地，人生且自舒眉。</strong><br /></li><li>月有阴晴月缺，人有悲欢离合，此事古难全。<br /></li><li>蜂 唐 罗隐<br />不论平地与山尖，无限风光尽被占。<br /><strong>采得百花成蜜后，为谁辛苦为谁甜。</strong><br /></li><li>《终南别业》唐王维<br />中岁颇好道，晚家南山陲。<br />兴来每独往，胜事空自知。<br />行到水穷处，坐看云起时。<br />偶然值林叟，谈笑无还期。<br /></li><li>谢灵运为人清狂，恃才傲物，曾于饮酒时自叹道：“天下才共一石，曹子建（即曹植）独得八斗，我得一斗，自古及今共分一斗。”<br /></li><li><strong>生活就像一面镜子，你对它微笑，它就会对你微笑。</strong><br /></li><li>处世让一步，待人宽一分；<br /></li><li>天之道，损有余而补不足；<br /></li><li>所谓“仁者见仁，智者见智”，每个人的眼光或者经历不同，看同样的事物就会有差别，甚至于天壤之别的观点。就好像“一千个人眼里就有一千个哈姆雷特”一样。<br />鲁迅对《红楼梦》的评价，“一部红楼梦，经学家看见《易》，道学家看见淫，才子看见缠绵，革命家看见排满，流言家看见宫闱秘事”。<br /></li><li>君子爱财，取之有道；<br /></li><li>量大福也大，机深祸亦深；<br /></li><li>水至清则无鱼，人至察则无徒；<br /></li><li>二鸟在林，不如一鸟在手<br /></li><li><strong>故君子事来而心始现，事去而心随空；<br />要有一颗拿得起放得下、不凝滞于外物的心。</strong><br /></li><li>人活着，有时靠的是一种精神气儿，常言道“人穷志不短”<br /></li><li>生于忧患，死于安乐<br /><strong>逆境更能激发人的求生、求胜心理，所以反倒比顺境更有利于磨炼强者的意志。</strong><br /></li><li>隐恶而扬善；<br /></li><li>既然你不可能让每个人都满意，那么不如坚持做你自己，有人不喜欢，且随他去；<br /></li><li>唐太宗曾问魏征：“人主何以为明，何以为暗？”魏征说：“兼听则明，偏听则暗。”<br /></li><li><strong>害人之心不可有，防人之心不可无； 这大概是《菜根谭》中流行最广的一句话</strong><br /></li><li><strong>宝剑锋从磨砺出，梅花香自苦寒来；</strong><br /></li><li>《孙子兵法·军争篇》说：“围师必阙，穷寇勿追，此用兵之法也。”所谓狗急跳墙，兔急咬人，当一个人被逼迫得无路可走时，往往会被激发出自身的野性，爆发出难以估量的潜力，势必会给压迫者造成更大的伤害。<br /></li><li>百无一用是书生<br /></li><li>克己奉公<br />严于律己，宽以待人<br /></li><li><strong>恩宜自淡而浓，先浓后淡者人忘其惠；威宜自严而宽，先宽后严者人怨其酷<br />人欲无穷，故而施恩宜用“递增法”；人性散漫，故而立威宜用“递减法”。</strong><br /></li><li>事缓则圆，急难成效<br /></li><li><strong>事稍拂逆，便思不知我的人，则怨尤自清。<br />心稍怠荒，便思胜似我的人，则精神自奋。</strong><br /></li><li>只有看淡自身的利害得失，才能以超然的姿态，获得彻底解脱；放下自己的心，超越有限的“我”，就会少一些感慨与抱怨，多一分轻松和快乐。<br /></li><li><strong>宠辱不惊，闲看庭前花开花落；去留无意，漫随天外云卷云舒；</strong><br /></li><li><strong>古往今来也只不过是一瞬之间，消解了时间的局限，也就获得了精神上的自由。</strong></li></ul><p><img src="/img/菜根谭.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人生 </tag>
            
            <tag> 格言 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《改变未来的九种算法》读书心得</title>
      <link href="/2019/07/21/%E3%80%8A%E6%94%B9%E5%8F%98%E6%9C%AA%E6%9D%A5%E7%9A%84%E4%B9%9D%E7%A7%8D%E7%AE%97%E6%B3%95%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2019/07/21/%E3%80%8A%E6%94%B9%E5%8F%98%E6%9C%AA%E6%9D%A5%E7%9A%84%E4%B9%9D%E7%A7%8D%E7%AE%97%E6%B3%95%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p><img src="/img/九种算法.svg" /> <span id="more"></span></p><h1 id="文章摘要">文章摘要</h1><ul><li>计算机模型 通讯 人工智能</li><li>计算机科学家们会将许多重要的思想形容为“算法”；算法是一种精确的机械化的处方。</li><li>硅谷的技术公司，惠普、苹果、谷歌都是从车库里面孵化出来的；<br /></li><li>将二进制数转化为小数位数的转换比例为30%；</li><li>可以说绝大多数图形识别任务都是分类问题</li><li><strong>在这里，我们遇到了人工智能研究人员在过去几十年中学到的最重要的教训之一：看似智能的行为有可能从看似随机的系统中浮现出来。如果我们有能力进入人脑，研究神经元之间连接的强度，其中绝大部分连接都会表现得很随机。然后，当作为集体行动时，这些连接的松散集合产生了人的智能行为！</strong></li><li>ACID 原子性、一致性、独立性（事务）、持久性</li><li>Brute Force 暴力破解</li></ul><p><img src="/img/九种算法1.jpg" /> <img src="/img/九种算法2.jpg" /> <img src="/img/九种算法3.jpg" /> <img src="/img/九种算法4.jpg" /> <img src="/img/九种算法5.jpg" /> <img src="/img/九种算法6.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
            <tag> 读书心得 </tag>
            
            <tag> 图形识别 </tag>
            
            <tag> RSA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《价值规律》读书心得</title>
      <link href="/2019/07/06/%E3%80%8A%E4%BB%B7%E5%80%BC%E8%A7%84%E5%BE%8B%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2019/07/06/%E3%80%8A%E4%BB%B7%E5%80%BC%E8%A7%84%E5%BE%8B%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<h1 id="故事">故事</h1><p>楚材晋用：<br />春秋时期，楚国大夫伍举因岳丈犯事而偷逃到晋国，遇到蔡国大夫声子，声子答应帮他回到楚国，便到楚国与令尹子木就楚晋两国人才问题进行探讨，说楚国人跑到晋国得到重用，对楚国十分不利。子木认为有理就去晋国接回伍举。</p><p>岳麓书院：<br />惟楚有材，于斯为盛； 惟是语首助词，无义；</p><p><strong>心有猛虎，细嗅蔷薇。是英国诗人西格里夫·萨松代表作《于我，过去，现在以及未来 》的经典诗句。原话是“In me the tiger sniffs the rose.”诗人余光中将其翻译为：心有猛虎，细嗅蔷薇。意思是，老虎也会有细嗅蔷薇的时候，忙碌而远大的雄心也会被温柔和美丽折服，安然感受美好。讲的是人性中阳刚与阴柔的两面。</strong><br /><span id="more"></span></p><h1 id="文章摘要">文章摘要</h1><ul><li>天下熙熙皆为利来； 天下攘攘皆为利往；<br /></li><li><strong>在江湖上，所有人永远都只愿意和强者合作，所以在做事的时候一定要高调，只有高调才能吸引更多的资源。所谓“人之道，损不足以奉有余”，你越弱别人越远离你，你越强别人越愿意把资源交给你，所以做事一定要得高调。</strong><br /></li><li>工农剪刀差，其实就是工业产品和农业产品的定价机制不同。 一个是国家定价，一个是市场定价；<br /></li><li>互联网经济可以把人、货物、现金、信息等一切有形和无形的东西“连接”起来，完全突破了物理空间的限制。工业抢空间，互联网抢时间，完全是不同层次的思维。<br /></li><li>知人者智，自知者明，胜人者有力，自胜者强；<br /></li><li><strong>如果你把困难当作一种雕刻，你就会变得越来越强大。</strong><br /></li><li><strong>基层社会拼能力和忠诚；中层社会拼关系；高层社会拼血缘；</strong><br /></li><li>物极必反，盛极而衰，月满必亏，否极泰来<br /></li><li>所有的问题都是人的问题，是人的品格、修养、格局的问题；<br /></li><li>一切变革的本质都是生产关系的调整，从而促进生产效率的提升。<br />生产关系和生产力的管理，好比鞋和脚的关系；<br /></li><li>自律的人才有资格谈自由。<br /></li><li>人类社会发展的一切障碍，从根本上来说就是“互不信任”导致的，因为在一个互不信任的社会里，人们会互相猜测，这是社会的运转效率就会非常低。<br /></li><li>区块链：分布式记账、去中心化<br /></li><li>数学锻炼你的逻辑，物理让你深刻，化学让你学会看微观，语文让你陶冶情操，历史让你看懂规律，地理让你看透万象，生物让你看透生命......<br /></li><li><strong>很多善于谋略的人，都会用利益分析法推导出一些结论之后，用第一套秩序包装起来（仁义道德）。说一套做一套。</strong><br /></li><li>原来狭长的“公司+雇员”的结构，现在变成了扁平的“平台+创客”结构。<br /></li><li><strong>VR是把你带到虚拟世界里，AR是把虚拟物品带到你的面前；</strong><br /></li><li><strong>传统企业的上下游就像是排纵队，每个人只能看到自己前面的人的后脑勺；互联网企业就像是排横队，大家都看到整个队伍的情况。</strong><br /></li><li>虚拟产业：线上的信息流、货币流；实体产业：线下的产品流、人群流；<br /></li><li>科技和金融，分别是实体经济和虚拟经济的核心支撑点。科技的本质是生产力，金融的本质是生产关系。对社会财富来说，科技和实业的作用是直接带来增量；互联网和金融的作用是优化存量、优化资源配置，从而促进增量增长。<br /></li><li><strong>投资就是跟人性博弈的过程，最强的对手一定是你自己。一旦你战胜了自己，如同跳出三界外，不在五行中。宠辱不惊，看庭前花开花落；去留无意，望天上云卷云舒。</strong><br /></li><li><strong>价值就是一个人的“能力”乘以“资源”；要想让价值流经你这里，你就必须具备可以放大价值的功能，而不是只充当一个传输节点。</strong><br /></li><li><strong>社交的本质就是价值交换。</strong><br /></li><li>天行健，君子以自强不息<br /></li><li>无效社交<ul><li>网络社交<br /></li><li>泛泛之交<br /></li><li>不平等社交<br /></li></ul></li><li>自古真心留不住，唯有套路得人心；<br /></li><li><strong>可靠：凡事有交代，件件有着落，事事有回音。</strong><br /></li><li>非凡的逻辑很简单：承认自己的平凡；寻找内心的安静；发现自己的不平凡。<br /></li><li>爱自己是一个艰难的过程，爱自己意味着要不断提高自己，要使自己变得强大，变得美好。但是爱别人，是只要发现别人的美好就可以了。<br /></li><li>一个人越缺少什么，越容易喜欢有这种特征的人，这不叫爱，这叫心理补偿。<br /></li><li><strong>升米恩，斗米仇</strong><br /></li><li><strong>不浮躁就是该吃饭吃饭，该睡觉睡觉，该看书看书，该洗澡洗澡；聊事时聊事，陪朋友时陪朋友。万事各得其所，专心在此时此刻，做每一件事。</strong><br /></li><li>学到的是技术，悟到的才是艺术。<br /></li><li>古代人是以“性情”为主动力，现代人是以“效率”为主动力；<br /></li><li><strong>凡是开始能让你痛苦的东西，最后一定也会让你快乐，比如良药苦口，忠言逆耳，寒门出贵子等。这就是世界的平衡法则，先苦就后甜，先甜就后苦。</strong><br /></li><li>人也有不断进步的需求，但那些能使一个人进步的行为，却不能让人在当下、在瞬间产生快感。比如晨练、运动、阅读、健身、瑜伽等，都能让人获得长足的进步，但这些需要一个人有强大的上进心、克制力、自律。<br /></li><li><strong>大格局的人和小格局的人，最根本的区别在于：小格局的人，总是以偏见为依据得出结论；大格局的人，总是能突破各种偏见，就事论事、就人论人。<br />层次越高的人，越没有傲慢和偏见。他们往往把自己的姿态放得很低，懂得尊重别人。</strong><br /></li><li>世人最难过的一关，就是总向别人解释自己。<br /></li><li>言多必失，人越描越黑，事越说越离谱。<br /></li><li>沉默才是最好的表达方式。<br /></li><li>过多的解释只会变成别人诬陷你的把柄。懂你的人不需你说，不懂你的人不相信你说的。<br /></li><li>优秀的人控制情绪，失败的人被情绪控制。<br /></li><li><strong>静气</strong><br /></li><li>百病生于气<br /></li><li><strong>世上本无事，庸人自扰之。每一个让你痛苦的人，必定会让你强大。每痛苦一次，你内心就强大一次。</strong><br /></li><li>苦难守恒定律<br /></li><li>命运平衡法则。<br /></li><li>心有猛虎，细嗅蔷薇</li></ul><p><img src="/img/价值规律1.jpg" /><br /><img src="/img/价值规律2.jpg" /><br /><img src="/img/价值规律3.jpg" /><br /><img src="/img/价值规律4.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 价值规律 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《大明王朝1566》读书摘要</title>
      <link href="/2019/06/22/%E3%80%8A%E5%A4%A7%E6%98%8E%E7%8E%8B%E6%9C%9D1566%E3%80%8B%E8%AF%BB%E4%B9%A6%E6%91%98%E8%A6%81/"/>
      <url>/2019/06/22/%E3%80%8A%E5%A4%A7%E6%98%8E%E7%8E%8B%E6%9C%9D1566%E3%80%8B%E8%AF%BB%E4%B9%A6%E6%91%98%E8%A6%81/</url>
      
        <content type="html"><![CDATA[<h1 id="文章摘要">文章摘要</h1><ul><li>授权柄于宦官，以家奴治天下；<br /></li><li><strong>做官要三思：思危、思退、思变；</strong><br /></li><li>孔曰成仁、孟曰取义<br /></li><li>官做到这个位置，“静气”二字已是必然的功夫；<br /></li><li><strong>做事时不问可不可能，但问应不应该；</strong><br /></li><li>民可使由之，不可使知之；<br /></li><li>共济时艰<br /></li><li>道德经云“天地不仁，以万物为刍狗；圣人不仁，以百姓为刍狗”。；<br /></li><li><strong>苏东坡：古人不见今时月、今月曾经照古人；</strong><br /></li><li>太史公：人固有一死，或重于泰山，或轻于鸿毛；<br /></li><li>爱吵架的从来就怕两种人，一种是任你暴跳如雷，他却心静如水；一种是挑你一枪，扬长而去。<br /></li><li><strong>枕戈待旦：并非拿着枪睁眼坐待天明，而是心如空城，枕着一杆枪也安然睡了。</strong><br /></li><li>在官场，礼节就是内容；<br /></li><li>铁打的营盘流水的兵；<br /></li><li>用对人是干大事第一要义；<br /></li><li><strong>不迁怒，不贰过；</strong><br /></li><li><strong>衣不如新人不如故</strong><br /></li><li>不痴不聋不做当家翁<br /></li><li>治大国如烹小鲜<br /></li><li>无心为过，虽过不罚；<br /></li><li><strong>有子万事足，无官一身轻；</strong><br /></li><li><strong>国之干城</strong><br /></li><li>行到水穷处，坐看云起时；<br /></li><li>该出手时就出手，得饶人处且饶人；<br /></li><li><strong>不能谋万世者不能某一时；不能谋全局者不能谋一隅</strong><br /></li><li>这个世上，真靠得住的就两种人，一种是笨人，一种是直人；<br /></li><li>天下无不是父母<br /></li><li>商鞅立木之法<br /></li><li><strong>巧言令色，大奸似忠；</strong><br /></li><li>圣人无恒心，以百姓之心为心；</li></ul><p><img src="/img/大明王朝1566.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 历史 </tag>
            
            <tag> 明朝 </tag>
            
            <tag> 嘉靖 </tag>
            
            <tag> 海瑞 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《社会机器》读书心得</title>
      <link href="/2019/06/12/%E3%80%8A%E7%A4%BE%E4%BC%9A%E6%9C%BA%E5%99%A8%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2019/06/12/%E3%80%8A%E7%A4%BE%E4%BC%9A%E6%9C%BA%E5%99%A8%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p><img src="/img/AI研究领域.png" /><br /><img src="/img/AI下棋算法.png" /></p><h1 id="模式识别">模式识别</h1><p>模式识别：这种解决一个新问题或者通过将新问题与一些之前已经解决过的问题或者已经观测到的模式相匹配的能力称之为模式识别。<br />模式识别依赖于以下几个方面，即我们的感官、感知以及认知系统从输入的数据中提取特征的能力；从数据中识别出一些规则的能力（建立一种模式）；判定一种新的模式是否与我们已遇见过的其它模式相类似的能力（建立模式分类）；以及在需要的时候，对比新模式与已知模式来支持学习与解决问题的能力。<br />拥有一套以某种方式编码于我们记忆之中的模式并且能够在模式识别的过程中使用这些模式，就是一类推理能力。<br /><span id="more"></span><br />推理能力：识别、特征、分类、解决问题</p><p>认知计算系统是由研究者构建的视图模拟人类大脑推理方式来解决问题的计算机系统。<br />有别于早期的专家系统，先将解决问题所需要的知识转化程计算机能理解的规则。</p><h1 id="图灵测试">图灵测试</h1><p>图灵测试假设，这台计算机知道它要模仿一个人，因而会有意地做错一些事情，例如，当被要求执行一个复杂地数学运算时，该计算机可能希望或者直接给出一个错误地答案，或者说一些类似“这太难了”的话，尽管实际上它可能会知道答案；例如，当问到计算机是否会死去，它的回答可能时会，即使它不会死去的。</p><h1 id="网络价值与用户数量的关系">网络价值与用户数量的关系</h1><p><a href="https://www.jianshu.com/p/c9f07196e99c">网络的价值（上）：从梅特卡夫及其定律说起</a></p><p>1980年,梅特卡夫(以太网的发明者)提出网络的价值V与其用户数量n的平方成正比,即 <span class="math inline">\(V∝n^{2}\)</span> 。该定律作为网络效应的一种最著名的体现,受到了很大的争议,许多学者将其称为"错的"或者"危险的",其他的定律相继提出:萨洛夫定律(V∝n)、里德定律(V∝nlog(n))、奥德利兹科定律(V∝2n)。<br />该定律已提出三十多年,但一直缺乏实证数据支持。直到2013年,梅特卡夫本人给出了梅特卡夫定律的第一个证据:<strong>脸书公司的收入与其月活跃用户数的平方成正比。</strong><br />本文采用腾讯公司(中国最大的互联网综合服务提供商之一)与脸书公司(全球最大的社交网络公司)过去十年的实际数据,拓展梅特卡夫的方法,得出了下列结果:<br />(1) 针对网络效应的四种体现给出了腾讯与脸书实际数据的拟合结果,并显示梅特卡夫定律的拟合误差远小于其他三个定律的拟合误差;<br />(2) 梅特卡夫定律对腾讯公司数据与脸书公司数据均成立;<br />(3) 腾讯和脸书公司的成本与其月活跃用户均非线性关系,而是与其月活跃用户的平方成正比;</p><h1 id="验证码的额外价值">验证码的额外价值</h1><p><a href="https://www.pingwest.com/a/181245">你以为自己在填验证码，其实你是在给Google义务劳动-reCAPTCHA验证码的众包价值</a></p><h1 id="文章摘要">文章摘要</h1><ul><li>宁为太平狗，不做乱世人<br /></li><li>乐观主义者们看到的是希望的理由，悲观主义者们看到的是绝望的理由。而真相，最终被发现时，往往是介于两者之间的一个非常微妙的境地。<br /></li><li>需要保护我们的隐私以降低我们的信息被某些敌对组织获取的概率。<br /></li><li>完全信息博弈：棋类； 不完全信息博弈：卡牌类<br /></li><li>记忆是一个重构的过程。在记忆重建的过程中，人类或许会“突变”他们的记忆。<br />记忆与我们对自己身份认定高度相关。<br /></li><li>人类惊人的特质之一就是可以通过情感来传达复杂的思想；人们有多种表达情感的方法，无论是语言的还是非语言的。<br /></li><li>人类是社会化的动物，人类倾向于在灾难面前团结起来从而缓解压力和脆弱感。<br /></li><li><strong>人工智能就是计算机目前还不能做的事情。</strong><br /></li><li>自然语言处理，分语音识别、语言理解；语音识别的识别儿童语音、噪声环境中的语音、多人交谈时的语音、带口音的语音仍有较大的难度。<br />语言理解的难点：声调、语言的歧义性、上下文； 让AI做高考的阅读理解：）<br /></li><li><strong>仅能够识别物体还不够，理解他们之间的相互影响的方式才能使得计算机在没有人类干涉的情况下，做出在这个世界中正常运转所需要的一类必要推理。（现实世界的物理规则）</strong><br /></li><li>三段论：<br />事实：若凡人终有一死；<br />事实：苏格拉底是人；<br />结论：苏格拉底终有一死；<br /></li><li>人类来创造，机器来管理。 维基百科使用了几百个Bots在管理内容更新，对一些拼写、违规内容进行自动检查。<br /></li><li>自主驾驶需要人类将控制权让渡给AI。<br /></li><li>技术往往是双刃剑，有好的一面，也有不好的一面。<br /></li><li>阿莫西夫的机器人三原则：<br />1、机器人不得伤害人类，或者看到人类受到伤害时袖手旁观；<br />2、机器人必须服从人类的命令，除非这条命令与第一条相违背；<br />3、机器人必须保护自己，除非这种保护与以上两条相违背。<br /></li><li>物联网IoT让私密数据的隐私性提出了挑战。<br /></li><li><strong>早期的专家系统通常由一个知识库和一个推理引擎构成。一旦运作起来，许多成功的专家系统将被用于培训新手，或者为解决某些特定的问题提供建议或提示。</strong><br /></li><li>我们将考虑人类所擅长的事情以及我们自身的局限性；我们也会用同样的方式来考虑机器。<br /></li><li>将感知到的内容进行“理解”，这一方面，计算机仍然落后于人类。</li></ul><p><img src="/img/社会机器.png" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
            <tag> 人工职能 </tag>
            
            <tag> 模式识别 </tag>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《变革之心》读书心得</title>
      <link href="/2019/06/09/%E3%80%8A%E5%8F%98%E9%9D%A9%E4%B9%8B%E5%BF%83%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2019/06/09/%E3%80%8A%E5%8F%98%E9%9D%A9%E4%B9%8B%E5%BF%83%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p><img src="/img/八大步骤.png" /><br /><img src="/img/变革模式.png" /><br /><img src="/img/阻碍变革四种行为.png" /><br /><img src="/img/投资规划的三种心态.png" /><br /><a href="/img/变革之心.xmind">变革之心笔记脑图</a><br /><span id="more"></span></p><h1 id="愿景方案规划">愿景方案规划</h1><p>方案通过对应的几个主要的维护来描绘。<br /><img src="/img/变革之心2.jpeg" /></p><p>如果说预算是一项数字练习的话，规划就是一个逻辑推理的线性过程。战略人员需要掌握大量关于客户和竞争对手的信息，同时，还需要具有很高的概念化能力。而确定愿景则需要一种完全不同的能力，它需要愿景的确定者能够详细的预见到可能的未来，因此就不可避免地包含了创造性和情感性的成分。<br />愿景：1页纸；<br />战略：10页纸<br />规划：笔记本<br />预算：更大的笔记本</p><h1 id="八大步骤">八大步骤</h1><p>增强紧迫感、建立指导团队、确立变革愿景、有效沟通愿景、授权行动、创造短期成效、不要放松、巩固变更成果。<br />先是建立指导团队，再确立变革愿景，和我们通常理解的不同。</p><h1 id="文章摘要">文章摘要</h1><ul><li>在这个纷纷提倡削减开支的时代，为什么仍需要给执行官们配备一间豪华办公室。<br /></li><li>在当今的时代里，我们每一天都必须讨论变革。 ---- 杰克·韦尔奇<br /></li><li>把客户放到核心的位置<br /></li><li><strong>你必须在扑灭大火的同事，消除那些可能再次引起火灾的隐患。</strong><br /></li><li>苦心婆口的说服毫无意义，只有火烧眉毛的时候，大家才能意识到变革的必要性；<br /></li><li>没有一次大型的变革，是把恐惧作为一种首要而持久的推动力而获得成功的。紧迫感能维持企业变革的动力；<br /></li><li><strong>过多的恐惧会使人们将精力放在自我保护上，而不再去考虑整个组织的利益。</strong><br /></li><li>是否存在别的组织正在充分利用，而你的组织却忽视了的技术断层。<br /></li><li>个人感想：管理首要的是思考人性的问题！<br />管理从人来出发，社区也是需要考虑人，以人为本。<br /></li><li>如果一个人一辈子只是在小心翼翼的明哲保身，那样的生命还有什么意义呢？<br />坦诚表达自己的想法；坦诚的对话、互相信任。<br /></li><li>人就是如此，当观点相同的人聚集在一起时，他们原来的观点就会进一步得到加强。<br /></li><li>行不通的方式：由于担心遭到反对，所以在开展工作的时候忽视或绕过部门领导。<br /></li><li>以降低成本、简化组织或提高效率为愿景的变革，根本不可能从一开始就收到欢迎。但通过提高客户服务水平的视角出发，就能通过优化客户服务方式间接达到降本增效的目的。<br />如果一个愿景在一个人数庞大的群体中引起愤怒和恐慌，那么，它就根本不可能实现。<br />通过正向的激励方式，引导大家正向的成就感。<br /></li><li>关注速度，通过速度来克服组织的惰性，避免大家熟视无睹，被同化的危险。<br /></li><li><strong>人类历史很好地证明了成语的影响和圣经的力量：绝对不要低估一个好故事的力量。</strong><br /></li><li>变革，需要考虑可能对人们产生怎样的影响。 可以通过角色扮演的方式来思考。<br />在面对人们的提问时，需要正确的应对他们的情绪；<br /><strong>自信往往是最关键的因素。<br />自己要充满信心，积极乐观：可以思考3分钟之后再回答，并且要慢条斯理的回答；回答的时候简单、明白、精确就行了，不需要长篇大论。</strong><br />从高维度思考，进行降维攻击。<br /></li><li>“成为第一”这一口号产生的激励作用几乎是无限的。<br /></li><li>世界范围内的竞争。<br /></li><li>我们知道，一旦人们发现你在某一点上说了假话，他们就很难相信你所传达的其他任何信息。<br />我们的原则是：千万不要撒谎，但你一定要对自己发送的信息保持乐观态度。<br />任何的夸张都是要不得的。<br /></li><li>站在老板的角度思考、站在投资人的角度思考<br /></li><li>你必须在内心有一种真正的紧迫感，意识到，我们的潜力还没有完全发挥出来，我们需要做的事情还很多，应该更加努力。<br /></li><li>在资本收益率中，我们需要做各行各业的比较，而不仅限于本行业。<br /></li><li>授权行动<br /></li><li>找到一些有变革经验的人，他们可以提高人们的自信心；<br /></li><li>建立适当的奖励系统，鼓励和提高人们的乐观心理，进而在他们心目中建立必要的自信；<br /></li><li>收集那些能够帮助人们做出更好变现，而且与愿景相关的反馈；<br /></li><li>通过调换工作岗位的方式（从而让他们意识到进行变革的必要性）来“改变”那些消极的经理。<br /></li><li>幽默能够打消人们的防御心理。<br /></li><li>在变革过程中，文化是最后而非最先需要考虑的问题。<br />实际上，只有当一种新的运作方式经过一段时间的验证，被证明很成功，公司的文化才有可能随之而变化。在改变运作方式之前就试图改变公司的价值标准是根本行不通的。<br /></li><li>环境的变化是永无止境的。<br /></li><li>榜样不是影响其他事物的主要因素，而是唯一因素。 阿尔伯特·史怀哲<br /></li><li>大脑中负责复杂分析的部分在革新方面比较迟钝。感官接受的刺激会直接反映到情绪，再由情绪命令人的行动。</li></ul><p><img src="/img/变革之心1.jpeg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 变革 </tag>
            
            <tag> 目睹 </tag>
            
            <tag> 感受 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《知行合一（实现价值驱动的敏捷和精益开发）》读书心得</title>
      <link href="/2019/05/26/%E3%80%8A%E7%9F%A5%E8%A1%8C%E5%90%88%E4%B8%80%EF%BC%88%E5%AE%9E%E7%8E%B0%E4%BB%B7%E5%80%BC%E9%A9%B1%E5%8A%A8%E7%9A%84%E6%95%8F%E6%8D%B7%E5%92%8C%E7%B2%BE%E7%9B%8A%E5%BC%80%E5%8F%91%EF%BC%89%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2019/05/26/%E3%80%8A%E7%9F%A5%E8%A1%8C%E5%90%88%E4%B8%80%EF%BC%88%E5%AE%9E%E7%8E%B0%E4%BB%B7%E5%80%BC%E9%A9%B1%E5%8A%A8%E7%9A%84%E6%95%8F%E6%8D%B7%E5%92%8C%E7%B2%BE%E7%9B%8A%E5%BC%80%E5%8F%91%EF%BC%89%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<h1 id="敏捷开发宣言">敏捷开发宣言</h1><p><img src="/img/知行合一5.jpg" /></p><h1 id="敏捷开发框架">敏捷开发框架</h1><p>本书描述中的敏捷管理活动多以Scrum为核心，而工程活动则多以极限编程的方法为主。<br /><img src="/img/Scrum.png" /><br /><img src="/img/进入迭代前的准备共工作.png" /></p><span id="more"></span><h1 id="项目管理铁三角">项目管理铁三角</h1><p>著名的项目铁三角：成本、进度、需求范围；<br /><img src="/img/知行合一2.jpg" /><br />新的项目管理铁三角（敏捷铁三角）：价值驱动<br />价值目标、质量目标、约束目标（成本、进度等）<br />新的项目铁三角要求我们用投资回报分析（return on investment,ROI)作为管理者的决策方式；<br /><img src="/img/知行合一3.jpg" /></p><h1 id="需求和技术的不确定性">需求和技术的不确定性</h1><p>需求的自然进化：客户对自己真正需求的产品需要一个认识过程，这个过程将产品的愿景（vision）逐步细化到产品特性。需求的自然进化。<br />需求蔓延：客户单方面随意追加及需求，不做必要的价值分析，不考虑实施成本；<br /><img src="/img/知行合一4.jpg" title="客户需求的不确定性、技术平台的不确定性" /></p><h1 id="迭代">迭代</h1><p>敏捷开发的两个重要特征是迭代开发和需求特性驱动管理；我相信这句话：看到错误的才知道什么是正确；<br />迭代的方式：小步快跑，在开发中学习、成长、调整和完善；<br />1、快速磨合<br />迭代也是团队从项目开始就有不断磨合的机会；而瀑布方式，只有在项目结束时，整个团队不同只能小组才有一次完整的配合机会；<br />2、快速反馈</p><p>持续集成、构建、自动化测试、自动化部署。（Jenkins）<br />每次构建都可以生成可发布的代码，修复失败的构建是优先级最高的事情。<br />80%的测试都是小而快的单元测试；凡是可以自动测的都自动测，手工测试仅覆盖需要人观察接入的测试（例如用户体验）。</p><h1 id="技术债务">技术债务</h1><p>技术债务：质量目标不仅是科发布（功能正确），同时也是可维护的；技术债务可以看作是设计、开发不足的累积总和，技术债务的管理是决定敏捷成败的一个重要因素，带病迭代是敏捷第一杀手。<br />盲目的砍掉必要的文档工作会增加技术债务；文档的目的是为了方便沟通、方便运维以及支持沉淀；<br />有些技术债务也是可以的，毕竟它能够加快开发速度。就像我们用信用卡借钱花一样，有时候我们确实需要超前消费。。关键是要几十还款，否则利息会压垮我们。</p><h1 id="精益文化">精益文化</h1><p>lean精益文化：流程可视化、持续优化、减少浪费（减少前置时间 lead time）、限制在制品（WIP work in processs）、消除瓶颈、拉动计划系统。</p><h1 id="管理变革">管理变革</h1><p>著名的成功变革管理的五要素及其某一要素缺失时的后果：<br /><img src="/img/知行合一8.jpg" /></p><h1 id="问题">问题</h1><p> <font color="#4590a3" size = "3px">内部团队开发人员容易做敏捷开发，但是外部公司因为商务原因还是比较困难，需求不明确，成本合同额难以定下来。<br /></font></p><h1 id="用户故事">用户故事</h1><p><a href="https://www.jianshu.com/p/2ca3e172755b">用户故事 扑克牌估算法</a></p><h1 id="文章摘要">文章摘要</h1><ul><li>极限编程（extreme），意思就是如果某个实践好，那就将其做到极限。例如，如果集成测试重要，那我们每天都做几次测试和集成（持续集成）<br /></li><li>响应变化高于遵循计划：变化是软件开发过程中的一个常态：客户对业务领域的理解加深会导致变更；商业环境的变化会导致变更；技术的变化也会导致变更；人员的变动也会带来变更。<br /></li><li>软件开发的软、易变、非线性增长复杂度； 制造业的生产过程高度重复，而<strong>软件开发中的不确定性</strong>，导致了过程重复是有限的。任何两个项目都不完全一样，不可能走过同样的步骤，这个差异是“明确定义的过程”不适用于复杂的项目管理类的重要原因<br /></li><li>需求蔓延（requirement creep)是软件开发中最常见的风险之一；<br /></li><li><strong>如何度量价值不是件容易的事，我们可以从这几方面来考虑：产品的销售额、对品牌及竞争力的影响、通过创新给企业带来的新的机会；</strong><br />创新有风险，但也给企业带来新的机会。<br /></li><li>敏捷开发有点类似美国电视剧的制作模式，电视剧是一集一集的拍，一集一集的播；根据观众的反馈进行调整；或者取消拍摄止损。<br /></li><li>信息流失（药方抄3遍毒死人）：5个人参加的电视节目，第一个人做动作给第二个人看，然后第二个人努力将看到的做给第三个人看，直到最后一个人做给观众看。往往最后一个人展示的动作已经和第一个人的原始动作相差很大了。<br /></li><li>不存在完美的产品，从商业角度来说，这其实是一个好事，不完美意味着机会，它能给我们带来商机。<br /></li><li>正确的可执行代码是项目进展状况最准确的度量；<br /></li><li><strong>管理者应该是领导者，为团队定目标期望，而不是天天告诉团队如何做具体工作； 管理者的勇气，是做有远见的智慧型领导者。</strong><br /></li><li>让团队同时做多个项目，不会提升团队效率，这样的安排实际是效率杀手。<br /></li><li>瀑布模式解决进度问题的方法是砍掉任务（通常是测试任务），而敏捷解决进度的方法是减少用户需求特性（低价值的需求）；前者牺牲的是质量，而后者砍掉了价值最低的用户需求；<br /></li><li>一鸟入林，百鸟无声；<br /></li><li>成就感让大家变得更主动；<br /></li><li>CCB 变更控制委员会<br />PB product backlog 产品需求列表<br />BS Business Specification 业务需求书<br />FS Functional Specification 功能需求书<br /></li><li>使团队成为真正的一个拳头而不是5个分开的指头。<br /></li><li>出了问题不要责难，而是要鼓励；<br /></li><li>如果发现自己不能解决一个问题，要有勇气不耻下问，寻求帮助。<br /></li><li><strong>从I型人才到T型人才，复合型人才；</strong><br /></li><li><strong>谁报告坏消息，谁就要被处罚，结果是没人愿意报告坏消息了</strong><br /></li><li><strong>看过几期《非常勿扰》，如果男嘉宾是软件工程师，基本的结果都是最后遭遇全部灭灯。（加班太多）</strong><br /></li><li>华为是最有勇气的IT企业，它把变革当作学习的机会、超越竞争对手的机会、发展壮大的机会。<br /></li><li>Scrum鼓励的是团队英雄行为，奖金的分发会更大程度上基于团队的表现，如果一个团队有优异的表现，那就请鼓励团队的每一个成员。<br /></li><li>学会从全局看问题，学会平衡短、中、长期利益。<br /></li><li><strong>IT的价值观：支持组织的业务目标及客户的期望。</strong><br /></li><li>合理不合法：合理说的是项目组被逼做出必要的裁剪（不一定是合适的），不合法讲的是实际的过程是不符合组织过程要求的。<br /></li><li>Donald Reinertsen:每个成功的产品都会有一个简单清晰的价值定位（value proposition)，消费者从众多竞争产品做选择时，往往只会根据三四个因素。<br /></li><li><strong>电梯陈述法：你必须能够用1-2分钟的时间（也就是从进入电梯开始到电梯落地），向一个高层讲清楚你的产品的价值定位。<br />产品名称、价值、差异（优势）<br />例如：牙医预约系统：为了帮助牙医及其助手能够方便地提前预约病人治疗时间，我们开发出牙医预约系统，它是个能够支持办公室电脑或者通过互联网预约地软件系统，和其他所有系统比，牙医预约系统非常简单易用。</strong><br /></li><li>心平气和对待冲突，不要害怕冲突；坚持原则。<br /></li><li>不要把迭代回顾会议变成一个抱怨会议，如果识别出需要改进的都是团队外的人和事，这样的会议会变成没有价值的会议，很可能是在浪费时间。<br /></li><li>用户故事：作为“角色”，我希望完成“活动”，这样可以获取“业务价值”<br />我们需要从正常业务流程及异常处理两个角度挖掘用户故事，避免遗漏。<br /></li><li>架构设计<br />架构设计的主要价值在于，它对系统核心基础设施做一序列关键决策：哪里需要泛化？要使用分层模式吗？如果使用，每一层的职责是什么？每一层包含哪些模块以及为什么要创建这些模块？如果在层和组件之间之间划分系统的职责？如何将模块进行大规模部署？信息如何在模块之间以及系统与外围系统之间流转？<br /></li><li>跑马拉松比赛，没有人像跑百米一样地冲刺，否则你一定会倒在中途。让一个团队连续几个月天天加班，也很难开发出高质量地产品。软件开发主要是脑力活动，像我每天集中精力有效工作时间也就6各小时，过了这个界限，脑子就明显钝化了。<br />软件开发的节奏意味着阶段、活动的可预测性：固定的迭代周期、固定的回顾会、规定的评审会等，都会帮助团队形成自己的开发节奏。<br /></li><li>成功是团队的成功，任何人不能由于团队的失败而受益。<br /></li><li>收集信息（审查）的目的是为了根据信息完成必要的工作（调整）<br /></li><li><strong>例会的真正结果应该是确定会后要开哪些后续会议；例会上不要做深入的讨论，识别问题但不在例会上解决问题，例会后的会议才是问题解决的时机。</strong><br /></li><li>种子团队<br /></li><li>scrum中的共享团队资源<br />1、公共模块团队<br />2、架构团队<br />3、测试团队（测试资源紧张的情况下）<br />4、维护团队（缺陷修复、紧急功能实现等）<br /></li><li>统计数据显示37%的软件开发问题和需求有关，其中13%源于无效的用户需求收集，12%源于需求遗漏，12%源于需求变更。需求问题带来的返工可能占整个返工的75%-85%<br /></li><li>Ron Radice关于技术评审的书名就叫“高质量低成本”，因为数据显示和测试相比，技术评审是一个高性价比的质量控制活动。<br /></li><li>开发过程中发现的问题不应该算作缺陷，修复这些问题本来就是要做的事情。团队应该重点分析客户所报的缺陷。<br /></li><li>导入敏捷<br /></li><li>很多所谓的评估项目很可能是花瓶项目。<br />类似体验，目前不是得到体验报告，关键是要根据这个报告进行调整和治疗；<br />许多软件项目都处于2级迷茫，这也是许多软件项目往往需要追加成本的原因。<br /></li><li>没有改进（改动）过的过程，很有可能是没有在项目中真正被使用。<br /></li><li>QA票是任务书的补充文档。<br /></li><li>软件开发过程碰到的4级迷茫<ul><li>0级迷茫：开发过程需要做的一切都很清楚，没有迷茫之处；<br /></li><li>1级迷茫：很清楚地直到自己有一些不明确之处；<br /></li><li>2级迷茫：没有意识到自己有不清楚之处；<br /></li><li>3级迷茫：没有掌握有效的方法以发现自己不清楚之处<br /></li></ul></li><li>他说自己做得好的一点是讲清楚了许多人觉得理应如此的实践。<br /></li><li>看板的实际意思是信号卡，看板代表其精益生产体系中的信号系统，精益拉动计划系统。<br /></li><li>在维护好客户关系的前提下，客户不付钱的事少做，客户付钱的事情多做； 投资回报的角度来考虑。</li></ul><p><img src="/img/知行合一1.jpg" /> <img src="/img/知行合一6.jpg" /> <img src="/img/知行合一7.jpg" /> <img src="/img/知行合一9.jpg" /> <img src="/img/知行合一10.jpg" /> <img src="/img/知行合一11.jpg" /> <img src="/img/知行合一12.jpg" /> <img src="/img/知行合一13.jpg" /> <img src="/img/知行合一14.jpg" /> <img src="/img/知行合一15.jpg" /> <img src="/img/知行合一16.jpg" /> <img src="/img/知行合一17.jpg" /> <img src="/img/知行合一18.jpg" /> <img src="/img/知行合一19.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 敏捷开发 </tag>
            
            <tag> 知行合一 </tag>
            
            <tag> 精益开发 </tag>
            
            <tag> 价值 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《悲剧的诞生》读书心得</title>
      <link href="/2019/05/11/%E3%80%8A%E6%82%B2%E5%89%A7%E7%9A%84%E8%AF%9E%E7%94%9F%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2019/05/11/%E3%80%8A%E6%82%B2%E5%89%A7%E7%9A%84%E8%AF%9E%E7%94%9F%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>《悲剧的诞生》表面上是研究希腊艺术，实际上是借题发挥尼采自己的人生哲学。<br />在书中，尼采用希腊神话中的酒神狄奥尼索斯（Dionysus）和日神阿波罗象征人生的两种基本艺术冲动，阐述了艺术拯救人生的主张，提倡一种审美的人生态度，并且以苏格拉底为靶子批判了科学理性主义世界观，在他看来苏格拉底是此种世界观的始作俑者。以这种方式阐述古希腊文献，无疑是与古典语文学传统的决裂。<br /><span id="more"></span><br />艺术形而上学是由日神精神和酒神精神组成，日神和酒神是作为人生的两位救世主登上尼采的美学舞台的。<br />日神：美的外观 幻觉 梦<br />酒神：情绪放纵 本体 醉 悲剧性情绪 复归自然的体验 痛苦和狂喜交织 （类似李白醉后作诗）<br /><img src="/img/悲剧的诞生2.jpeg" /><br /><img src="/img/悲剧的诞生3.jpeg" /><br /><img src="/img/悲剧的诞生4.jpeg" /><br /><img src="/img/悲剧的诞生5.jpeg" /></p><p>悲观主义者看来，世界是无意义的，人生是无意义的；尼采认为需要通过艺术的谎言来赋予人生的意义。</p><p>日神精神沉湎于外观的幻觉，反对追究本体，酒神精神却要破除外观的幻觉，与本体沟通融合。前者用美的面纱遮盖人生的悲剧面目，后者揭开面纱，直视人生悲剧。前者教人不放弃人生的欢乐，后者教人不回避人生的痛苦。前者执着人生，后者超脱人生。前者迷恋瞬时，后者向往永恒。与日神精神相比，酒神精神更具形而上学性质，具有浓郁的悲剧色彩。<br />外观的幻觉一旦破除，世界和人生露出了可怕的真相，如果再肯定人生呢？这正是酒神要解决的问题。</p><p>一个人在顺利的时候，就像是日神状态，大家享受日神的幻觉（因为这非人生的悲剧本质）；挫折的时候，幻觉破灭，人往往放纵（例如借酒消愁），回归人生的痛苦本质，通过放纵的痛苦解除一切痛苦的根源；<br />就像不如意事常八九，可以语人无二三，悲剧才是人生的本质。</p><p>酒神精神的潜台词就是：就算人生是幕悲剧，我们也要有声有色地演这幕悲剧，不要失掉了悲剧的壮丽和快慰。这就是尼采所提倡的审美人生态度的真实含义。<br /><strong>以强大的生命力对抗人生悲剧，赋予人生以意义。</strong></p><p>不管现象如何变化，事物基础中的生命仍是坚不可摧和充满欢乐的。</p><p>苏格拉底命题的结论：“知识即美德”“罪恶仅仅源于无知”“有德者即幸福者”<br />科学精神实质上是功利主义，它旨在人类物质利益的增值，浮在人生的表面，回避人生的根本问题。尼采认为，科学精神是一种浅薄的乐观主义，避而不见人生悲剧面目，因而与悲剧世界观正相反。科学精神恶性发展的后果，便是现代人丧失人生根基，灵魂空虚，无家可归，惶惶不可终日。</p><h1 id="文章摘要">文章摘要</h1><ul><li>没有音乐的生活简直是一个错误、一种灾难，一次流放。<br /></li><li>自幼折磨着他的生命之谜一直萦绕在他心中，他的使命乃是解开这个谜，为人生寻求一种真实的意义。<br /></li><li>感受一种被理解的快乐。<br /></li><li>作为一个哲学家，必须摆脱职业、女人、孩子、祖国、信仰等而获得自由。<br /></li><li>与瓦格纳的交往是他一生唯一的一次幸遇，其余的交往不过是可有可无的“零头”罢了。<br /></li><li>孤独是缺乏爱和被爱的呈现。 ... <strong>孤独使他发疯，终于在疯狂中摆脱了孤独。</strong><br /></li><li>尼采一反传统，认为希腊艺术的繁荣不是缘于希腊人内心的和谐，反倒是缘于他们内心的痛苦和冲突，因为过于看清人生的悲剧性质，所以产生日神和酒神两种艺术冲动，要用艺术来拯救人生。<br /></li><li><strong>不如意事常八九，可与语人无二三</strong><br />最早语出《晋书·羊祜传》，其他出处：南宋词人辛弃疾词《贺新郎·用前韵再赋》：”叹人生，不如意事，十常八九。“辛弃疾之后，有 宋人方岳诗：“不如意事常八九，可与语人无二三。”（《别子才司令》） [1] 。元明以来戏曲和小说里常见的“不如意事常八九，可与语人无二三”这一联，就化用了方岳的诗句。<br /></li><li>重估一切价值，重点在批判基督道德。基督教对生命做伦理评价，视生命本能为罪恶，其结果是造成普通的罪恶感和自我压抑。审美的人生要求我们摆脱这种罪恶感，超于善恶之外，亨受心灵的自由和生命的欢乐。<br /></li><li>每个人在创造梦境方面都是完全的艺术家，而梦境的美丽外观是一切造型艺术的前提。<br /></li><li>一个在大自然怀抱中受教育的艺术家爱弥儿。<br /></li><li>在艺术中，音乐是纯粹的酒神艺术，悲剧和抒情诗求诸日神的形式，但在本质上也是酒神艺术，是世界本体情绪的表露。<br /></li><li>瓦格纳在论及文明时说，音乐使之黯然失色，犹如日光使烛火黯然失色。<br /></li><li>对于真正的诗人来说，借喻不是修辞手段，而是取代某一观念真实浮现在他面前的形象。<br /></li><li>鲁迅《药》中“分明有一圈红白的花，围着那尖圆的坟顶。 ”有什么寓意？<br />《呐喊》序言中说的很明白，鲁迅自称是听将令的，也就是说不能太悲观，所以在小说中运用了曲笔，平白加上了一个花圈，意味着革命后继有人，不是所有民众都毫无觉悟的。<br /></li><li>艺术家的生成之快乐，反抗一切灾难的艺术创作之喜悦，毋宁说只是<strong>倒映在黑暗苦海上的一片灿烂的云天幻境</strong>罢了。<br /></li><li>积极的罪行当做普罗米修斯的真正德行这种崇高的见解；<br />类似善意的谎言。</li></ul><p><img src="/img/悲剧的诞生1.jpeg" /><br /><img src="/img/悲剧的诞生6.jpeg" /><br /><img src="/img/悲剧的诞生7.jpeg" /><br /><img src="/img/悲剧的诞生8.jpeg" /><br /><img src="/img/悲剧的诞生9.jpeg" /><br /><img src="/img/悲剧的诞生10.jpeg" /><br /><img src="/img/悲剧的诞生11.jpeg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 哲学 </tag>
            
            <tag> 尼采 </tag>
            
            <tag> 悲剧的诞生 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《马云：未来已来》读书心得</title>
      <link href="/2019/04/06/%E3%80%8A%E9%A9%AC%E4%BA%91%EF%BC%9A%E6%9C%AA%E6%9D%A5%E5%B7%B2%E6%9D%A5%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2019/04/06/%E3%80%8A%E9%A9%AC%E4%BA%91%EF%BC%9A%E6%9C%AA%E6%9D%A5%E5%B7%B2%E6%9D%A5%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>马云说了，他干活干不过年轻人，所以他就负责吹牛皮。不过吹着吹着，吹出了哲理来。不过话说回来，讲话有哲理的人，一般是吃苦吃得比较多的人。回头看看马云的经历，除了我们能看到的光鲜之外，背后的磨难、失败、失意、迷茫、痛苦、失望更是数不胜数。马云说过：“我不认为世界上有很多人被拒绝过30次（注：马云自嘲因为丑，多次应聘被拒绝，并且多次都是唯一被拒绝的那个）。我所有的只是坚持，像阿甘一样，我不抱怨，无论成功还是失败。如果一个人失败后老是抱怨别人，那么这个人永远不会成功；如果他一直反省自己，那么他就有希望。”<br />阿里现在是一个伟大的技术公司，一直在改变着世界。但是，真正改变世界的不是这些伟大的技术，而是阿里改变世界的梦想。他们20年如一日，20年一直不忘初心的为了实现让天下没有难做的生意的使命而奋斗着，为了帮助中小企业而努力着。<br />马云根据自己创业的经验，和年轻人分享了自己的心得：第一，要乐观地看待未来；第二，少一些抱怨，认真检查自己的问题；第三，要有超越常人的坚持。没有这些素质，你是走不远的。</p><p>马云在书中提到了老子对孔子说的关于以柔克刚的故事，补充如下：</p><blockquote><p>传说孔子曾带领一班学生找老子请教，老子很老了，正在闭目养神，大概听到了响动，抬起眼皮看了看，孔子于是非常恭敬的请安说：“弟子孔丘特来候教。”过了很长时间，老子才张开嘴，用手指着自己的嘴问：“你看我的牙齿怎么样？”孔子说：“已经掉了，又问：“那我的舌头呢？”孔子说：“还好。”老子又合上眼皮，静养去了。</p></blockquote><span id="more"></span><h1 id="关于梦想">关于梦想</h1><p>一个人心有多大，就能做多大的事情。改变世界的不仅是技术，更是你相信自己可以改变世界的梦想。</p><ul><li>相信自己正在做的事情是对的，不管人们是否喜欢。</li></ul><h1 id="关于创业">关于创业</h1><blockquote><p>职业经理人跟企业家的差别就好比大家都上山打野猪。如果职业经理人开枪以后，野猪没有被打死，冲了过来，职业经理人会把枪一扔就跑了；而如果是企业家，看到野猪冲过来，他会拿起柴刀冲上去。</p></blockquote><p>在中国，当人们抱怨的时候，机会就出现了。处理人们的不满，解决存在的问题，这就是我们的机会。我们不是为了创业而创业，而是为了解决某个问题而创业，不能本末倒置。<br />并且，做企业需要巨大的乐趣驱动，你做了你热爱的事情，做了你所相信的事情，你把相信你的人，相信你做的这件事情的人聚集在一起，你才有机会，你才有能量。所以，阿里最主要的财富就是有一批有理想主义情怀的员工，他们满怀激情的为了自己的理想拼搏着。<br />一个优秀的企业家一定是善于倾听，并且仍然能够拥有自己想法的人。企业家和经济学家之间的差异就是在这儿：经济学家讲完了，这件事情就结束了；企业家讲完了，事情才刚刚开始。所以，企业家要做的是知行合一。知很容易，行也很容易，但是要合在一起，非常难。</p><blockquote><p>当年，毛泽东在延安做了三件非常重要的事情：第一是建立抗日军政大学，用以培养干部；第二是开展延安整风运动，以统一价值观和使命感，重塑理想主义；第三是南泥湾开发，要先活下来。今天，在经济形势不好的情况下，企业也要思考这几个问题。</p></blockquote><ul><li>人性：所有成功的创业者情商都极高，对人性的问题把握得很好。<br /></li><li>模式：管理模式、组织模式、文化模式。<br /></li><li>如果把在美国做企业比作100米赛跑，那么在中国做企业就是越野赛。我们面临很多障碍，但这些障碍也是机会。<br /></li><li>做企业，有三个要素很重要，第一个是经营模式，第二个是产品，第三个管理；<br /></li><li><strong>想要走得快，那就一个人走；想要走得远，那就一群人一起走。要一群人一起走，就一定要有组织。</strong></li></ul><h1 id="关于战略">关于战略</h1><p>做事情要有前瞻性，需要考虑十年后需要什么，我们今天就开始做！今天的你是你10年前的想法和行动造就的，10年后的你是你今天的想法和行动造就的，做人如此，做企业也如此。所以企业家需要立足今天，也当思考十年之后的事。<br />所以要有战略，战略是基于对未来的判断，战略从愿景来，愿景从使命来。所以，使命、愿景、战略、组织、文化、人才这一整套体系的建设都要完善和强大，你才有可能做好。</p><ul><li>就我自己来说，通常我会预判三五年之后政府会做的事情，然后立刻开始行动并不断修正；三五年后，当政府开始号召大家一起行动的时候，我会选择退出。<br /></li><li>要思考你到底需要什么，你要什么，你可以放弃什么；<br /></li><li>未来30年，商业的模式是什么？跨界、小组织、快速文化创新；<br /></li><li>人工智能和生命科学是未来技术的两大前沿；</li></ul><h1 id="关于创新">关于创新</h1><p>创新就是使用独特、高效的方法去解决问题，未来的创新离不开数据，有了数据以后，创新、创意和创造才会层出不穷。</p><h1 id="关于管理">关于管理</h1><p>管理，简单说管是管人、管文化，理是理制度、理模式；管理就是借力，借助人的力量、资本的力量和知识的力量。</p><blockquote><p>关于增加人的问题：方法是想出来的。因为你把门堵住了，员工过就会想尽一切方法在技术上完善，在产品上完善，在制度上创新，否则他们做起来就很简单，加人、加原材料就是了，可那就乱套了。</p></blockquote><p><img src="/img/马云未来已来2.jpg" /></p><ul><li>没有KPI考核，理想就变成空想<br /></li><li>居安思危：要在阳光灿烂的日子修屋顶<br /></li><li>危机关头，能熬过去的企业才有“抗体”<br /></li><li><strong>决定企业成功与否的，不是规模多大、速度多块，而是产品有多好，是否有良好的消费者体验。</strong><br /></li><li>转型一定是有代价的，就像拔牙一样，一定会痛；<br /></li><li><strong>小企业考虑的是要用什么样的人，而大企业考虑的是要开掉的什么样的人。</strong></li></ul><h1 id="关于担当">关于担当</h1><p>高情商是怎么来的？磨难、失败、失意、迷茫、痛苦、失望，所有这些凑在一起，就能造就高情商。勇气也很重要，你有没有勇气，敢不敢担当？如果你愿意为自己担当，你就是普通人；<strong>如果你愿意为5个人担当，你可能就是一个团队的领头人，如果你愿意为13亿人担当，你就是国家领导人。</strong><br />一把手一定要有担当。什么叫担当？就是能承担责任，出现问题的时候，对不起，我来担，这是我的问题。</p><h1 id="关于挫折">关于挫折</h1><p>知识让人聪明，聪明的人知道自己需要什么；经历挫折让人更具智慧，智慧的人才知道什么时候去放弃；<strong>一个人只有经过生活的磨练、生活的挫折，学校教育教给你的东西才能真正变成你自己的知识。</strong><br />就像阿甘说的，人生就像一盒巧克力，你永远不知道下一颗会是什么滋味。<strong>所以我对未来是充满信心的，但对今天、明天，我胆小谨慎，如履薄冰。今天很残酷、明天更残酷，后天很美好。</strong></p><h1 id="文章摘要">文章摘要</h1><ul><li><strong>其实压力不可怕，可怕的是诱惑。</strong><br /></li><li><strong>想一想你来这家公司应聘时的理想，以及那时候你对公司的承诺，这家公司要的不仅仅是承诺，还有对承诺的坚持。</strong><br /></li><li><strong>公益的本质是唤醒善良。</strong><br /></li><li><strong>每位老师都有机会点燃别人心中的灯。</strong><br /></li><li>看书很重要，但是看世界更重要，读万卷书也要行万里路；<br /></li><li>你去星巴克并非仅仅为了品尝它的咖啡是多么美味，那也是一种生活方式。<br /></li><li>习惯于被安排好人生的人，不可能会成功。<br /></li><li>平台的思想就是，让别人越来越强大，去服务别人。<br /><strong>基础架构就像做大平台，服务别人，让别人更强大。</strong><br /></li><li><strong>今天网上的商品交易，背后的本质是信任关系在流转。</strong><br /></li><li><strong>沟通是消除误解的唯一钥匙。</strong><br />我认为贸易是最好的沟通方式，是承载文化最好的方式，可以让我们尊重和理解不同的文化；<br /></li><li>我认为“一带一路”更重要的是东西文化的交流。只有文化的交流、思想的交流，才有可能带来经济的交流。<br /></li><li>聪明是看见别人没看见的东西，智慧是看见了也当没看见。<br /></li><li>我给大家一个建议：永远相信你的对手不在你边上，在你边上的，都是你的榜样，哪怕这个人你特讨厌。<br /></li><li>要有抗击打能力，既不要盲目乐观，也不要盲目悲观。<br /></li><li>如果地球病了，没有人会健康。<br /></li><li>中国应该注意经济增长质量，就像人长大，不应该只长身体，也要提升内在修养、文化素质和智慧。<br /></li><li>政府考虑的是如何把事情做得更为公平，科学家是考虑如何把事情做得更为准确，而企业家考虑的是如何把事情做得更为有效。<br /></li><li>如果说我跟别人哪里不一样的话，就是我观察问题的角度和别人不一样，看问题的深度和别人不一样。<br />如果一个人经常有好运气的话，那他背后一定有很多东西是你没发现的。<br /></li><li>五新<br />新零售：线上线下融合 （鼠标+水泥）<br />新制造：智能化、个性化 （不仅仅是由石油和店里驱动，还要靠数据来支撑。）<br />新金融：信用体系、普惠的<br />新技术：AI<br />新能源：数据 （大数据要赋予技术以灵魂）</li></ul><p><img src="/img/马云未来已来1.jpg" /><br /><img src="/img/马云未来已来3.jpg" /><br /><img src="/img/马云未来已来4.jpg" /><br /><img src="/img/马云未来已来5.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 哲学 </tag>
            
            <tag> 马云 </tag>
            
            <tag> 讲话 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《商战》读书心得</title>
      <link href="/2019/03/17/%E3%80%8A%E5%95%86%E6%88%98%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2019/03/17/%E3%80%8A%E5%95%86%E6%88%98%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p><img src="/img/《商战》脑图.png" title="《商战》脑图" /><br /><a href="/img/《商战》读书笔记--脑图.xmind">《商战》脑图下载</a><br /><span id="more"></span></p><h1 id="文章摘要">文章摘要</h1><ul><li>武器可以日益先进，但是战争本身基于两个亘古不变的特征：战略和战术；<br /></li><li>战场是在：顾客心智<br /></li><li>品牌延伸陷阱<br /></li><li>泰勒 《科学管理原理》 分工<br />德鲁克 知识工作者<br />农业社会-&gt;工业社会-&gt;知识社会<br /></li><li>哑铃型社会 橄榄型社会<br /></li><li>组织存在的目的，不在于组织本身，而在于组织之外的社会成果<br /></li><li>品牌：代表企业或服务的符号 （标签） 通过广告占领用户心智（注意力经济）； 独特的价值、差异化的能力；<br /></li><li>历史充满了偶然，历史的前进更往往是在悲剧中前行；<br /></li><li><strong>不要试图改变别人的心智：厄运在山顶聚集以等待世界末日的教徒们，次日的到来不会动摇他们的信念。第二天下山时，他们对全能的上帝的慈悲更加深信不疑。</strong><br /></li><li>每家企业都有三类产品：一种是打广告的产品，另一种是做销量的产品，还有一种是赚钱的产品。仅仅因为某个产品卖得出去或者能赚钱，甚至能赚大钱，就为它打广告，是一种浪费。<br /></li><li>在战争中，出于仁慈之心而犯的错误是最严重的。<br />仁不带兵，义不行贾（gǔ）；<br />商贾 本义:做买卖<br /></li><li>判断一个人在企业是否处于“作战一线”很容易。假如他因为完不成营销目标而被解雇，那么他就处于这条线之内；假如他可以因为某人没有完成营销目标而有权解雇此人，那么他就处在这条线之外。<br /></li><li>优秀的将领拥有无限的勇气，会直面上级和同事提出的不同方案。<br /></li><li>思维方式：（立体的思维方式） 内、外；短期、长期；正面、反面；</li></ul><p><img src="/img/商战2.jpeg" /><br /><img src="/img/商战3.jpeg" /><br /><img src="/img/商战4.jpeg" /><br /><img src="/img/商战1.jpeg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 商战 </tag>
            
            <tag> 战略 </tag>
            
            <tag> 战术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《杀死一只知更鸟》读书心得</title>
      <link href="/2019/02/10/%E3%80%8A%E6%9D%80%E6%AD%BB%E4%B8%80%E5%8F%AA%E7%9F%A5%E6%9B%B4%E9%B8%9F%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2019/02/10/%E3%80%8A%E6%9D%80%E6%AD%BB%E4%B8%80%E5%8F%AA%E7%9F%A5%E6%9B%B4%E9%B8%9F%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>在30年代，美国南部的萧条时期，作者哈珀·李（Happer Lee)以一个7岁孩童斯库特（Scout）以及一个成年女性的混合视角，以第一人称的方式，回顾了发生在那个特殊岁月的关于种族问题，通过她父亲阿提克斯（Atticus Finch）为一个黑人强奸疑犯的辩护，塑造了一个公平、正义、平等的律师形象，通过对子女的教育，一个慈爱、平和的绅士父亲形象跃然纸上。<br />就像书上所说，知更鸟只负责给我们歌唱，善良而又对人类无害，如果我们因为偏见、种族原因而杀死一个人，那和杀死一只知更鸟一样是罪恶的。<br />在陪审团裁决黑人罗宾逊（Robinson）罪名成立的时候，阿提克斯知道，在人们内心深处的那个秘密法庭里，从一开始就已经裁决了最终的结果。因为，在白人和黑人的诉讼当中，一直在循环着这样的一个结果。<br />阿提克斯说，我不知道，可是他们做了。他们以前做过，今天又做了，他们将来还会再做，当他们这样做的时候......好像只有孩子在哭泣。 <span id="more"></span></p><h1 id="文章摘要">文章摘要</h1><ul><li>罗斯福：除了恐惧本身，没有什么可恐惧的。</li><li>阅读就像一个人的呼吸，即使不喜欢也不能不做。</li><li>你永远不可能真正了解一个人，除非你从他的角度去看问题。</li><li>有时候，某个人手中的《圣经》，比鄙人手中的威士忌还要恶劣。</li><li>还有一些人，他们太当心来世了，以至于都没学会怎样在这个世界上好好活着。</li><li>我已经长大了，不应该再做那种孩子气的事，而且我越早学会克制自己，别人的日子就越好过。</li><li>抬起头、放下拳头，不管别人对你说什么，都不要发火，试着用头脑去抗争。... 昂起头来，做个绅士，不管她对你说什么，都不要生气。</li><li><strong>不能因为我们在此之前已经失败了一百年，就认为我们没有理由去争取胜利。</strong></li><li>关于耐心：人在狩猎时，最好不要着急。什么也不用说，猎物早晚会禁不住好奇心冒出来的。</li><li>如果说你们父亲有什么特殊的地方，那就是他那颗文明高贵的心。好枪法是上帝赋予人的礼物，是一种才能...我想，他也许意识到上帝给予了他一个对其它物种不公平的优势，于是就把抢放下了。</li><li><strong>有一种东西，不能遵循从众原则，那就是人的良心。</strong></li><li>她说，她要干干净净的离开这个世界，不欠任何人，不依赖任何东西。...她说她决意要在死前戒掉吗啡，那就是她所做的。</li><li><strong>我想让你见识一下什么是真正的勇敢，而不要错误的认为一个人手握枪支就是勇敢。勇敢是：当你还未开始就已知道自己会输，可是依然要去做，而且无论如何都要把它坚持到底。你很少能赢，但有时也会。 （知其不可而为之）</strong></li><li>优秀的人，就是指那些根据自己的见识尽力而为的人。</li><li><strong>在交叉讯问证人时，千万，千万，千万，不要去问一个你事先不知道答案的问题，这是我从吃奶时就领悟到的一个信条。如果你问了，常常会得到一个你不想要的答案，这个答案很可能会毁掉你的诉讼。</strong></li><li>如果我们一直跟着感觉走，那我们就像追着自己尾巴的猫一样。</li><li>他说话的时候，那是一种平静的自卫。</li></ul><p><img src="/img/杀死一只知更鸟2.jpg" /> <img src="/img/杀死一只知更鸟1.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 美国 </tag>
            
            <tag> 种族歧视 </tag>
            
            <tag> 公平 </tag>
            
            <tag> 平等 </tag>
            
            <tag> 正义 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《深度工作7步法》读书心得</title>
      <link href="/2019/02/10/%E3%80%8A%E6%B7%B1%E5%BA%A6%E5%B7%A5%E4%BD%9C7%E6%AD%A5%E6%B3%95%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2019/02/10/%E3%80%8A%E6%B7%B1%E5%BA%A6%E5%B7%A5%E4%BD%9C7%E6%AD%A5%E6%B3%95%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<h3 id="第1步-准备工作合理管理精力和时间">第1步 准备工作：合理管理精力和时间</h3><p>全景思维<br />留白有助创新</p><h3 id="第2步-已经开始如何坚持把重点从结果转到过程">第2步 已经开始，如何坚持：把重点从结果转到过程</h3><p>远大目标、勇于开始、关注过程</p><span id="more"></span><h3 id="第3步-坚持的过程中如何把握方向提出指引你前进的问题">第3步 坚持的过程中，如何把握方向：提出指引你前进的问题</h3><p>灯塔问题：不断提醒自己，什么是对你最重要的那个问题。</p><h3 id="第4步-深入过程中如何规避风险做好收益规划">第4步 深入过程中，如何规避风险：做好收益规划</h3><p>投资组合、股权</p><h3 id="第5步-初具规模如何做大分配角色设定进度">第5步 初具规模，如何做大：分配角色，设定进度</h3><p>项目经理，负责协调、管理，这样会让其他人有时间专注于工作本身。 （抬头看路、低头拉车； 分工有效提高工作效率的一个体现）<br />好的经理人的特质：<br />1、构想愿景和设定目标<br />2、重视个人<br />3、为团队工作创造抱持性环境</p><p>柳传志著名的管理三要素：搭班子、定战略、带队伍</p><h3 id="第6步-规模扩大构建商业模式成为企业艺术家">第6步 规模扩大，构建商业模式：成为企业艺术家</h3><p>产品+商业模式 类似 写信+信封<br />让你施展商业模式设计才华的另一项练习是混搭商业模式，看会发生什么。选取某领域的一种商业模式，把它运用到另外一个行业中，那将是怎样的情况呢？<br />跨界！</p><h3 id="第7步-反思培养跨学科思维的超通才">第7步 反思：培养跨学科思维的超通才</h3><p>商业和艺术的融合<br />将学生改造成能以领导者的思维方式思考的领导者，而不是独立思考的技术专家。<br />专注于抽象思维</p><h1 id="文章摘要">文章摘要</h1><ul><li>商学院教的全是框架。<br /></li><li>雇主权：个人在工作中的发明，知识产权属于雇主。<br /></li><li>最小可行产品（Minimum viable product） 简称MVP<br /></li><li>倘若确实付出了真诚的努力，且不遗余力，那么，失败看上去就跟成功一样令人兴奋。<br /></li><li>将密码分开（非对称性加密 RSA），是自文艺复兴以来最具有革命性的加密概念。<br />它解决了，置身于一个到处是不可信之人的世界，你如何与一个值得信任的人做交易。<br /></li><li>马基雅维利：只要目的正当，可以不择手段。 （守正出奇）<br /></li><li>管理是一门艺术，而非科学。我们要慎防那些试图把人类行为数字化和量化的人...<br /></li><li>固定成本、变动成本：固定成本指不管你的产量大小都会产生的费用，而变动成本指原料等的费用，只有生产时，它才会产生。<br /></li><li>从A点到B点<br /></li><li>学习心态：失败是学习的一个过程<br /></li><li>两种方式成长：提高效率（ 1 -&gt; N ）、创新（0 -&gt; 1)<br /></li><li>对待新事物，就像新生儿，允许他们犯错，给时间给他们成长。 任何新事物，要抱着让子弹飞一会的态度去接纳他们。<br /></li><li>拥抱不确定性</li></ul><p><img src="/img/深度工作7步法2.jpg" /><br /><img src="/img/深度工作7步法3.jpg" /><br /><img src="/img/深度工作7步法4.jpg" /><br /><img src="/img/深度工作7步法5.jpg" /><br /><img src="/img/深度工作7步法6.jpg" /><br /><img src="/img/深度工作7步法7.jpg" /><br /><img src="/img/深度工作7步法1.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 工作 </tag>
            
            <tag> 效率 </tag>
            
            <tag> 谷歌 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《阿米巴经营》读书心得</title>
      <link href="/2018/10/28/%E3%80%8A%E9%98%BF%E7%B1%B3%E5%B7%B4%E7%BB%8F%E8%90%A5%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2018/10/28/%E3%80%8A%E9%98%BF%E7%B1%B3%E5%B7%B4%E7%BB%8F%E8%90%A5%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>【<strong>中心思想</strong>】<br /><strong>所谓阿米巴经营就是：分部门核算，全员参与经营的一种新的管理会计制度和经营体制。说穿了就是公司内部买卖（采购）关系！</strong><br />通过内部的买卖关系，采取与市场直接挂钩的分部门的核算制度，把市场的压力传递到内部部门。</p><h1 id="京瓷的成功因素">京瓷的成功因素</h1><ul><li><strong>经营哲学</strong>：作为人，何谓正确；<br /></li><li><strong>经营体制</strong>：阿米巴经营；</li></ul><p>两者缺一不可。</p><h2 id="经营哲学">经营哲学</h2><ul><li>成功的方程式<br /><strong>人生·工作的结果 = 思维方式 × 努力 × 能力</strong><br />思维方式：就是人生态度，从负100分到正100分打分。因为是相乘关系，稍稍负面的“思维方式”，就会带来负的人生结果。具备正确的思维方式：<strong>作为人，何谓正确。</strong><br /></li><li>领导者<br />拿出勇气，把正确的事情贯彻到底，这一点非常重要。就是：公平、正义、勤奋、谦虚、勇敢、知足、乐观、自利利他等普通正确的价值观。<br />领导人的魅力就是以身作则（人格魅力），并且有不达目的誓不罢休、渗透到潜意识的强烈而持久的愿望和洞穿岩石般的坚韧不拔的意志。<br />领导者必须具备热情、信念（理想、梦想）、意志；</li></ul><span id="more"></span><h2 id="经营体制">经营体制</h2><p>阿米巴经营就像IT中的微服务架构，小快灵。通过内部核算，取代目前的<strong>官僚式经营</strong>；<br />导入阿米巴经营的最大难点是要重新划分组织，重新分配权力。</p><ul><li>阿米巴的划分标准<br />市场上有没有类似的公司存在。<br /></li><li>阿米巴之间的定价<br />单位的时间成本、市场同类产品的定价。<br /></li><li>阿米巴之间的冲突处理机制<br />利己利他主义、高层领导的公平协调，同时满足个体和整体的利益；（避免囚徒困境：个人的理性选择，往往不是集体的最优选择。）</li></ul><h1 id="阿米巴管理体制">阿米巴管理体制</h1><h2 id="组织构建">组织构建</h2><p>首先有职能，然后依赖职能构建组织。<br /><img src="/img/阿米巴经营组织架构.png" /></p><h2 id="考核机制">考核机制</h2><ul><li>1、 单位时间核算表：<br />人均单位时间产值（不含劳务费）；<br /></li><li>2、结算销售额比率；<br />即附加价值除以总生产。</li></ul><h2 id="库存管理">库存管理</h2><p>阿米巴经营中，由销售部门发出订单，制造部门按订单生产，完成后交付给销售部门的产品由销售部门负责库存。<br />一旦销售预测和价格预测发生差错，库存产品成为废物或者必须削价处理，那么由此产生的损失及处理责任由销售部门承担；<br />还有，在单位时间核算制度中，针对库存金额设定了公司内部利息，这个利息高于一般的银行利息，作为销售部门的经费予以征收。</p><ul><li>1、管理库存的成本：管理人员费用、场地费用、时间；<br /></li><li>2、资金的占用：库存也是一种资金的占用；<br /></li><li>3、价格风险：可能会因为市场上价格下行，导致利润损失；<br /></li><li>4、规格风险：可能会出现规格变化，导致库存的型号过期；</li></ul><h2 id="目标管理">目标管理</h2><p>量化的目标；通过设定目标来统一组织的方向；</p><h1 id="文章摘要">文章摘要</h1><ul><li>所谓阿米巴经营，就是把组织划分成一个个小的团体，各自独立核算，同时在公司内部培养具有经营者意识的领导人，让全体成员参与经营，实现全员参与型经营。<br /></li><li>思维方式：做正确的事。<br /></li><li>中小企业像脓包，一大就破； 中小企业做的是“盖浇饭”式的笼统帐，在这过程中规模膨胀，管理跟不上，就要垮台。<br /></li><li>利益共享、风险共担的跟投制度。<br /></li><li>先是把工作做好，薪酬自然提升，而不是反过来。<br /></li><li><strong>京瓷的经营理念是：在追求全体员工物质和精神两方面幸福的同时，为人类社会的进步发展做出贡献。</strong><br /></li><li>一个不断成长的公司究竟应该怎么去运营。<br /></li><li>经营者意识：销售最大化、费用最小化。<br /></li><li>不忘初心：思考目的、目标，手段是为目的服务。<br /></li><li>产品销售的价格与成本无关，而是由市场决定；定价是由产品的价值决定，而不是成本。<br /></li><li>IT部门如果实行阿米巴经营，体制上与市场上的其它公司竞争，如何进行保护和孵化？<br />IT公司的服务清单、服务目录、计价；<br />面向客户、一站式服务。<br /></li><li>现在的京瓷，细分后的阿米巴数量经营达到了3000个；<br /></li><li>不能建立现在就能战斗的体制，就会在竞争中落败。出于这种危机感，我经常改变和调整组织形态。目的是不能让组织僵化，需要让组织时刻在进化。<br /></li><li>费用最小化，关键是要减少浪费，同时优化成本。<br /></li><li>多重确认原则<br /></li><li>坚持原则，思考自己的原则是什么来进行决策；（不同文化背景、不同公司、不同人适合的风格不一样，也不能一概而论）<br />对于来自其它部门的不合理要求，不可轻易妥协，不能唯唯诺诺当好人，哪怕来自事业部长的无理要求，也要据理力争，即使吵架也无所畏惧。<br /></li><li>我的观点是：实际从事产品生产的制造部门才是利润的源泉，制造部门应该直接获取市场信息，并迅速将这种信息反映到生产活动中去。<br />在经营上追求速度的今天，应对市场变化敏感度的差异，直接体现了企业竞争力的差异。因此，让制造部门具备市场意识，提高制造部门的核算意识，就会不断强化制造部门的体质。<br /></li><li>通过经费的转移分摊，可以唤起大家的成本意识，同时可以防止间接部门的臃肿化的倾向。<br />所谓阿米巴经营，现场才是经营的主角，间接部门应该彻底压缩。归根到底，间接部门只能是一个“小政府”。<br /></li><li>销售的使命是要看透客户能够接受的最高价格，让客户认为“这个价格我们接受”。<br />定价是决定企业生死的大问题，领导人必须全神贯注、全力以赴。<br /></li><li>要超越自己的专业技术的范围，去新的领域闯荡，人们难免犹豫不决。但是封闭在自造的硬壳中，只做传统业务，直到变成化石，那么技术进步等一切就无从谈起。<br /></li><li>我在当社长的时候，不管是销售、研发还是生产，只要发生问题，我就会亲临现场指挥，在客户和现场间奔走巡回。只要有空，我就涉足现场，视察有问题的部门，全力帮助解决。虽然把经营委托给各阿米巴长，但并不是一切放任不管。<br />同时，作为经营者，我不断思考公司将来应该如何发展，思考公司前进的方向，针对有关公司整体的重大问题做出判断等等，履行对经营者所要求的高层次的职责。</li></ul><p><img src="/img/阿米巴经营1.JPG" /><br /><img src="/img/阿米巴经营2.JPG" /><br /><img src="/img/阿米巴经营3.JPG" /><br /><img src="/img/阿米巴经营4.JPG" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 阿米巴经营 </tag>
            
            <tag> 内部买卖 </tag>
            
            <tag> 全民经营 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《失控》读书心得</title>
      <link href="/2018/09/23/%E3%80%8A%E5%A4%B1%E6%8E%A7%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2018/09/23/%E3%80%8A%E5%A4%B1%E6%8E%A7%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<h1 id="蜂群思维">蜂群思维</h1><p>低层思维的自治个体形成高层群体思维的行为（群态）；<br />去中心化、分布式管理</p><h1 id="生物圈的实验">生物圈的实验</h1><p>一个生态环境的形成有着特定的次序，甚至有些在过程中已完成了使命，已经消失，所以重建生态是很困难的。</p><h1 id="达尔文的进化论">达尔文的进化论</h1><p>自然选择的问题：自然选择是编辑，而不是作者。<br />进化论仅仅证明了微进化（从红玫瑰到白玫瑰），而不能证明宏进化（从虫子到蛇）；在地质史上，也从没有发现相关的中间物种，也就是所谓的“化石断代”。</p><span id="more"></span><h1 id="文章摘要">文章摘要</h1><ul><li>一个生态环境的形成有着特定的次序，甚至有些在过程中已完成了使命，已经消失，所以重建生态是很困难的。<br /></li><li>大型任务通过去中心化的方法并借助最少的规则来完成。<br /></li><li><strong>低层级的存在无法推断出高层级的复杂性；</strong><br /></li><li>分布式系统的四个突出特点；<ul><li>没有强制性的中心控制；<br /></li><li>次级单位具有自治的特质；<br /></li><li>次级单位质检彼此高度连接；<br /></li><li>点对点间的影响通过网络形成了非线性因果关系。<br /></li></ul></li><li>如今，经济学家们认为，只有把产品单座服务来做，才能取得最佳的效果。你卖给顾客什么并不重要，重要的是你为顾客做了些什么。<br /></li><li>时至今日，自然科学一直未能解决的一个难题，就是如何建立一种纯意识。<br /></li><li>人们对他们不想要的东西总是视而不见。<br /></li><li>收益递增法则：拥有者得到更多。 （马太效应）<br /></li><li>氧气的浓度是20%，低于这个含量是贫氧，高于就易燃。<br /></li><li>囚徒困境的策略：“一报还一报”，以合作回报合作，以背叛回报背叛，往往产生一轮轮的合作。 （但是前提是重复性游戏，例如黑社会）<br /></li><li>均衡即死亡；<br /></li><li>四个阻断<ul><li>地球与宇宙 哥白尼解决<br /></li><li>人类与生物界 牛顿解决<br /></li><li>理性与无意识的非理性 弗洛伊德解决<br /></li><li>人类与机器人 ？<br /></li></ul></li><li>正是因为有了电这个令人惊叹的奇迹的后代----弱电，才有了我们对整个社会的重新构想。 （信息技术）<br /></li><li>任何一个稳定的封闭生态系统的基础，基本上都是某种微生物。<br /></li><li>食物链的冗余路径。<br /></li><li>生态系统的建设，要想保证繁殖，雌性的比例应该高一点。原则上，雌性与雄性的比例达到5：3；而不是人类从政治意义上的4：4。<br /></li><li>地球上，我们的碳银行就是阿拉伯沙漠地下的石油。<br /></li><li>生命是终极技术，机器技术只不过是生命技术的临时替代品而已，随着我们对机器的改进，他们会变得更有机、更生物化，更近似生命，因为生命是生物的最高技术。<br /></li><li>生命具有自治性，它们能够自立，更重要的是，它能够自主学习。<br /></li><li>虽然生命构建在碳元素之上，它却不以碳为驱动力，而是碳氢化合物中的氢。<br /></li><li>产品设计意味着要让营销、法律和工程团队同时参与进来，而不是像过去那样顺序完成。<br /></li><li><strong>存在两种类型复杂的系统：连续性的和非连续性的。例如，汽车性能随速度变化的变化而变化，就是连续性的；而软件系统BUG，就是非连续性的。非连续系统种，断点始终存在。</strong><br /></li><li>文章本天成，妙手偶得之。 陆游《文章》<br /></li><li>研究从生物时间（树木得千年寿命）转到进化时间（树种得百万年寿命）。<br /></li><li><strong>所有成功得系统都会吸引寄生虫，这似乎是生命的普遍属性。</strong><br /></li><li>海量连接中涌现出秩序的人工智能方法被冠以“连接主义”的名号。<br /></li><li>机器人会像匹诺曹一样获得生命吗？<br /></li><li>机器人三定律：是科幻小说家艾萨克·阿西莫夫（Isaac Asimov）在他的机器人相关作品和其他机器人相关小说中为机器人设定的行为准则<ul><li>第一法则：机器人不得伤害人类，或坐视人类受到伤害；<br /></li><li>第二法则：除非违背第一法则，否则机器人必须服从人类命令；<br /></li><li>第三法则：除非违背第一或第二法则，否则机器人必须保护自己。<br />这三大法则是不是机器人的宿命？<br /></li></ul></li><li>生命是否能够从碳结构转向硅结构？<br /></li><li>新想法：有机的碳基生命只不过是超生命进化为物质形式的第一步而已。生命征服了碳。而如今，在池塘杂草和翠鸟的伪装下，生命骚动着想侵入水晶、电线、生化凝胶、以及神经和硅的组合物。<br /></li><li>科学工作的三种方式：<ul><li>理论<br /></li><li>实验<br /></li><li>仿真 （借助计算机）<br /></li></ul></li><li>大自然用以无中生有的九条规律（九律）<ul><li>分布式<br /></li><li>自下而上的控制<br /></li><li>递增收益<br /></li><li>模块化生长<br /></li><li>边界最大化<br /></li><li>鼓励犯错误<br /></li><li>不求最优化，但求多目标<br /></li><li>谋求持久的不均衡态（均衡即死亡）<br /></li><li>变自生变</li></ul></li></ul><p><img src="/img/失控.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 失控 </tag>
            
            <tag> 蜂群思维 </tag>
            
            <tag> 进化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《产品思维》读书心得</title>
      <link href="/2018/07/25/%E3%80%8A%E4%BA%A7%E5%93%81%E6%80%9D%E7%BB%B4%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97%20/"/>
      <url>/2018/07/25/%E3%80%8A%E4%BA%A7%E5%93%81%E6%80%9D%E7%BB%B4%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97%20/</url>
      
        <content type="html"><![CDATA[<p>严格来说是听书心得，是在得到上听的梁宁说的产品思维的课程。</p><h1 id="摘要">摘要</h1><ul><li>认知框架，内心驱动力</li><li><strong>角色 去角色化交流 朋友</strong></li><li>愉悦 恐惧</li><li>用户画像 羊 狼 大明 笨笨 小闲</li><li>产品 关键是运营场景 例如餐厅 场 时空 景 情绪触发</li><li>理性把人往回拉 情绪推动人</li><li>服务 而不是 产品</li><li>产品 核心服务 战略价值 运营 可用性 用户区隔</li><li>企业是社会分工的产品，你的效率高就给你做</li><li>领导力就是带领大家穿越生死线的能力</li><li><strong>向互联网公司转型 扁平化 速度</strong></li><li>书 用户体验要素</li><li>用户体验地图 第一只羊 用户故事</li><li>用户画像 用户目标 用户路径 用户故事</li><li>服务蓝图 客户服务节点</li><li>口碑 是把事情做过头</li><li><strong>创新重要的是面对痛苦，起心动念</strong></li><li>婴儿恒温箱 跨物种</li><li>亚朵酒店</li><li>价值网</li><li>扑克牌 确定 依赖</li><li><strong>商业是以利润为中心，人生是以意义为中心</strong></li></ul>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 读书心得 </tag>
            
            <tag> 产品思维 </tag>
            
            <tag> 梁宁 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《Head First PMP》读书笔记</title>
      <link href="/2018/04/28/%E3%80%8AHead%20First%20PMP%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
      <url>/2018/04/28/%E3%80%8AHead%20First%20PMP%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="pmp各类图表">PMP各类图表</h1><p><img src="/img/项目管理过程与知识领域.png" /></p><span id="more"></span><p>PMP - ITTO，這是PMBOK 裡每個流程的標準模型，Input 、Tool&amp;Technique、 Output<br /><a href="/img/PMP-ITTO.xlsx">PMP-ITTO</a><br /><img src="/img/沟通管理.png" title="沟通管理" /><br /><img src="/img/人力资源直方图.jpg" /><br /><img src="/img/项目管理计划与项目文件的区别.png" /><br /><img src="/img/挣值管理.jpg" /><br /><img src="/img/RFI、RFQ、RFP和IFB的对比.png" /><br /><img src="/img/WBS模板.jpeg" /><br /><img src="/img/风险分解结构（RBS）.jpg" /><br /><a href="/img/PMP读书笔记.xmind">读书笔记脑图</a></p><h1 id="文章摘要">文章摘要</h1><ul><li>渐进明细：开始时，你只有目标和一个计划，不过随着项目的进行，往往会有需要处理的新信息。<br />滚动计划！敏捷开发就是这种方式！<br /></li><li><strong>正确的做法是讲故事，而不是做报告。</strong><br /></li><li>bullet point 要点<br /></li><li>把运维划分为多个发布版本，每个发布版本都有一个明确的起点和重点。<br /></li><li>项目中完成的所有工作都有一个模式（pattern）,先计划，再去做。过程框架（过程组和知识领域）正是让这一切平稳进行的关键。<br /></li><li>每个过程都包括输入、工具、用来完成工作的技术及输出。ITTO<br /></li><li>可以把过程认为是《PMBOK指南》的核心信息，而过程组和知识领域是对这些过程分组的两种不同方法。类似，一群人可以按照国籍分，也可以按照种族分。<br /></li><li>项目章程会给你管理项目的全力。<br /></li><li>项目管理计划使你在问题发生之前作出规划。项目管理计划就是要对问题作出规划，并在问题出现时提供所需的信息来纠正这些问题。<br />项目管理计划是一个文档，不过它要分解为一组辅助计划，对于各个其他知识领域分别有一个辅助计划。<br /></li><li>项目管理计划与项目进度不是同一回事。<br /></li><li>项目管理信息系统是公司企业环境要素的一部分，通常也是所有变更控制系统的一部分。它定义了如何分派工作。<br /></li><li>项目管理计划包括基线：范围、进度和预算的快照。<br /></li><li>作为项目经理，你的任务中很大一部分就是明确如何避免变更，如何在项目具体实现之前避免变更呢？一种方法就是尽可能地作出计划，因为大量变更都是因为缺少计划而发生。<br /></li><li>如果你能自己作出一个变更，而不影响项目约束，那么作为项目经理的你完全有权利这样做。<br /></li><li>在做出变更之前，你需要确定它会对项目约束有什么影响，并确保你的赞助人和干系人接受这种影响。这正是变更控制所做的工作。<br /></li><li>工作包与活动的区别<ol type="1"><li>活动是由工作包分解而来，是实现工作包所需的具体工作。<br /></li><li>工作包是WBS底层的可交付成果，是WBS的一部分，活动不是WBS的一部分。<br /></li><li>工作包不表示时间、也不表示顺序，只表示项目范围；活动可表示时间、顺序、是资源估算、历史估算、费用估算的重要依据。<br /></li></ol></li><li>活动工期估算一定是工期估算，而不是工作量估算，所以他们会给出日历时间，而不是多少人时（工作量单位）；工期*需要人力资源=工作量<br /></li><li><strong>光环效应：指只是因为某个人对某一个工作很擅长，就把他置于另一个他无法驾驭的位置。</strong><br /></li><li>对项目实际测量的这些数据被称为工作绩效数据，你计算的预测信息则称为工作绩效信息（有EAC和ETC计算出来的结果）。<br /></li><li>PMBOK中的重要会议<ol type="1"><li>干系人识别会议<br /></li><li>项目启动会议<br /></li><li>项目管理计划编制会议<br /></li><li>项目开踢会议<br /></li><li>项目状态审查会<br /></li><li>项目变更控制会<br /></li><li>项目收尾总结会</li></ol></li></ul><p><img src="/img/PMP%20-%202.jpg" /><br /><img src="/img/PMP%20-%201.jpg" /><br /><img src="/img/PMP%20-%203.jpg" /><br /><img src="/img/PMP%20-%204.jpg" /><br /><img src="/img/PMP%20-%205.jpg" /><br /><img src="/img/PMP%20-%206.jpg" /><br /><img src="/img/PMP%20-%207.jpg" /><br /><img src="/img/PMP%20-%208.jpg" /><br /><img src="/img/PMP%20-%209.jpg" /><br /><img src="/img/PMP%20-%2010.jpg" /><br /><img src="/img/PMP%20-%2011.jpg" /><br /><img src="/img/PMP%20-%2012.jpg" /><br /><img src="/img/PMP%20-%2013.jpg" /><br /><img src="/img/PMP%20-%2014.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PMP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《原则》读书心得</title>
      <link href="/2018/04/13/%E3%80%8A%E5%8E%9F%E5%88%99%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2018/04/13/%E3%80%8A%E5%8E%9F%E5%88%99%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p><strong>[中心思想]</strong><br />处理一件事情时，需要思考你决策背后的逻辑和因果关系，然后归纳出相关的原则和标准。在将来面对类似问题时，你能够重复利用这些原则来解决（情景再现）。<br />强烈推荐这本书。本书可以作为一本人生手册，放在你的手边，随时翻翻，有疑问的时候可以从书中找找灵感和答案(类似赵普的半部论语治天下)。<br />个人可以养成习惯，在你面对和处理问题时，一旦做了决定，想想这个决策背后依赖的原则，这样既能保证你处理问题的合理性，也能坚定你的信心。<br />书中，作者总结了在他跌宕起伏的一生中，在面对各种问题时，背后遵循的一些原则，通过这些原则，作者简化了其决策思考的过程，并保持自己价值观的一致性。<br />作者分了生活原则和工作原则来分别表述，当然，工作原则很多和生活原则是重复的，因为我们的目标是让生活和工作中保持一致的价值观和行为准则。作者讲了很多，从理解的角度来说，自己摘录了几个印象特别深刻的部分，其它的部分，可以随时翻阅书本进行查找。<br /><img src="/img/原则.png" /><br /><span id="more"></span></p><p><strong>同时，看完这本书之后，也给儿子未来的高中生活写了一篇寄语。</strong></p><blockquote><p>桐桐，<br />一转眼你已经上了高中，从高中开始，你就要迈入人生新的阶段。并且，从高中开始，你就要离开爸妈的身边，在学校住宿了。<br />在家里，一切都是简单的、容易的，爸妈对你无条件的爱，让你感觉到生活、人际关系都是很简单的、一切都自然而然。但是，俗话说，在家百日好、出门一日难，在你住宿的过程中，总会碰到很多意外和磕磕撞撞，作为过来人，爸爸想和你聊聊一些为人处世的道理（或者说原则），希望在你面对一些问题的时候，能想想这些道理，然后在这些道理的指导下处理人际关系。<br /><strong>1、换位思考</strong><br />《原则》这本书说得好，人和人言行举止的不同，是因为人与人的思维是不同的，这又是因为人和人的生理构造的不同，例如有人是左脑思维、有人是右脑思维，并且因为不同的生活环境和生活经历，也导致人与人的大不同。<br />面对这些不同时，你要学会理解和包容，生理的不同，就像个子的高矮，是先天的，我们要站在对方的立场去思考和理解他们，理解他们的一切都是事出有因的。<br /><strong>2、分歧处理</strong><br />人因为思维的不同，在相处中分歧总是难以避免的，包括价值观和原则。有分歧是很正常的，在有分歧的时候，首先要避免先入为主的认为自己是正确的，就像盲人摸象，可能每个人都只能看到事情的局部。所以我们需要心平气和的讨论，目的是为了探寻真相，而不是为了分出胜负。<br />如果能通过分歧，找出事情的真相，那就是我们进步的动力；如果分歧不能消除，那就心平气和的停止对话。<br /><strong>3、目标管理</strong><br />一个人，没有目标，就像一叶扁舟，随风飘荡。只有思考自己想要什么，树立目标，才能坚定自己努力的方向。在达成目标的路上，总是有意外、有流言蜚语、有你不想面对的事情，但是要学会聚焦目标，去做为达到目标应该做的东西，而不是你喜欢做的东西。在你受干扰、犹豫的时候，想想你的目标，然后摒弃杂念，勇往直前。<br />桐桐，这么多年来，你的表现一直都很好，爸爸妈妈也相信你能在未来的日子里表现得更好。</p></blockquote><p><strong>下面是本书摘录的知识领域。</strong></p><h1 id="如何面对失败和痛苦">如何面对失败和痛苦</h1><ul><li><strong>改变自己的心理反应模式，使痛苦成为某种你渴望的东西。</strong><br /></li><li><strong>痛苦+反思=进步。</strong><br /></li><li><strong>我养成的最有价值的习惯就是利用痛苦来激发高质量的思考。</strong><br /></li><li>痛苦按钮：当某个人在经历痛苦，那就是让他把经历什么样的痛苦记录下来的最佳时机，但还不是反思的合适时机，因为当时很难保持头脑清醒。这时候可以让员工把他们的感受（气愤、失望、沮丧等）记录下来，以后再利用反思所用的问题进行回顾反思。<br /></li><li>在痛苦中反思的能力，正确的失败，就是能够在经历痛苦的失败过程中吸取重要的教训。<br /></li><li><strong>同时，与失败比起来，我对乏味和平庸的恐惧要严重得多。对外而言，很好的事要比糟糕的事好，而糟糕的事要比平庸的事好，因为糟糕的事至少给生活增加了滋味。</strong><br /></li><li>我做事的方式是试错：犯错，找出错误原因，总结出新的原则，最终成功。<br />（错误的严重性要控制，不能冒不能承受的错误）；例如，我能接受把车蹭掉了漆或者撞凹了一块，但不能冒大风险给把车给全毁了。<br /></li><li><strong>在形成这个视角的过程中，我开始以一种截然不同的方式体会痛苦的时刻。我不会感到沮丧或透不过气，而是把痛苦视为大自然的提醒，告诉我有一些重要的东西需要我去学习。体验痛苦，然后探索大自然希望通过痛苦给我的教益，开始成为我的一项游戏。这项游戏我做得越多就越擅长，也就越不会对这些情况感到痛苦，同时思考、总结出原则、利用原则获得回报的过程也变得越来越有收获。我学会了喜爱自己的痛苦，我想这是一种健康的视角，就像学会喜爱锻炼身体一样。（痛并快乐着）</strong><br /></li><li>没有痛苦就没有收获。<br /></li><li>虽然我们不喜欢痛苦，但自然的一切创造都有目的，所以自然让我们痛苦是有意图的，那么意图是什么？痛苦让我们清醒，帮助我们进步。<br /></li><li>自然的一项基本法则是，为了赢得力量，人必须努力突破极限，而这时痛苦的。<br /></li><li>荣格说：人需要困难，这对健康来说是必需的。<br /></li><li>但多数人本能地躲避痛苦。不论是锻炼身体还是历练头脑，人都会躲避痛苦，如举重的痛苦，如沮丧、思想挣扎、尴尬、耻辱的痛苦。当人面对自身存在缺陷这个严酷现实时，这一点就体现得更明显了。<br /></li><li>直面你的问题，错误和弱点导致的痛苦现实，会大幅度提高你的效率。<br /></li><li>天将降大任于斯人也，.....<br /></li><li>如果你没有经历过失败，就说明你没有努力突破极限，而如果你不努力突破极限，你就不能最大极限地挖掘自己的潜能。<br /></li><li>你可以选择健康并痛苦的事实，也可以选择不健康但舒适的幻觉。<br /></li><li>成长的痛苦，它将考验你的个性，当你忍痛前行时给你回报。<br /></li><li>乔丹简直沉醉在自己的错误中，他把每次错误都当成改进的机会。乔丹明白一个道理，错误就想是玩小拼图游戏，每完成一个，就会得到一个宝贝。当你每犯一个错误并从中吸取教训，就会使你在未来免于再犯几千个类似的错误。<br /></li><li>失败是成功之母，但是只有从失败中吸取教训并加以改进，才能成为成功之母。<br /></li><li>爱迪生说过：我没有失败，我只是发现了一万种不成功的方法。<br /></li><li><strong>对我而言，如果你回顾一年前的自己，而没有为自己所做的傻事感到震惊，就说明你还没有吸取足够多的教训。</strong><br /></li><li>不要纠结于一时的成败，要放眼于达成目标。<br /></li><li>不要灰心丧气。如果现在尚未遇到坏事，那就再等等，坏事迟早要上门，这就是现实。我的生活态度在于，问题总归会发生，对我来说最重要的是弄清楚如何应对，而不是花时间在那里抱怨慨叹并期待不要发生。</li></ul><h1 id="面对缺点">面对缺点</h1><p>但能够真实展现自我的人是最快乐的，如果你能以开放的心态看待自身的缺点，这将解放你，帮助你更好地应对缺点。我建议你不要为自己的缺点感到羞愧，要明白任何人都有缺点。把缺点摆上桌面将帮助你戒掉坏习惯，养成好习惯，获得真正的能力，拥有充足的理由保持乐观。<br />把你最大的弱点写下来（如找出问题、规划解决方案、执行落实），并写下其原因（如你被情绪左右，你无法预见各种合理的可能性）。尽管包括你在内的大多数人面临的重大障碍不止一个，单如果你能消除或者想方法规避这个最大的弱点，你将大大改善你的人生。<br /><strong>建议写下自己最大的三个坏习惯，然后从中选一个，下决心戒掉。</strong><br />（自信的讲出自己的观点、高标要求自己和别人、坦诚面对自己的弱点、冒险或者高目标、太顾及家庭）</p><h1 id="分歧处理">分歧处理</h1><ul><li>理解人与人大不相同。<br />观点不同 &lt;-- 思维不同 &lt;-- 大脑结构不同（生理性的差异）<br /></li><li>分歧是一个好的学习机会。<br /></li><li>记住，你不是在争论，而是在开放地探求事实。<br /></li><li>你要保持理性，并期待对方也保持理性。如果你冷静，平等的对待对方，尊重对方，效果就会好得多。<br /></li><li>复述对方的观点<br />一种检验你做得好不好的方式是，把和你有分歧的人的观点，向对方复述一遍。如果他说你复述得对，就说明你做得很好。<br />我还建议双方都遵守<strong>“两分钟法则”</strong>，两分钟内不要打断对方，以便对方有时间把想法说清楚。（<strong>对方说话时，当你想插话时，先数5秒，对方还不说的时候，你再接着说。</strong>）<br /></li><li>如果你们发现讨论陷入了僵局，就商定一个你们都尊重的人，让他帮忙主持讨论。最没有成效的方式是你自己脑子里试图把事情想明白（这是大多数人的倾向），或者在讨论收效已不断减少的情况下继续浪费时间。发生这种情况时，你们应该转向更有效的方式----达成相互理解，这不等于达成一致。例如，你们都同意保持分歧。<br /></li><li>头脑封闭的人，不喜欢看到自己的观点被挑战。他们通常会因无法说服对方而感到沮丧，而不是好奇对方为何看法不同。他们在把事情弄错时会产生坏情绪，更关心自己能不能被证明是正确的，而不是提出问题，了解其他人的观点。<br /></li><li>头脑开放的人更想了解为什么会出现分歧。当其他人不赞同时，他们不会发怒。他们明白自己总有可能是错的，值得花一点时间考虑对方的观点，以确定自己没有忽略一些因素或犯错。<br /></li><li>头脑封闭的人更关心自己是否被理解，而不是理解其他人。头脑开放的人，经常觉得有必要从对方的视角看待事物。<br /></li><li>头脑开放的人，总是更喜欢倾听而不是发言。</li></ul><h1 id="五步流程实现人生愿望">五步流程实现人生愿望</h1><ul><li>1、有明确的目标<br /></li><li>2、找到阻碍你实现这些目标的问题，并且不容忍问题<br /></li><li>3、准确诊断问题，找到问题的根源<br /></li><li>4、规划可以解决问题的方案<br /></li><li>5、做一切必要的事来践行这些方案，实现成果<br /><img src="/img/原则12.jpg" /></li></ul><p>记住：这五步中的每一步都源自你的价值观。你的价值观决定了你想要什么，即你的目标。<br />设定目标（如果确定你希望自己的生活是什么样子）需要你擅长更高层次的思考，如设想未来、优先排序。</p><h2 id="技巧">技巧</h2><ul><li>1、你必须按顺序一步步来，做完某一步再做下一步，并且做某一步的时候，不要考虑任何下一步的问题；<br /></li><li>2、如果你被自己的情绪压倒，就退后一步，暂时停下来，直到恢复清醒的思考能力。必要时，向冷静、善于思考的人请教。<br /></li><li>3、目标管理，要制定伟大的目标，要是自己全力跳跳就能摸得着的目标，而不是明知自己能实现的目标。<br />你必须养成一种对任何性质的恶习都绝不容忍的习惯，无论其是重还是轻。<br /></li><li>4、问题管理：区分直接原因和根本原因<br />直接原因通常是导致问题的行动（或不行动），所以通常用动词描述（我因为没有查列车时刻表而错过了火车）。根本原因是更深层的原因，通常用形容词描述（我因为健忘没有查列车时刻表）。只有消除根本原因才能真正解决问题，为此，你必须区分症状和疾病本身。<br /></li><li>5、任务是方案和目标之间的纽带。<br /></li><li>6、大多数人犯的错误是，一心只想着执行，所以几乎不花时间来规划。谨记：规划先于行动。（谋定而后动）</li></ul><h1 id="如何做事">如何做事</h1><p><strong>让你的思维慢下来，以注意到你正在引用的决策标准。把这个标准作为一项原则写下来，当结果出现时，评估结果，思考标准，并在下一个“类似情景”出现之前改进标准。</strong> （情景再现）<br /><img src="/img/如何做事.png" /></p><h1 id="如何管理">如何管理</h1><ul><li>像操作一部机器那样，进行管理以实现目标。<br />出色的管理者把自己视同工程师，把机构当做机器看待，兢兢业业进行维护和改进。他们设计出流程图，展示机器如何运作，如何对设计进行评估。他们开发出量化工具来评价每个独立的机器部件（最重要的是人）和整部机器的运转情况。他们不断对设计和人员进行完善以便更好发挥作用。<br />如果出现问题，要在两个层面进行讨论（1）机器层面（为什么）；（2）案例层面（怎么办）<br />从手头的事例中提炼出相关的原则，把流程系统化，从而相当于建造出一部机器。<br /></li><li>不断把结果和你的目标进行对照。<br /><img src="/img/原则17.jpg" /><br /><img src="/img/原则18.jpg" /><br /><img src="/img/原则19.jpg" /><br /><img src="/img/原则20.jpg" /></li></ul><h1 id="量化指标">量化指标</h1><ul><li>如果没有数据，就很难对业绩进行客观的、不偏不倚的、不带感情色彩的对话，也很难追踪进展。<br /></li><li>设计好量化指标的4个步骤<ul><li>了解公司的目标是什么<br /></li><li>了解达成目标的程序（你的机器，包括人和设计）<br /></li><li>找到程序中最适合量化的关键部分，以便了解机器如何运作以达成目标<br /></li><li>研究如何在关键指标上发挥杠杆效用，以便调整程序、改变结果。</li></ul></li></ul><h1 id="如何讲述">如何讲述</h1><p><strong>用“基线以上”和“基线以下”来确定谈话位于哪一层。基线以上的谈话应当以井然有序的方式走向结论，只有在必要说明某个要点的细节时才可以走到基线以下。</strong><br /><img src="/img/原则15.jpg" /><br /><img src="/img/原则16.jpg" /></p><h1 id="聚焦目标">聚焦目标</h1><ul><li>我们选择自己追求什么（我们的目标），而这会影响我们走的道路。目标对一个人是很重要的，就像灯塔，指引着我们前行的路。<br /></li><li>聚焦目标而不是任务。<br />问一个高层次的问题：“目标xxx的进展如何？”好的而回答是先指出关于xxx整体进展的总体情况，如果需要，再分述各任务情况来支持论证，只见任务不见目标的人只会讲任务完成情况。<br /></li><li>不要担心你的员工是不是喜欢你。不能取悦所有的人，一旦你认为正确，就必须有在质疑中孤独前行的勇气。<br /></li><li>人往往无意识的倾向于做自己喜欢的事而非需要做的事。如果他们忽视了做事的优先顺序，你需要为他们重新指引方向。这也是为什么经常听取员工汇报工作进展很重要的部分原因。<br /></li><li>捍卫你认为正确的事情虽然在短期内比较艰难，但是我愿意接受挑战。我关切的是要做正确的事情，而不是别人怎样看我。<br /></li><li>最重要的责任人在最高层负责订立目标、规划成果和组织实施。高层管理者必须能进行高层次的思考，清楚目标与任务之间的差别，否则你就不得不去做本该由下属完成的事。...管理者要聚焦目标。<br /></li><li>任务和目标是不同的。目标导向的人，能从日常的任务中跳出来，思考未来做什么、怎么做。他们最适合创造新东西（新组织、新计划），管理频繁变化的组织。因为视野宽阔、通观全局，他们通常会成为最能勾画未来蓝图的领导者。</li></ul><h1 id="文章摘要">文章摘要</h1><ul><li>我一生中学到的最重要的东西是一种以原则为基础的生活方式。<br /></li><li>做一个有原则的人意味着，总是依据可以清晰解释的原则做事。<br /></li><li><strong>我的希望是，阅读本书将促使你以你认为最合适的方式发现自己的原则，并最好把他们写下来。...遇到更多情况时，改进自己的原则，反思自己的原则，这将帮助你做出更好的决定。... 在做任何决定时，仔细思考并写下自己的决策标准，是价值无限的。</strong><br /></li><li>独立思考并决定：（1）你想要什么 （2）事实是什么 （3）面对事实，你如何实现自己的愿望。<br /></li><li>以系统化的方式来决策：方法是以算法的形式把决策标准表达出来，从而植入计算机。同时运行两套决策系统----头脑中的一套和计算机的一套。<br />虽然计算机在很多方面比我们的大脑棒得多，但我们拥有的想象力、理解力和逻辑能力是它不具备的。正因如此，我们的大脑与计算机的合作才是绝配。<br />通过系统模型来辅助决策。<br /></li><li>独立的思考者。<br /></li><li>想象一下，为了拥有美好的生活，你必须穿越一片危险的丛林。你可以安全地留在原地，过着普通的生活；你也可以冒险穿越丛林，过着绝妙的生活。你将如何对待这一选择。...这就是你生活的价值观的体现，没有好坏，只有选择。<br /></li><li>我发现，想要追求卓越，你就必须挑战自己的极限，而挑战自己的极限可能会使你一蹶不振，这将造成很大的痛苦。你会觉得自己已经失败了，但这不一定是失败，除非你自己放弃。...你能做的最重要的事情是总结这些失败提供的教训，学会谦逊和极度开明的心态，从而增加你成功的概率，然后继续挑战自己的极限。<br /></li><li>几乎总是存在一条你还没有发现的有利的道路，所以你需要不断的找下去，直到找到它，而不是满足于最初对你显而易见的那种选择。<br /></li><li>对冲，就像赌大小，你有赌资100元，你看好是开大，但是你只在大那里押了60元，而在小那里押了40元，这就是对冲风险。<br /></li><li>我懂得了最重要的事情并不是预知未来，而是知道在每一个时间点上如何针对可获得的信息作出合理的回应。（随机应变）<br /></li><li>成熟意味着你可以放弃一些好的选择，从而追求更好的选择。（舍得，有舍才有得）<br /></li><li>我相信拥有把某件事情探究明白的能力，要比拥有如何做某件事的具体知识更重要。<br /></li><li>拥有几个良好的、互不相关的回报流，要比只有一个好，而且，知道如何结合不同的回报流，要比能够选出好的回报流更有效果。<br /></li><li><strong>每个人的大脑都有两部分----层次较高的逻辑部分与层次较低的情绪部分。称之为“两个我”，他们会争夺对于一个人的控制权。大脑的逻辑部分可以轻易理解了解自身弱点是一件好事（因为这是克服弱点的第一步），但大脑的情绪部分通常讨厌这么做。</strong><br /></li><li>现金是长期投资产品里面最差的一种，因为它会因为通胀和税收因素而不断贬值。<br /></li><li>如果管理者不知道员工的不同的思维方式，他就不知道员工将如何处理不同的情况，就像一个工程领队不知道他的设备将如何工作一样。<br /></li><li><strong>MBTI职业性格测试</strong>。<br /></li><li><strong>痛苦和迫切的压力最能激发人的学习动力。</strong><br /></li><li>在我看来，一个管理者能够实现的最大成功就是能够使其他人在没有你的情况下把事情做好。<br /></li><li>人生由三个阶段组成：在第一个阶段，我们依赖其他人，我们学习；在第二个阶段，其他人依赖我们，我们工作；在第三个阶段，当其他人不再依赖我们、我们也不必工作时，我们就可以自由地体验生活了。<br /></li><li>塑造者<br />一个塑造者通常会在别人的质疑和反对之下提出独特和有价值的愿景，并以美好的方式将其实现。<br />他们都能同时看到大图景和小细节（以及中间的层次），并能综合在不同层次上总结的观点，而大多数人通常是见此不见彼。<br />马斯克要发火箭，我问他在火箭学方面有没有背景，他说没有，我刚开始读这方面的书。 这就是塑造者典型的思考和行为方式。<br /><strong>在塑造者自我评估中，他们都在一项上打了低分："顾及他人"。但这并不代表他们真的不关心他人。他们只是，每当面对是实现自己的目标还是取悦他人（或不让他人失望）时，他们都会选择实现自己的目标。</strong><br />塑造者=理想家+务实思考者+坚毅者<br /></li><li>好思维源于探求原则背后的逻辑。<br /></li><li><strong>消费支出和通胀的驱动因素是消费支出水平，而消费支出是货币和信贷之和，不仅仅是货币量。</strong><br /></li><li><strong>视频《经济机器是如何运行的》 30分钟</strong><br /></li><li>国家的行为方式更为自私，将自身利益最大化是唯一的目标，他们的行为是为了支持自身选民的利益。大多数决策者在踏入职业生涯的时候是理想主义者，离开时已经幻灭。<br /></li><li>创始人离开企业后，企业要想成功转型，只需做两件事：让胜任的人做CEO，同时拥有一套有效的治理机制，可以在CEO无法胜任的情况下替换他。<br /></li><li>幸福是奋斗出来的，其实真正的幸福是奋斗的过程，而不是结果。结果的持续是不长的。<br /></li><li>生活总有顺境和逆境，努力拼搏并不只会让你的顺境变得更好，还会让你的逆境变得不那么糟糕。<br /></li><li>拥有更多东西所产生的边际收益会很快下降。事实上，得到适量的东西比得到太多的东西更好，因为后者会伴随着沉重的负担。<br /></li><li><strong>我不能说一种充满成就的紧张的人生就一定比充满享受的轻松人生更好，但是我敢说，坚强比软弱好，而拼搏让人坚强。<br />发现自己的性格，过与性格相适应的生活，才是最幸福的。</strong><br /></li><li>传递知识就像传递基因，其意义超过了单个的人，因为人死了基因还会延续下去。<br /></li><li>我已经是一个超级现实的人，所以我学会了欣赏所有现实的美好，包括严酷的现实，并开始鄙视不现实的理想主义。<br /></li><li><strong>梦想+现实+决心=成功的生活。</strong><br /></li><li>当真相与愿望不符时，大多人人抗拒真相。这很糟糕，因为好东西会自己照顾自己，而理解和应对不好的东西才是最重要的。<br /></li><li>不要担心其他人的看法，使之成为你的障碍。<br /></li><li>努力从自然的角度观察事物，自然会走向整体的最优化，而不是个体的最优化，但多数人只是根据事物对自身的影响判断好坏。<br />自然创造了各种激励机制，促使追求自身利益的个体带来整体的进步。<br />所以，我发现现实和自然的规律---包括我以及一切东西如何分解、重组----都是美好的。尽管在情感上，对于将与我关心的东西最终分离，我很难接受。<br />当我开始从理解现实规律的视角看待现实，而不是认为现实不对时，我发现几乎所有起初看起来“不好”的东西，如雨天、缺点甚至死亡，都是由于我对于自己像要拥有的东西持有先入为主之见。我逐渐意识到，我产生这些最初的反应，是因为我没有把事物放到大背景下看待，即显示的构造是让整体实现最优，而不是尽力实现我的愿望。<br /></li><li>我们可以看到完美是不存在的，它是一个目标，激发永不停息的进化过程。<br /></li><li>你在一生中取得什么样的成就，将取决于你如何看待事物，以及你关心什么人，什么东西。<br /></li><li>弗洛伊德：爱与工作是人性基石。<br /></li><li>更高层次的思考：人类拥有独特的从更高层次俯视的能力，这不仅适用于理解现实和现实背后的因果关系，也适用于俯视自身和周围的人。<br /></li><li><strong>成功有两条路：（1）自己拥有成功所需的要素；（2）从其他人那里得到成功所需的要素。第二条路你要谦逊。谦逊和你自己有能力一样重要，甚至比你有能力更重要。（借力）</strong><br /></li><li><strong>影响合理决策的两个最大的障碍是你的自我意识和思维盲点。<br />自我意识障碍，是指你潜意识里的防卫机制，它使你难以接受自己的错误和弱点。</strong><br /></li><li>有创造力的人通常持有发散思维，而线性思维的人往往更值得信赖。<br /></li><li><strong>亚里士多德：悲剧就是人的致命缺陷导致的可怕结果。</strong><br /></li><li>头脑极度开放。<br />决策分两步：分析所有相关信息，然后决定。<br /></li><li><strong>团队就像一个交响乐团，有一个统一的指挥（管理者），有不同技艺、风格不一的乐手。<br />乐队指挥者是勾勒结果，并确保乐队所有成员一起发力实现目标。指挥要确保每个乐队成员知道自己的长处和短处，以及各自的职责。不是每个人都自己演奏得最好，而是通过合作实现1+1&gt;2的效果。指挥最吃力不讨好的工作之一是开除总是不能好好演奏或合作的人。</strong><br /></li><li>很多很有创造力的人都曾患有双相障碍。<br /></li><li>人类永远处在塑造人类的两极力量之间：“一极是激发罪恶的个体选择，另一极是激发美德的群体选择。”<br />集体利益不仅对组织是最好的，对组织里的每个人也是最好的。<br /></li><li>如果你能坚持某种行为约18个月，你就会形成一种几乎要永远做下去的强烈倾向。<br /></li><li>习惯能让你的大脑进入“自动导航模式”。用神经科学的术语说，就是让基底核从大脑皮层那里接管控制权，这样你想都不用想就能执行。（相当于加载到内存中，执行速度更快。）<br /></li><li>从身体锻炼到学习冥想，很多种训练都能给人脑带来物理性和生理性的变化，从而影响人的思维与记忆能力。<br /></li><li>内向与外向。内向者聚焦于内心世界，从思想、记忆和经验中，汲取能量；外向者聚焦于外部，从与人相处中汲取能量。<br />内向者更喜欢独立思考，想明白了才和别人交流。相比在群体中发言，内向者通常更喜欢书面沟通（如电子邮件），并倾向于不公开自己的批评性想法。<br /></li><li><strong>一个能从木工中得到内心满足的木匠，能轻松拥有和美国总统一样美好甚至比美国总统更好的生活。大自然塑造万事万物皆有目的。你最需要的勇气不是驱使你战胜别人的勇气，而是不管其他人对你有何冀望，你始终坚持做最真实的自我的勇气。</strong><br /></li><li>有效决策<br />用系统化、可复制的方式把你的所有决策做好，同时还能以非常清晰准确的方式描述决策程序，从而让处在同样情况下的任何人都能做出同样的高质量的决策。<br />影响好决策最大威胁是有害的情绪，始终提醒自己，至少听一下某种相反的观点，永远都没有害处。<br />你能做的最重要的决定之一是决定问谁。<br /><strong>不要听到什么信什么，许多人会把观点表述为事实。</strong><br /><strong>所有东西放在眼前看最大。在生活中的所有方面，正在发生的事情似乎很大，回头来看则不然。所以你应该跳出去以看到全局，有时候可以过一段时间再做决定。（事缓则圆）</strong><br />不要过度分析细节，综合分析的时候始终要看到大局。<br />高效地综合考虑各个层次，就像在谷歌地图上看你的家乡，不同层次看到的东西是不一样的。<br /><strong>预期价值 = 奖励 * 赢的概率 - 惩罚 * 输的概率<br />预期价值最高的决策是最好的决策。</strong><br /></li><li>荣格：除非你意识到你的潜意识，否则潜意识将主导你的人生，而你将其称为命运。<br /></li><li>我们在桥水利用计算机系统，就像司机利用GPS一样，是用系统来辅助我们的导航能力，而不是替代它。例如前面桥突然断了，人会看到，判断然后停车，而GPS不会。<br /></li><li><strong>生活原则总结：拥抱现实并妥善对应现实，实现你愿望的五步流程，头脑极度开放，理解人与人大不相同。</strong><br /></li><li><strong>一个时代有一个时代的问题，一代人有一代人的使命。</strong><br /></li><li><strong>创意择优 = 极度求真 + 极度透明 + 可信度加权的决策。</strong><br /></li><li><strong>如果有人把工作当做一场燃烧激情、完成使命的游戏，那么可以说，工作原则就是为了这样的人而写的。</strong><br /></li><li>若不想当面议论别人，背地里也不要说，要批评别人就当面指出来。<br /></li><li>像家庭意味着存在无条件的爱和永恒的关系，像团队则表明如果每个人的贡献越多，整个团队就越强。<br /></li><li>新创公司的带头人最重要的素质就是对利益分配的处理方式。俗话说，财散人聚、财聚人散，例如刘邦和项羽就是典型的结果。<br /></li><li>3-5个精明能干且善于思考的人以开放心态讨论，通常能找到问题的最佳答案。<br /></li><li><strong>远离心态封闭的人。<br />价值观不同，会导致很多痛苦的经历和不好的结局，最终还是会让你们疏离。一旦你发现此种情形，最好尽快分道扬镳。</strong><br /></li><li><strong>要更关注发言人的推理过程，而非其结论。 用的是什么数据，基于什么样的逻辑得出结论。</strong><br /></li><li>如果无法达成共识，你应该把分歧提交给合适的上级去决策。<br /></li><li>如果不解决重大分歧，在短期内看似容易避免对抗，但长期可能导致非常具有破坏性的结果。<br /></li><li><strong>着眼大局：从部门的角度看待个人的问题，从公司的层面看待部门的问题，从社会的层面看待公司的问题。</strong><br /></li><li>要考虑你寻找的人应该具备什么样的价值观、能力和技艺（按此顺序）<br />选那些会问很多好问题的人。聪明人总能问出有思想的问题，而非对凡事都能给出答案。与好答案相比，好问题是表明未来成功的更好的指标。<br /></li><li><strong>依人发薪，而非依岗位发薪。</strong><br /></li><li>要更多想着如何把蛋糕做大，而非怎样切蛋糕才能使自己获得最大的一块。<br /></li><li>正确面对批评。<br />严厉的爱既是最难给的，也是最重要的爱（因为它很不受欢迎）。虽然大多数人爱听好话，但准确的批评更加难得。<br /></li><li>出色的管理者重视协调，而非亲力亲为。就像乐队的指挥，他们并不演奏某个乐器，却能指挥整个乐队演奏精彩的乐章。<br />出色的管理着的标志是他不必亲自做任何事。管理者应视自己陷入细枝末节为不良信号。<br /></li><li><strong>允许你下属随时越级向你汇报。这是一个非常好用的提升责任心的方法。</strong><br /></li><li>别想当然的认为员工的答案都是正确的，你应该时不时进行交叉核验。<br /></li><li><strong>欢迎被问责。欢迎被人问责你很重要，因为没有人能够客观看待自己。当你被问责时，保持冷静至关重要。如果你自己能经受此类问责，这会提升你的人格素质，让你保持镇定。</strong><br /></li><li><strong>像公司拥有者那样思考，例如休假也不应忘记责任！ （从公司利益出发，工作外的时间也需要考虑公司的事情。）</strong><br /></li><li>做标准、做制度、做平台、自动化<br /></li><li>如何从你公司总体的目标和价值观中，推导出具体的目标----节约成本，让客户更加满意，对需要帮助的人提供帮助等。<br /></li><li><strong>关心员工：在一个人身处困境时，必须对其进行私下慰问。</strong><br /></li><li>拥有好的挑战者胜过拥有好的追随者。<br /></li><li>要经常向有智慧的人寻求建议。<br /></li><li><strong>有状况及时反馈给领导。要把自己的忧虑提出来，使得老板知道风险所在，让老板和上级责任人能够就下一步的工作达成共识。</strong><br /></li><li>问题（解决问题）是驱动公司前进的动力。直面问题激发出了重要的创新和改进。<br /></li><li>担心出差错为你提供了一种保护，而对差错毫不担心，则可能让你暴露于风险。<br /></li><li>在设计组织架构时，要围绕目标而不是围绕任务。<br /></li><li><strong>每个人都必须由一位具有可信度的、奉行高标准的人来监督。<br />缺乏严格的监督，就可能导致质量控制不到位、培训不足、对优异业绩缺乏认可。不要盲目地相信人们自然地能把工作做好。（关键节点必须有跟进，结果必须要有验收）</strong><br /></li><li>为确保正确完成关键任务，宁要“做两遍”而不要“二次确认”。<br /></li><li><strong>尽管临时工和顾问有助于迅速解决问题，但不能培养公司的能力。</strong><br /></li><li>桥水公司的价值观和战略目标--- 通过极度求真和极度透明，创造佳绩，从事有意义的工作，保持有意义的人际关系。<br /></li><li>进行调查并让员工知道你将开展调查。作为一种机制，而不是突然袭击，这样员工也明白，这是一种机制的要求。<br /></li><li><strong>要知道几乎做每件事所花费的时间和资金都比你预期的更多。<br />几乎没有一件事情能够完全按计划进行，这是因为先前的计划并未考虑过失败的情况。我个人假定，每件事与计划相比实际都要花费1.5倍的时间和金钱。</strong><br /></li><li><strong>时间管理</strong><ul><li>通过优先级排序或者拒绝来减少工作量<br /></li><li>授权给合适的人去做<br /></li><li>提高工作效率<br /></li></ul></li><li><strong>鸣钟庆祝：当你和你的团队经过艰苦的努力成功地达成目标，庆祝吧。</strong><br /></li><li><strong>你读过多少本关于奉劝你改变行为的书，而你也希望改变但最终未能改变？如果没有工具和计划的帮助，你觉得阅读本书你能改变多少？我猜的是几乎没有什么改变。</strong><br /></li><li><strong>治理是一套监督系统，能够把表现不佳的员工、运行不理的流程清除掉。这是一个制衡的程序，能够确保公司的原则和总体的利益总是置于个人或少部分人的利益之上。</strong></li></ul><p><img src="/img/原则1.jpg" /><br /><img src="/img/原则2.jpg" /><br /><img src="/img/原则3.jpg" /><br /><img src="/img/原则4.jpg" /><br /><img src="/img/原则5.jpg" /><br /><img src="/img/原则6.jpg" /><br /><img src="/img/原则7.jpg" /><br /><img src="/img/原则8.jpg" /><br /><img src="/img/原则9.jpg" /><br /><img src="/img/原则10.jpg" /><br /><img src="/img/原则11.jpg" /><br /><img src="/img/原则13.jpg" /><br /><img src="/img/原则14.jpg" /><br /><img src="/img/原则18.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 生活 </tag>
            
            <tag> 工作 </tag>
            
            <tag> 原则 </tag>
            
            <tag> 桥水 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《反脆弱》读书心得</title>
      <link href="/2018/03/26/%E3%80%8A%E5%8F%8D%E8%84%86%E5%BC%B1%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2018/03/26/%E3%80%8A%E5%8F%8D%E8%84%86%E5%BC%B1%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>【<strong>中心思想</strong>】全书主题就是一句话：“杀不死我的，只会让我更坚强。”就像作者书后总结的：“<strong>验证你是否活着的最好方式，就是查验你是否喜欢变化。请记住，如果不觉得饥饿，山珍野味也会味同嚼蜡；如果没有辛勤的付出，得到的结果将毫无意义；同样的，没有经历过伤痛，便不懂得欢乐；没有经历过磨难，信念就不会坚固；被剥夺了个人风险，合乎道德的生活自然也没有意义。”</strong><br />根据作者给反脆弱下的定义：“我们几乎总能使用一个简单的不对称测试来检测反脆弱性和脆弱性：<strong>从随机事件（或一定冲击）中获得有利结果大于不利结果的就是反脆弱的，反之则是脆弱的。”例如人体就是反脆弱的，适当的压力会给人体带来好处，而瓷器就不是</strong>。在一个不对称系统中，有利因素超过不利因素，选择权赋予人获取有利因素而不受不利因素影响的机会，选择权就是反脆弱性的武器。<br />在对波动性、随机性、压力、挫折等面前，作者是典型的斯多葛主义，强调需要驯化自己的情绪，并且作者强调这些（适度的前提下）是有利人的肌体的。<br />作者后面讨论到科学时，强调自然主义、经验主义的观点。作者推崇人类以自然的规律来生活，对一些小病通过医学的处理是持怀疑态度的，按照作者说法，私人医生（家庭医生）只会缩短你的寿命。对于科学，作者认为是实践出真知的，而不是因为有了理论才有实践。<strong>理论、模型的东西，往往是参数过度简化过之后的复杂算法模型，按照模型得出的结论，往往与现实的混沌世界背道而驰。</strong><br />总的来说，<strong>对于随机性、不确定性和混沌也是一样：你要利用它们，而不是躲避它们。你要成为火，渴望得到风的吹拂。</strong></p><span id="more"></span><h1 id="文章摘要">文章摘要</h1><ul><li>尼采有句话，杀不死我的，只会让我更坚强。<br /></li><li>复杂系统内部充满着难以察觉的相互依赖关系和非线性反应。<br /></li><li>我们发展到今天是得益于一些冒甘愿冒险、甘愿承担失误后果的人，他们是值得社会去鼓励、保护和尊重的人。<br /></li><li><strong>事实上，越简单越好。复杂机制会导致意想不到的连锁反应。</strong><br /></li><li>乔布斯就认识到：“你必须努力理顺你的思维，才能使其更简单明了。”<br /></li><li>现代社会已经用法律条文取代了道德，但法律是可以被一个高明的律师玩弄于股掌之中的。<br /></li><li>三元结构根据特征将事物分为三类：脆弱类、强韧类、反脆弱类。<br /></li><li><strong>事物的反脆弱性一般以某个压力水平为限，对肌体的打击往往会让身体受益，但以一定的程度为限。</strong><br />系统在缺乏压力的情况下会被削弱，而在遭受压力的时候变得更为坚强，一个典型的例子就是人体。<br /></li><li>有一则古代传说，是希腊流传的古闪米特族和埃及传说，说的是凤凰，传说中霓裳羽衣的鸟儿。每次他被焚毁，都会从灰烬中重生，并恢复到新生的状态。<br /></li><li><strong>名词是思想的载体。</strong><br /></li><li>所谓“领域”就是某一类别的活动。<br /></li><li><strong>从挫折中产生的强于常人的动力和意志力也是额外的能力。</strong><br /></li><li><strong>如果你想做出改变，并且不关心未来结果的多样可能性，认为大多数结果都会对你有利，那么你就具有反脆弱性。</strong><br /></li><li>启蒙运动的一大贡献就是将个体置于更重要的位置，其权利、自由、独立、对幸福的追求，以及最重要的隐私都得到了保护。<br /></li><li>简化导致最严重问题的情况往往出现在用线性来简化和代替非线性的情况。<br /></li><li><strong>压力就是信息</strong> 在一个自然的环境中，压力源就是信息。重要的信号，总有方法触动你。<br /></li><li><strong>波动性有助于改善系统</strong><br /></li><li><strong>拥抱随机性、波动、挫折。</strong> 我爱随机性，压力与不确定性在生活中大有裨益。 没有波动就没有稳定。<br />概括地说，人为的压制波动性不仅会导致系统变得极其脆弱，同时系统也不会呈现出明显的风险。我们说过波动性就是信息，事实上，这些系统往往过于风平浪静，而其表面之下却暗流涌动。<br /></li><li>帝国和集权是不同的，集权制国家依赖于中央集权官僚制度，而帝国，如罗马帝国和奥斯曼王朝则更依赖于当地的精英，事实上，帝国允许城邦的繁荣发展并保留一定的有效自治权----对和平更有利的是，这些自治是商业自治而不是军事自治。<br /></li><li>戴维·休谟在《英格兰史》中就主张国家要小，因为大国更容易受到战争的诱惑。<br /></li><li><strong>事缓则圆：事实上，人类是非常不擅长过滤信息的，特别是短期信息。而拖延则是帮助我们筛选信息的较好方式，他们避免我们由于冲动而轻信某个信息。<br />从容镇静的人是懂得对真实信息做出反应的人，而神经过敏的人则大多是在对噪声做出反应。 </strong><br />因个人能力和智力的欠缺，无法区分噪声和信号，往往是过度干预背后的原因。<br /></li><li><strong>依赖于外部的认可，有损健康。<br />一个人所受的尊重，是与他为了自己的理念而承受的风险成正比例的。 </strong><br /></li><li><strong>胖子托尼使用的模型很简单，他识别脆弱性，在脆弱事物的崩溃上下赌注。</strong><br /></li><li>坏事对人的触动远大于好事。 塞翁失马<br /></li><li><strong>斯多葛主义的主旨就是情绪的驯化，而不一定是情绪的消除。</strong><br /></li><li><strong>我们可能会被剥脱一些东西----但是，善行和美德是不会被剥夺的</strong><br /></li><li>路径依赖性。<br /></li><li>乔布斯的理念是，人们根本不知道他们想要什么，直到你提供给他们。<br /></li><li><strong>参与到合理的试错活动中，失败了也不觉得耻辱，而是重新来过，再次失败，再次重来。</strong><br />为了让试错过程不完全是随机性的，你需要保持理性。每一次失败都能提供额外的信息，每一次信息都比前一个信息更有价值。<br /></li><li>去聚会的可选择性，对于希望从不确定中收益以及受不利因素的影响最小的人来说，参加聚会是最好的建议。<br /></li><li><strong>选择权=不对称性+理性</strong><br />基本的不对称性：有利因素超过不利因素，选择权赋予人获取有利因素而不受不利因素影响的机会。选择权就是反脆弱性的武器。<br /></li><li>技术的定义：科学知识在实际项目中的应用。<br /></li><li>那些拥有可选择权的人，只会报告最符合其目的的东西。把故事好的一面呈现出来，把糟糕的一面隐藏起来。<br /></li><li><strong>大脑中装满太多复杂的技巧和方法，往往会使人忽略基本的东西。</strong><br /></li><li>模式识别，洞察事物的内在逻辑。<br /></li><li>交易员的推理比公式有效得多。 经验的作用，直觉比公式更有用。<br /></li><li>思想根植于传统的启发法（经验法则）所扮演的角色。<br /></li><li>实践出真知，我们是在实践中创造出理论。<br /></li><li>现在人们无条件的相信有组织的科学发展，这几乎取代了对有组织宗教的无条件信仰。<br /></li><li>没有证据表明，战略规划起到了作用----否定它的证据倒是有很多。规划使公司无视选择权，因为它的行动方针已经过于僵化以至于无法把握稍纵即逝的机遇了。<br />个人注：规划还是有用，至少让你想清楚的将要做什么，目标是什么，只是说规划必须与时俱进。不能受规划的束缚。<br /></li><li><strong>风险投资：寻找一个在其职业生涯中能换6种或7种，甚至更多种工作的人。<br />我学会了怎样挑选那些在坐着什么也不做的情况下，有能力和别人打成一片的人。我以人们的周旋能力为筛选标准进行选择，好学的人一般不善于与人周旋：他们需要一个明确的任务。</strong><br /></li><li>虽然我的父亲似乎并不重视教育，但却重视文化和金钱----他鼓励我去寻找这两样东西。他总是对学者和商人很感兴趣。<br /></li><li>追随自己的爱好来选择阅读。<br /></li><li><strong>不让别人来构架问题的框架，问题的答案永远根植于问题之中：千万不要直接回答一个对你来说毫无意义的问题。</strong><br /></li><li>有些事情我不能理解，但这不一定表示我是无知的。<br /></li><li>概率在现实世界中并不重要，重要的事件带来的影响。例如，飞机95%的安全，你也不会去坐。而赌注有51%的胜率，你可能就去参与了。<br /></li><li>在银行家的眼里，我就像混在一年一度前往麦加朝圣的伊斯兰教徒中的耶稣会传教士一样。<br /></li><li>在项目管理中，项目规模增加将带来不良后果，而且项目延误导致的成本在总预算中的占比会增高。<br /></li><li>项目的非线性结构，最弱的环节往往决定了项目的成败。<br /></li><li>忍痛效应：出事的时候，额外的安全措施成本昂贵，而这些额外的费用也并未被纳入他们最初作决定时所做的那种在电脑上看起来很不错的成本效益分析。<br /></li><li>脆弱性总是隐藏在非线性中。<br /></li><li>哪些模型是脆弱的，哪些不是脆弱的。只需对假设进行给一个小小的变更，然后看看影响有多大，以及这种影响是否会持续加剧。<br /></li><li>如果你拥有有利的不对称性，或正凸性（选择权是特例），从长远来看，你会做得相当不错，在不确定的情况下表现优于平均数。不确定性越强，可选择性的作用越大，你的表现越好。这个属性对人生来说非常重要。<br /></li><li><strong>教皇问米开朗基罗，他如何雕刻出了大卫雕像，这个被视为所有杰作中的杰作的雕像。米开朗基罗回答：“这很简单，我只是剔除了所有不属于大卫的部分。”</strong><br /></li><li><strong>很少有人知道，我们在许多事情上正逐渐从80/20变成更不均衡的99/1，也就是99%的互联网流量都是不到1%的网站创造的，99%的图书销量是不到1%的作者贡献的。当今，几乎所有东西都有赢者通吃的效应。</strong><br /></li><li>时间有撕碎一切的锋利牙齿。<br /></li><li>只要踏进博物馆，你内在的审美思维就与这些古人相通了。<br /></li><li><strong>让孩子们多读经典著作，未来包含在过去之中。</strong><br /></li><li>勒·柯布西耶将房屋称为“居住的机器。”<br />城市生活原本应该是在大街小巷中品味的。<br /><strong>对雅各布斯来说，城市是一个有机体，但对摩西来说，城市则是一台需要改良的机器。</strong><br /></li><li>脆弱的事物过度依赖于所谓的科学方法，而不是经时间验证的启发法。<br /></li><li>安逸的本身就会使人脆弱。<br /></li><li>如果病人几近健康，那么大自然才是真正的医生。<br /></li><li>进化是靠无定向的、凸性的自由探索或试错来推进的，因此本质上是强韧的，因为它能从连续的、重复的、细小的、局部的错误中获得潜在的随机收益。<br /></li><li><strong>养生法则是：愉悦的心情、充足的休息以及适当的缺乏营养。</strong><br /></li><li>社交网络阻碍了人们的正常社交。<br /></li><li>其实罗马人不喜欢舒适的生活，因为了解它的副作用。<br /></li><li>宗教的目的就是保护我们免受科学主义的伤害。<br /></li><li>不规律在某些领域是有其益处的；规律性有时也有其危害。<br /></li><li>按古人的逻辑行事。<br /></li><li>伦理与法律的逐渐割裂。<br /></li><li><strong>对斯多葛学派的人来说，审慎是勇气（战胜自己冲动的勇气）的固有要素。</strong><br />现代人对勇气的定义：勇敢的站起来支持一个想法，并在激动的状态中享受死亡，只因为获得了为真相而死的权力或站起来维护自己价值观的权力。<br /></li><li>除非努力进取，否则尊严将一文不值；除非你愿意为它付出代价。<br /></li><li><strong>如果你承担风险，那么，那些不承担风险的人带给你的侮辱只不过如同牲畜的吠叫：你不可能因为狗朝着你狂吠而感觉受到了侮辱。</strong><br /></li><li>所谓“知识世界”会导致知识和行动的分离（同一个人不是既懂得知识又知道行动），并导致社会的脆弱性。<br /></li><li>文字总有这样的特性，不是最正确，却是最迷人的----或者说，那些能拿出最冠冕堂皇的说辞的人才会获胜。<br /></li><li><strong>愚蠢的人总是力图证明自己正确，而聪明人则力图赚钱。或者：愚蠢的人总是力图赢得辩论，而聪明人则寻求获利。再换句话说：辩论失利未尝不是一件好事。</strong><br /></li><li>哲学是唯一不需要与现实连接起来的领域。<br /></li><li>预言是对信念的承诺，仅此而已。先知不是第一个产生某个想法的人，他是第一个相信这个想法的人，而且始终相信。<br /></li><li><strong>他们不是创业家，只是演员而已，并且是华而不实的演员（商学院更像是表演学校）。</strong><br /></li><li>一个公司没有自然伦理，它只服从于资产负债表。<br /></li><li>如果你不得不在流氓的的承诺和公务员之间做出选择，那就选择前者吧。任何时候，机构都是没有荣誉感的，个人才会有荣誉感。<br />只有荣誉感才会促进商业的发展。<br /></li><li><strong>只有有勇气的人才敢于自由地发表意见。</strong><br /></li><li>法律规定越复杂，社会网络越官僚，就有越多深谙系统该漏洞和缺陷的主管官员从中受益，因为他的主管优势将是其专业知识的凸性函数。<br /><strong>法律规定越复杂，业内人士越容易找到套利的机会。</strong><br /></li><li>他们提出意见只是为了其本人的利益，但他们却将自己粉饰成为了集体的利益而呼吁。<br /></li><li><strong>一切宗教律法都可以归结为一条黄金法则的细化、应用和诠释，即“己所不欲勿施于人”。</strong><br /></li><li>达摩克里斯之剑 The Sword of Damocles<br />少即是多：less is more.</li></ul><p><img src="/img/反脆弱1.jpg" /><br /><img src="/img/反脆弱2.jpg" /><br /><img src="/img/反脆弱3.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 反脆弱 </tag>
            
            <tag> 斯多葛主义 </tag>
            
            <tag> 不对称 </tag>
            
            <tag> 随机性 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《DevOps软件架构师行动指南》读书心得</title>
      <link href="/2018/03/18/%E3%80%8ADevOps%E8%BD%AF%E4%BB%B6%E6%9E%B6%E6%9E%84%E5%B8%88%E8%A1%8C%E5%8A%A8%E6%8C%87%E5%8D%97%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2018/03/18/%E3%80%8ADevOps%E8%BD%AF%E4%BB%B6%E6%9E%B6%E6%9E%84%E5%B8%88%E8%A1%8C%E5%8A%A8%E6%8C%87%E5%8D%97%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>【<strong>中心思想</strong>】<br />本书讨论了DevOps的定义、范围、实践以及相关案例。作者从目标的角度，给DevOps下了如下定义：<strong>DevOps是一套实践方法，在保证高质量的前提下缩短系统变更从提交到部署至生产环境的时间。</strong>其核心是缩短部署时间，根据这一目标，就必须减少部署过程的运维和开发的协同时间、必须减少构建、测试和发布的时间。根据这一目标，通过DevOps的实践，通过重新定义运维和开发的职责来减少协同，通过自动化的工具，来提高构建、测试以及发布的效率，同时，通过自动化的工具也把运维和开发的显示协同变成了隐式协同。<span id="more"></span><br />在职责分工方面，DevOps包括：</p><ul><li>1、让开发人员和运维人员互相沟通；<br /></li><li>2、运维人员提前介入，在需求阶段提出系统的运维性需求；（例如日志及监控相关的需求、维护发布向后兼容及软件可切换的特性）<br /></li><li>3、允许开发团队自动化地部署到生产环境；<br /></li><li>4、当生产环境中发现错误时，让开发团队成为第一个响应者。（新部署的内容在开始一段时间内由开发人员负主要责任，之后由运维人员负主要责任。</li></ul><p>另外，本书也提出在组织内推动方案落地的工作方式，具体见下图：<br /><img src="/img/DevOps11.jpg" /><br /><img src="/img/DevOps12.jpg" /></p><h1 id="文章摘要">文章摘要</h1><ul><li>脚本和文件也应该进行版本控制，也应该进行错误检查。这个术语常常称为：“<strong>基础架构即代码（infrastructure-as-code)</strong>”。<br /></li><li>5个为什么（5 Whys）是一个确定根本原因的技巧。不停的问为什么，直到发现原因。<br /></li><li>确定一个组织的文化的方法之一是看它的激励产生什么样的结果。<br /><strong>一般来说，开发人员收到的激励是做出变更（发布新代码），运维人员收到的激励是不要变更。这两种不同的激励机制培育出不同的态度，可能成为文化冲突的原因。<br />组织中的两个部门有一个共同的目标----确保组织取得成功。</strong><br /></li><li>一般来说，一个人对自己的团队最忠诚，其次才是整个组织。<br /></li><li>使用工具意味着需要遵守隐含在工具中的策略。<br /></li><li>DNS服务器可能会轮换服务器的顺序以便提供某种程度的负载均衡。<br /></li><li>NoSQL 起初，这个名字的意思是No SQL，由于现在有些系统支持SQL，所以现在它代表Not Only SQL。<br /></li><li>模块是一个具有一致功能的代码单元，而组件是一个可执行单位。<br /></li><li>开发团队应该实践一些防御性编程。<br /></li><li><strong>测试框架的一个关键特征是，它产生报表。</strong><br /></li><li><strong>环境拆解：在资源的目的已经实现后，很容易失去对资源的跟踪。处于安全的目的，每个虚拟机都必须打补丁，不使用、失去跟踪的资源为恶意用户提供了一个攻击点。</strong><br /></li><li>功能开关：常见的实践是把功能的开关放到配置文件中。<br /></li><li><strong>配置参数是一个能够更改系统行为的外部设置变量</strong>。配置设置可以是：希望展示给用户的语言、数据文件的位置、线程池的大小、屏幕上背景颜色或功能开关设置。<br />可以按照使用时间把配置参数分组。<br /></li><li><strong>构建是从源代码和配置等输入创建一个可执行工件（例如二进制可执行文件）的过程，包括编译和打包等过程。构建的目的是创建适合于部署的东西。</strong><br /></li><li>集成测试是测试已构建的可执行工件的步骤。环境包括与外部服务的联系，例如代理数据库。<br /></li><li><strong>一个开发人员把测试代码连接到了生产数据库，这样的例子我们听到多次了。</strong><br /></li><li>服务的注册可以包含版本号，然后客户端就能够请求具体版本的服务。<br /></li><li>虚拟机是运行在虚拟机监视器上的一个镜像，虚拟机镜像可以包含多个独立的进程----每一个进程就是一个服务。<br /></li><li><strong>计算的目的是洞察世界，而不是计数。</strong><br /></li><li>性能度量包括：延迟、吞吐量、使用率。<br />吞吐量一般用每秒事务数（TPS）；<br />使用率是资源使用的相对量，通常是通过在感兴趣的资源中插入<strong>探针</strong>来度量。<br /></li><li>如果系统主动提供了被监控的数据（例如简单网络管理协议SNMP，因为网络设备经常作为一个封闭的系统出现），那么监控是入侵式的且影响系统设计。如果系统不主动提供被监控的数据，那么监控是非入侵式的，且不影响系统设计。<br /></li><li><strong>监控是基于时间或基于事件的。</strong><br /></li><li>微服务架构，会让你的系统变成扇形系统或深层次系统。<br /></li><li><strong>对虚拟机强制一个较短的生命周期是一个用来防止虚拟机泛滥的技术。</strong><br /></li><li>如果你不能将你所做的事情描述为一个过程，那么你不知道你在做什么。<br />过程模型是一组活动的规格说明，当这些活动执行时，导致完成所需的结果。<br /></li><li><strong>审计记录必须加密，并且要独立于被审计系统进行存储，同时保护对这些记录的访问。<br />不要把审计追踪和日志混为一谈。审计追踪持续存在数月或者数年，且有法律地位，主要用于安全目的。日志持续存在时间是以日（甚至更短）来计算的，它主要用于支持运维和开发的需求。</strong><br /></li><li>云平台的一个核心特征是编程接口和自动化框架的广泛可用性。<br /></li><li>协同工具：JIRA，问题追踪和软件开发<br /></li><li><strong>最小化权限原则。</strong><br /></li><li>Notation：符号<br />JSON：JavaScript Object Notation。 小的JavaScript对象表示法。<br /></li><li>定时任务 cronjob<br /><strong>守护程序：daemon</strong><br />平面文件：flat file<br />领域特定语言：Domain Specific Language，DSL<br /></li><li>ITIL是对运维组的最重要功能进行组织的体系。<br /></li><li><strong>发布计划的规定之一就是测试回滚计划。回滚意味着恢复到之前的发布。也可能前滚，也就是说，修正错误并生成包含错误修正的新版本。由于回滚的敏感性以及前滚的可能性，所以自动触发回滚很少。</strong><br /></li><li>管理依赖关系！<br /></li><li>数据模式！<br /></li><li>团队之间的协作是最消耗时间的。如果开发团队接受DevOps的职责，即开发团队交付、支持并维护服务，那么因为所有所需的知识都保留在开发团队，所以向运维团队和支持人员转交知识也就少了。不需要转移知识也就省掉了部署过程中的大量协作步骤。<br />（其实类似敏捷的全功能团队也是为了减少协同的成本）<br />DevOps的目标的一部分是通过使用隐形和不经常的协作代替直接的协作来实现。</li></ul><p><img src="/img/DevOps1.jpg" /><br /><img src="/img/DevOps2.jpg" title="DevOps生命周期价值链" /><br /><img src="/img/DevOps3.jpg" title="部署流水线" /><br /><img src="/img/DevOps4.jpg" title="生产环境引流测试" /><br /><img src="/img/DevOps5.jpg" title="服务持续改善流程（ITIL）" /><br /><img src="/img/DevOps6.jpg" title="部署流水线" /><br /><img src="/img/DevOps7.jpg" title="功能开关" /><br /><img src="/img/DevOps8.jpg" title="监控目的" /><br /><img src="/img/DevOps9.jpg" title="监控架构" /><br /><img src="/img/DevOps10.jpg" title="威胁模型" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DevOps </tag>
            
            <tag> 运维 </tag>
            
            <tag> 开发 </tag>
            
            <tag> 软件架构师 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《自控力》读书心得</title>
      <link href="/2018/03/14/%E3%80%8A%E8%87%AA%E6%8E%A7%E5%8A%9B%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2018/03/14/%E3%80%8A%E8%87%AA%E6%8E%A7%E5%8A%9B%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>【<strong>中心思想</strong>】<br />本书主要是通过案例分析如何提高自己的意志力和自控力，书中提出了很多方法。针对我想要、我要做、我不要等方面来提高自己的意志力，并且每个章节都总结出提高意志力的日常实验方法，指导读者去锻炼提高自控力。<strong>提高意志力有几个重要的方面，锻炼、冥想（深呼吸）、看书等等，只要自己持之以恒，总是在潜移默化中得到提高。</strong><br />作者提到，<strong>人是一个矛盾的综合体，自控力最强的人，不是从与自我的较量中获得自控，而是学会了如何接受相互冲突的自我，并将这些自我融为一体。</strong><br /><span id="more"></span></p><h1 id="冥想训练">冥想训练</h1><p><strong>冥想训练里做的事正是我们在生活中也要面对的 ---- 把自己的注意力收回，专注于最初的目标（在冥想训练中目标就是专注呼吸）。</strong><br />冥想不是让你什么都不想，而是让你不要太分心，不要忘了最初的目标。如果你在冥想时没法集中注意力,别担心，你只需多做练习，将注意力重新集中到呼呼上。<br />将呼吸频率降低到每分钟4 ~ 6次，也就是每次呼吸用10 ~ 15秒的时间。</p><p><img src="/img/自控力4.jpg" /><br /><img src="/img/自控力5.jpg" /></p><h1 id="我想要的力量">我想要的力量</h1><p><strong>把“我不要”变成“我想要”</strong><br /><strong>一个重要的意志力规则是：你若你觉得自己没有时间和精力去处理“我想要”做的事，那就把他安排在你意志力最强的时候。</strong><br />（例如坚持背单词，可以放在早上意志力最坚强的时候）<br />自控力就像肌肉一样有极限，它被使用之后会渐渐疲惫。如果你不让肌肉休息，你就会完全失去力量，就像运动员把自己逼到筋疲力尽时一样。<br />所以，需要把意志力用到刀刃上。<br /><img src="/img/自控力6.jpg" /></p><h1 id="意志力极限">意志力极限</h1><p>疲惫不是一种身体反应，而是一种感觉，一种情绪。<br />疲惫只不过是大脑产生的某种反应，好让我们停下来。<br /><strong>动员知道，第一波疲惫感绝对不是自己的极限，只要有了足够的动力，他们就能挺过去。（当你感到无法坚持的时候，再坚持一会）</strong></p><h1 id="缓解压力的方法">缓解压力的方法</h1><p><strong>最有效的解压方法包括：锻炼或参加体育活动，祈祷或参加宗教活动、阅读、听音乐、与家人朋友相处、按摩、外出散步、冥想或作瑜伽，以及培养有创意的爱好。（找出自己的方法，我自己就是锻炼、看书）</strong><br /><strong>失败的时候，请原谅自己</strong>：你只是个凡人，没有人都会有失去自控的时候，这只是人性的组成部分。你能想象你尊敬、关心的其他人也经历过同样的抗争和挫折吗？这个视角会让自我批评和自我怀疑的声音变得不那么尖锐。<br /><img src="/img/自控力7.jpg" /></p><h1 id="意志力训练">意志力训练</h1><p><img src="/img/自控力8.jpg" /><br /><img src="/img/自控力9.jpg" /></p><ul><li><strong>预先承诺：</strong>要实现自己的目标，我们就必须<strong>限制自己的选择</strong>，这称为“预先承诺”。<br /></li><li><strong>自豪感的力量：</strong>（想象<strong>别人的目光是很有激励作用的</strong>），为了让自豪感发挥作用，我们必须认为别人都在监视自己，或我们有机会向别人报告自己的成功。<br />让自己坚定决心的有效策略是：公开你的意志力挑战。如果你相信别人会支持你走向成功并观察你的行为，你就会更有动力区做正确的事。<br /></li><li><strong>变成集体项目</strong></li></ul><h1 id="讽刺性反弹">讽刺性反弹</h1><p>当人们不再试图控制那些不希望出现的想法和情绪时，它们也就不会来烦你了。（类似睡眠一样）<br />如果不和焦虑对抗，焦虑就会自然离去。<br /><strong>不要抑制想法，接受它的存在，但不要相信它。你要接受的想法是：这些想法总是来来去去，你无法控制会出现什么想法，但你不必接受它的内容。换而言之，你可以对自己说：”好吧，那种想法又来了，又得心烦。不过，这就是思维的运作方式，它并不一定意味着什么。”</strong><br /><img src="/img/自控力10.jpg" /></p><h1 id="文章摘要">文章摘要</h1><ul><li><strong>所谓意志力，就是控制自己的注意力、情绪和欲望的能力。 </strong><br /></li><li>自知之明是自控的基础，认识到自己的意志力存在问题，则是自控的关键。自己的意志力缺陷是人之常情。<br /></li><li>神经学家发现，如果你经常让大脑冥想，他不仅会变得擅长冥想，还会提升你的自控力，提升你集中注意力、管理压力、克制冲动和认识自我的能力。<br /></li><li>科学研究发现，自控力不仅和心理有关，更和生理有关。只有在大脑和身体同时作用的瞬间，你才有力量克服冲动。<br /></li><li><strong>适度的压力是有意义的健康生活不可或缺的一部分。</strong><br /></li><li>大部分人认为，取得进步会刺激我们获得更大的成功。但心里学家知道，我们总是把进步当做放松的借口。在完成某个目标过程中取得的进步，会刺激人们做出妨碍完成目标的行为。</li></ul><p><img src="/img/自控力1.jpg" /><br /><img src="/img/自控力2.jpg" /><br /><img src="/img/自控力3.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 心理学 </tag>
            
            <tag> 自控力 </tag>
            
            <tag> 意志力 </tag>
            
            <tag> 冥想 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《系统思维》读书心得</title>
      <link href="/2018/03/10/%E3%80%8A%E7%B3%BB%E7%BB%9F%E6%80%9D%E7%BB%B4%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2018/03/10/%E3%80%8A%E7%B3%BB%E7%BB%9F%E6%80%9D%E7%BB%B4%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>【<strong>中心思想</strong>】<br />我们人类思维是<strong>从分析思维（处理多个独立变量之科学）到整体思维（处理相互依赖变量之科学和艺术）的转变。</strong><br />分析思维是一个包含三个步骤的思维过程。首先，它把需要了解的东西给拆开；接着尝试去单独解释被拆开的每一块的行为表现；最后，再把对各个快的理解汇集到一起，构成对整体的解释。而<strong>系统思维则采用不同的过程，它把系统放到它所属的一个更大的环境中，去理解和学习它在这个更大整体内所扮演的角色。</strong><br />全书从开放性、目的性、反直觉行为、多维度、突现性五个维度对系统思维进行了阐述，同时，结合人类社会、商业社会，对几个典型的案例的系统设计思维进行分析说明。<br />最开始以为本书是讲信息化系统（IT系统）的整体思维以及相关的设计思想，看了之后，才知道主要是针对人类社会以及商业社会的整体思维进行讲述的。<br /><span id="more"></span></p><h1 id="系统思维的五个维度">系统思维的五个维度</h1><p><img src="/img/系统思维4.jpg" title="系统思维的五个维度" /></p><ul><li><strong>对系统的管理需要加强对交互环境的管理，即向上管理。系统所需的领导力也应该是如何影响不可控关系人的能力。</strong><br /></li><li>理智选择是可以规避风险的，而情感选择不会，风险恰恰是刺激与挑战的一个重要属性。<br /></li><li>目的性系统是一个价值导向的系统。<br /></li><li><strong>世界并不是由那些所谓正确的人所掌控的，而是由那些可以说服他人认同自己的人来掌控的。（所以，要学会说服别人的能力）</strong><br /></li><li>突现性的本性决定了我们无法对其进行分析，也无法利用分析类的工具进行操作，也不符合任何因果关系的解释。比如生命的现象，这个最值得注意的突现性属性，却没有人能够找到生命真正的起源。<br /></li><li><strong>各部分之间良好的协调和增强的交互作用会产生一种共鸣。而这种共鸣的力量比每个部分的力量相加之和大一个数量级。（类似球队的化学反应）</strong><br /></li><li>反直觉行为的含义是行为产生与预料完全相反的结果。就如常说的地狱之路是由善意铺成的。事情可能在转机之前就变得更糟，或者反之亦然，我们的成败都有可能源于错误的理由。<br /></li><li>混沌理论肯定了迭代在发现复杂模式过程中的基础性作用。这与我长久以来坚信<strong>迭代是整体方法的本质以及设计方法学的核心</strong>的观点不谋而合。<br /></li><li>我们永远不能低估自身对变化的抵抗程度。传统的观点就像守旧派，他宁愿牺牲也不会投降。<br /></li><li><strong>对熟悉环境的舒适感加上对未来的恐惧产生了一种强大的力量，甚至会超越变化可能带来的潜在的自身利益。人们可能因为美好的初衷而被打动，甚至全心全意地给予支持。但是当这个想法开始临近实施时，不安全感和自我怀疑便开始产生。支持者们潜意识中可能已经开始破坏自己的努力成果，并阻止改变的发生。</strong><br /></li><li>系统动力学中一个被普遍认同的观点是，量变在某个关键点之后就会带来质变。<br /></li><li>有时候，超前于时代的人往往比落后的更加可悲。<br /></li><li><strong>被动适应一个恶化的环境，是通向灾难之路。</strong></li></ul><h1 id="文化与技术">文化与技术</h1><p><strong>系统思维认为文明是文化（软件)和技术(硬件)交互中涌现出的产物。技术是无国界的，不受阻碍的增长，而文化是局部的，固执的抗拒变化。</strong><br /><img src="/img/系统思维12.jpg" title="文化和技术" /></p><h1 id="系统思考的四大基础">系统思考的四大基础</h1><p><img src="/img/系统思维3.jpg" title="系统思考的四大基础" /></p><ul><li>整体思维，把系统放到更大的、包含它的系统之中，去研究它所产生的影响。</li></ul><h1 id="文章摘要">文章摘要</h1><ul><li><strong>生物系统主要通过遗传密码来达到自组织，社会系统则通过文化密码，而社会系统的DNA就是他们的文化。</strong><br /></li><li>系统思维经历了三代不同的变化：<br />第一代系统思维（运筹学）处理在机械（决定性）系统中，相互依赖性带来的挑战。<br />第二代系统思维（控制论和开放系统）处理在生命系统中，相互依赖性和自组织（负熵）带来的双重的挑战。<br />第三代系统思维（设计）处理在社会文化系统中，相互依赖、自组织和选择带来的三重挑战。<br /></li><li><strong>文化决定了一个群体的伦理规范。如果程序员使用一种高级计算机语言编写代码，如果不进行额外配置，该语言就会使用期默认的参数。同样，如果不进行明确的选择，文化就会将它的默认价值观强加给其中的参与者。（文化作为默认的决策系统）</strong><br /></li><li>人要学着时不时停下来，审视自身，然后再出发。<br /></li><li>以服务于企业的需求来服务自身。<br /></li><li><strong>大规模生产不仅产生了量变，还引起了问题本身的质变。问题已经不再是如何生产了，而是如何销售。</strong><br /></li><li>组织规模和组织控制系统的效能是反比关系，这使得大企业不得不走向分权之路。但是这样的结果又违背了无差异及统一指挥原则。<br /><strong>权利下放会导致混乱和局部优化。解决生产问题的最佳答案可能和解决营销问题的最佳答案相冲突，可能也不是财务和人事想要的最佳答案。这或许就是大多数企业总是不断在集权和分权之间摇摆震荡的原因。</strong>（分久必合、合久必分）<br /></li><li><strong>选择在人的成长中处于核心位置</strong>，成长是对选择能力的提升；设计是选择和整体思维的载体。<br />（个人注：类似黑客帝国中Leo的世界）<br /></li><li>控制，意味着采取必要且充分的行动获得所期望的结果。而影响，以为这采取的行动是非充分的，只是所获得结果的合力之一。<br /></li><li>伟大的热力学第二定律断言，宇宙作为一个封闭的系统，有着消除所有特异性的趋势。这样的话，宇宙的终极状态便是同一和随机；但是量子理论的一个重要发现就是，认识到宇宙其实是一个开放系统，开放系统是负熵并且趋于秩序化的。<br /></li><li>生命系统和环境之间的交互是认知活动，生命的过程就是认知的过程。生活就是一个知识的积累过程。意识是一种特殊的认知过程，是认知达到一定复杂性后呈现出的结果。<br /></li><li>过去，空间是宇宙的主要成分，因为原子的99.999%都是空的。但是在量子世界观中，空间被赋予了新的意义，不再有所谓的空无一物，空间中的任何地方，现在都被认为充满了各种“场”。不可见，不能摸、听不见、尝不着、没气味的“场”。我们的五感感觉不到，却如同存在于其中的粒子一般真实。<br /></li><li>没有欲望的能力是无力的，就如同没有能力的欲望是无望一样。<br /></li><li><strong>化冲突为竞争关键是寻找共同的目标，寻找的层次越高，找到共同目标的可能性就越大。</strong><br /></li><li>复杂性是一个相对的概念，它依赖于所牵涉的变量的数目和交互特性。<br /></li><li>需要注意的是，我们的数学工具大多是基于线性的。而不是非线性的。<strong>在线性系统里，整体的取值可以由其部分的值累加而得到。相反，非线性系统是典型的突显性属性，整体由其部分的交互产生。 </strong><br /></li><li>现实中的数学模型对参数、变量及相互关系进行简化模拟，但是，现实社会是一个混沌系统，对初始差异是非常敏感的。<br /></li><li><strong>本质上，那些交互定义了相关性，进而导致了系统中的非线性。正是相关性造成了我们认知能力的最大挑战，而这种挑战就需要我们以运筹建模来应对。模式识别是理解和改变这种不受欢迎行为的关键，这直接促使我们产生了对要研究的现象，开发交互式运筹绘图的需求。</strong><br /></li><li>成本的75%是由设计决定的。<br /></li><li><strong>平台是为某一类模块预先定义好的宿主。这类模块中的每一个都可能有自己独特的功能，但是他们都有一个共同特征，这个特征定义了他们与系统的其他平台和模块的关系性质。从本质上说，平台是一个预先定义好的接口，他的关键功能是简化和管理其所有模块类与整体框架中其余部分之间的交互。</strong><br /></li><li>目的指出了系统存在的原因，他回答的是“为什么”的问题。规格则回答“是什么”的问题。设计回答了“如何做”的问题。<br /></li><li>孤立的局部优化一组相互依赖的变量中的每一部分，并不能够优化整个系统。<br /></li><li>分别对待成本最小化和收入最大化这两个目标，在系统内部产生了一个基本的矛盾。为实现收入最大化，销售人员就偏向于增加产品的种类，添加定制化的功能，短时间内交付产品，等等。而另一方面，为实现生产成本最小化则可以简单的通过标准化生产过程来达到，但这也意味着减少产品的数量和长期的生产进度。因此基本矛盾就产生了。解决市场问题的最佳答案是牺牲生产。反之亦然。...对这个问题的一个完全不同的解决方案是寻求绩效指标的兼容性，而非在不兼容的集合之间寻找折中。...我们关注的真正重点事项相容性，也就是绩效指标必须按照一方成功，不会导致另一方失败的原则来设计。<br /></li><li>在市场经济中，价值是由用户来定义的，随着我们朝着经济全球化的发展，价格逐渐变成不可控变量。而成本，因为技术的进步，正逐渐变为可控变量。<br /></li><li><strong>组织理论主要处理两种关系：职责（谁对什么负责）和权力(谁对谁汇报）。</strong><br /></li><li>在选择任何度量标准时，一个重要的考虑是其简单性。产生一个度量标准的成本，不应该超过其产生信息的价值。尽管应该更倾向客观度量，但如果获取客观度量结果的代价过高，那么就可以使用一个主管度量代替。记住，集体的主观便是客观。比如在评价一个体操运动员的表现时，我们是基于多个不同的裁判的集体裁决。<br /></li><li>在社会学领域里，因果在时空上是分离的。一个行动所带来的后果可能过一段时间后才能体现。<br /></li><li>医疗系统面向处于最脆弱状况的人群，涉及生活中最为敏感的方面。<br /></li><li><strong>质量、成本和时间相互关联，并且形成了一个互补的整体。牺牲其一，必然损失其他。</strong><br /></li><li><strong>组织将变得扁平化，决策能在相关的最低层作出。 </strong><br /></li><li><strong>一蹴而就！</strong></li></ul><p><img src="/img/系统思维1.jpg" /><br /><img src="/img/系统思维2.jpg" /><br /><img src="/img/系统思维5.jpg" /><br /><img src="/img/系统思维6.jpg" /><br /><img src="/img/系统思维7.jpg" /><br /><img src="/img/系统思维8.jpg" /><br /><img src="/img/系统思维9.jpg" /><br /><img src="/img/系统思维10.jpg" /><br /><img src="/img/系统思维11.jpg" /><br /><img src="/img/系统思维13.jpg" /><br /><img src="/img/系统思维14.jpg" /><br /><img src="/img/系统思维15.jpg" /><br /><img src="/img/系统思维16.jpg" /><br /><img src="/img/系统思维17.jpg" /><br /><img src="/img/系统思维18.jpg" /><br /><img src="/img/系统思维19.jpg" /><br /><img src="/img/系统思维20.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 系统思维 </tag>
            
            <tag> 运筹学 </tag>
            
            <tag> 模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《聘谁》读书心得</title>
      <link href="/2018/02/24/%E3%80%8A%E8%81%98%E8%B0%81%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2018/02/24/%E3%80%8A%E8%81%98%E8%B0%81%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p><strong>【中心思想】</strong><br />本书介绍了如何挑选A级选手的方法，按作者的说法，选材的成功来自于4项重要的步骤：建标、判别、当断和融入。针对每个步骤进行了详细的说明。很关键的一步是建标（积分卡），也就是说你必须知道自己需要的人才是怎么样，然后才能找到合适的人才。<br /><span id="more"></span></p><h1 id="筛选面试提问">筛选面试提问</h1><p>1、职业目标<br /><strong>英才知道自己想做什么，并且无惧于告诉你他们的目标！</strong><br />A:CIO<br />2、职业专长<br />要求选手讲8-12项；<br />要他们举例说明自己的强项是如何发挥作用的。如果他们说自己果断，就要他们举出哪次果断让其获益匪浅。<br />A:<br />韧劲：清理数据；<br />创新：看板<br />责任感：PM<br />团队建设<br />人才培养<br />坚持：跑步、看书及写读书笔记<br />有计划性：每天晚上想好第二天要做的事情，早上列出来，最条处理。<br />服务意识：<br />适合和业务沟通：共同的成功，能获和业务建立信任。<br />正直<br />3、不擅长什么，或对什么不感兴趣<br />5-8项缺点<br />A:<br />事务性的工作:之前太最求完美，所以很多事情都事必躬亲，最近也慢慢学会了授权，慢慢把自己从事务性的工作中抽身。例如，开始每次需求都要仔细过一遍，UAT前功能都要过一遍。后来，慢慢就只需要在项目经理报告需求的关键控制点、功能点就行了；<br />应酬？<br />嘴笨<br />谨慎：运维的习惯，安全第一。<br />顾家<br />4、过去老板给你的打分。<br />A:8分，问出为何你觉得老板会给你8分？会说明自己的强项和弱项。</p><h1 id="升级面试提问指南">升级面试提问指南</h1><p>1、聘你去做什么的？<br />打探他昔日的积分卡<br />勾画出之前的使命和关键成果。<br />A:上线、数据清理、重构。<br />2、最骄傲的成就<br />A:管理创新：看板、量化考核；看板，项目管理好了，信息透明，阳光是组好的杀毒剂；量化考核，提高项目的管理，进度和成本，质量通过了量化的考核。<br />3、工作的低谷<br />A:中间有一个领导，我那时候是负责系统上线，因为我们当时按照需求、开发、实施来分工；俗话说，三分技术、七分管理、十分实施；但是我们领导是银行过来的，银行的业务很规范，实施的工作基本上是业务在推动，但是在房地产行业，业务的人员的系统思维和素质相对较低，需要IT人员带领他们、推动他们上线。<br />我们领导开始认为这个工作是没有必要的，所以开始的观念上还是有冲突，当时把整个实施部门转为了技术支持部门。<br />但是在过程当中，确实出现很多系统上线不理想，后来在项目组中还是增加了实施人员，不过，后来我在负责老大难的系统PM，还是比较好的完成任务。<br />后来领导走的时候，他也发了一封邮件给我，点评了我，说当时他对技术上更看重。不过后来，他还是比较欣赏我的责任心，也看到我在技术能力方面的进步，也鼓励我多关注技术方面的能力。<br />不过那段时间，确实是，我也看了很多技术方面的书。自己的提升了很多。<br />回头看，低谷确实是一个人成长的最好阶段！<br />4、跟谁共事<br />老板叫什么，感觉如何；老板认为你的强项和弱项。<br />A:<br />阮：学院派，系统性强。执行力强，理论不足；<br />万：实战派，推动能力强，文化上不适应；责任心强，技术不足；<br />李：运维，稳扎稳打，魄力不够；正直、管理能力强，冲劲不足；<br />5、为何终止那份工作<br />A:<br />小孩初中毕业，发展的天花板，并且老板也离职，正好换一个环境。<br />导火线：理念不合；自建，明源的基础上优化，合作的态度。<br />6、职业生涯的介绍<br />按时间顺序（成果）<br />1）运维、实施<br />2）开发Java<br />3）基础架构，数据中心建设<br />4）信息化实施</p><p>5）开发管理</p><h1 id="专项面试提问">专项面试提问</h1><p>1、<br />2、最大成就<br />A:<br />数据中心建设（监控zabbix）、销售系统上下（最好的项目）、PM系统（保证了稳定可用）、管理创新（看板、绩效考核）<br />遗憾：敏捷转型，领导比较保守，没有推进。 因为这个是涉及整个公司的。<br /><strong>现状图</strong><br />3、最大的错误和教训<br />A:<br />被动，受业务影响。 只考虑了应用哦规划、技术规划，没有考虑数据规划。导致后期的数据问题、报表问题满足不了业务的要求。<br />容量规划<br />要做好规划（和业务沟通，根据业务的目标和业务的核心问题），然后实施</p><h1 id="文章摘要">文章摘要</h1><ul><li><strong>充满好奇：“什么”，“如何”，“告诉我更多”</strong><br /></li><li>在当下资源及资讯同质化的竞争格局中，人成了跨度和变数最大的因素，甚至是决定性因素。优秀的人更能够善用有效地资源及资讯，使其产生预期甚至是超出预期的效能，从而赢得胜利，为企业、为自己赚取更多的利益。<br /></li><li>优秀的人才本身就应具有独立思考的能力和行为经验<br /></li><li>别盲目崇信技术，若不系统化地运用软件，世界上再先进的高科技跟踪方法也无法发挥作用。 <strong>软件只是工具，关键还是使用的人！</strong><br /></li><li>跳槽就跟结婚一样，我会告诉他们我是谁，是什么样的人，如果你们觉得不适合，可以不要我。<br /></li><li>如果你觉得他符合要求，那他很可能就是那个合适的人，如果你有任何犹豫，或者你觉得还需要进一步考察，那么，毫不留情地将其剔除。只邀请那些自身情况跟积分卡要求强烈吻合的候选人。<br /></li><li>花60%的时间都用于考虑“人”的问题，把“人”排进头3号要事。 请关注“人”，别光盯着“事”<br /></li><li>董事会喜欢“绵羊型人才”，他们乐于接受反馈，擅长倾听，尊重他们，并精通软手腕。因为他们容易相处。我们的研究发现：绵羊型成功的概率是57%，这还不错。 （中规中矩）<br />第二种人才是“猎豹型人才”，他们行动迅速、进攻性强、工作卖力、坚韧不拔、并对人们高标准要求、严格考核。 根据我们的研究：猎豹型人才的成功率是100%。每只猎豹都能为投资者创造非凡的价值。 （敏捷专注）<br /></li><li>能力、团队精神、态度！<br /></li><li>让人达到理想业绩的重要因素是能力，而让人能够保持理想业绩的重要因素是动力。<br /></li><li>积分卡描述了岗位使命、所需实现成果、工作能力以及对企业文化的适应性。</li></ul><p><img src="/img/聘谁1.jpg" /><br /><img src="/img/聘谁2.jpg" /><br /><img src="/img/聘谁3.jpg" /><br /><img src="/img/聘谁4.jpg" /><br /><img src="/img/聘谁5.jpg" /><br /><img src="/img/聘谁6.jpg" /><br /><img src="/img/聘谁7.jpg" /><br /><img src="/img/聘谁8.jpg" /><br /><img src="/img/聘谁9.jpg" /><br /><img src="/img/聘谁10.jpg" /><br /><img src="/img/聘谁11.jpg" /><br /><img src="/img/聘谁12.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 招聘 </tag>
            
            <tag> 简历 </tag>
            
            <tag> 面试 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《目标II》读书心得</title>
      <link href="/2018/02/18/%E3%80%8A%E7%9B%AE%E6%A0%87II%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2018/02/18/%E3%80%8A%E7%9B%AE%E6%A0%87II%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p><strong>【中心思想】</strong><br />本书从思考的方法入手：<strong>要改善些什么？要改善成什么样子？怎样才能有效地、一步一步地执行改革？</strong>然后通过讲故事的方式，引出了如何在管理过程中，通过冲突图、现状图、过渡图、未来图、条件图等方式来找出问题、解决问题的过程。其中的核心是冲突图和现状图。<br /><strong>冲突图</strong>能定义出分歧的共同目标，然后找出方案，在满足共同目标的前提下，消除冲突。画出冲突图总是开始的第一步，是我们解决问题的第一步。<br /><strong>现状图</strong>，通过列出负面清单，找出负面清单的因果关系，从而发现核心问题，然后聚焦解决核心的问题，带来革命性的变化。</p><p>本书也提出了一个很有价值的观点，那就是“价值的认知”。<strong>公司对产品价值的认知出自于已付出的努力，而市场对产品价值的认知来自使用产品所带来的好处。</strong>所以，满足市场对产品价值的认知成为更重要的成功之钥。我们需要采取行动，有效提高市场对产品价值的认知。（换位思考，从客户的角度来思考问题，思考价值）。<strong>从市场的观点来看，对产品的看法包括了相关的服务、财务条件、保证等，产品包括了整套的交易！产品之外的价值可以称之为周边效益，这一部分的改进相对容易，但是能带来很大的价值认知改变</strong>。而周边效益又分为正面效应和负面效应，在消除负面效应的效果（核心的问题）更能达到立竿见影的效应。<br />同时，同一个市场内不同部分可能对于同一个产品有不同的价值认知。（例如机票就是这样，购买时间、购买地点、个人购买或团体，价格都可以不同），这些就是区隔市场的概念：当一部分的市场的价格变动不会造成另一部分的市场价格变动时，我们称之为彼此区隔的市场。可以通过区隔市场，和周边效益一并考虑来提高公司的效率。<br /><span id="more"></span></p><h1 id="冲突图">冲突图</h1><p>冲突图并非都是第一个步骤，只有当现有状况在你的心里架构完整时，才能使用冲突图。</p><h1 id="现状图">现状图</h1><p>使用一套非常系统的方法，建立所谓的现状图，清楚列出所处状况里所有问题的因果关系。一旦做完，你会了解到你根本不需要处理这么多问题，因为到最后，你会发现，“核心问题”只有一两个。</p><ul><li>1、一开始，你先列一张“不良效应”的清单，大概5-10项问题。<br /></li><li>2、找出其中的因果关系（可以先找出其中两项的因果关系）<br />可以在两项因果关系之间，加入一个中间效应，以说明这两项没有直接联系的因果关系；<br />可以补充一些我们认为缺少的东西；<br />在复杂的个案中，至少都会发现一个严重的循环效应；<br /></li><li>3、找出核心的问题。</li></ul><h1 id="过渡图">过渡图</h1><p><strong>在给客户讲的时候，不能直接讲好处，而是通过提出客户的问题，我们的解决，这样引出公司的方案。这样比直接罗列公司的优点有效得多！（这个，类似是过渡图的方式）。这样，是帮客户解决问题。</strong> 问题可以是客户导致的问题，也可以是公司导致的问题，但是我们党的方案是解决了这些问题，来提供客户对产品价值的认知。</p><h1 id="未来图">未来图</h1><p><strong>欲擒故纵：业务人员讲完之后，提出让买主考虑一些时间，然后再约时间会面。这样能增加买主对方案的信任。（其实也是表达了业务人员对自己方案的信任！）</strong></p><h1 id="条件图">条件图</h1><p>目标-&gt;目标的障碍-&gt;中期目标（消除障碍的任务）-&gt;达成目标。<br />找到什么职位，障碍：有力人士的推荐；好的业绩；找出类似的空缺；</p><h1 id="文章摘要">文章摘要</h1><ul><li><strong>经理人的目标：平时能做出好的决定，需要的时候，能找出突破性的解决方案。</strong><br /></li><li>最佳效益是指在一个框框内做得最好，但有时候，危机来临时，需要突破性的解决方案，必须在框框外寻找解决方案。<br /></li><li>我们列出的不良效应，都来自同一个问题：经理人以达成局部效益为经营公司的目标！<br /></li><li><strong>用财务报表的数字来衡量公司的经营效益，就像用成绩来衡量一个学生一样。因为分数是最容易标准化、比较的指标。但是一个人的核心能力、素质是分数中体现不出来的，衡量分数是捡了芝麻丢了西瓜。但是话说回来。有分数衡量，至少能保证下限。</strong><br /></li><li>产品服务化。<br /></li><li>我们无法只单独谈目标，我们必须在一些限制条件之下谈目标。若定义个目标，但却没有定义达到目标过程中涉及的范围，那就没有意义。<br /></li><li>一个公司的目标有三个：保护股东利益、保护员工利益、满足市场需求。<br /></li><li><strong>一旦你发现目前的谈判已经陷入无法达成妥协的僵局时，就采取第一个步骤：马上停止对话。<br />第二个步骤：调整心态。</strong><br /></li><li>朱莉可以把任何可能的问题都转变成<strong>双赢</strong>的局面。<br /></li><li>通常的情况是，在我们动笔之前，总觉得有一堆理由，但等我们真正写下来的时候，理由却少了很多，而且更难为情的是，大部分理由都是些可怜的借口。<br /></li><li>目前的竞争激烈到为了要渗透市场，我们常常必须以成本价卖出基本配备，只能靠后续附加的配备及零件来赚钱。<br /></li><li><strong>高管的工作：思考、谈话、做决定。</strong><br />股东（董事），负责管理金钱，就像看门狗，看管投资的公司，使其产生做好的局部效益。<br /></li><li>养老基金的投资，是要确保相关人员在退休的时候，能拿到<strong>等额购买力</strong>的金钱，而不是等额的金钱。<br /></li><li>黑手党提案：令人无法抗拒的提案。<br /></li><li>迫使商店囤积更多的库存，只是将我们自己和市场的分隔得更远！<br /></li><li>我们应该以买主购买之后所能得到的益处来衡量公司的价格。而这些利益并不只限于收购回来的公司直接产生的利润！价值衡量。<br /></li><li><strong>曾经出现了一个天才，他发明了比较麦子和羊群价值的方法，并因为发明了金钱这个抽象的单位，一种货币</strong>，只是还没有人发明衡量安稳度或满意度的单位罢了。<br /></li><li>人们太习惯于怪罪无法改变外界环境，却不责怪自己未事先防范。就像蚱蜢责怪冬天，而蚂蚁却丰衣足食。<br /></li><li>歇后语：秋后的蚱蜢——蹦不了几天了。</li></ul><p><img src="/img/目标II1.jpg" /><br /><img src="/img/目标II2.jpg" /><br /><img src="/img/目标II3.jpg" /><br /><img src="/img/目标II4.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 目标 </tag>
            
            <tag> 冲突图 </tag>
            
            <tag> 现状图 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《灰犀牛》读书心得</title>
      <link href="/2018/02/14/%E3%80%8A%E7%81%B0%E7%8A%80%E7%89%9B%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2018/02/14/%E3%80%8A%E7%81%B0%E7%8A%80%E7%89%9B%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p><strong>【中心思想】</strong><br />灰犀牛式的危机就是那些大概率发生，而又会造成重大影响的危机事件（相对来说，黑天鹅事件是那些极小概率的事件）。<br />应对危机，需要我们有 <strong>冷静的头脑；理性的思维；积极的行动。</strong></p><span id="more"></span><h1 id="如何应对灰犀牛式危机">如何应对灰犀牛式危机</h1><ul><li>1、承认危机的存在；</li><li>2、定义灰犀牛式危机事件的性质；</li><li>3、不要静立不动 一定要提前制定一个计划，并充分利用这个计划；</li><li>4、不要浪费危机；</li><li>5、站在顺风处； 最好的领导会在危险尚未靠近的时候就采取行动；</li><li>6、成为发现灰犀牛危机的人，成为控制灰犀牛式危机的人。</li></ul><h1 id="灰犀牛的难点是">灰犀牛的难点是</h1><ul><li>1、你远远看见一个黑点，但是不能肯定的确就是黑犀牛；</li><li>2、而等你确认是黑犀牛的时候，已经来不及反应，或者反应的难度已经非常大了。</li></ul><h1 id="文章摘要">文章摘要</h1><ul><li><strong>幡然悔悟： fān 思想转变很快，彻底悔悟。幡然 : 同‘翻然’</strong></li><li><strong>以邻为壑，hè，是一个成语，意思是原谓将邻国当作沟坑，把本国的洪水排泄到那里去；后比喻把困难或灾祸推给别人。</strong><br /></li><li>未雨绸缪远胜于亡羊补牢。</li><li>黑天鹅事件是我们无法预见的事件，而灰犀牛是我们本来应该看到但是却忽略了的风险。</li><li><strong>15分钟沙漏，提醒自己去思考最简单的也是最基本的目标。</strong></li><li>卡桑德拉：凶事预言家，这个名词已经逐渐被用来指称那些持续对未来悲观失望的人----总的来说是一个不值得信任的人。<br />如今西方文化的核心理论之一：不要相信总是唱反调的人。</li><li><strong>词汇是思想的载体。</strong></li><li>有时候，预言没能应验，是因为我们事先发现了危险，并且采取了措施，它才没有真的发生。<br />但是，如果不发生，那预言还是准确的吗？</li><li>失败同胜利一样，可以撼动灵魂，释放荣耀；<br />打击可以帮助我们进一步看清问题，激发出人们的行动力。</li><li>当我们面临的问题迅速地从“慢性病”转变成“急性病”，那么我们用来解决问题的事件就会被极大地缩短，我们被打败的概率也会极大地增加。</li><li>如果我们能做一项全球范围的民意测验，评选出五个最容易令人产生幸福感的声音，我断定，葡萄酒瓶开启时，木塞砰的一下跳起来的声音，一定是其中之一。</li><li>在对的时间做对的事情！</li><li>你不会总是一次就能找到解决问题的方法。每一次错误都是通往正确答案的必要过程。有时候，那个当时看起来是个错误的举动，可能其实正是通往成功的必经之路。（失败是成功之母）</li><li>在压力面前，女性会趋于保守，而男性则相对趋于冒险！</li><li><strong>黎明前的黑暗，触底反弹。</strong></li><li>零废料填埋的标准。</li><li>中美两国是温室气体排放量最大的两个国家，约占了1/3的排放量。</li><li><strong>未来分析师：<br />最重要的是关注未来人口的结构变化，因为人口的结构变化把个人同主要的经济发展和政治变化趋势联系在一起，而且关联到其他的变化，尤其是科学技术和人工智能等在未来扮演了越来越重要的角色。</strong></li><li>犀牛没有汗腺，不能靠出汗来散发体热，所以就在雨后的地上挖坑，就在里面的泥浆里打滚，以此来给自己降温解暑，缓解皮肤上的不适。</li></ul><p><img src="/img/灰犀牛1.jpg" /> <img src="/img/灰犀牛2.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 危机 </tag>
            
            <tag> 应对 </tag>
            
            <tag> 灰犀牛 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《架构即未来》读书心得</title>
      <link href="/2018/02/06/%E3%80%8A%E6%9E%B6%E6%9E%84%E5%8D%B3%E6%9C%AA%E6%9D%A5%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2018/02/06/%E3%80%8A%E6%9E%B6%E6%9E%84%E5%8D%B3%E6%9C%AA%E6%9D%A5%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p><strong>【中心思想】</strong><br />本书从人员、组织、流程、技术等方面进行扩展性的讨论，有别于通常从技术的考虑。另外，全书贯穿了对股东利益的重视！</p><p>确实，像康威定律说的，产品就是组织架构的缩影。所以，人员、组织、流程对产品扩展性的影响不逊于技术带来的影响。做一个类比，一个球队要获得成功，必须要好的队员（人员）、好的阵型（组织）、好的战术（流程）。<br /><strong>简单的说，要达到可扩展，需要做如下两件事：建立敏捷性团队，使用可扩展性的架构原则（具体见下文）。</strong><br />技术团队需要明白，实现股东价值是第一要义，所以要经常思考用什么指标来衡量IT工作对业务的意义。技术团队能够理解业务所面临的挑战、风险、障碍和策略。<br />技术高管理解赚钱的业务手段、收入的驱动力、目前的财务现实、竞争格局和当年的财务目标非常关键，只有具备这种知识和能力的CTO才能制定出合理的技术战略来实现企业的目标。<br /><span id="more"></span></p><p><strong>敏捷团队的定义：我们把跨职能同时符合服务架构的组织，标示为敏捷性组织。</strong><br />见下图：<br /><img src="/img/敏捷型团队1.jpg" /><br /><img src="/img/敏捷型团队2.jpg" /><br />敏捷型组织专业技能的提高问题：<br /><img src="/img/敏捷型团队技能提高.jpg" /></p><p><strong>目标树：通过目标树的方法，可以很容易把一个组织的目标和公司的大目标结合起来。</strong><br /><img src="/img/目标树.jpg" /></p><p><strong>变更的管理：经验告诉我们，生产环境中发生的事故，有很大一部分是有软件和硬件的变更引起或触发的。</strong><br /><img src="/img/变更.jpg" /><br /><img src="/img/变更日历.jpg" /></p><p><strong>架构原则</strong><br /><img src="/img/架构原则的韦恩图.jpg" /><br /><img src="/img/架构原则.jpg" /></p><p><strong>外购或自建：</strong><br />聚焦成本策略：聚焦成本策略的主要弱点是不能专注于战略或差异化竞争；如果只注重成本可能就会导致之间系统的决策。<br />聚焦战略策略：我们是否是相关技术里最好的（前两名或者三名）供应商或开发商？研发或提供相关技术是否有助于可持续的差异化竞争。<br /><img src="/img/外购或自建.jpg" /></p><p><strong>风险模型：把故障隔离架构比喻成泳道，记住，泳道之间不要进行任何同步通信！</strong><br /><img src="/img/风险模型.jpg" /></p><p><strong>AKF扩展立方体：</strong><br /><img src="/img/AKF扩展立方体.jpg" /><br /><img src="/img/AKF扩展立方体1.jpg" /><br /><img src="/img/AKF扩展立方体总结.jpg" /></p><p><strong>NOSQL数据库</strong><br /><img src="/img/NOSQL数据库.jpg" /></p><p><strong>组织内的两种基本冲突类型：情感型、认知型； 情感型通常是不好的冲突，常常带有责任推诿的冲突，例如部门间的冲突；认知型通常是好的冲突，类似头脑风暴类的。</strong><br /><img src="/img/组织冲突图.jpg" /></p><h1 id="文章摘要">文章摘要</h1><ul><li>随着资源的逐渐枯竭，企业更倾向于哪些面向客户的短期的功能研发，而不是长期的扩展性项目。结果是满足了短期的目标，却牺牲了平台长远的活力。<br /></li><li>中国还缺少架构师，特别是有领导力的架构师。<br />架构师的责任是确保系统的设计和架构可以随着业务的发展而扩展。...架构是也负责制定代码设计和系统实施的技术标准。<br /></li><li>DevOps通常负责配置、运行和监控生产系统。<br /></li><li>管理工具能减低沟通成本；沟通成本 = n(n-1)/2 n代表人数；<br /></li><li><strong>亚马逊的“两张披萨饼团队”规则：任何一个团队的规模不能大过两张披萨所能喂饱的人数。</strong><br /></li><li><strong>管理是与“推”（pushing）相关的活动，而领导是与“拉”（pulling）相关的活动。领导设定目的地和通往目的地的路线图。管理设法达到目的地。...风格代表着个人对领导或管理任务的理解。如果一个人聚焦在事务处理方面，那么他就是一个经理，如果一个人更具有远见卓识，那他就是一个领导。...如果管理是推动组织爬坡，那么领导就是选择山头，鼓励员工一起努力翻越山头。...好的领导创造文化，聚焦打造具有高可扩展性的组织、流程和产品而取得成功。这种文化靠激励体系来确保公司能够在成本可控的情况下扩展，同时不影响用户体验和出现扩展性问题。</strong><br /></li><li>组织的架构很少有对或错之分，任何的结构都有利有弊。<br /></li><li>很多成功的领导者都具备“高管盘问”（executive interrogation)这个关键的能力。比尔盖茨版本的盘问技能叫“比尔·盖茨审查”。懂得什么时候去调查、去哪里调查，知道找出满意的答案。<br /></li><li>组织的两个关键属性：规模和结构。团队规模的下限是6人，上限是15人。<br /></li><li>一个不注重代码、文档、规范和配置标准的制定、发布和应用的组织，研发效率和质量必然低下，生产中出现严重问题的风险很大。<br /></li><li>功能点和场景点是度量功能的两个不同的标准化方法。功能点从用户角度出发，而场景点从工程师的角度出发。<strong>工程师在本性上对他们认为能够完成的任务过于乐观。</strong><br /></li><li><strong>技术人员开始思考如何做一个好的服务者而不是软件开发者。 产品思维！！</strong><br /></li><li>关系良好，差异大的团队可以更早的发现项目中的障碍和问题。<br /></li><li>领导力：影响一个组织或者个人达成某个特定目标的行为的力量。<br /></li><li>最好的领导在为股东争取利益方面大公无私。<br /></li><li><strong>如果愿景是描述你要到哪里，而使命是一个指引你怎么到那里的大方向（个人觉得使命应该是告诉你为什么到那里），那么目标就是旅行中确保我们走在正确道路上的路路标或者里程碑。</strong><br /></li><li>长期的实践让我们领悟到，对表现差的人要尽早淘汰。（不要尝试改变一个人的道理）<br /></li><li><strong>技术和业务建立信任的唯一途径就是通过共同的成功。</strong><br /></li><li>好的过程产生好的结果。...过程有助于增加组织的产出和规范重复性或不明确的任务，但太多的过程会拖累组织并增加成本。<br /></li><li>造成过程不适应或不匹配的最大问题是文化冲突。<br /></li><li>当危机发生时，我们要在最短时间内解决问题，而不考虑资源的回报和效率。<br />浪费危机是一件可怕的事情，这句格言支出了一个强烈的愿望，要通过事后处理、分析尽最大可能从危机中学习，以此改善我们的人员、过程、技术和架构。<br /></li><li><strong>危机中，需要领导者内在有令人难以置信的平静。 （胸有惊雷而面如平湖）</strong><br />最有效的领导人在危机中保持冷静，在整个危机管理的过程中能有力地维护秩序。他们必须有敏锐的业务头脑和技术经验，可以承受压力。<br /></li><li>工程团队有一种自然的倾向，他们相信自己能在没有外界或者管理团队的帮助下解决问题。这可能做得到，但仅仅解决是不够的，更需要快速和有效地解决问题。这通常超过工程团队能够掌握的，尤其是设计第三方供应商的时候。<br /></li><li><strong>承认错误，明确计划，确保不再发生。</strong><br /></li><li>商业在本质上是一种冒险的尝试。<br /></li><li><strong>如质量专家所熟知的，即使是中度复杂程度的系统，要确保软件无缺陷，在数学上也是不可能的。</strong><br /></li><li><strong>最好的负载是那些从真实的用户流量中复制出来的。有时，可以从应用或负载均衡器的日志里收集取得。</strong><br /></li><li>疏于度量注定你永远不会改进，疏于管理注定你迷失目标和愿景。<br /></li><li><strong>网站平台和后台系统的数据库模式不兼容是不能实施回滚过程最常见的原因。 通常的做法是，在早期的版本中修改应用（保持数据库不变），在稍后的版本中修改数据库。</strong><br /></li><li>数据库同步<br />复制数据库在同一数据中心一般在5秒内同步完毕，跨区域的一般在10秒内同步完毕。<br /></li><li>很多数据库的复制技术支持以表为单位的复制！<br /></li><li><strong>缓存是可扩展性工具箱中最好的工具之一</strong><br /></li><li>对象几乎都被处理或压缩成序列化格式，以最大限度地减少内存占用。Memcached是当今最流行的缓存方法之一。这是一个高性能，分布式的内存对象缓存系统。<br /></li><li>对许多系统而言，随着时间的推移，数据的价值在逐渐降低。虽然旧的客户联系信息可能还有价值，但却不如最新联系信息的价值高。<br /></li><li><strong>大数据的定义：大数据是指一个数据的集合，这个数据的集合如此庞大和复杂以致很难用传统的数据处理技术处理。</strong><br /></li><li><strong>是架构而不是技术要为产品的可扩展能力负责。</strong><br /></li><li><strong>虚拟化的概念是云计算背后真正的核心架构原则。</strong><br /></li><li>云和网格是不同的目的，云计算允许你扩大和缩小架构，网格将工作分解成许多并行的单位。<br /></li><li>技术人员仍然要对其应用的可扩展性和可用性负责。云端只是可以用来实现高可用性和可扩展性的另一种技术或架构，但它不能保证应用的可用性或可扩展性。<br /></li><li>数据中心通常考虑的因素：带宽的情况、电力成本、人力成本。<br /></li><li>监控必须监控业务数据方面的变化，这个才是反映股东价值的东西，而不是仅仅从技术层面来考虑监控指标！</li></ul><p><img src="/img/架构即未来1.jpg" /><br /><img src="/img/架构即未来2.jpg" /><br /><img src="/img/架构即未来3.jpg" /><br /><img src="/img/架构即未来4.jpg" /><br /><img src="/img/架构即未来10.jpg" /><br /><img src="/img/架构即未来20.jpg" /><br /><img src="/img/架构即未来22.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 架构 </tag>
            
            <tag> 扩展 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《企业IT架构转型之道》读书心得</title>
      <link href="/2018/01/19/%E3%80%8A%E4%BC%81%E4%B8%9AIT%E6%9E%B6%E6%9E%84%E8%BD%AC%E5%9E%8B%E4%B9%8B%E9%81%93%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2018/01/19/%E3%80%8A%E4%BC%81%E4%B8%9AIT%E6%9E%B6%E6%9E%84%E8%BD%AC%E5%9E%8B%E4%B9%8B%E9%81%93%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>作者结合阿里巴巴在架构转型的经验，谈了阿里从单体的架构转型到分布型架构的历程。书名中提到的中台，其实是阿里的IT架构中的中台，按作者的说法就是共享服务（IT服务）中心，具体见下图：<br /><img src="/img/IT架构转型之道2.jpg" /><br />本书总体的思路是，见下图：<br /><img src="/img/IT架构转型之道13.png" /><br /><span id="more"></span><br />在谈到共享服务中心的建设时，需要遵循以下几个原则：高内聚、低耦合原则；数据完整性原则；业务可运营性原则；渐进性的建设原则。<br /><img src="/img/IT架构转型之道12.jpg" /></p><p>以上内容提到的主要是技术层面，作者也顺便提到了IT核心竞争力打造的问题，IT的视角必须从技术转向业务创新，通过IT来驱动业务，这样才有可能提高IT的地位。所以<strong>企业信息化部门从以项目管理、系统运维等业务支持的职能向企业核心业务服务能力的沉淀和打造，并基于这些服务能力进行对内对外的运营职能转变；</strong>通过产品服务化，通过服务创新来体现IT的价值。</p><h1 id="文章摘要">文章摘要</h1><ul><li><strong>其实服务的重用将比数据重用带来更多的好处，数据只是原始生产资料，服务则包含逻辑，是工厂的加工车间，如果加工过程也一样可以复制，将带来生产效率的大幅度提升。</strong><br /></li><li>厚平台、薄应用<br /></li><li>IT系统建设中实现的业务得不到沉淀和持续发展，是对企业最大的伤害。<br /></li><li><strong>懂业务是指，能对业务下一步发展有着自己的理解和看法，对业务流程如何进一步优化能更好的提升业务，甚至对企业现有的业务提出创新的想法，为企业带来新的业务增长点</strong>。<br /></li><li><strong>SOA的初衷是服务的重用以快速响应业务的变化，但实际应用中，SOA的项目基本是沦为多个系统间的集成！</strong><br /></li><li>一个小小的功能改动可能给其他功能带来的未知风险，整个平台给人一种“<strong>牵一发而动全身</strong>”的感觉。<br /></li><li>给飞行中的飞机换发动机。<br /></li><li>服务器水位<br /></li><li>微服务是SOA的一种演化后的形态。<br /></li><li>数据模型、存储模型、对外提供的服务接口<br /></li><li>服务中心一定是实现通用的能力，个性化尽量在业务层实现。<br /></li><li>异步化与缓存两个技术都与系统的性能有很大的关系。<br /></li><li>分布式一致性协议Paxos。<br />分布式系统的事务处理：Google Chubby的作者Mike Burrows说过这个世界上只有一种一致性算法，那就是Paxos，其它的算法都是残次品。<br /></li><li>柔性事务的总结：<ul><li>应用程序一定要做到幂等实现，特别是对数据库进行数据修改操作时；<br /></li><li>远程模块之间用异步消息来驱动，异步消息还可以起到检查点的作用。<br /></li></ul></li><li>数据操作的时间<ul><li>内存数据库 纳秒级<br /></li><li>SSD盘数据 微秒级<br /></li><li>SATA盘数据 毫秒级<br /></li></ul></li><li>应用规模的扩大和复杂度的增加会造成开发和维护成本的急剧上升，这是软件工程学早就得出的结论。<br /></li><li><strong>岗位轮转推动真正的换位思考</strong>，让双方在实际工作中更真切地感知到处于不同岗位时对业务的理解和出发点。<br /></li><li><strong>该公司花了更多的时间在于应用的推广。毕竟，相对于技术上的变化，人的思维变化更不容易。</strong><br /></li><li>CRM（客户关系管理） 进化成SCRM（社会化关系型客户管理），通过直接与客户互动的社交性的方式进行客户关系的管理，而不只限于传统的数据分析、挖掘的管理。<br /></li><li>BCP平台让整个平台稳定性的能力从技术维度延伸到了业务维度，完善了平台稳定性的覆盖广度，是平台稳定性体系化的一个非常重要的拼图。</li></ul><p><img src="/img/IT架构转型之道1.jpg" /><br /><img src="/img/IT架构转型之道3.jpg" /><br /><img src="/img/IT架构转型之道4.jpg" /><br /><img src="/img/IT架构转型之道5.jpg" /><br /><img src="/img/IT架构转型之道6.jpg" /><br /><img src="/img/IT架构转型之道7.jpg" /><br /><img src="/img/IT架构转型之道8.jpg" /><br /><img src="/img/IT架构转型之道9.jpg" /><br /><img src="/img/IT架构转型之道10.jpg" /><br /><img src="/img/IT架构转型之道11.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 架构 </tag>
            
            <tag> 阿里巴巴 </tag>
            
            <tag> SOA </tag>
            
            <tag> 中台 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《目标》读书心得</title>
      <link href="/2018/01/09/%E3%80%8A%E7%9B%AE%E6%A0%87%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2018/01/09/%E3%80%8A%E7%9B%AE%E6%A0%87%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>本书通过故事的方式讲述了主人公在管理工厂过程中，扭亏为盈的历程。<br />在主人公和导师（姑且这么称呼）钟纳的交互过程中，钟纳<strong>通过苏格拉底式的方式，通过不断的提出问题，引导主人公找出工厂的问题、找出解决问题的方法，甚至找出如何去找出问题的方法。</strong><br />在钟纳的指引下，主人公找出了组织的目标以及衡量这些目标的指标，简单的说，组织的目标就是赚钱（而不是降低成本），然后在这个目标的前提下，如何让公司的日常营运和公司整体绩效之间找到一些逻辑关系，作者提出了<strong>三个重要的衡量日常运营工作的指标，那就是：有效产出（Throughput）、库存（Inventory）和营运费用（Operating Expense）</strong>。按照作者的定义，有效产出就是整个系统通过销售而获得金钱的速度；库存就是整个系统投资在采购上的金钱；营运费用，就是系统为了把库存转化为有效产出而花的钱。<br />为了满足这几个指标，有效的方法是提高有效产出、降低库存、降低营运费用。<br />作者在不考虑降低营运费用（通常的做法是裁员，作者反对这种做法），提出了能有效提高有效产出、降低库存的方法，那就是TOC制约法（瓶颈管理法）。 <span id="more"></span><br />因为瓶颈其实决定了组织的有效产出，所以，提高有效产出的关键是提高瓶颈的产出，所以需要专门对瓶颈进行管理。<br />在TOC制约法的理论中，只有瓶颈的优化才是真正的优化，其它的优化都是幻象，作者还有创意的提出：<strong>工厂的产能就是瓶颈的产能，瓶颈每小时的生产量就等于工厂每小时的生产量。所以，假如瓶颈损失一个小时的生产时间，就等于整个系统损失了一个小时。</strong>为了有效管理瓶颈，需要遵循以下两个原则：</p><ul><li>绝对不可以浪费瓶颈的时间；<br /></li><li>减轻瓶颈的负担，把部分工作移交给非瓶颈的生产资源；</li></ul><p>另外，瓶颈是会移动的，所以，持续的瓶颈管理也是管理不断完善的过程。<br />同时，书中也结合实际，提出了类似丰田精益生产的关于小批量、拉动式生产、全局优化、持续改善等概念。<br />本书最后，作者通过哲学的方式，思考管理的问题，作为一个管理者最重要的事情是什么？思考再三，主人公提出了<strong>管理者最基本的几项能力就是：要找出需要改变什么、朝什么方向改变以及如何改变的能力。</strong></p><p>五步法：<br /><img src="/img/目标2.jpg" /></p><p>看了这本书以后，结果自己公司情况，也考虑了一下公司的变革问题，如下图：<br /><img src="/img/公司变革.png" /></p><h1 id="文章摘要">文章摘要</h1><ul><li>古希腊责任苏格拉底的著名指导方法是：只问问题，不提供答案，要学生自己思考、摸索、假设，以行动印证，最后找出答案（不要给答案，只要问问题就好！）。<br />假如不是因为我们必须苦思冥想，才能找到解决方案，我们会有足够的勇气实施这些方案吗？很可能不会。假如不是源自我们在辛苦挣扎中所获得的信念，也就是在过程中发展出来的对问题的责任感，我不认为我们会大胆地实施这些方案。<br /></li><li>你不应该在产能和需求之间求取平衡，你们需要做的是，<strong>产品在工厂中的流量与市场需求之间求取平衡。</strong><br /></li><li>传统的成本法的问题。<strong>改善的意思不是节省成本，而是增加有效产出。从成本世界跨入有效产出世界。</strong><br /></li><li><strong>丰田精益生产（TPS）在很多企业实施失败，关键是不同企业的生产环境之间存在根本性的差异。</strong><br /></li><li>常识其实并非那么平常，常识是我们给予逻辑推演出来的结论的最高礼赞。...马克吐温说过，常识其实一点都不平常。<br /><strong>当我们说某件事是常识的时候，只不过是因为那件事情符合了我们的直觉。</strong><br /></li><li>复杂的解决方法是行不通的，问题越复杂，解决方法越要简单。<br /></li><li>机械手臂则随着设定好的程序跳舞。<br /></li><li>每隔半年，公司里似乎总会有人提出新计划，作为解决一切问题的万灵丹。有些计划似乎一时奏效，但是没有一个计划带来真正打好处。<br /></li><li><strong>就好像拼命鞭打一匹已经全力奔驰的马一样，只是徒劳无功。</strong><br /></li><li>我总是拼命赶路，结果和许多人一样错过了周围的奇景。<br /></li><li>有时候，我们会把数字修饰一下，但是每个人都玩这种把戏。<br /></li><li>如果你和世界上其他人几乎没有什么两样，你毫不质疑地就接受了很多事情，表示你没有真正地思考。<br /></li><li>生产力是把一家公司带向目标的行动，每个能让公司更接近目标的行动都是有生产力的行动。<br /></li><li>机器的折旧要算营运费用，机器中仍然保有的价值也就是可以变卖的部分，就要算库存。...库存就是我们可以卖到市场上的那些东西。<br /><strong>任何我们花掉的钱都是营运费用，任何我们可以借销售而回收的投资都算库存。</strong><br /></li><li>假如你输入的资料是垃圾，那么输出的也会是垃圾。<br /></li><li>在树林的小径上，你可以借着队伍的间隔找出谁走得慢，走得逾慢的人，他和前面那个人之间的距离就逾大。如果拿我们工厂来看，间隔就是库存。<br /></li><li>假如你现在就靠赶工来解决问题，那么你以后会不停地重施故技，情况会变得更糟。<br /></li><li><strong>每个人都忙碌不堪的工厂是非常没有效率的工厂。</strong><br /></li><li>永远要试着从对方的角度来看事情，而不是只从自己的角度出发。<br /></li><li>当你逐渐往上爬，肩膀上的责任越来越重时，你应该学会越来越依赖自己的力量。<br /></li><li>财务数字只能显示一小部分真相。<br /></li><li><strong>当重担落在别人肩上时，要提出建议是多么轻松的事。</strong><br /></li><li>身为团体一份子的安全感，大家共同分摊责任，没有一个人会特别受到责怪。（法不责众）<br /></li><li>几乎每一家大公司的政策都在不停地变化，每5-10年就从中央集权转为分权政策，然后又再回头采取中央集权。<br /></li><li><strong>我要怎样才可以继续保持这么高昂的士气？ 永远不满足！<br />昨日是，今日非！ 克服惰性，不断创新。</strong><br /></li><li>外面是狗咬狗的世界。<br /></li><li>物理学界研究一个主题的时候，和工商界的做法大不相同，他们不是一开始就收集一大堆资料，相反的，他们从几乎是随意选择的一些现象、有关生命的一些发现着手，然后提出假设。这是最有趣的部分，所有的假设似乎都以一个基本关系为基础：如果......那么......。<br /></li><li>细节不要紧，我们应该找出核心问题。<br /></li><li>我所看过的所有长期计划，都是一派胡言。（其实长期计划只是想理清自己的长期思考，而不是真的一定能实现的）<br /></li><li>总公司到处弥漫的心态，又要怎么说呢，就是自以为是，<strong>委过于人</strong>的心态。<br /></li><li><strong>假如我们在执行计划以前，就先有系统地思考，而不是只靠事后之明的话，我们就可以避免很多问题。</strong><br /></li><li><strong>当思维完全着眼于局部时，每个人都希望工作的设计对自己最有利，这就造成了混乱。</strong><br /></li><li>制约因素是杆杠支点。<br /></li><li><strong>关于请咨询公司的理由，“这件事我可以办到，但是如果有人帮忙就会更快办到。”<br />如果我受邀参加客户的初步会议，我就问问题，这有助于我鉴别有没有让我发挥的空间。一般来说，我会试着帮助客户明白，如果解决的是核心问题，而不是很多人关注的表面的征兆，几乎可以肯定拿到好的结果。</strong><br /></li><li>所以，你所说的就不是一本书，而更像一个图书馆了。<br /></li><li><strong>建立客户关系要比贷款本身更有价值。</strong><br /></li><li>高德拉特说过，他生命中最重高的目标就是教这个世界怎样去思考。<br /></li><li>这是个关于世界文化的课程，谈的是从不同观点看事物。<br /></li><li><strong>这是合作式学习的一个很好的例子，因为每个人都需要思考，即使他们已经知道答案，他们也得努力思考怎样引导其他人找到答案。<br />如果我们教育孩子的方式是让他们自己找答案，而不只是给他们答案，并让他们记住，那么从这些孩子身上能够释放出什么来呢？是人的潜能。</strong><br /></li><li>全球制造业，受两位伟大的思想家的影响极大：亨利·福特和大野耐一。<br /></li><li><strong>看板系统就成为一个实际的机制，告诉员工何时不生产（防止过度生产）。大野成功的将福特的概念扩展了，将机制的基础由空间改为库存。<br />聚焦于流动，以及不强调局部的成本考虑，反而令成本下降，员工的效率也大大提升了。</strong><br /></li><li>物料需求计划 Material Requirements Planning MRP。<br /></li><li>瓶颈为所有订单提供运作“节奏”，犹如鼓之于“鼓声”（Drum beat），瓶颈就是“鼓”（drum）。时间缓冲（Buffer）将承诺交货期转换为物料发放日期，而抑制物料的发放，这行动就是一根绳子（Rope），将订单和发料绑起来。因为，这个以时间为基础的TOC应用专题就被称为鼓-缓冲-绳子（Drum-Buffer-Rope）系统，简称DDR。</li></ul><p><img src="/img/目标1.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 目标 </tag>
            
            <tag> 制约法 </tag>
            
            <tag> TOC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《一分钟经理人》读书心得</title>
      <link href="/2018/01/05/%E3%80%8A%E4%B8%80%E5%88%86%E9%92%9F%E7%BB%8F%E7%90%86%E4%BA%BA%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2018/01/05/%E3%80%8A%E4%B8%80%E5%88%86%E9%92%9F%E7%BB%8F%E7%90%86%E4%BA%BA%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>很薄的一本书，和看一本杂志的某篇文章差不多。<br />整本书，其实可以归集为一页内容：分别是<strong>一分钟目标、一分钟称赞、一分钟更正。</strong><br />目标关键是要清晰和明确，称赞、更正要及时。 <img src="/img/一分钟经理人2.jpg" /> <span id="more"></span></p><h1 id="文章摘要">文章摘要</h1><ul><li>员工是企业获得成功的动力，吸引并留住人才是第一要务。</li><li><strong>寻找一种经理人，他们在瞬息万变的时代中既领导有方又能引导人们平衡工作和生活，将一切变得更有意义、更有乐趣。</strong></li><li>高效的经理人应该可以很好的管理自己和他的工作伙伴，从而让公司和员工实现双赢。</li><li>我们原来是一家“自上而下”严密管理的公司，这种架构在当时很适用，现在就有点过时了，不能激发员工潜力还扼杀创造力。消费者要求更快的服务更好的产品，所以我们需要每个人都贡献出自己的才能。<strong>才智不应被困在某间办公室里----应该样洋溢在整个公司里。</strong></li><li>现在速度就是金钱，协作模式比控制模式的效率要高得多。</li><li>自我感觉良好的人，才能做出优秀的业绩。</li><li>如果你说不出你所期望的情形，那就说明还没有真正发现问题，你只是在抱怨而已。只有现实情况与你期望的情况有出入时，问题才可能存在。</li><li><strong>引导式提问题的方式。 苏格拉底的方式，而不是直接给出答案。</strong></li><li>通过发现别人作对的事来帮助他们充分发挥潜力。</li><li>经理人给你机会独立解决问题，而不是参与你的决定。这样节约了经理人的时间，团队也得到了锻炼。</li><li>以建设性的方式来更正错误。<strong>做错事之后自嘲一下，再用更好的工作成果修正错误。</strong></li><li><strong>我最好的投资就是把时间花在人的身上。</strong></li><li>人们行动的最大动力来自结果的反馈，大家都想知道自己干得怎么样。实际上，套用一句老话：反馈是成功的前奏。</li><li>在大多数公司里，为了让自己看起来是个称职的经理人，他们就不得不从下属身上挑毛病。</li><li>有人向爱因斯坦要电话号码，结果爱因斯坦翻出通讯录给他看，说他从不让能在别处查到的信息占用自己的大脑。</li><li>训练成功者最重要的是发现他们做对了的事情----开始也许只是大概做对，接着要逐渐引导他们做得越来越完美。</li><li><strong>设立一系列的小目标，可以帮助人们更顺利地完成整项工作。</strong></li><li><strong>我们的目的是帮助人们建立自尊自信，而不是贬低他们的价值。人格受到伤害时，哪怕扭曲事实，人们也会为自己和自己的行为开脱。一旦形成防备心态，就很难听得进去了。</strong></li><li>要把一个人的行为和他自身的价值区分开。指出行为上的错误，别贬低对方的人格，最后再肯定他们的价值。</li><li><strong>先严厉的指出客观上的错误行为，再肯定并鼓励他们，就能收到很好的效果。（先批评，再肯定）</strong></li><li>引导别人尝试新事物、变得更有创造力。 <img src="/img/一分钟经理人1.jpg" /></li></ul>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 管理 </tag>
            
            <tag> 一分钟 </tag>
            
            <tag> 经理人 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《看漫画学统计学》读书心得</title>
      <link href="/2018/01/04/%E3%80%8A%E7%9C%8B%E6%BC%AB%E7%94%BB%E5%AD%A6%E7%BB%9F%E8%AE%A1%E5%AD%A6%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2018/01/04/%E3%80%8A%E7%9C%8B%E6%BC%AB%E7%94%BB%E5%AD%A6%E7%BB%9F%E8%AE%A1%E5%AD%A6%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>大学学过统计学，但现在也都忘光光了，之前也看过《数学桥--对高等数学的一次观赏之旅》，不过这本书因为是专业的书籍，偏向于公式的推断，陷入了细节。所以一直对于统计学中关键的几个分布正态分布、二项分布、泊松分布，感觉还是没有很好的理解。<br />像本书作者所说<strong>太拘泥于公式，常常会看不到事物的本质</strong>，所以一直以来，对于统计学，感觉自己都是只见树木不见森林。<br />现在有空看了这本《看漫画学统计学》，终于可以跳出公式来看待公式的本质了。</p><span id="more"></span><p>作者提到，统计学上最重要的定理是中心极限定理，确实是，回头看来，中心极限定理其实是后面很多推断的基石，而之前我们往往忽视了中心极限定理的存在。</p><h1 id="中心极限定理">中心极限定理</h1><ol type="1"><li><p>大量相互独立的随机变量，在采样次数足够大的时候（一般要超过30次以上），其均值或者和的分布以正态分布为极限。</p><p>中心极限定理的有趣的地方在于，无论随机变量呈现出什么分布，只要你抽取次数无限大，抽取样本的均值就接近于正态分布。<br />对，mark一下重点就是：</p><ul><li>样本的平均值约等于总体的平均值；</li><li>不管总体是什么分布，但是样本的均值都会围绕在总体的整体平均值周围并呈现正态分布。</li></ul><p>eg：比如你投6枚筛子，对每次的6个数求平均<span class="math inline">\(X_{n}\)</span> ，则<span class="math inline">\(X_{1}\)</span> -- <span class="math inline">\(X_{n}\)</span> 的分布就满足与正态分布<br /></p></li><li><p>那中心极限定理的用处是什么呢？</p><p>eg：你要预测一件事情发生的概率，比如查验食品合格率，你只需要抽查部分就可判断整体合格率，这就用到中心极限定理了，因为样本的均值分布是在总体样本的均值附近呈现正态分布，这样你就知道有68%的样本在总体平均值的一个标准差范围内波动，有95%的样本平均值在总体平均值的两个标准误差范围内，99.7%的在总体平均值三个标准差单位内波动，如果一个样本均值与总体均值的差大于三个标准差，那么我们可以说这个样本不属于这个总体，所以这就是我们拿样本均值估计总体均值的原因所在（当然自我感觉其实再计算一下标准差对估计的评估效果会好一点）。</p></li></ol><blockquote><p><strong>中心极限定理</strong>：当样本容量n增大时，不论原来的总体是否服从正态分布，样本均值的抽样分布都将趋于服从正态分布。比如你要调查一个国家人民的体重的平均值，每次取1000个体重值作为样本，对应就有一个样本均值。你再从总体中重復抽取n多次1000个样本，就对应有n个样本均值。随着n增大，把所有样本均值画出来，得到的就是一个接近正态分布的图。</p></blockquote><p>而正态分布是我们日常生活中最常见的分布，很多生活现象都可以用正态分布来模拟。标准正态分布的概率（从左到右的面积）可以通过正态分布表来查到。<strong>在日常生活中，例如某个考试不同科目的含金量，可以通过正态分布表来比较，例如可以使用公式表示： (得分 - 平均分) / 标准差 （代表分数与平均分有几个标准差），来比较不同科目在整体中的位置，而不能纯粹以分数的绝对值来比较分数的含金量。</strong><br />另外，看了本书终于明白，<strong>正态分布、泊（bó）松分布是在不同情况下，来近似计算二项分布的一种方法</strong>。<br />有如此多种近似计算的方法，是因为准确计算二项分布非常困难，因此，祖先们绞尽脑汁想出了多种近似计算的方法。由于是近似，所以，可以根据情况选择“更加合适的近似”。而且，有些情况下也可以直接计算，而不是近似计算，因为，首先应该研究是否可以直接计算，因为这才是最准确的。然后在不能用手中的工具做计算时，就考虑做近似计算。<br /><strong>特别是泊松分布，在实验次数n非常大，发生概率P非常小的情况下可以通过泊松分布来近似计算二项分布，例如交通事故的发生概率就是满足以上条件的最著名的二项分布的例子。并且，在用泊松分布近似计算的时候，一个重要的要素，就是没有用到n的值，这样，对于没有总量的概率计算就很有效。一般泊松分布是在知道某个期间的平均概率的情况下，预测某个期间出现某状况次数为k的概率。这些在日常生活中的应用是非常广的。</strong><br />在讲了这些描述统计学之后，作者也提到了推断统计学。描述统计学和推断统计学最大的不同之处在于“是否掌握全部数据”。描述统计学主要讲的是怎样解释所收集信息的内容。而推断统计学是根据部分数据估计整体的统计学。关于推断统计，其基础还是概率计算。于是，最后还是回到大部分都服从正态分布的问题（<strong>例如体重的分布、财富的分布就不符合正态分布</strong>）。在推断统计学中，样本量少时服从t分布，当样本量较大时，就可以当做正态分布处理。准确的说应该是，无论n大或者小，使用的都是t分布。但n较大是也可以使用正态分布。</p><h1 id="概念">概念</h1><h2 id="直方图平均值方差标准差">直方图、平均值、方差、标准差</h2><p><img src="/img/看漫画学统计学3.jpg" /></p><h2 id="标准正态分布正态分布nμσ²">标准正态分布、正态分布（N(μ，σ²））</h2><p><img src="/img/看漫画学统计学4.jpg" /><br /><img src="/img/看漫画学统计学5.jpg" /><br /><img src="/img/看漫画学统计学6.jpg" /></p><h2 id="标准正态分布表">标准正态分布表</h2><p><img src="/img/看漫画学统计学7.jpg" /><br /><img src="/img/看漫画学统计学8.jpg" /><br /><img src="/img/看漫画学统计学9.jpg" /><br />我们认为标准的正态分布是彻底已知的，这就如同，测量物体是不会全部逐个测量尺寸，而是使用模型的数据进行倍数换算来测量的原理。因此，可以说标准正态分布是一把“尺子”。<br />正态分布和标准正态分布在<strong>横轴</strong>上是放大或缩小倍σ（1/σ倍）的关系。</p><h2 id="正态分布的推导">正态分布的推导</h2><p><img src="/img/看漫画学统计学10.jpg" /><br /><img src="/img/看漫画学统计学11.jpg" /></p><h1 id="二项分布">二项分布</h1><h2 id="二项分布-bnp">二项分布 B（n，p）</h2><blockquote><p>二项分布是指在只有两个结果的n次独立的伯努利试验中，所期望的结果出现次数的概率。在单次试验中，结果A出现的概率为p，结果B出现的概率为q，p+q=1。那么在n=10，即10次试验中，结果A出现0次、1次、……、10次的概率各是多少呢？这样的概率分布呈现出什么特征呢？这就是二项分布所研究的内容。</p></blockquote><p><img src="/img/看漫画学统计学12.jpg" /></p><h2 id="二项分布的近似计算">二项分布的近似计算</h2><h3 id="用正态分布近似计算二项分布">用正态分布近似计算二项分布</h3><p>用正态分布求解二项分布，二项分布可以用正态分布做近似计算.<br /><img src="/img/看漫画学统计学13.jpg" /></p><h3 id="用泊松分布近似计算二项分布">用泊松分布近似计算二项分布</h3><p>但实际上还有一种做近似计算的方法，那就是使用“泊松分布”。</p><p>用泊松分布做近似计算的条件如下：</p><ul><li>实验次数n非常大。比如1万次或以上；<br /></li><li>发生概率P非常小。 1/1000或者更小等；</li></ul><p>交通事故的发生概率就是满足以上条件的最著名的二项分布的例子。<br />在用泊松分布近似计算的时候，一个重要的要素，就是没有用到n的值，这样，对于没有总量的概率计算就很有效。一般泊松分布是在知道某个期间的平均概率的情况下，预测某个期间出现某状况次数为K的概率<br /><img src="/img/看漫画学统计学14.jpg" /></p><h1 id="推断统计学">推断统计学</h1><p>推断统计的前提是：</p><ul><li>以随机抽样为前提；<br /></li><li>以原始分布是正态分布为前提。</li></ul><p>描述统计学和推断统计学最大的不同之处在于“是否掌握全部数据”。描述统计学主要讲的是怎样解释所收集信息的内容。而推断统计学是根据部分数据估计整体的统计学。关于推断统计，其基础还是概率计算。于是，最后还是回到大部分都服从正态分布的问题（例如体重的分布、财富的分布就不符合正态分布）。<br /><img src="/img/看漫画学统计学15.jpg" /><br /><img src="/img/看漫画学统计学16.jpg" /><br />样本量少时服从t分布，当样本量较大时，就可以当做正态分布处理。准确的说应该是，无论n大或者小，使用的都是t分布。但n较大是也可以使用正态分布。</p><h1 id="文章摘要">文章摘要</h1><ul><li><strong>处理数据的第一步是画直方图，实际上，判断直方图是否是单峰非常重要，因为统计学基本上是围绕单峰型的直方图构成的学问。直方图或者次数分布表，在多数情况下使用比率来表示会更方便。用比例画出的直方图叫做“概率分布”。</strong><br /></li><li>按照某一期间分组的方法叫做“组距分组”。好的组距分组是：组距小到可充分说明数据的特点，同时让观者容易理解的分组。<br /></li><li>尝试----错误法<br /></li><li>要意识到，不正常的直方图一定有其原因。<br /></li><li>大数据除了样本数据量大之外，还包括数据的要素信息也很多（包括不同的维度标签）<br /></li><li><strong>运行程序越复杂伴随的漏洞也就越多（换句话说：越简单越稳定）</strong><br /></li><li><strong>用曲线的面积表示概率是件非常了不起的创意。...这种函数由于“其面积表示概率”，因此称之为“概率密度函数”。...“概率密度函数”的曲线图表中，“高度”是概率密度，即“曲线的面积是概率”。</strong><br /></li><li><strong>正态分布，简单地说就是“所收集的数据以平均值为中心前后大致对称分散的状态</strong><br /></li><li>这个复杂的正态分布公式，并不是某位天才数学家的一时灵感，而是通过质朴且零碎的理论一一堆砌而成，即<strong>它不是神的启发，而是人类的智慧。</strong><br /></li><li>正态分布的特点：<ul><li>有一个最高点；<br /></li><li>以最高点为中心左右对称；<br /></li><li>以指数函数的速度趋近于0；<br /></li><li>面积为1（因为是概率分布），<br /></li></ul></li><li>要求事事完美也是失败的原因。<br /></li><li>大数定律，它说的是，实验次数无限多时，接近概率的理论值。<br /></li><li>重要的是，但你走投无路时退一步，站在远处俯瞰整体。</li></ul><p><img src="/img/看漫画学统计学2.jpg" /><br /><img src="/img/看漫画学统计学1.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 统计学 </tag>
            
            <tag> 漫画 </tag>
            
            <tag> 分布 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《丰田生产的会计思维》读书心得</title>
      <link href="/2017/12/27/%E3%80%8A%E4%B8%B0%E7%94%B0%E7%94%9F%E4%BA%A7%E7%9A%84%E4%BC%9A%E8%AE%A1%E6%80%9D%E7%BB%B4%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2017/12/27/%E3%80%8A%E4%B8%B0%E7%94%B0%E7%94%9F%E4%BA%A7%E7%9A%84%E4%BC%9A%E8%AE%A1%E6%80%9D%E7%BB%B4%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>传统的会计只考虑资金的成本，没有考虑时间的影响因素，在丰田的观点看来，时间是最宝贵的，所以在成本考核的时候，必须加上对应的时间维度，作者把这个视角的成本称为J成本。<br />J成本定义，如下图：<br /><img src="/img/丰田生产的会计思维6.jpg" /><br />在J成本体系下，<strong>缩短前置时间（过程时间）就可以提高“收益性”。以前被认为互为对立的“成本改善和前置时间改善”，实际上几乎有着相同的效果。（俗话说，时间就是金钱。）</strong>基于此，作者提出<strong>丰田生产方式的现场改善，是通过自动化在确保质量（Q)的基础上，彻底实现缩短过程时间（D)，这样，收益（C)就会随之而来。</strong><br /><span id="more"></span><br />而资金的成本，在制造业主要体现在库存方面，只要我们把“物品”和“资金”等同起来思考，就能避免浪费性闲置，从而降低成本。<br />利润是考核一个企业的关键要素，如果来评价一个企业，指标很重要，传统做法主要是使用利润率来考核，基于J成本，基于时间要素的考虑，作者独创性的提出了效益性的概念。其定义如下：<br />利润率： 每次营业活动赚多少钱；<br />收益性：某一定期间（通常是某一会计期间）赚多少钱。<br />收益性公式，如下图<br /><img src="/img/丰田生产的会计思维9.jpg" /><br />在这个收益性的公式里，作为评价指标分支的利润，在J成本论中是使用“毛利”。<strong>在企业众多的利润当中，之所以选择毛利=销售总利润，是因为想尽可能如实反映出生产现场努力的结果。</strong>如果将其换成比如营业利润，则在计算成本时，也要包括一般管理费等于生产现场没有直接关系的费用。因此，为了排除这些因素，便使用了“毛利”。</p><ul><li>三个重点：<br />1、掌握客户的需求，专注于产品质量与准时交货，利润自然会来；<br />2、目标放在企业整体团队的运营绩效，而不是某个单位或部门的业绩；<br />3、把时间因素计算在会计成本里。（导入时间轴的新管理会计法，J成本。）<br /></li><li><strong>当“认可改善本身，但结果却不理想”时，你是怎么想的？<br />1、评价方法不正确；<br />2、改善本身不正确；<br />实际上，1、2是相互交杂的。</strong><br /></li><li>将1万元库存放置1天会损失多少钱的问题，变为，1万元的这些库存本来一天应该赚多少钱。（机会成本，而不仅仅是利息的问题）<br /></li><li>几乎所有重视成本降低的企业，都忘掉了时间的概念。因为，时间是包含在D（交货期）中，而不是C（成本）中。<br /></li><li><strong>构成丰田生产的两根支柱：<br />“自动化”----绝对确保品质（内建质检）<br />“准时化”----挑战缩短交货期</strong><br /></li><li>持有大量库存 = 损失<br /></li><li>员工要理解自己工作的意义，通过自己的努力获得成长，该员工才是贵公司的动力，是宝贝。<br /></li><li>事物的观察方法和思考方法。<br /></li><li>原则辨识简易，即要一简单明了为主，而不是追求流行。<br /></li><li><strong>超市战略和便利店战略的比较：</strong><ul><li>大量采购低价购买，增加毛利；<br /></li><li>增加买卖次数，加快自己周转；<br /></li></ul></li><li>归类为材料、半成品、成本，从材料到成品，公司内的库存在会计上都被称为“存货”。<br /></li><li><strong>不接受不良、不流出不良、不制造不良。</strong><br /></li><li>要发挥人的主观能动性，不要把员工当成按部就班、不需要思考的纯执行的工具，否则就变成“机器人集团”了。<br /></li><li><strong>本位主义实际上是产生重大问题的温床。各部门只顾着一味提高本部门的业绩，却没有思考公司整体利益的机制。</strong><br />在判断公司全体的盈利状况时，不能只关注“这个材料1个是多少钱？”而关注“1年前投资多少钱，现在赚了多少钱？”的“收益性”才是最重要的。<br /></li><li>一想到需要这么多繁琐的程序，很可能就会想，"还是维持现状吧！"于是置之不理。<br /></li><li>陈腐化风险。随着技术的进步，使用的办公用品会突然发生变化，这样，以前准备的大量库存就有陈腐化的风险。比如，电脑软盘就是个很好的例子。<br /></li><li>至今还没有一个有效的手法测定全局最优。<br /></li><li><strong>丰田的做法是，本来应该5个人做的工作，有意让4个人来做。也就是说，塑造如果大家不下功夫想方法进行改善，就无法完成这项工作的环境。遇到压力，人就一定会涌出智慧来。</strong><br /></li><li><strong>不变才是错误！</strong><br /></li><li>逼迫现场去降低成本的做法会使公司走向毁灭。</li></ul><p><img src="/img/丰田生产的会计思维2.jpg" /><br /><img src="/img/丰田生产的会计思维3.jpg" /><br /><img src="/img/丰田生产的会计思维4.jpg" /><br /><img src="/img/丰田生产的会计思维5.jpg" /><br /><img src="/img/丰田生产的会计思维7.jpg" /><br /><img src="/img/丰田生产的会计思维8.jpg" /><br /><img src="/img/丰田生产的会计思维1.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 丰田生产 </tag>
            
            <tag> 会计 </tag>
            
            <tag> J成本 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《精益思想》读书心得</title>
      <link href="/2017/12/18/%E3%80%8A%E7%B2%BE%E7%9B%8A%E6%80%9D%E6%83%B3%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2017/12/18/%E3%80%8A%E7%B2%BE%E7%9B%8A%E6%80%9D%E6%83%B3%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>一本介绍精益思想方面的书，按照作者的概括，<strong>精益思想可以概括为5个原则：精确地定义特定产品的价值；识别出每种产品的价值流；使价值流不断地流动；让客户从生产者方面拉动价值；永远追求尽善尽美。</strong>作者按照这5个原则，分别进行了阐述，并且在阐述的过程中，通过举例的方式，对不同类型的企业采用精益思想获取的进步进行了说明。证明了作者提出的精益思想是适应各类型企业的观点。</p><p>为了满足精益化思想，作者认为价值流动要打破部门的壁垒，所以改革组织架构，成立跨职能的全功能团队，行成以产品单元为核心的单件流工作方式是实现精益思想的首要条件。必须把目前的以部门为单位的批量作业，转化为生产单元的“单件流”作业。<br /><span id="more"></span><br />价值流是精益思想的核心，我们在改革前，首先要对公司的价值流进行定义和分析，同时，一个常见的经验是，需要把对物流和信息流的描述加入到价值流图中。<br /><img src="/img/精益思想8.jpg" /><br /><img src="/img/精益思想9.jpg" /></p><ul><li><strong>可乐加入一氧化碳是起泡用的</strong><br /></li><li>对“预测”来说唯一肯定的事情是，预测的结果是错误的。<br /></li><li>应该准确地概括总结出“精益思想”，为管理者提供一种类似北极星那样可靠的行动指南。<br /></li><li>任何一项商务活动的三项关键性管理任务所必需的一组特定活动：解决问题的任务；信息管理的任务；物质转化的任务；<br /></li><li><strong>大野耐一把这种主张批量生产（一年一度的农作物收获）和储存（收粮入仓）的过程中，丢掉了猎人们那种一物一猎的明智。</strong><br /></li><li>美国每年印制的图书有一半还没有找到读者就被人送粉碎机了。<br />技术的进步也使得单件流成为可能，例如定制印刷图书。<br /></li><li>让价值流动得快一些总能暴露出价值流中隐藏的浪费。<br /></li><li>发达国家持续的经济停滞已经导致了寻找政治替罪羊的后果。停滞也使企业界热衷于削减成本（由流程再造者领导）<br /></li><li>大野耐一在现代超市里发现这一有利位置时非常兴奋，从而启发他在1950年创造了我们现在称之为准时生产（JIT）的新的流动管理系统。<br /></li><li>专职的全功能团队的好处是，高效，消除浪费（大家步调一致，减少了等待的成本）<br /></li><li>由工作团队制定标准，而不是由某些离得很远的行业工程技术人员制定标准。<br /></li><li><strong>芝加哥大学任教的波兰籍心理学家...的研究，改变了心理学家的重点，他并没有去寻找什么使人们感觉不好（以及如何改变这种感觉），而是探求什么使人们感觉良好，从而将积极因素用到日常生活中来。</strong><br /><strong>世界各地的人们一致报告为最有益的活动，也就是使他们感觉最好的活动，包括：一个明确的目标；必须非常专注做事而无暇分心的需要；很少干扰和分心的事；对达标进度明确而又及时的反馈；挑战感----一个人的技能适于，但也只是适于，应付手头任务的那种感觉。</strong><br /></li><li>拉动式生产，必须考虑生产适配拉动的方式，同时，运输也必须匹配每日的小批量运输。<br /></li><li>芝加哥的配送中心全部自动化了，尽管节省了一些直接劳动力，但是为了维护这个系统的技术支持量却抵消了节省的直接劳动所得到的好处，而资金成本则使整个方法并不经济。精益系统应当采用“适当”的技术。<br /></li><li>自然界非线性力量的潜在影响有可能会使在北京的一只蝴蝶影响到纽约几天后的气象----用于商业，今天的管理者似乎就要生活在面对蝴蝶的恐惧之中了。<br /></li><li>对准时生产的应用大多数也只涉及准时供应，而不是准时生产。<br /></li><li>请记住，你要为尽善尽美去竞争，而不要为现在的对手去竞争，所以你必须有能力测定现实和尽善尽美之间的距离。 个人注：做最好的自己。<br /></li><li><strong>形成一个理想，选择两三件最重要的事去干并要干成，而把其他事情放到以后去办。</strong><br /></li><li>变革的代理人通常都是专横的人，只有慷慨的专横者才能成功。<br /></li><li>没有哪家企业仅靠降低成本和节省开支就可以得到挽救。<br /></li><li>花岗岩脑袋<br /></li><li>改善的权力下放， 让听见炮火的人来做决策。<br /></li><li>成功的三个条件：远见卓识、精湛的技能技巧、对获得成功的强烈愿望。<br /></li><li>显然，每一项改革、改善都会带来风险，至少一定程度上是这样的。<br /></li><li>在试图创建连续流的过程中，可能会出现“进两步退一步”的现象；<br />伴随着前进的步伐，还有后退的现象，问题是要保持绝对稳定的方针。<br /></li><li>到处都是将不同部门和不同职能分开的厚厚的“墙”，不但阻碍了价值的流动，而且也使人们看不到价值的流动。<br /></li><li>等级制度对精益思想有很大的损害。<br /></li><li>大变革需要信念上的大飞跃，也就是说，即使“改善方法”看上去有悖常理，首席执行官也必须说“就这样干”。<br /></li><li>业绩考核指标：（<strong>IT团队也可以参考此来考核</strong>）<ul><li>产品团队生产率（用人均销售额来表示）<br /></li><li>客户服务（用按时交付产品的百分比来表示）<br /></li><li>存货周转次数<br /></li><li>产品质量（由团队造成的产品缺陷来表示）<br /></li></ul></li><li>极大柔性的生产系统；<br /></li><li>利润分享计划；<br /></li><li><strong>先进的硬技术是有用的，而且在许多情况下是非常重要的；但是，如果不能与一个能充分利用这些技术的组织架构有机结合起来，这些技术的功用是很有限的。</strong><br /></li><li><strong>一个老笑话：一般零件在普惠（飞机发动机制造商）工厂的生产过程中所走过的路程比它为航空公司服役所走过的路程还要长。</strong><br /></li><li>年生产小时数。<br /></li><li>那些诚心诚意尝试用新的更好的方式进行管理的人员，不会因为失败而受到惩罚。<br /></li><li>市场退让战略。<br /></li><li>对于落后的原因，我们也有一些笼统的认识，但是我们缺乏解决生产率问题和产品一次作业质量问题的方法，而且我们也不知道该优先考虑什么。也就是说，当你在每一项竞争指标上都落后的时候，你怎么开始行动？又从哪儿开始呢？<br /></li><li><strong>成套零件本身就是一个防错装置，因为手推车上的零件是严格按照装配顺序放置的，任何遗漏的零件会马上被发现。</strong><br /></li><li>存在一种更高形式的技艺，那就是要不断地对工作组织和价值流动给予重新思考，消除浪费，同时积极地预见并防止团队上下会出现的新问题<br /></li><li>员工的创造性、持续改善、学习型组织。<br /></li><li>定位：有所为，有所不为；<br /></li><li>纠正过去那种用工程师对价值的定义来代替客户对价值的定义的倾向。<br /></li><li>窄如烟囱的晋升道路。<br /></li><li>如果大部分的浪费都消除了，而成本还是超过价值，那么，这时的问题就是：德国人给自己付的工资是否太多了。<br /></li><li>中尾千弘是大野最得意的学生之一，他与这位导师共同工作了20年，从不记得得到过大野对其工作的任何形式的称赞，但他却记得几乎每天都受到责骂。...如果仅仅是忍受辱骂的问题，那么山本认为，与获得的回报相比这一代价是值得的。<br /></li><li>“高技术”的批量生产：<ul><li>违背了机动、灵活的思想；<br /></li><li>导致库存积压；<br /></li><li>问题掩盖到了后期；<br /></li></ul></li><li><strong>大野最爱说的一句话：常识往往是错误的。</strong><br /></li><li>强有力的领导人，不是做好人，而是有明确的目标和信念，然后带领大家朝目标前进的人。<br /></li><li>把精益技术和高技术的批量生产相结合是有可能的。<br /></li><li>例如，iPhone是技术领先，而不只是管理上的领先。<br /></li><li>计算报酬最简单和最便宜的方法一般反而是最好的方法。<br /><strong>如果奖金在各个产品系列之间有差别，那么在重新分配人员时，将会产生一系列的矛盾。</strong><br /></li><li>真正的变革以及形成坚实的基础需要时间。<br /></li><li>变革的难点：常见的是，咨询公司一走，企业又马上变回原样，所以持续是很难的，因为这需要改变整个公司的文化。<br /></li><li>那些工程师们是保持了领先的技术能力呢，还是仅仅反复应用他们已经掌握了的技术呢。<br /></li><li><strong>为了防止世界破裂，有必要保持某种最低程度的合作（于是有了“热线”和根据对第三方国家的不确定意向搜集到的情报达成的默契协议，如不结盟国家）。</strong><br /></li><li>从流程到流动。<br /></li><li><strong>各职能部门应做的事情是考虑未来。做标准、做平台、做系统等。</strong><br />流程再造运动已经认识到部门化想法不够好，并且已经在尝试将重点从组织分工（部门）转向创造价值的“过程”上。<br />精益方法重新定义职能、部门和企业的作用。<br /></li><li><strong>在一个基于横向运作的社会中，如何发挥垂直机构的作用（这种垂直机构积累知识、传授知识并推动知识的发展），通常的做法是，技能必须不断升级，并且通过定期分派给职能性任务达到最佳状态，以克服“万金油雇员”的问题。<br />另外，是否考虑保持小规模的职能部门，在产品单元和职能部门之间定期的交流人员，以保证技能得到升级，同时，技能也能带到产品单元中去。</strong><br /></li><li>达到 顾客--所有者--雇员--协作厂 四赢的目的。<br /></li><li>使变革制度化。<br /></li><li><strong>通过抓住危机或者制造危机来寻找一个变革的杠杆。</strong><br />衰退是宝贵的，因为他动摇了传统的认识。<br /></li><li>精益思想的一个核心原则是：一个价值生成系统必须是柔性的和响应敏感的，因为预测通常是不正确的。<br /></li><li>动态的计划很有价值，但固定不变的计划却毫无用处。</li></ul><p><img src="/img/精益思想1.jpg" /><br /><img src="/img/精益思想2.jpg" /><br /><img src="/img/精益思想3.jpg" /><br /><img src="/img/精益思想4.jpg" /><br /><img src="/img/精益思想5.jpg" /><br /><img src="/img/精益思想6.jpg" /><br /><img src="/img/精益思想7.jpg" /><br /><img src="/img/精益思想10.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 精益思想 </tag>
            
            <tag> 丰田 </tag>
            
            <tag> 准时生产 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《丰田模式--精益制造的14项管理原则》读书心得</title>
      <link href="/2017/11/27/%E3%80%8A%E4%B8%B0%E7%94%B0%E6%A8%A1%E5%BC%8F--%E7%B2%BE%E7%9B%8A%E5%88%B6%E9%80%A0%E7%9A%8414%E9%A1%B9%E7%AE%A1%E7%90%86%E5%8E%9F%E5%88%99%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2017/11/27/%E3%80%8A%E4%B8%B0%E7%94%B0%E6%A8%A1%E5%BC%8F--%E7%B2%BE%E7%9B%8A%E5%88%B6%E9%80%A0%E7%9A%8414%E9%A1%B9%E7%AE%A1%E7%90%86%E5%8E%9F%E5%88%99%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>一本介绍丰田精益生产原则的书，作者根据对丰田公司的调研，针对丰田精益生产的14项原则进行了分类，一共有四种，分别是理念、流程、员工/合作伙伴、解决问题。<br /><img src="/img/丰田模式9.jpg" /><br />其实关于理念、员工/合作伙伴、解决问题（学习型组织）这些都是管理学上老生常谈的通用的管理思维，但是关于流程，在制造业里面，自福特公司提出的批量生产以来，通常理解上的批量生产能有效降低成本等原则，丰田却反常识的提出了颠覆性的理念。受美国超市补货思想的启发，丰田公司创始人提出了连续生产、单件流、拉动式生产等具有划时代意义的流程理念。在流程方面，核心的思想是消除浪费。通过对价值流的分析，找出浪费点，减少产品的前置期有效的降低浪费；通过看板、标准化等措施，让问题可视化；通过单件流让问题尽快的被发现，越早发现问题，解决问题的成本就越低，关于解决问题，丰田提出的现地现物、问5个问什么等方法，能让组织进行持续的改善。<br />其实，书中也提到，这些工具都是浮在水面的冰山的一角，真正能改变一个企业的必须从重塑企业文化着手，文化才是一个企业的基石。<br /><img src="/img/丰田模式29.jpg" /><br />在IT业中，很多思想都是从精益生产中学习过来的，例如敏捷开发的迭代（单件流）、看板、全功能团队（连续生产）等。另外，在信息化规划中常见的架构屋，也是发源于丰田公司的管理理念架构。<br /><img src="/img/丰田模式6.jpg" /><br />房地产企业作为一个粗放型的产业，在流程方便倒也可以学习精益生产的思想，通过分析房地产的价值流，找出浪费的地方，同时，通过可视化进行管理。<br /><span id="more"></span></p><h1 id="文章摘要">文章摘要</h1><ul><li>丰田生产方式（卓越的创造价值流程）是建立在以顾客为导向（顾客拉动）、以人为中心（尊重员工，释放员工潜能）、坚持挑战（不满现状、追求卓越）、持续改善和创新的可持续发展理念的基础上的。<br />丰田模式的本质是“通过释放人的潜能，追求卓越。”<br />丰田生产方式的核心：杜绝浪费<br />丰田生产方式：Toyota's Production System, TPS。<br />精益生产：Lean Production。<br />前置期：Lean Time<br /></li><li><strong>丰田公司不是生产技术系统，而是制造汽车，你应该告诉我制造汽车的流程，以及信息系统如何帮助我们对流程加以完善，信息技术专家制作的大型制造流程图，应该是上方显示丰田制造企汽车的车体、烤漆、组装等流程，下方的图则显示各种信息技术及其如何支持汽车的生产。这张图显示信息技术所扮演的正确角色----为生产线提供帮助。</strong><br /></li><li>我们把这14项原则区分成四大类，全部都是以P这个字母开头---- 理念（Philosophy）、流程（Process）、员工/合作伙伴（People、Partners）、解决问题（Problem solving）<br /></li><li>精益制造的五个步骤：定义顾客的价值（customer value）、定义价值流程（value stream）、建立连续的作业流程（flow）、拉动式（pulling）生产方式、努力追求卓越。<br /></li><li>改善作业流程：<br />杜绝时间与资源浪费。<br />在工作场所的体制中内建质检（built-in quality）。<br />寻找低成本但可靠的方法以替代昂贵的新技术。<br />力求作业流程的尽善尽美。<br />建立追求持续改善的学习文化。<br /></li><li>在现今还无法实现连续作业的地方，还是需要明智审慎地使用存货作为缓冲。...事实上，在正确且必要之处建立缓存货，将有助于企业整体流程的改善。<br />切记，当无法实现单件流时，次佳选择是设计维持最低存货水平的拉动式生产方式。<br /></li><li><strong>丰田生产方式并不是一面倒的坚持拉动式的生产方式来避免生产过剩。在丰田公司，仍然有许多依照预定生产进度表的推动式生产方式，例如从日本把零部件输出至美国。</strong><br /></li><li>存货会使人养成不能立即面对面处理问题的坏习惯，如果你不处理问题，就无法改善你的流程，单件流和持续改善就像连体婴！<br /></li><li>丰田公司愿意冒着生产停顿之风险，以使问题浮现，并挑战员工解决问题。存货会使问题隐藏，降低解决问题的紧迫感，丰田模式就是要在每个问题一暴露后，立即暂停，解决问题。<br /></li><li>在丰田模式中，“拉动”意指“准时生产”的理想状态：在顾客（包括生产流程中下一个步骤的“内部顾客”）需要时，才提供其所需数量的东西。<br /></li><li>看板泛指发出信号的标志、公告板、门牌、布告、公布栏、卡片等；送回一只空箱子（一只看板），所发出的信息是“请补充某数量的零件”；或是送回一张卡片，上面详细记载某项零件的信息及地点。<br /></li><li><strong>使工作进程均衡化，也可能是丰田模式中最有违直觉的一个原则。</strong><br />相较于快速、起跑领先，但偶尔停下来打盹的兔子，慢但稳定持续的乌龟产生的浪费更少，表现也更令人满意。<br /><strong>均衡化是使生产量和产出组合都能平均化，它并不是根据顾客订单的实际流量来制造产品，顾客订单流量可能会出现明显波动，均衡化系拿一段时间内的总订单量来平均化，使每天的产量与产出组合相同。</strong><br /></li><li><strong>那些成功应用丰田生产方式的公司多半会结合按单定制与维持一定水平的最终产品存货，可以对需求量高的季节性产品采取预备存货的方式，其他产品则采取按单定制模式，这种结合能够帮助我们实现生产的均衡化，并以按单定制的模式来制造大多数产品。</strong>（个人注：不过这也对生产线的切换提出很高的要求！）<br /></li><li><strong>现地现物，亲自到现场查看以了解实际情况，就像刑侦专家在现场调查犯罪一样。</strong>书面报告和数字不会显现自己在现场看到的东西（虽然他仍然需要看到这些报告和数字表格），表格和数字可能代表结果，但它们不会显现每天的实际流程细节。收集材料和深入分析才能告诉你，你的常识到底正不正确。<br /></li><li><strong>作战室、全功能团队、面对面、看板等敏捷开发思想！</strong><br /></li><li>丰田家族是创新者、务实的理想主义者，他们在实践中学习，始终秉持奉献社会的使命，为达成目标而奋斗不懈，最重要的是，他们是以身作则的领导者。<br /></li><li>早年接受的专业训练是社会科学，他们看事情讲究宏观，却不忘追究事情的根由。<br /></li><li><strong>丰田生产方式最重要的概念之一是从美国超级市场的作业流程中学到的拉动式生产方式。</strong><br /></li><li>看板类似“油量仪表盘”，只有油料不多了，你才需要到加油站加油。<br /></li><li><strong>下个流程是顾客！</strong><br /></li><li>重视质量管理可以更进一步降低成本。<br /></li><li>工业时代向知识经济时代转型。<br /></li><li>此制度必须每天一贯一致的态度实行，而非只是一阵旋风。<br /></li><li>靠不断尝试的行动以获得改善，就会使自己的能力与知识得以提升。<br /></li><li><strong>最好能选择性的使用信息技术，而且在许多时候，纵使可以采用自动化，以降低劳工人数与成本，最好还是使用人工流程。人是最具弹性的资产，如果你未能了解人工流程并使之变得更有效率，就无法知道流程的哪些部分需要自动化作为支撑。</strong><br /></li><li>亨利·福特首创批量生产方式。<br /></li><li><strong>每个人在其一生当中都应该有一个宏伟的规划，我把毕生投入与发明新的织布机，现在轮到你了，你应该努力达成能对世界有所贡献的事。---丰田佐吉给儿子的寄语</strong><br /></li><li>安全是所有活动的基础。<br /></li><li><strong>xx以主动积极、不十分友善、对负责的任务有强烈的达成决心而闻名。</strong><br /></li><li>顾客感受到的噪声与震动，其根本源头是引擎。<br /></li><li><strong>在丰田领导高层看来，最严重的危机是丰田的同仁并不认为存在危机，或缺乏必须持续改善的急迫感。</strong><br /></li><li><strong>大部屋的主要功能有两个：信息管理与现场决策。团队成员聚集在此大房间的频率如何呢？视情况而定，不过，通常整个团队至少每两天就聚集一次，一天聚集在大部屋，另一天，总工程师待在自己的办公室里。</strong><br /></li><li>我访谈过的每位丰田员工都具备赚钱以外的目的感，他们对公司有极大的使命感，且能够根据公司使命来辨别是非对错。<br /></li><li>团结将近25万名员工朝着比赚钱更重要的目标共同努力。丰田公司经营企业的起始点是为顾客、社会与国家创造更高的价值。<br /></li><li>丰田公司了解到维持员工的饭碗是它对社区与社会的责任之一。<br /></li><li>丰田的战略决策以其长期理念为导向，它不会随意、轻易地放弃这些理念，唯有当世界发生根本性变迁而威胁到公司之长期生存时，丰田才有可能改变它在制造、投资，以及人员管理方面的理念，而且它会先经过充分分析后，才做出改变。<br /></li><li>生存是进化的第一要义。<br /></li><li><strong>若船上有一位不服从指挥的浆手，划动速度比其他人快，会发生什么结果呢？船的划动会乱了方寸，速度会慢下来。换句话说，一位浆手的速度过快，将会使整艘船的速度减缓。</strong><br /></li><li>员工从事更多真正创造价值的工作，且能够即时看到自己努力的结果，使其获得更高的成就感与工作满意度。<br /></li><li>长鞭效应：长鞭效应是对需求信息扭曲在供应链中传递的一种形象的描述。其基本思想是：在供应链上的各节点，企业只根据来自其相邻的下级企业的需求信息进行生产或者供应决策时，需求信息的不真实性会沿着供应链逆流而上，产生逐级放大的现象。当信息达到最源头的供应商时，其所获得的需求信息和实际消费市场中的顾客需求信息发生了很大的偏差。由于这种需求放大效应的影响，供应方往往维持比需求方更高的库存水平或者说是生产准备计划。<br /></li><li>存货的种类：<br />1、真正百分之百按单定制的产品，必须摆在待命区以便立即装上卡车；<br />2、此工厂知道一到春季与夏初就会需求大增的产品，应该在整年稳定地生产，以累积应付旺季的缓冲存货；<br />3、安全存货，用以应付意外的产品需求增加，这种需求并非季节性的波动，而是顾客需求的突然增加；<br />4、缓冲性存货，用以应对工厂或者设备发生故障的情况，是顾客在机器维修之际，仍然得以继续获得产品供给，这种情况是工厂本身造成的变异。<br /></li><li><strong>标准化的工作本身就是应对质量问题的一种对策。</strong><br /></li><li><strong>使团队把重心放在解决问题上，而不是一味地讨论是哪些人的责任，因为这种咎责只不过是另外一种形式的浪费。</strong><br /></li><li><strong>美国人往往以为引进新技术是解决问题的好方法，丰田则是偏好先通过对“人”与流程的改善来解决问题，然后才以技术作为补充。...改善质量最重要的是人与流程。</strong><br /></li><li><strong>标准工作说明表及其中包含的信息是丰田生产方式中的要素，生产一线上的员工要写出一张让其他人能够了解的标准工作说明表，他必须相信这张表的重要性。</strong><br />如果你把标准化视为现在你能想到的最佳境界但却是未来可以做到的改善，你就能各有所精进；但是，如果你把标准当成设定种种限制，那么，改善的进程就会停滞不前。<br /><strong>丰田并非实行可能使工作变成刻板且降级的僵化标准，而是把工作的标准当成对员工的授权，以及促进员工在工作上额创新的基础。</strong><br />任何流程除非标准化，否则不可能达到真正的改善；一个流程若杂乱无章、经常改变，那么，任何针对此流程进行的改善只不过是多增加了一项变化种类。<br /><strong>你必须先把流程标准化，继而使其稳定，然后才能谈持续改善。</strong><br />获取知识并不困难，困难部分在于是员工使用数据库里的这些标准，并持续完善这些标准。<br />以标准化作为授权的工具，首先，工作标准必须够明确，方能作为有用的指引，但同时也必须够概略化，以保留某种程度的弹性。在重复性质的人力工作方面，工作标准是相当明确的；在工程作业方面，由于没有固定的数量，因此，标准必须更具有弹性变化。举例来说，知道车盖的曲率和风阻之间的关系，远比知道车盖弧度线的特性参数来得重要。其次，执行工作者必须对工作标准加以完善。<br /><strong>先规范操作流程，再谈自动化，尽可能地使制度弹性化，这样，你才能随着业务的变化而持续完善流程。此外，随时辅以现地现物所获得的信息。</strong><br /></li><li><strong>最佳的选择往往是低科技解决方案。</strong><br /></li><li>标准化加上创新，再转化为新标准。<br /></li><li>可视化管理制度的目的是改善价值流<br />可视化管理也可能显示材料或工具等项目该放置于何处、某个项目在此处之数量有多少、执行某件工作的标准程序如何、进行中的工作情况如何，以及其他种种工作流程的重要信息。就最广义的定义而言，可视化管理指的是所有种类的实时信息显示设计，以确保作业与流程的快速且适当执行。<br /></li><li><strong>日常运营作业的主要的方法则是实现流程的可视化。 个人注：房地产的流程也可以参考。</strong><br /></li><li><strong>以一页报告呈现你必须知道的东西。少即是多！</strong><br /></li><li>执行工作的是人，传送与流程信息的是计算机。<br /></li><li>每当新领导走马上任而企图改变公司文化时，只不过是表面性地撼动公司，根本无法产生深层的文化变革，或使员工产生忠诚度。<br /></li><li><strong>他们多半会以提问题的方式来领导，他们询问有关实际情况的问题，以及员工的行动策略，但即使他们具备相关知识，也不会直接为这些问题提供答案。</strong><br /></li><li>把问题视为训练与教导员工的机会。<br /></li><li>成功的开始主要取决于建立丰田文化，而不是建立一个有正确技术的工厂。...我们建立的不是一座仓库厂房，而是在建立文化，这是我们成功的原因。<br />先花时间发展文化是值得的，因为“我们只有一次建立正确文化的机会。”<br /></li><li>不到手弄脏，如何能把工作做好。<br /></li><li>思维地理学：一系列实验获得具体证据显示，在观看相同场景时，西方人多半浅显地看到景物的一般类别，而亚洲人多半更好更仔细地看景物以及景物之间的关系。<br /></li><li><strong>基于人为视觉导向的事实，丰田新进的员工必须学习如何尽可能精炼文字，并佐以视觉辅导的沟通方法。</strong><br /></li><li><strong>我们要的是有创意、思考型的专业人员。</strong><br /></li><li>创造自治团队，方能具有弹性与竞争力。<br /></li><li><strong>由于丰田公司把为顾客创造价值视为公司存在的最主要的目的之一，而作业员是实际执行创造价值工作者，因此，它把作业员放在组织层级的最上层，其他层级都是扮演支持性角色。</strong><br /></li><li><strong>会议管理：把信息分享和解决问题开来：应该在会议之前尽可能多分享信息，使会议时间集中于解决问题。</strong><br /></li><li><strong>最明显的力量是有人能坦诚指出不正确的事情，勇于负起责任，提出对策以防止这些事情再发生。</strong><br /></li><li><strong>丰田公司最强调的并不是工具、技巧、方法，而是思考问题的方式与解决问题的方法，在丰田公司，解决问题时，思考占了80%，工具只占20%。不幸的是，我发现，许多公司在实施六西格玛时，使用了所有时髦的分析工具。他们解决问题时，工具占了80%，思考只占20%。（个人注：其实对管理者也一样，可能思考占比会比事务的工作多。）</strong><br /></li><li>未去过日本的人可能不了解，反省的目的并不是要伤害个人，而是要帮助个人改进；<br /></li><li><strong>你考核什么，就会获致什么。</strong><br /></li><li>工厂里的重复性工作和自己技术与服务性质的工作显著不同，就如同他们在黑夜和白昼的生活迥异一样。<br /></li><li>认识到任何流程皆可做到某种程度的重复，这是起点。<br /></li><li>医院的比喻<br />开发是医生，运维是护士。<br /></li><li>任何复杂的服务事业，改善之路的第一项行动是绘制整个系统的价值流程图。<br /></li><li><strong>前置期：指产品在系统中停留的时间。</strong><br /></li><li>建议考核指标越少越好，因为追踪记录这些指标会占用人们的工作时间。<br /></li><li><strong>作业优化，优先于计算机、系统的优化。</strong><br /></li><li>那应该是建立一种制度，坚守此制度，并持续不断的完善它。絮乱无章地赶搭一波又一波的热潮，不可能成为学习型组织。<br /></li><li><strong>六西格玛，这是全面质量管理的延伸工具，指的是平均每100万个产出中，只有不超过3.4个次品。</strong><br /></li><li><strong>从客户的角度设定考核指标，同时，从客户的角度来设置对应的组织架构及工作安排。</strong><br /></li><li>筚路蓝缕：bì lù lán lǚ ，筚路：柴车；蓝缕：破衣服。 意思是指驾着简陋的柴车，穿着破烂的衣服去开辟山林道路。形容创业的艰苦。<br /></li><li>Nova 诺娃，是新星的意思。</li></ul><p><img src="/img/丰田模式1.jpg" /><br /><img src="/img/可视化.png" /><br /><img src="/img/丰田模式2.jpg" /><br /><img src="/img/丰田模式3.jpg" /><br /><img src="/img/丰田模式4.jpg" /><br /><img src="/img/丰田模式5.jpg" /><br /><img src="/img/丰田模式6.jpg" /><br /><img src="/img/丰田模式7.jpg" /><br /><img src="/img/丰田模式8.jpg" /><br /><img src="/img/丰田模式9.jpg" /><br /><img src="/img/丰田模式10.jpg" /><br /><img src="/img/丰田模式11.jpg" /><br /><img src="/img/丰田模式12.jpg" /><br /><img src="/img/丰田模式13.jpg" /><br /><img src="/img/丰田模式14.jpg" /><br /><img src="/img/丰田模式15.jpg" /><br /><img src="/img/丰田模式16.jpg" /><br /><img src="/img/丰田模式17.jpg" /><br /><img src="/img/丰田模式18.jpg" /><br /><img src="/img/丰田模式19.jpg" /><br /><img src="/img/丰田模式20.jpg" /><br /><img src="/img/丰田模式21.jpg" /><br /><img src="/img/丰田模式22.jpg" /><br /><img src="/img/丰田模式23.jpg" /><br /><img src="/img/丰田模式24.jpg" /><br /><img src="/img/丰田模式25.jpg" /><br /><img src="/img/丰田模式26.jpg" /><br /><img src="/img/丰田模式27.jpg" /><br /><img src="/img/丰田模式28.jpg" /><br /><img src="/img/丰田模式29.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 丰田模式 </tag>
            
            <tag> 精益生产 </tag>
            
            <tag> 4P </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《分布式服务架构》读书心得</title>
      <link href="/2017/10/13/%E3%80%8A%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2017/10/13/%E3%80%8A%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>外国人写的书偏重于思想，中国人写的书偏向于操作层面，所以国人书里面的案例、代码会多点，可以当手册用;-)。<br />本书，介绍从单体架构演变到SOA架构，再到微服务架构的历程，然后针对微服务架构中最重要的一致性问题进行阐述。同时，针对互联网企业对服务化系统高性能要求，针对容量的评估和性能保障，通过实际的案例进行讲解，对日常的工作有实际的参考意义。<br />在运维环节，描述了如何搭建目前流行的ELK大数据日志系统，以及目前市面上常见的应用性能管理系统（APM）产品的实现。最后还提供了作者积累的一系列的运维脚本、工具给到读者。<br />文章最后详细说明了容器及敏捷开发的一些工具。<br />总体来说，本书适合作为一个工具书，理论层面只是做了简单的介绍。<br /><span id="more"></span><br /><img src="/img/架构发展图.png" title="架构发展图" /></p><ul><li><strong>ATAM 互联网架构权衡分析法（Archiecture Tradeoff Analysis Method) 本书提供了一个架构评审的方法案例。</strong><br />具体见书的第89页。其实简单的思想就是，业务场景的高峰要求，转换成非性能指标，结合单机的性能指标，推算出需要配备的机器。<br />具体见：<a href="http://www.jianshu.com/p/fbf56ccb4ebe">互联网性能与容量评估的方法论和典型案例</a><br /></li><li>压测方案和压测场景<br />具体见书的119页，举了一个例子。简单的实现就是拿一个正常的基准测试的响应时间，然后拿目标的吞吐量的+-梯度值来计算出并发数。（吞吐量=并发数/响应时间）；先采用小的吞吐量场景，压测出对应的响应时间，然后拿这个时间，来计算出目标吞吐量的并发数，压测出响应时间；以此类推，不断往上加并发数。测出系统在符合响应时间要求内的最大吞吐量（并发数）。<br />然后在CPU和内存使用率70%的情况下的负载来测试系统稳定性。<br />[关于并发数的一篇好文章]（https://ruby-china.org/topics/26221）<br /></li><li>APM 应用性能管理<br />开源的的<a href="/img/learning-pinpoint.pdf">pinpoint</a><br />国内的应用性能监控平台：<a href="http://www.tingyun.com/">听云</a></li></ul><h1 id="文章摘要">文章摘要</h1><ul><li>JEE平台是典型的二八原则的一个应用场景，它将80%通用的与业务无关的逻辑和流程封装在应用服务器的模块化组件里，通过配置的模式提供给应用程序访问，应用程序实现20%的专用逻辑，并通过配置的形式访问应用服务器提供的模块化组件。事实上，应用服务器提供的对象关系映射服务、数据持久服务、事务服务、安全服务、消息服务等通过简单的配置即可在应用程序中使用。<br /></li><li>由于面向对象领域模型与关系型数据库存在天然的屏障，所以对象模型和关系模型之间需要一个纽带框架，也就是我们常说的ORM框架，它能够将对象转化成关系，也可以将关系转化成对象，于是，Hibernate框架出现了。Hibernate通过配置对象与关系表之间的映射关系，来指导框架对对象进行持久化和查询，并且可以让应用层开发者像执行SQL一样执行对象查找。这大大减少了应用层开发人员写SQL的时间。然而，随着时间的发展，高度抽象的ORM框架被证明性能有瓶颈，因此，后来大家更倾向于使用更加灵活的MyBatis来实现ORM层。<br /></li><li>在微服务架构中，提倡运维人员也是服务项目团队的一员，倡导谁开发、谁维护，实施终身维护制度。（全功能团队）<br /></li><li>核心链路以外的服务可以使用异步的消息队列进行异步化。<br /></li><li>我们一般会将同一类功能划分在一个微服务中，尽量避免过细而导致成本增加，适可而止。<br /></li><li>切记，永远不要在本地事务中调用远程服务，在这种场景下，如果远程服务出了问题，则会拖长事务，导致应用服务器占用太多的数据库链接，让服务器负载迅速攀升，在严重情况下会压垮数据库。<br /></li><li><strong>Mule ESB：MuleSoft公司出品的基于Java语言的企业服务总线产品。它的优点是可以把现有的不同技术栈历史遗留系统与新增的服务化系统通过总线进行串联和编排，来满足日新月异的业务功能需求。</strong><br /></li><li>访问数据库操作无论是查询还是更新，原则上都是短小操作，不需要异步化。不推荐将大数据存储到关系型数据库中，关系型数据库只存储相关的最小化核心信息。<br /></li><li>在刚上线且不稳定的项目中通常设置成Debug级别日志，便于查找问题；在线上系统稳定后使用Error级别日志即可，这样能有效提高效率。<br /></li><li>我们在调用远程服务和被远程服务调用时，都需要打点耗时日志，这能够帮助我们排查接口调用的错误或者超时等问题。<br /></li><li><strong>对异常等错误信息必须打印错误级别及以上的日志，对线上日志要定期检查，没有异常日志产生的服务器才是健康的服务。</strong><br /></li><li><strong>QA环境可以是用debug及以下级别的日志。<br />刚刚上线的应用还没有到稳定期，使用debug级别的日志。<br />上线后稳定的应用，使用info级别的日志。<br />常年不出现问题的应用使用error级别的日志即可。</strong><br /></li><li>推荐使用日志框架原生的按照日期滚动的Appender来记录日志，这样不对I/O产生冲击，是轻量级的日志滚动功能实现。<br /></li><li>每一次上线都必须有快速回滚的方案。<br /></li><li><strong>容器是对应用层的抽象，而虚拟机是在物理硬件层面上的虚拟化。<br />Docker将集装箱思想运用到了对软件的打包上，为代码提供了一个基于容器的标准化运输系统，可以将任何应用及其依赖打包成一个轻量级的、可移植的、自包含的容器中，可以运行在几乎所有操作系统上。</strong><br /></li><li><strong>海恩法则(Heinrich's Law)</strong>，是德国飞机涡轮机的发明者德国人帕布斯·海恩提出的一个在航空界关于安全飞行的法则，海恩法则指出： 每一起严重事故的背后，必然有29次轻微事故和300起未遂先兆以及1000起事故隐患。法则强调两点：一是事故的发生是量的积累的结果；二是再好的技术，再完美的规章，在实际操作层面，也无法取代人自身的素质和责任心。<br /></li><li><strong>墨菲定律的本质其实是：你只会注意到那些你不愿意接受的结果，而把出现概率相同的满意的结果视为理所当然并在统计的时候忽略掉。</strong>最明显的例子就是开车变道。每个人都说：我换到哪条道上，哪条道就忽然变成最慢的了。事实上，他至少有一半以上的机会是变道之后比刚才快了，但他不会注意到，不会在心里说：“我擦，这次选对了”，只会一边超过旁边的车一边寻找更快的一条道。<br /></li><li><strong>墨菲定律指“凡是可能出错的事都会在未来出错。”（Anything that can go wrong will go wrong.）。</strong>引申为“所有的程序都有缺陷”，或“若缺陷有很多个可能性，则它必然会朝往令情况最坏的方向发展”，讲白话一点“明明之前应该都没问题，偏偏关键时刻就是出错了”。<br />行政管理涉及的因素非常复杂，单就人为而言，管理学家也是极难解释，故此，管理者自不能避免目标制订和执行永不出错，这个管理原则说明，如果一个危机将要发生，它总会出事，换言之，管理者需要时时刻刻做好准备，面对到来的失误和失败。<br />墨菲理论没带有事情必坏或必好的成果，他只是让管理者知道，能发生的事，总会发生，换言之，管理者必须对所有可能会发生的事情作好周全的准备，这也就是为何泳池等场所也要配备灭火器等设备的原因之一。<br /></li><li><strong>幸存者偏差:</strong>最早来源于英军对战斗机改进做的统计，根据对飞回来的受损灰机的统计发现其主要受损部位集中在机翼，所以结论是应当减少机腹的装甲加强机翼的装甲？这个结论显然是可笑的，造成这种偏差的原因是机腹中弹的灰机大多数都坠毁了，统计结论产生了偏差，这个偏差被命名为“幸存者偏差”。在现实生活中该偏差比比皆是，举个最简单的栗子，老有人说“读书有什么用，我的小学同学XXX，他从小成绩一塌糊涂，初中都没念完就退学了，现在生意做得可大了，我本科毕业还不是租着房吃着泡面朝九晚五。”实际情况是一个班会读书的那帮孩子日后也有生意做得不错的、也有租房吃泡面的、也有在家啃老的，平均生活水准高于不读书的，但是不读书的孩子中有一些已经吸毒吸成鬼了、赌博欠一屁股债躲起来了、在家乡加入黑社会被砍死了，这些人你看不到，你只能看到那些幸存者，生意做得可大了。</li></ul><p><img src="/img/分布式架构1.jpg" /><br /><img src="/img/分布式架构2.jpg" /><br /><img src="/img/分布式架构3.jpg" /><br /><img src="/img/分布式架构4.jpg" /><br /><img src="/img/分布式架构5.jpg" /><br /><img src="/img/分布式架构6.jpg" /><br /><img src="/img/分布式架构7.jpg" /><br /><img src="/img/分布式架构8.jpg" /><br /><img src="/img/分布式架构.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 读书心得 </tag>
            
            <tag> 架构 </tag>
            
            <tag> 分布式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《持续交付--发布可靠软件的系统方法》读书心得</title>
      <link href="/2017/10/01/%E3%80%8A%E6%8C%81%E7%BB%AD%E4%BA%A4%E4%BB%98--%E5%8F%91%E5%B8%83%E5%8F%AF%E9%9D%A0%E8%BD%AF%E4%BB%B6%E7%9A%84%E7%B3%BB%E7%BB%9F%E6%96%B9%E6%B3%95%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2017/10/01/%E3%80%8A%E6%8C%81%E7%BB%AD%E4%BA%A4%E4%BB%98--%E5%8F%91%E5%B8%83%E5%8F%AF%E9%9D%A0%E8%BD%AF%E4%BB%B6%E7%9A%84%E7%B3%BB%E7%BB%9F%E6%96%B9%E6%B3%95%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>看了这本书，总结的一些脑图：<br /><img src="/img/工作思路.png" /><br /><img src="/img/DevOps的理解.png" /><br />这本书之前看过，不过没有做笔记。在看板管理这本书里面有推荐，就顺便再看了一遍，温故而知新。<br /><strong>本书主要出发点就是敏捷思想的那个原则：If it hurts，do if more often 。</strong><br />在软件项目中，发布是风险最高的一个环节，所以，根据敏捷的思想，怎么把发布的东西提前、频繁的做就成了关键，也就是所谓的持续交付。<br />而要达到频繁、重复做一件事，核心就是自动。只有自动化，才能重复的、可靠的去做。所以持续交付的核心就是全面版本控制和自动化。版本的管理不单是程序代码和对应配置信息，还包括运行环境及其配置信息、部署脚本的版本控制；自动化包括自动化构建、自动化单元测试、自动化验收测试、自动化部署、自动化发布等。通过持续的构建、部署，能提供一种快速的反馈，能让修复的成本最低，并且让软件一直保持在一种可用的状态。<span id="more"></span></p><p>现在很多项目都是开发者开发后将困难交给测试者，而测试者又在发布时将困难转嫁到运维团队。当出现问题时，人们花费大量的时间来修复错误，并用同等的时间来互相指责。其实，这些错误就是这种各自为政的工作方式所不可避免的结果。为了解决这个问题，为了加强合作，将敏捷思想注入了系统管理和发布中，就是我们说的DevOps运动。</p><p>要达到持续发布的目标，需要开发、测试的通力合作，编写能自动运行的测试脚本；需要运维、开发的通力合作，编写相关的自动构建和部署脚本、自动化的环境准备（虚拟化技术）。只有这样，才能让开发人员简单的、可重复的把代码部署到各种环境。为了完成这些任务，离不开开发人员、测试人员、运维人员的脚本编写能力。再说虽然有很多自动化工具的支撑，但是还是需要有一定的脚本编写能力才能更好的发挥相关工具的作用。</p><p>在实际发布过程中，环境的管理以及对应的配置管理最容易给忽略。为了达到持续发布的目的，所有的环境变更在上线之前必须经过测试，因而要将其编成脚本，放在版本控制系统中。这样，一旦修改被认可，就可以通过自动化的方式将其放在生产环境中。这样，对环境的修改和对软件的修改就没什么分别了。</p><p>像作者所说，<strong>持续集成就是不断对程序进行测试；持续部署就是对部署过程进行测试；</strong>只有这些测试在发布前频繁的、重复的做，那么到了发布那天，一切也水到渠成了。</p><h1 id="文章摘要">文章摘要</h1><ul><li><strong>一个可工作的软件包括：可执行的代码、配置信息、运行环境和数据。<br />软件部署包括：<br />1、提供并管理呢的软件所需要的运行环境，包括硬件配置、所依赖的软件、基础设施以及所需要的外部服务；<br />2、将你的应用程序的正确版本安装在其之上；<br />3、配置你的应用程序，包括它所需要的任何数据及状态。</strong><br /></li><li>自动化：可靠、可重复、可审计。<br /></li><li>敏捷宣言的第一原则：我们的首要任务是尽早持续交付有价值的软件并让客户满意。<br /></li><li>精益软件开发运动告诉我们：整体优化是非常重要的。<br /></li><li>持续集成，就像盖房子的时候，每道工序（类似软件开发中的功能点）完成就做验收，而不是等房子都建好再去验收。<br /></li><li>精益和迭代软件开发理论：快速并迭代地交付有价值且可工作的软件，并持续不断地从交付流程中消除浪费。<br /></li><li>性能：是指对处理单一事务所花时间的一种度量，既可以单独衡量，也可以在一定的负载下衡量；<br /></li><li>吞吐量：是指系统在一定时间内处理事务的数量，通常它受限于系统中某个瓶颈。<br /></li><li><strong>容量：是指在一定的工作负载下，当每个单独请求的响应时间维持在可接受范围内时，改系统所能承担的最大吞吐量。</strong><br /></li><li>找出有多少中负载，以及每种负载有多大，并考虑覆盖不同路径的场景，既是一门艺术，也是一门科学。完全复制真实生产环境的流量是不可能的，所以需要做流量分析，并结合经验和直觉来达到尽可能接近与真实环境的模拟。<br />另外，也不要依据硬件的某些特定参数对应用程序的扩展性作出线性推论，这是在蒙蔽你自己。...复杂系统的行为很少是这种线性相关的。<br /></li><li>我们不建议通过UI进行容量测试。<br /></li><li>自动化部署（脚本），一方面把部署专家从这些重复的、枯燥的工作中解放出来；另一方面能把部署专家的知识沉淀下来。<br /></li><li><strong>伏尔泰说过，追求完美是把事情做好的大敌。</strong><br /></li><li>精益制造的目标是确保快速交付高质量的产品，它聚焦于消除浪费，减少成本。<br /></li><li>配置管理是指一个过程，通过该过程，所有与项目相关的产物，以及它们之间的关系都被唯一定义、修改、存储和检索。<br /></li><li>可以将整个环境（包括配置基线上的操作系统）做成一个虚拟镜像，放在版本控制库中，这可以作为更高级别的保证措施，并且可以提高部署的简单性。<br /></li><li>持续集成，就要避免参建分支（除了发布分支之外），因为分支会带来合并的问题。<br /></li><li>提交的时候，要使用意义明显的提交注释，并且这个注释还需要包括一个链接，可以链接到项目管理工具中的一个功能或者缺陷，从而知道为什么要修改这段代码。<br /></li><li>可配置的软件并不总是像它看起来那么便宜。更好的方法几乎总是先专注于提供具有高价值且可配置程度较低的功能，然后在真正需要时再添加可配置选项。<br /></li><li><strong>不要把密码签入到版本控制系统中，也不要把它硬编码到应用程序中。用户在部署时每次都手工输入密码。</strong><br /></li><li>配置管理中不同环境配置管理的重要性。 个人注：有一次，测试机的配置文件用错了生产环境的配置，导致流程测试的短信通知都发送到了实际的员工那里。<br /></li><li>要为每个应用程序维护一份所有配置选项的索引表，记录这些配置保存在什么地方，它们的生命周期有多长，以及如何修改它们。<br /></li><li>我们要把应用程序的配置信息当做代码一样看待，恰当的管理它，并对他们进行测试。<br /></li><li>配置管理包括<strong>环境的配置(硬件、依赖软件、基础设施、外部系统等)以及应用程序的配置</strong>，两者都同样重要。<br /></li><li>对于跨地区的团队管理，让各团队之间的人员做定期的轮换也是非常有必要的，这样每个地方的成员都能与其它地方的团队成员建立起一些私人交情。<br /></li><li>我们一般将代码覆盖率高于80%的测试视为“全面的”测试。<br /></li><li><strong>大多数界面测试工具与界面本身紧紧耦合在一起，其后果就是，一旦界面改变了（哪怕是一点儿），测试也会被破坏，这会导致很多假阳性，因为你会经常遇到这种情况，即测试被破坏的原因并不是应用功能不正确，而只是由于某个复选框的名字被修改了。</strong><br /></li><li>需要些的最重要的自动化测试是哪些对Happy Path的测试。每个需求或者用户故事都应该有对Happy Path的自动化验收测试，而且至少有一个。这些测试应该被美味开发人员当做冒烟测试来使用。<br /></li><li>遗留系统的代码通常没有标准组件化，结构比较差。所以修改系统某部分的代码却影响了另一部分代码的事情经常发生。<br /></li><li>可以将应用程序分成两部分，一部分是实现系统功能的具体代码，另一部分则是在这些代码之下，为实现系统功能提供支撑的框架代码。<br /></li><li>编译出来的二进制包应该具有与环境无关性。<br /></li><li>会变动的与不变的东西分离。<br /></li><li><strong>在部署应用程序是，应该用一个自动化脚本做一下冒烟测试，用来确保应用程序已经正常启动并运行了。这个测试应该非常简单，比如只要启动应用程序，检查一下，能看到主页面，并在主页面上能看到正确的内容就行了。这个冒烟测试还应该检查一下应用程序所依赖的服务是否都已经启动，并且正常运行了，比如数据库，消息总线或外部服务。</strong><br /></li><li>尽管验收测试非常有价值，但他们的创建和维护成本也是非常高的。所以要时刻牢记，自动化验收测试也是回归测试。不要幼稚地对照着验收测试条件，盲目地把所有东西都自动化了。<br /></li><li><strong>生产环境应该是完全受控的，即对生产环境的任何修改都应该通过自动化过程来完成。这不仅包括应用程序的部署，还包括对配置、软件栈、网络拓扑以及状态的所有修改。</strong><br />通过自动化的环境准备和管理、最佳的配置管理实践以及虚拟化技术，环境准备和维护的成本会显著降低。<br /></li><li>所有的构建工具都有一个共同的核心功能，即可以对依赖关系建模。<br />每个任务都包括两点内容，一是它做什么，二是它依赖于什么。<br /></li><li>幂等：多次运算结果是一样的，例如绝对值运算。abs(x)=abs(abs(x))<br /></li><li>Ant 很快成了Java项目构建工作的事实标准。现在很多IDE和其它工具都支持Ant。<br /></li><li><strong>Maven 这种流行的“惯例胜于配置”（convention over configuration)的原则意味着，只要项目按Maven指定的方式进行组织，它就几乎能用一条命令执行所有的构建、部署、测试和发布任务，却不用写很多行的XML。</strong><br /></li><li>如果你刚开始一个Java项目，或者想找Ant或Maven的替代品，我们强烈推荐Builder.<br /></li><li><strong>“使用同样的脚本部署每个环境”和“环境配置信息的不同（比如服务器URI或IP地址）这两件事应该分开管理，即将配置信息从脚本中分离出来，并将其保存再版本控制库中。</strong><br /></li><li>“脚本”这个属于被广泛应用，通常是指辅助我们进行构建、测试、部署和发布应用程序所有自动化脚本。<br /></li><li>尤其注意的一点是，运行的单元测试不应该与文件系统、数据库、库文件、框架或外部系统等打交道。所有对这些方面的调用都应该使用测试替身代替，比如模拟对象（Mock）和桩等。<br /></li><li>通常我们会让提交测试在10分钟内完成。<br /></li><li>当使用XUnit Test这类测试框架时，可以将验收条件写在测试的名字中，然后通过XUnit Test测试框架直接运行验收测试。<br /></li><li><strong>首先，抵制使用生产数据的备份作为验收测试的测试数据库的诱惑（尽管有时它对吞吐量测试是有用的）。相反，我们要维护一个受控的数据最小集。</strong><br /></li><li><strong>原子测试会创建它所需要的一切，并在运行后清理干净。 ...一个常用的技术就是在测试开始时创建一个事务，在其结束时将其回滚。这样，数据库就回到了测试之前的状态。</strong><br /></li><li>自动化验收测试不应该运行在包含所有外部系统集成点的环境中。相反，应该为自动化验收测试提供一个受控的环境，并且被测系统应该能在这个环境上运行。<br /></li><li>不断运行这些复杂的验收测试，的确会花费开发团队很多时间。然而，根据我们的经验，这种成本投入是一种投资，会节省很多倍的维护成本。<br /></li><li>在项目开始就识别出哪些是重要的非功能性需求，这一点至关重要。<br /></li><li><strong>从审计人员的角度来捕获非功能性需求。</strong><br /></li><li>部署与发布之间的主要区别在于回滚的能力。<br /></li><li>服务器应用程序不应该有GUI。 个人注：PM的工具就是这样，导致出错的时候，会在GUI弹出对话框。导致程序无法运行。<br /></li><li>MTBF(Mean Time Between Failure，平均无故障时间)<br />MTTR(Mean Time To Repair，平均修复时间)<br />RPO(Recovery Point Objective,恢复点目标)<br />RTO(Recovery Time Objective,恢复时间目标)<br /></li><li><strong>基础设施访问控制<br />在没有批准的情况下，不允许他人修改基础设施；<br />制定一个对基础设施进行变更的自动化过程；<br />对基础设施进行监控，一旦发生问题，能尽早发现。</strong><br /></li><li>PXE是通过以太网启动机器的一个标准。当在机器的BIOS中选择通过网络启动的话，那实际上就是PXE。<br /></li><li><strong>很多企业有一个双重身份的试运行环境，既承担生产环境部署的测试Udine，也可以作为故障备份。</strong><br /></li><li>备份网络与生产环境网络也是物理隔绝的，以便当备份时大量数据的移动不会影响性能或管理网络。<br /></li><li>SNMP是监控领域最常见的标准。在SNMP中，所有的都是变量，通过查看这些变量来监控系统。<br /></li><li><strong>“向前兼容性”是指应用程序的早期版本仍旧可以工作在后续版本的数据库上的一种能力。</strong><br /></li><li><strong>有些人把组件叫做“模块”（Module）。在windows平台上，一个组件通常是以DLL形式打包的。在Unix上，它可能就被打包成SO文件了。而在Java的世界里，它可能就是一个JAR包。</strong><br /></li><li>基于组件的设计通常被认为是一种良好的架构，具有松耦合性，是一种鼓励重用的设计。<br /><strong>对组件的一个要求就是它应该可以独立部署。</strong><br />组件为其他系统提供一个接口（比如提供某个API的框架或服务）。<br /></li><li>依据功能领域而不是组件来组件团队确保了每个人都有权利修改代码库的任何部分，同时在团队之间定期交换人员，确保团队人员之间的良好沟通。<br /></li><li><strong>悲观锁、乐观锁是版本控制系统的概念。乐观锁，就是允许大家同时修改一个文件。乐观锁通常假设某个文件中的某一行是一个可变的最小单位。</strong><br /></li><li><strong>主干开发模式：在主干上开发，而只有发布的时候创建分支，并且在分支上修复BUG，然后合并到主干上。</strong><br /></li><li>版本好的格式：w.x.y.z 其中w是主版本，x是一个发布，y是某个客户的表示，而z是一个构建。<br /></li><li>像精益制造业一样，没有频繁交付的软件就是仓库中的库存。它已经花钱制造完了，却还没有为你赚钱，实际上保管它也是花钱的。<br /></li><li><strong>本书主要关于与被称做服务转换（service transition）的ITIL阶段。</strong><br /></li><li>持续集成的目标是让正在开发的软件一直处于可工作状态。<br /></li><li>自动化测试，就类似Dos里面的批处理文件，可以重复的、自动的执行；而人工测试，就类似Windows的界面操作，只能靠人工处理。 可以想象，要把一个目录copy到另外一个盘符的对应目录，两者的操作方式是有很大的区别。<br /></li><li>治理：Governance。</li></ul><p><img src="/img/持续交付1.jpg" /><br /><img src="/img/持续交付2.jpg" /><br /><img src="/img/持续交付3.jpg" /><br /><img src="/img/持续交付4.jpg" /><br /><img src="/img/持续交付5.jpg" /><br /><img src="/img/持续交付6.jpg" /><br /><img src="/img/持续交付7.jpg" /><br /><img src="/img/持续交付8.jpg" /><br /><img src="/img/持续交付9.jpg" /><br /><img src="/img/持续交付.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 持续集成 </tag>
            
            <tag> 持续交付 </tag>
            
            <tag> 自动化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《看板方法》读书心得</title>
      <link href="/2017/09/20/%E3%80%8A%E7%9C%8B%E6%9D%BF%E6%96%B9%E6%B3%95%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2017/09/20/%E3%80%8A%E7%9C%8B%E6%9D%BF%E6%96%B9%E6%B3%95%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>软件开发的工作是无形的，不像生产线那样能看到有形的产品。所以软件开发的进度和情况对客户、对管理者来说都是一块心病，往往都不知道开发同事在忙什么、忙得怎么样了。<br />而借鉴自精益生产的看板管理，恰好能解决这个问题，让软件开发的工作可视化，让客户、管理者、其它团队成员能直观的看到项目的情况，有助于整个项目的价值流流动。<br />在这本《看板方法》中，有三个核心的概念：可视化、拉动系统、在制品限额（WIP)。<br />看板系统，他的定义就是：首先以价值流（Value Stream)对软件开发生命周期的工作流程进行建模，然后建立一个可视化跟踪系统，此后，当新工作“流”经该系统时，通过跟踪其状态的变化，便可识别出瓶颈。<br />在实际过程当中，看板是一个工具。就像丰田生产方式的创建者之一大野耐一曾说过：“丰田生产方式的两大支柱，是及时生产和有人工介入的自动化。<strong>驱动这个体系运转的工具，便是看板。</strong>”</p><span id="more"></span><p>在软件开发过程当中，涌现出了很多编程的理论，包括从早期的瀑布模型到现在的敏捷模型，其核心就是想让软件的开发更接近工程科学的精确性和可预测性，提高软件开发的质量和减少浪费。在作者看来，软件开发成功有以下6个秘诀：</p><ul><li>专业于质量<br /></li><li>减少进行中的工作<br /></li><li>频繁交付<br /></li><li>根据交付速率来平衡需求请求量<br /></li><li>进行优先级排序<br /></li><li>消除变异性的根源，提升可预测性。</li></ul><p>以上秘诀加上看板这个工具，能有效的提高软件开发的效率和成功率，提高客户满意度。<br />看了这本书以后，在我们预结算项目组采用了电子看板系统Trello，整体感觉还不错。知道大家都在干什么了、各项工作的进展怎么样了、瓶颈的情况是在哪里等等。并且结合每日10分钟的晨会，整个项目组对整体的工作目标及优先级、各自工作在整体工作目标中的位置和重要性都一目了然。<br /> <font color="MidnightBlue" size = "3px"><strong>BTW：后来想了想，其实看板就是我们老板以前提出的地铁报站指示图，就是车辆到达一站，该站就变红色，未到的站点是绿色。房地产的全流程（价值流图），从拿地、规划、设计、报建、施工、验收、销售、物业等也是一种价值流。并且对一个地产项目来说，每个环节都有很多工作，通过看板就能简单实现（地铁那个因为只有一个红绿色，站点下面不能再分，不符合业务实际），并且每个工作都能通过定义卡片来识别工作事项、负责人、要求完成时间、当前状态、工作的前置条件、后置条件等。</strong></font></p><p>以下是比较好的资料：<br /><code>&#123;% post_link 平安7年精益敏捷转型之路 %&#125;</code> <a href="http://www.cnblogs.com/bhlsheji/p/4909170.html">《精益开发实战——大项目看板管理》读书笔记</a></p><h1 id="文章摘要">文章摘要</h1><ul><li><strong>精益方法是以快速高质量交付、减少浪费为目的。</strong><br /></li><li>报告准时交付的达成情况（due-date performance)<br /></li><li><strong>破坏负载（Failure Load）：因早前的质量问题引生的工作。 指返工活动，或者那些为了修复早前的不良实现而提交到看板系统中的需求开发工作。</strong><br /></li><li>大型软件开发是一场马拉松，不是短跑冲刺。<br /></li><li>看板（kan-ban)是一个日语词汇，英文字面意思是“信号卡”。<br /></li><li>人们只有释放组织中的大部分压力，才能够集中精力准确高质量地完成工作。<br /></li><li>为了能够得到持续改善，需要具备富余时间。<br /></li><li><strong>在新系统发布一个月后，这些应用程序开发团队一般都会解散，而源代码则转交到维护团队手上。（超过15个开发或测试天数的较大请求，必须作为正式项目提交立项，按照项目进行管理。）</strong><br /></li><li>不管怎么变，情况都不会变得比当前更糟！是该有人去尝试变革了。<br /></li><li>任何处于代办项列表中已经超过6个月的请求项，都可以从列表中清除出去。<br /></li><li>在工作中，如果全体员工能够持续专注于提高质量、生产率和客户满意度，那么这种文化便可称为改善文化。<br /></li><li><strong>在敏捷软件开发圈子里形成的一个基本共识是，稳定的节奏十分重要。...稳定的“心跳”对于项目十分重要。</strong><br /></li><li>减小用户故事的规模，使其粒度变得更细，降低用户故事规模的变异性。<br /></li><li>对于固定交付日期类的工作，团队会倾向于提早启动工作，以确保能够做到准时交付。这并非最优解。通过改善估算质量，才能提升以价值和交付速率来衡量的全局效能。<br /></li><li>受到充分激励且经验丰富的员工对组织绩效有着巨大的影响，人才保留对于组织是至关重要的。<br /></li><li>只有超负荷工作才能完全挖掘出知识工作者的产能----这是一种极其错误的观点。超负荷工作的状态，维持一两天或许可行，但可持续性不会超过一两周。为员工创造工作/生活平衡，决不让他们超负荷工作，这才是明智公司的经营决策。<br /></li><li>在项目过程中需要舍弃一些东西进行权衡时，传统的项目经理可能会选择延期交付、增加资源投入、缩减范围或者三者不同程度的兼而有之；<strong>敏捷项目的明确共识是缩减范围，保障交付。</strong><br /></li><li><strong>编写一个用户故事的模板：作为一名<用户>，我希望有一个<特性>，以便于<交付一些价值>。</strong><br /></li><li>同一时间只能有一个加急事项。<br /></li><li>lead time:交货时间 ，引申为前置时间。<br />backlog 积压<br />grooming 梳理<br />sprint 冲刺</li></ul><p><img src="/img/看板方法.jpg" /><br /><img src="/img/看板方法1.jpg" /><br /><img src="/img/看板方法2.jpg" /><br /><img src="/img/看板方法3.jpg" /><br /><img src="/img/看板方法4.jpg" /><br /><img src="/img/看板方法5.jpg" /><br /><img src="/img/看板方法6.jpg" /><br /><img src="/img/看板方法7.jpg" /><br /><img src="/img/看板方法8.jpg" /><br /><img src="/img/看板方法9.jpg" /><br /><img src="/img/看板方法10.jpg" /><br /><img src="/img/看板方法11.jpg" /><br /><img src="/img/看板方法12.jpg" /><br /><img src="/img/看板方法13.jpg" /><br /><img src="/img/看板方法14.jpg" /><br /><img src="/img/看板方法15.jpg" /><br /><img src="/img/看板方法16.jpg" /><br /><img src="/img/看板方法17.jpg" /><br /><img src="/img/看板方法18.jpg" /><br /><img src="/img/看板方法19.jpg" /><br /><img src="/img/看板方法20.jpg" /><br /><img src="/img/看板方法21.jpg" /><br /><img src="/img/看板方法22.jpg" /><br /><img src="/img/看板方法23.jpg" /><br /><img src="/img/看板方法24.jpg" /><br /><img src="/img/看板方法25.jpg" /><br /><img src="/img/看板方法26.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 看板 </tag>
            
            <tag> 拉动系统 </tag>
            
            <tag> 在制品限额 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《凤凰项目：一个IT运维的传奇故事》读书心得</title>
      <link href="/2017/09/05/%E3%80%8A%E5%87%A4%E5%87%B0%E9%A1%B9%E7%9B%AE%EF%BC%9A%E4%B8%80%E4%B8%AAIT%E8%BF%90%E7%BB%B4%E7%9A%84%E4%BC%A0%E5%A5%87%E6%95%85%E4%BA%8B%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2017/09/05/%E3%80%8A%E5%87%A4%E5%87%B0%E9%A1%B9%E7%9B%AE%EF%BC%9A%E4%B8%80%E4%B8%AAIT%E8%BF%90%E7%BB%B4%E7%9A%84%E4%BC%A0%E5%A5%87%E6%95%85%E4%BA%8B%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>本书主要讲述IT运维如何为了达到企业目标，通过借用工厂管理的约束理论、精益生产以及全面质量管理的理论，来改进IT的运维。<br />像作者说的，CEO和CIO的关系被描述为一场不和谐的婚姻，也就是说双方都感到无能为力，并感到自己被另一方所挟持。作者通过将故事的方式，把主人公在与CEO斗智斗勇的过程中，借助三步工作法，使IT的价值得到了体现，间接的也让业务的价值得到了体现。</p><p><img src="/img/三步工作法.png" /><br /><a href="/img/三步工作法.mmap">三步工作法脑图</a><br /><span id="more"></span></p><h1 id="文章摘要">文章摘要</h1><ul><li>作为IT运维部，主要的工作是确保形成一条迅速、可预测、持续不断的计划内工作流，从而向业务部门交付工作价值，同时尽可能降低计划外工作的影响与破坏，那样IT运维部才能提供稳定的、可预期的、安全的IT服务<br /></li><li><strong>管理运动：约束理论、精益生产或者丰田生产系统，以及全面质量管理。虽然每个运动的起源地各不相同，但他们都赞成一点：半成品是个隐形杀手。因此，管理任何一家工厂最关键的机制之一，就是工作任务和原材料的发布。没有这个机制，就无法控制半成品。</strong><br /></li><li>像工厂经理那样去思考（跨越IT部门和业务部门的界限）。他们着眼于整个工作流，确认约束点的位置，并且尽其所能地运用各种技术和流程知识来确保工作得到有效执行。<br /></li><li><strong>《团队发展的五大障碍》中提到，想要在团队中达到互相信任，你需要展现出自己脆弱的一面。</strong><br /></li><li><strong>举办了弱点分享会，分享自己最脆弱的故事，去掉身上的盾牌。分享最多的、最感人的还是发生在家人身上的故事。</strong><br /></li><li>开发提供代码、基础架构要求（硬件参数、配置参数、网络、防火墙等）<br /></li><li>控制IT运维部的工作导入量，确保绝大多数受约束的人力资源都只能投放在为整个系统的目标所服务的工作上，而不只是为了一个部门的目标服务。<br /></li><li>这也是一种系统思维，始终确保整个企业达成共同的目标，而不只是其中的一部分。<br /></li><li>相比于向系统中投入更多的工作，将无用的工作剔除出系统更为重要。<br /></li><li><strong>变更就是对应用程序、数据库、操作系统、网络或硬件进行的物理、逻辑或虚拟操作，并且这样的操作可能对相关服务产生影响。</strong><br /></li><li>80%的风险是由20%的变更产生的，需要对变更按照风险进行分类，对不同类别进行不同的授权。<br /></li><li>工作计划表：工作的分类，工作需求，优先级，工作进度，可用资源<br /></li><li>不应该根据第一个工作站的效率来安排工作，而是根据瓶颈资源所能完成的工作的速度来安排工作。<br /></li><li>关键资源所从事的任何活动都必须通过看板，不可以通过电子邮件、即时信息、电话，诸如此类的渠道。<br /></li><li>变更管理：按照风险分类、合规审计系统分类的变更授权管理<br /></li><li><strong>你最希望我做什么？最不希望我做什么？</strong><br /></li><li><strong>我强迫自己深吸一口气......我缓缓的点头，不上她的当。“我期待你提出的任何建议。” ... 我在心里默数三下才开口。</strong><br /></li><li>总是IT运维部在通宵达旦地为那些糟糕的代码埋单，每隔一小时重启一次服务器，就像电影里的超级英雄那样，尽可能向世人隐瞒糟糕的真相。<br /></li><li>日期驱动的项目。<br /></li><li>追求完美是成事的大敌。<br /></li><li>这种全员出动的工作状态是IT人的生活的一部分。<br /></li><li>ITIL代表IT基础架构库，记录着许多最好的IT实践和流程。<br /></li><li>事故及故障修复工作，占用了运维员工大概75%的工作时间，并且这些常常涉及关键业务系统，优先级最高。...导致总是无法把精力集中到对公司最为重要的事情上。<br /></li><li><strong>创建约束理论的艾利·高德拉特告诉我们，在瓶颈之外的任何地方做出的改进都是假象。...在瓶颈之前做出的任何改进只会导致瓶颈处堆积更多的库存。</strong><br /></li><li><strong>汇报的时候，一要做演练，二要考虑各种可能的应对，并且要考虑最坏的情况。就像足球比赛，要考虑领先、落后、落后几个球的打法等等，做到有备无患。</strong><br /></li><li>在发射的当口，他们还在往太空飞船上安装零件，这可不是个好兆头。<br /></li><li>提出问题的同时，最好有理有据，然后带着解决方案来汇报。 而不只是嚷嚷，我早就告诉过你了。<br /></li><li>就像免费赠送的小狗，搞死你的不是前期的投入，而是后台的运行和维护。（总体拥有成本）<br /></li><li><strong>她就像个不粘锅，什么都粘不到她的身上。</strong><br /></li><li>合作而不是对立，人情往来的重要性。<br /></li><li>预防措施有个问题，就是你很少能知道自己究竟避开了哪些灾难。<br /></li><li>开发部把所有的工作周期都花在了功能开发，而没有用在稳定性、安全性、可扩展性、可维护性、可操作性、持续性以及诸如此类的美好性能上。<br /></li><li>故障报告：故障时间线（含场景）<br /></li><li>IT很重要。公司的每一项重大活动都有IT的参与，而且IT对日常运作的方方面面起到关键作用。... IT不只是一个部门，IT是我们在整个公司层面需要发展的一种能力。<br /></li><li>当每个人都习惯于相信，说“不”是不能够接受的回答，我们就都成了百依百顺的接单员，盲目地按照既定路线办事。...我们付给你们工资，是为了让你们思考，而不是执行。<br /></li><li>管理就是为了达到业务目标，有效组织以及激发员工，体系的完成工作。<br /></li><li>可以得到他人诚实的反馈也是一种恩赐。<br /></li><li><strong>CFO的工作职责：管理公司财务风险、领导财务计划和运营流程。</strong><br /></li><li>和高层打交道时，告诉他们坏消息要找适当的时机、适当的场合。<br /></li><li>5W1H：<br />高层思维<br />Why：目的， 达成公司的目标<br />What：做什么，把业务的重要评估指标作为IT任务的前提条件<br />具体行动<br />How： 包括 Who、When、Where等。<br /></li><li>他终于把头从头从屁股上抬起来，开始正眼看这个世界了。<br /></li><li><strong>再没什么比吐槽IT更能让人团结一致了。</strong><br /></li><li><strong>访谈：业务的考核指标是什么，完成考核指标有哪些困难，IT在其中有没有什么障碍，IT可以提供什么样的帮助。</strong><br /></li><li>技术改变也要工作方式的改变，不然不会真正减少工作上的桎梏。<br /></li><li><strong>在IT部门创建一个向前的工作系统，记住，目标是单一工作流！一旦看到工作向后移动，我就会想到浪费，想到不合格品。</strong><br /></li><li>也许IT工作比生产制造复杂得多。IT工作不仅是无形的，因此更难追踪，而且可能出错的地方也要多得多。<br /></li><li>把环境标准化，支持同一时刻构建开发、QA、生产环境，并且让他们保持同步。 在部署的时候，开发除了提供代码（最好是打包好的代码），还需要提供部署这些代码需要的确切环境，并且在版本控制中一并对部署环境的情况进行检查。<br /></li><li>t+1的大规模运算可以考虑放云端。<br /></li><li>库存型生产 订单型生产<br /></li><li>上<strong>游的开发团队不再给下游的工作（比如QA、IT运维以及信息安全部）造成麻烦，开发部将20%的时间用于帮助确保工作顺利地通过整个价值流，加快自动化测试，改进部署基础架构，并确保所有应用程序建立有用的产品遥测。...每个人都像重视功能性需求一样重视非功能性要求。...因为非功能性需求对于实现业务目标同样重要。</strong><br /></li><li>开发运维（DevOps）是在IT价值流中应用精益理论的结果<br /></li><li><strong>我现在的愿望是，在生活的各方面，都不要惧怕冲突，不怕说出真相，不怕说出真实的想法。当然，完全做到这些是痴心妄想，但我认为这仍然是一个有价值的目标。</strong><br /></li><li>Wip WIP（Work In Process，在制品）<br />WIP的意思即为Work In Process ，也就是在制品的意思。在制品指的是正在加工，尚未完成的产品。有广狭二义:广义的包括正在加工的产品和准备进一步加工的半成品;狭义的仅指正在加工的产品。</li></ul><p><img src="/img/凤凰项目2.jpg" /><br /><img src="/img/凤凰项目3.jpg" /><br /><img src="/img/凤凰项目4.jpg" /><br /><img src="/img/凤凰项目5.jpg" /><br /><img src="/img/凤凰项目6.jpg" /><br /><img src="/img/凤凰项目7.jpg" /></p><p><img src="/img/凤凰项目1.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DevOps </tag>
            
            <tag> 精益生产 </tag>
            
            <tag> IT运维 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《让云落地--云计算模式》读书心得</title>
      <link href="/2017/08/23/%E3%80%8A%E8%AE%A9%E4%BA%91%E8%90%BD%E5%9C%B0--%E4%BA%91%E8%AE%A1%E7%AE%97%E6%9C%8D%E5%8A%A1%E6%A8%A1%E5%BC%8F%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2017/08/23/%E3%80%8A%E8%AE%A9%E4%BA%91%E8%90%BD%E5%9C%B0--%E4%BA%91%E8%AE%A1%E7%AE%97%E6%9C%8D%E5%8A%A1%E6%A8%A1%E5%BC%8F%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>这本书主要从宏观上讲述了云计算落地需要考虑及决策的问题，没有涉及到具体的技术细节。<br />云计算，不管你认不认同，都已经开始走入了我们的生活。云计算就像自来水公司取代家家户户的水井，就像电力公司取代家家户户的发电机一样，必然会给IT的基础架构、IT架构带来革命性的变化。</p><p><strong>云计算有公有云、私有云、混合云三种部署模式，有Saas、Paas、Iaas三种服务模式。</strong>在做决策时候，需要从业务需求的驱动出发来考虑需要采取的策略。<br /><span id="more"></span><br />如果是新创公司，云计算肯定是不二的选择。云计算弹性（灵活）、敏捷（速度），能让公司专注于核心业务，从复杂的IT架构中解放出来，让专业的公司做专业的事情；云计算共享、租用的思想能降低公司的成本；云计算按需付费的模式，使资本费用化，让公司能把有限的资金投放到核心业务上面，提高公司的竞争力。</p><p>如果是有历史遗留架构的公司，在做云计算的决策时，需要考虑的事情会更多。历史遗留的系统，很多并不是RESTful的无状态架构，迁移到云计算也不能做到自动的弹性扩展和迁移；在这些公司，可以考虑先把一些非核心业务的应用（例如人力、CRM、行政），一些非核心的IT管理（例如异地备份、性能测试、测试机、监控）等功能先放在云端。通过这种模式，培养公司的云计算能力和意识。针对需要对现有架构进行改造以便能够与新的云服务进行整合的系统，需要分析，结合业务的需求及公司的现状，再决定采取何种处理方式。<strong>迁移到云端涉及到组织的变革，包括技术、流程、人的变革</strong>，为了顺利推进，可以采取将云计算方案分解成多个更小一些的可交付项，这样可以尽快交付商业价值。</p><p>另外，在考虑云计算时，也需要对云服务提供商的安全性、合规性、服务水平进行考虑。<br />现在的云计算服务商都会通过相关的认证，<strong>对于大多数云服务商而言，安全是他们的核心竞争力。所以可以说，公有云比绝大多数的本地数据中心更安全。</strong>同时，大多数云服务提供商能够提供的服务等级，与多数单独用户自己做到的相比，即便没有更好，但也绝对不会更差。所以，我们需要用更积极的态度拥抱云计算，把IT的复杂性、专业性交给专业的云计算公司来处理。</p><p>在考虑云计算时，数据保密也是一个重要的选项，不同行业、不同业务，对数据的保密性要求是不一样的。例如，对于PII（个人身份认证信息）数据，许多公司拒绝将这些敏感和私有数据存放在公有、多租户环境中。所以很多公司会选择私有云和混合云，把敏感的数据放在私有云里面。针对这种情况，混合云的最佳实践方式是在利用快速伸缩性和资源池这些云计算的优势方面尽可能多地使用公有云，而在数据所有权和隐私这些公有云中风险较高的领域使用私有云，从而既能充分利用云计算的优势，又达到保密的目的。</p><p>现在，在云计算落地的过程中，有个误区，很多公司只是简单的把现有应用迁移到云端。这些公司将应用迁移至云端并非为了弹性，而是不想再管理和维护基础设施。其实他们最需要的是托管解决方案（托管只是在托管服务提供商处租用或购买基础设施和地面空间）。</p><p>云计算给IT运维带来了革命性的变化，<strong>作为IT运维最重要的一项能力规划，需要从以前预测将来需要的学问转变成实时自动扩展的学问。</strong>同时，迁移到云计算，对IT的流程管理、监控、安全等都提出不一样的挑战，IT运维同事，需要做好相关的知识储备。不过，百变不离其宗，一个系统，最根本的还是架构设计。一个良好的、符合云计算弹性要求的业务架构、IT架构以及数据架构，才是决定一个系统是否能真正发挥作用的根本。所以，在做任何决策之前，需要从架构设计的角度出发来思考问题，从满足业务目标的角度来决策问题。</p><p>变化才是永恒不变的真理，让我们以开放、主动的心态来拥抱云计算的到来吧。</p><h1 id="文章摘要">文章摘要</h1><ul><li>如何以适当的解决方案来解决业务问题；<br /></li><li><strong>企业能否在云中搭建出真正解决业务问题的可行的解决方案，取决于是否进行了合理的架构设计；</strong><br /></li><li>像架构师一样思考；<br /></li><li>架构师在IT部门中的位置比较特别，因为他们对业务和技术都有涉足。他们必须走出IT人员对技术的痴迷，把眼光放远，从细节上了解什么可行、什么不可行；但同时也必须立足市场，熟知业务，知道<strong>企业的战略、目标及问题</strong>。<br /></li><li><strong>我们是探索者，我们只能按照探索者的方式来赚取经验值：试错！</strong><br /></li><li>成功使用云计算最关键的技术决策之一，就是基于业务、技术和组织需求等各方面情况选择正确的云服务模式。<br /></li><li><strong>使用熟悉的业务场景讨论技术会使人对概念产生画面感。</strong><br /></li><li>我记得，当人们把互联网吹捧为巨大的技术革新时，有些权威人士举起了安全的大旗来表达他们的反对意见。现在当我们今天面对云计算的普及时，同样的事情又在发生。<br /></li><li><strong>只要专注于应用架构和用户体验这两项他们所擅长的事情上。</strong><br /></li><li>当人们知道要进行改变时，通常他们的第一反应就是拒绝改变。<br /></li><li><strong>每种云服务模式都通过某种程度上的资源抽象，来降低消费者构建和部署系统的复杂性。</strong><br /></li><li>对账（Balancing the bill） 即确保一天结束之后钱箱里的现金与收据一致的一种说法。<br /></li><li><strong>OpenStack是一个开源项目，提供IaaS功能。</strong><br /></li><li>你可以找出可以重复的步骤来将其自动化。一旦将创建环境和部署软件的过程自动化，他们就能够在自动进行的步骤中落实适当的安全控制和流程。（可重复-&gt;自动化）<br /></li><li><strong>系统恢复考虑的要素：RTO（恢复时间目标）、RPO（恢复点目标）、恢复价值</strong><br /></li><li>在云中处理敏感数据时，应始终对数据进行加密。<br /></li><li><strong>将所有的密钥存储在应用外部，提供自应用内请求密钥的唯一的安全方法。</strong><br /></li><li>建议是基于请求的资源内容而非URL进行认证。URL更容易被发现，并且比内容资源要脆弱得多。<br /></li><li><strong>自动化基础设施：通过各种API对基础设施进行抽象，从而使我们能够像对待代码一样来对待基础设施。鉴于配备和撤销基础设施可以通过脚本进行，不实现环境创建的自动化真的就没什么理由了。</strong><br /></li><li>分离日志信息与产生这些日志的物理服务器成为一种需要，这样这些信息在云资源消失时才不会丢失。...最好的方法是构建一个公用服务，以常见的日志消息格式书写应用消息。（集中化的日志解决方案，这样可以给开发只开放日志服务器权限）<br /></li><li><strong>管理没有监控策略的云方案就像关了车灯夜晚行驶在高速公路上。</strong><br /></li><li>如果现有的激励措施不能鼓励人们做出改变，那么认为一切都将魔法般地自动改变都无异于痴人说梦。<br /></li><li>正如我们多年来应对的其他转型一样，一切归根到底都是人、流程和技术的问题。<br /></li><li>企业应该通过使用SaaS来将所有非核心竞争力的应用、功能和服务外包出去。<br /></li><li><strong>在云中编写松耦合软件的关键办法之一就是将应用的状态存放在客户端而非服务器端，这样就打破了硬件和软件之间的依赖性。（RESTful）</strong><br /></li><li>在这个“高可用但最终一致”的世界里，成功的秘密就是构建无状态的、松耦合的、RESTful服务。架构师必须接受这种构建软件的方法，来充分利用云所提供的弹性。<br /></li><li>DevOps心态：开发人员不再只对代码负责，测试人员不再只对测试负责，运维人员也不再只对系统的运维负责。在DevOps文化里，每个人都对整个系统负责并承担后果。...每个人都对交付和质量负责。<br /></li><li>企业应该通过使用SaaS来将所有非核心竞争力的应用、功能和服务外包出去。<br /></li><li>无状态（RESTful）架构比有状态架构更适合云。<br /></li><li><strong>5W1H 谁（who）、什么时间（when）、什么地点（where）、为什么（why）、做什么事（what），如何做（how）。</strong><br /></li><li><strong>集中化、标准化、自动化</strong><br /></li><li><strong>PDP：保护、检测、预防</strong><br />健康系统的基准指标。<br /></li><li>一般情况下，大企业会选择混合云服务，把数据保存在私有云里，然后把不重要的组件迁移至公有云中。<br /></li><li>大多数服务中断本可以轻易避免----只要客户对此有所预期并进行了<strong>故障设计</strong>。<br /></li><li>需求驱动决策。...在进行架构设计时，有时架构最优的方案未必业务最优。关键是，在制定架构决策时满足业务目标必须始终放在第一位。<br /></li><li>在一个公司内，组织变革管理对于任何变革方案的成功都至关重要。<br /></li><li>数据所有权是非常重要的特性。<br /></li><li>记下经常会被问到的问题，随时查阅，能够解答典型客户对于基于云方案经常会有的疑惑或顾虑。<br /></li><li>如果在开发初期就考虑审计需求，在设计时就可以把流程和控制设置在核心应用部分，从而可以较轻地降低风险，提高可审计性和减低审计成本。（合规路线图）<br /></li><li><strong>设计功能标记：当今部署方法论的另一个新的趋势是使用功能标记。功能标记允许对功能特性进行开启或关闭的配置，或者只对特定群组的用户可用。</strong><br /></li><li>横向扩展：Scaling out<br />沉没成本：sunk cost<br />云服务提供商（CSP）<br />未雨绸缪。<br />竖井 silo 或者 孤岛。</li></ul><p><img src="/img/《让云落地--云计算模式》读书心得.jpg" /></p><h1 id="文章截图">文章截图</h1><p><img src="/img/云计算落地1.jpg" /><br /><img src="/img/云计算落地2.jpg" /><br /><img src="/img/云计算落地3.jpg" /><br /><img src="/img/云计算落地4.jpg" /><br /><img src="/img/云计算落地5.jpg" /><br /><img src="/img/云计算落地6.jpg" /><br /><img src="/img/云计算落地7.jpg" /><br /><img src="/img/云计算落地8.jpg" /><br /><img src="/img/云计算落地9.jpg" /><br /><img src="/img/云计算落地10.jpg" /><br /><img src="/img/云计算落地11.jpg" /><br /><img src="/img/云计算落地12.jpg" /><br /><img src="/img/云计算落地13.jpg" /><br /><img src="/img/云计算落地14.jpg" /><br /><img src="/img/云计算落地15.jpg" /><br /><img src="/img/云计算落地16.jpg" /><br /><img src="/img/云计算落地17.jpg" /><br /><img src="/img/云计算落地18.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 云计算 </tag>
            
            <tag> Iaas </tag>
            
            <tag> Paas </tag>
            
            <tag> Saas </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《我在通用汽车的岁月》 读书心得</title>
      <link href="/2017/08/10/%E3%80%8A%E6%88%91%E5%9C%A8%E9%80%9A%E7%94%A8%E6%B1%BD%E8%BD%A6%E7%9A%84%E5%B2%81%E6%9C%88%E3%80%8B%20%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2017/08/10/%E3%80%8A%E6%88%91%E5%9C%A8%E9%80%9A%E7%94%A8%E6%B1%BD%E8%BD%A6%E7%9A%84%E5%B2%81%E6%9C%88%E3%80%8B%20%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>德鲁克在调研完通用公司，并写了著名的《公司的概念》（具体见<a href="/2017/07/18/%E3%80%8A%E5%85%AC%E5%8F%B8%E7%9A%84%E6%A6%82%E5%BF%B5%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/" title="《公司的概念》读书心得">《公司的概念》读书心得</a>）之后，其中提到了通用公司的一些问题。斯隆对德鲁克书中提到的一些观点不是很认同，后来也推出了这本《我在通用汽车的岁月》 。<br />斯隆把通用公司从创立初期到他离任时，其中在管理方面的各种变革、通用公司的发展历程，通过他的视角，娓娓道来。<br />本书可以作为一本企业高层人员的管理指导书，书中把斯隆在<strong>组织、产品政策、财务控制</strong>等方面的开创性的变更，通过斯隆的亲身经历，展现在读者面前。<br />在组织模式上，斯隆推崇<strong>“在政策上统一，在管理上分权”</strong>的管理模式，简单说，就是在协调控制下的分权运营模式。在协调方面，作为委员会管理的推崇者，斯隆通过委员会处理事业部间的协调管理，但是委员会只是处理一些公司层面的、基本的、共性的问题。<br /><span id="more"></span><br />在产品政策方面，斯隆提出了结合公司整体目标综合考虑而不是孤立地考虑产品政策，终结了通用汽车各事业部各自为政的产品定位的历史。从整个公司的层面规划产品线、分析不同的细分市场，使通用公司在不同的细分市场上有明确的车型和主导的公司，并且能够根据细分市场的竞争对手进行不同的竞争策略。<br />事实表明，公司不仅在具体产品的层面上竞争，也同样需要在宏观政策上进行竞争。而宏观方面的竞争往往决定了最后的胜负，福特公司就是因为在宏观政策上的欠缺，最后在和通用的竞争中败下阵来。</p><p>在财务控制与运营方面，首先斯隆明确了对事业部的考核标准，那就是投资回报率，公司必须追求长期平均投资回报率的最大化；同时，为了使股东利益和管理层利益一致，通用公司推动了股权激励计划的实现；这些都是通用公司的开创性举动，对通用公司的成长起到了非常大的推动作用。在运营方面，斯隆还提到了相关的立项原则、销售量预测的原则等，但是，不管运营过程的技术环节是怎么处理，斯隆提出了，<strong>其实管理的核心还是应对变化</strong>。一个公司是生活在一个动态的环境中（人也一样），动态的环境包括社会环境、经济环境、经济周期、竞争对手等。一个公司必须有预测、有计划的安排工作，但是，俗话说，计划没有变化快，关键是在变化的时候，如何缩短反应的时间，需要构建一套机制和管理来达到快速反应的目的。</p><p>在本文的最后，斯隆提出，没有一个公司是一成不变的。改革有可能带来好处，也可能带来坏处。我也希望我没有留下组织可以自行运作的印象。组织并不会做出决策，它的作用是提供一个框架，在这个基础上建立一些规范和标准，再根据这些标准来做出新的决策。每个人都需要做出决策，并且要为自己的决策负责。<br />管理层的任务不是教条地应用公式，而是在基于个案分析的基础上做出正确的决策。</p><h1 id="个人总结出来的公司管理与部队的类比">个人总结出来的公司管理与部队的类比</h1><table><thead><tr class="header"><th style="text-align: center;">公司</th><th style="text-align: center;">部队</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">组织结构</td><td style="text-align: center;">作战阵型</td></tr><tr class="even"><td style="text-align: center;">产品策略</td><td style="text-align: center;">武器</td></tr><tr class="odd"><td style="text-align: center;">激励机制（高管持股计划）</td><td style="text-align: center;">激励机制</td></tr><tr class="even"><td style="text-align: center;">问题管理（委员会）</td><td style="text-align: center;">问题管理</td></tr></tbody></table><h1 id="文章摘要">文章摘要</h1><ul><li>斯隆先生一遍一遍地研读美国宪法，以便为通用汽车以及所有大公司制定出管理组织和管理思想。<br /></li><li><strong>职业经理人的工作不是去讨好人，不是去改变人，而是要激发他们的能力去工作。...业绩导向</strong><br /></li><li>对变革的惰性导致冒险精神的逐渐散失；...从容应对变革的能力则是优质管理的标志。<br /></li><li>经济衰退有淘汰弱者的作用。<br /></li><li>圆满完成某项工作的责权必须明确地集中到某一个人手里。个人注：责任主体要明确，类似矩形的项目模式。<br /></li><li><strong>协调问题实际上就是寻找一种能够将诸多管理职能关联到一起的方法。</strong><br /></li><li><strong>集体可以指定政策，但是推行政策却只能靠个人。...执行委员会将自己限制在政策事务领域，而将行政职权交给了总裁。 个人注：类似立法权和行政权分立一样。</strong><br /></li><li>我认为我们的管理长期以来一直都面临着不敢直面变革，不及时有效的处理已经存在的问题等。...现阶段管理层应该将精力转移到通过改善效率、降低支出以提高盈利的能力上来。 个人注：精细化管理。<br /></li><li><strong>我们必须从原则上思考、处理这些问题，而不是陷入具体的细枝末节。</strong><br /></li><li>历史表明，我们的工作既得到股东的信任，又没有疏忽对员工、客户、经销商、供应商和社会的责任。<br /></li><li>悬挂方式，即连接车轮和底盘的方式。<br /></li><li>通用汽车是一个工程组织，我们的工作是切削金属并通过这个过程来使之增值。<br /></li><li>基础研究：出于自身的兴趣来追求知识。<br /></li><li>一段时间内的最佳政策与实践可能在后来就不再是最好的解决方案了。<br /></li><li>当你的竞争对手开始仿效你的时候，这就是最好的勋章。<br /></li><li><strong>在纸面上写下一条政策是件非常简单的事情，但是在美国这么大的地方、在我们这种从事复杂业务的公司里，采用行政管理的手段推行这种政策必然是一个循序渐进的演进过程----而不是一个革命的过程----耐心是非常必要的，这一点无论怎么强调都不过分。在这些困难之上，还有一件更困难的事情，即改变一个大组织在用某种特定的方式去处理某件特定事情上的观点。我们都知道人类思想的惰性是多么大。</strong><br /></li><li>通常情况下，总是资本流向这些人才，而不是这些人去迁就资本。<br /></li><li>必须用开放的心态去研究、处理每件事，不要带有偏见，唯一的目的就是掌握事实，而不要考虑它们将把我们引向何处。<br /></li><li>我相信，考虑任何问题的唯一出发点应该是从长期的观点看。 个人注：还有整体考虑。<br /></li><li>只有标准化才可以实现因大批量生产而带来的规模经济。<br /></li><li>人类的天性决定了人们更愿意在激励下工作。<br /></li><li><strong>经济学的公理指出，专业化和劳动分工可以促成成本的降低和贸易的产生。</strong><br /></li><li>管理是能带来效益的，一个公司的竞争力体现在良好的管理（当下）和卓有成效的规划能力（未来）。<br /></li><li>对于一个企业的管理者来说，动机和机遇是其走向成功的两个非常重要的因素。前者通过某些方面的激励性报酬得到很好的应用，后者则是通过分权管理体制。<br /></li><li>好的管理在于集中和分权管理的协调，或者说是“基于协调控制的分权管理”。<br /></li><li><strong>立项的四项基本原则：</strong><ul><li>项目是否合乎逻辑？或者是否值得进行风险投资？<br /></li><li>项目在技术方面是否已经有了适当的进展？<br /></li><li>考虑到公司的整体利益，项目是否合适？<br /></li><li>和其它待考虑的项目相比，该项目是否具有相对价值？</li></ul></li><li>个人注：<br />委员会就是某一领域集中总部、各事业部对应高层组成的一个议事机构，主要讨论基于公司基本公共层面的问题，起到协调各事业部之间的利益问题。<br />委员会管理的模式，类似工作组的方法，不过委员会应该是规划出来的，而不像工作组这样是根据临时的工作组建的。<br />事业部间委员会是公司在处理事业部间协调关系的第一个重大理念...并且只处理基本问题。<br /></li><li>预测：预期销售量。<br /><strong>首先是预测的艺术，其次是证明预测失误之后缩短反应时间的问题。</strong> 快速反应，通过机制和管理来达到。 预测变化和应对变化的决策机制和程序。<br />预测需要考虑宏观经济环境、经济周期的影响，竞品的影响等。<br /></li><li>未雨绸缪<br /></li><li>Motro 既有汽车之意，也有发动机之意。</li></ul><p><img src="/img/我在通用汽车的岁月.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 管理 </tag>
            
            <tag> 德鲁克 </tag>
            
            <tag> 通用 </tag>
            
            <tag> 斯隆 </tag>
            
            <tag> 组织 </tag>
            
            <tag> 产品 </tag>
            
            <tag> 财务 </tag>
            
            <tag> 汽车 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《伟大的博弈》 读书心得</title>
      <link href="/2017/07/27/%E3%80%8A%E4%BC%9F%E5%A4%A7%E7%9A%84%E5%8D%9A%E5%BC%88%E3%80%8B%20%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2017/07/27/%E3%80%8A%E4%BC%9F%E5%A4%A7%E7%9A%84%E5%8D%9A%E5%BC%88%E3%80%8B%20%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>开始以为是关于博弈方面的书，拿到手一看，才知道主要是描写华尔街历史的书，也是一本关于美国金融史和经济史的书。<br />本书讲述了一帮天才在华尔街这个大舞台上挥洒人类贪婪和恐惧的故事，讲述了华尔街在历经了火灾、恐慌和萧条的洗礼之后，最终真正成为美国金融市场代表的过程。<br />华尔街从一个完全自由的市场，一个由贪婪的投机者和腐败的政客操控的市场，到现在受监管的相对成熟的市场，中间经历了多次的股灾和起伏。在漫长的发展过程中，华尔街不断从灾难中吸取教训以防灾难再次发生，不过就像因为“泰坦尼克号”的沉没才建立了北大西洋冰层巡逻制度一样，监管的加强往往只有在一些痛苦的经历，尤其是灾难性的时间之后才会来临。相信华尔街未来还会继续遇到新的挑战，也必须学会应对这些挑战。<br />不过就像作者所说，尽管有数不清的海难，人类依然扬帆出海，同样的道理，尽管有无数次金融危机，人们依然会进入这个市场，辛勤的买低卖高，怀着对美好未来的憧憬，将手里的资金投入到股市，去参与这场伟大的博弈。事实上，在资本市场上博弈和到未知世界去探险都源于我们人类的同一种冲动，因为市场的地平线之外也是一个未知世界----未来。<br /><span id="more"></span></p><h1 id="文章摘要">文章摘要</h1><ul><li><strong>金融在现代经济中的处于核心地位；</strong><br /></li><li>2008年爆发的国际金融危机一个重要而深刻的教训是：如果虚拟经济的发展脱离了服务于实体经济的轨道，它的发展一定会遭受挫折，并有可能带来严重的危机。<br /></li><li><strong>正如马克思理论所揭示的那样，矛盾是事物存在的特征之一。</strong><br /></li><li>历史是最好的教科书。<br /></li><li>用故事来诠释历史。<br /></li><li><strong>人类经济活动的主体从使用铁犁的农民变成了使用电脑的白领；人类对外层空间的观察工具，从伽利略手工制造的两英寸望远镜变成了半径10米的成对凯克天文望远镜；观察视野从尚不能清晰辨认土星的光环，扩展到了距地球120亿光年的太空；人类的信息从以马车的速度传递到以光速传播。</strong><br /></li><li><strong>经典政治学给世界强国所下的定义为：“其利益必须被其他国家所考虑的国家。”</strong><br /></li><li>在现实生活中，政客们和资本家们差不多一样自私自利。<br /></li><li>在自由市场的博弈中，如果其参与者是完全理性的，并拥有完备的信息，那么博弈的结果有可能是所有的参与者都是赢家。然后，在现实生活中，完全理性的人是不存在的，信息完备也只是理想状态----这种状态可以接近，但却永远无法达到。<br /></li><li><strong>运气往往是所有伟大事物必不可少的标志。（无巧不成书）</strong><br /></li><li>荷兰人，尤其是阿姆斯特丹的市民是现代资本主义制度的真正创造者。...世界上的第一个证券交易所----阿姆斯特丹交易所。<br /></li><li><strong>这种总是期望有人会愿意出价更高的想法，长期以来被称为投资的博傻理论。</strong><br /></li><li>在每个人身上都可以依稀看到他的孩提时代（三岁定八十）。<br /></li><li>有限责任制度----现代企业制度的基石。<br /></li><li>他们有过一夜暴富的辉煌，也会经历倾家荡产的劫难，在天堂和地狱之间轮回。<br /></li><li>投机是股票市场不可分割的一部分。<br /></li><li>正如20世纪初伟大的英国金融家欧内斯特·卡塞尔(Ernest Cassel)爵士所说：“当我年轻的时候，人们称我为赌徒；后来我的生意规模越来越大，我被称为一名投机者；而现在我被称为银行家。但其实我一直在做同样的工作。”（成王败寇，以胜败论英雄）<br /></li><li>在金本位制下经济几乎不可能发生通货膨胀。<br /></li><li><strong>在每一个历史变革和经济形势乐观的时候，新股票都是投机者的至爱。</strong><br /></li><li><strong>战争融资：南北战争时期，华尔街帮助北方进行了大规模的战争融资，使它最终战胜了因大量印钞而引发大规模通货膨胀的南方。...购买这些战争债券不仅是一种爱国的表现，也是一笔很好的投资。...结果是北方的通货膨胀率是180%,而南方的是9000%</strong><br /></li><li><strong>摩根品格中两个最重要的特征：1、永远追求达到国际银行业的最高水准；2、如果要想在这个领域获得长期的成功，个人诚信至关重要。</strong><br /></li><li>通货紧缩意味着他们在还款的时候，要用比借款时更为值钱的美元来偿还贷款。<br /></li><li>货币的准确定义就是一种在与任何其它商品的交易中都能被普遍接受的商品。<br /></li><li>不管是由于什么原因引起的恐慌，当它一旦蔓延到整个经济体系当中时，人们便开始将手中的资产转换为流动性更高的资产。他们会将股票和债券卖出以换成现金和黄金。<br /></li><li>一个没有监管的自由市场在本质上是不稳定的，在压力面前它很容易崩溃。<br /></li><li><strong>人类社会有一条铁律：在没有外来压力时，任何组织的发展都会朝着有利于该组织精英的方向演进。</strong><br /></li><li>派一只狐狸去守鸡窝。<br /></li><li>投资中最为重要的原则是“该斩仓时要斩仓，见好就收。”（个人注：其实这个时机的把握才是最难的）<br /></li><li>滞胀：通货膨胀和经济萧条并存。<br /></li><li>最好的价格出自最大的市场。<br /></li><li>对付任何此类危机，你只需要开闸放水，让金钱充斥市场。（个人注：恐慌很多时候都是因为流动性问题引起。）<br /></li><li>全球的主要货币市场已不再由金本位制决定，而是由国际外汇市场决定。<br /></li><li>在欧洲，St.Nicholas 和 Santa Claus（圣诞老人）是一回事。<br /></li><li>经纪人：broker<br /></li><li>蓝筹股：blue-chip stock<br /></li><li>摩天大楼 skyscraper</li></ul><p><img src="/img/伟大的博弈.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 华尔街 </tag>
            
            <tag> 博弈 </tag>
            
            <tag> 股市 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《公司的概念》读书心得</title>
      <link href="/2017/07/18/%E3%80%8A%E5%85%AC%E5%8F%B8%E7%9A%84%E6%A6%82%E5%BF%B5%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2017/07/18/%E3%80%8A%E5%85%AC%E5%8F%B8%E7%9A%84%E6%A6%82%E5%BF%B5%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>作为受邀请到通用公司考察之后的产物，《公司的概念》面世之后就不受通用公司上下的青睐。不过，俗话说，墙里开花墙外香，《公司的概念》中提出的观念在日本受到了推崇，也为日本企业在二战之后崛起助了一臂之力。<br /><span id="more"></span><br />二战后，人类社会进入了大规模化的生产，而通用汽车公司又是其中的代表。在考察完通用公司之后，德鲁克构建了“企业（组织）、管理、工业社会”之间的内在联系。从企业的联邦制（分权管理），到管理中涉及的领导人培养、员工的绩效考核、工人的激励以及创造力等；从企业的经济责任、社会责任，到就业、失业基金、养老基金等；德鲁克提出了一系列让人耳目一新的管理思想。虽然这些思想在我们现在看起来耳目能详，但是，搁在二战之后那段时间，就连伟大如斯隆者，也无法接受德鲁克的这些观念。<br />从政治学的角度出发，作者创新性的提出，一个企业的是需要实现社会的信仰和承诺的，至少要在最低限度上实现，在员工管理、员工关系的处理方面，其实也是在实现社会的信仰和承诺。一个公司是组织人们努力实现共同目标的社会机构，所以它的本质和目标不仅仅在于它的经济业绩，还包括了人与人之间的关系，还包括了社会的信仰和承诺。<br />在作者看来，人是一个企业的核心，包括领导者、普通管理者、工人，一个企业的进步，依赖于全体员工的创造力和创新性。在作者看来，管理学是一门实践的学科，不是物理规律，所以需要在管理中提升管理。一个类似20年的制度规则，即使在通用公司发挥了重大作用，也没有了活力。但是，俗话说，船大难调头，通用公司在求变方面也是小心谨慎、尽量规避风险，从而错过了最好的机遇。<br />德鲁克作为管理学的大师，总是能洞察未来，他的思想能穿透时间。即使我们现在看回《公司的概念》，也不得不惊叹于德鲁克的洞察力和其思想的深度。</p><h1 id="文章摘要">文章摘要</h1><ul><li>政府的作用是在于设置一个引导工商活动的框架而不是直接插手商业活动。<br /></li><li>生存是任何社会的第一法则；<br /></li><li>重要的不是普遍现象，而是代表性事物。<br /></li><li>我们的问题不仅包括大企业是怎样运作的，还包括大型公司在美国的社会中是如何运作的。<br /></li><li><strong>从政治学角度（社会分析和政治分析）对公司进行分析：公司是一个组织人们努力实现共同目标的社会机构。公司的本质和目标不在于它的经济业绩，也不在于它形式上的准则，而是在于人和人之间的关系，包括公司成员之间的关系和公司和公司外部公民之间的关系。</strong><br /></li><li>所有关于领导者的问题中，最艰巨和最紧迫的恐怕是如何把企业中从事日常工作的专业技术人才培养为具备决策能力的训练有素的人才。<br /></li><li>没有什么比仅在某一层面发现完美的解决方法更简单的了；没有什么比建立一个政治上的“各个领域间的和谐”更加艰难的了，其困难就在于其对真正政治才能的无尽的挑战和冒险。（个人注：政治：一种平衡的艺术。）<br /></li><li>生存是公司的第一法则；而且生产具有最大经济回报的产品，实现它自己目标的能力就是评价公司业绩的首要标准。<br /></li><li><strong>如同军队或其他任何社会机构一样，公司中真正重要的不是个体的成员，而是成员之间的管辖和责任关系。借用现代心理学的说法打一个比喻，机构就像一首乐曲，他不是由单个音调组合而成，而是由不同音调按一定顺序排列而成的。</strong><br /></li><li><strong>公司的生存和成功运行取决于他是否能处理好三个互相依存的问题：领导问题、基本政策问题，以及行动和决策的标准问题。</strong><br /></li><li><strong>鼓励乐于负责的精神和行动。</strong><br /></li><li>建立一套评价其业绩的客观的标准...大公司确实存在这这样一种风险，它只能依靠个人的主观印象评价员工的能力和表现。无论管理者的本意有多好，这也必将造成组织内部的裙带关系和道德败坏。<br /></li><li>大型组织生来就有抑制创新、提倡服从的倾向。（个人注：官僚作风）<br /></li><li>通才正是领导的本质所在。通才：即具备判断力、决策力和知识的人才。<br /></li><li><strong>墨守成规会抹杀冒险和开拓精神，而这两种精神正是所有企业赖以发展的基础。</strong><br /></li><li>公司需要一个能消除外部波动评估竞争业绩的判断标准，以便公司有可能客观公正的评价经理人的表现。<br /></li><li>通用公司采取了联邦制的组织形式。通用公司把分权制度视为一个普遍适用的基本方法。（和社会文化有关，美国是联邦国家，本质是一种分权的形式；而中国本质是一种集权的组织，所以公司大部分也是采用集权的模式，这些和社会文化是相关的）<br /></li><li>在核心层管理的所有职责中，最重要的就算事先谋划的职责了，它把通用汽车公司统一起来，向着一个共同的目标奋斗。<br /></li><li>每一个管理人员，下至最低的工头助理，上至董事局主席，都同时拥有决策和管理的职责，但是，核心层规定了分部经理的决策范围和必须遵守的一般规则。<br /></li><li>若要贴切的描述分部经理的地位和运作，也许可以这么说：他们能在核心层管理制定的政策框架内独立自主的领导其下属开展工作。他们可以聘用、解雇和提拔员工；他们能自行决定员工的数量、必须具备的素质以及工资幅度。<br /></li><li>给予高层股权，能使高层考虑公司的整体的、长期的利益。<br /></li><li>通用公司对管理人员下的一个定义就是：如果反对一项政策决定，就应该正式提出意见的人。管理人员提出批评意见后不仅不会招致惩罚，而且还会受到鼓励，因为这表明了他们工作的积极，关心企业。他们的意见也总是能得到认真的对待。<br /></li><li>政治理论中有一条基本公理：只有在一个规则明确、权利和责任严格分工的体制内，人们才能享有通用汽车公司赋予其最高管理者那样的高度自主。...如果通用汽车公司真的依赖于个人的良好愿望而运作，那么他的寿命不可能比人更长。<br /></li><li><strong>如何在业绩评价中消除纯粹的外部波动----份额</strong><br /></li><li>对于内部单位的衡量标准：低于外部供应商的价格向汽车分部提供产品。<br /></li><li>如果一个人的观点或者提议遭到否决，那不应该是因为他级别低，而只能是因为他的不切实际。<br /></li><li><strong>无论一种制度有多完美，也无法预见将来可能发生的问题，并事前指定出解决问题的方法。最后，对于一个具体问题，纯理论制度关心的是它的解决方法是否与既定的原则相统一，而不是这种方法是否恰当。....通用汽车公司的力量源泉恰恰在于，他能运用原则和理论指导人们处理“计划制定者”无法预知和事先规划的具体情况。</strong><br /></li><li>分权的理念不该成为一成不变的规则，相反，他应该发挥罗盘的作用，引领人们翻越崇山峻岭。<br /></li><li>当某个部门的管理者离任后，应该从其他部门挑选继任者；在提升员工时，应该以他们需要的经验，而不是已有的经验为依据。（个人注：这一点在现实工作中太难实现了）<br /></li><li>“公共关系”意味着宣传----就其本质而言是广告的一种延伸，从推销商品延伸到推销商品生产者。<br /></li><li><strong>短期冲突依然可能发生，但是一旦这种威胁成为现实，双方将基于共同的长期利益解决问题。</strong> （个人注：求同存异）<br /></li><li>在政治体制中，个人利益不会长期服从于利他主义，反之亦然；最终的结果只能是两者合二为一，至少也要并行不悖。<br /></li><li>大型分部的个别部门或单位与市场联系甚少，他们的业绩也就很难与市场业绩挂钩，所以只能采取成本会计体系作为客观评价标准。通用汽车公司认为仅仅这一个标准还不够，必须以市场标准为补充，因此公司提倡通过分权管理促使个别生产单位直接与市场发生联系。（成本体系和市场双重调剂）<br /></li><li>对接班人的要求：积极、能干、勇于承担责任。<br /></li><li>人们对国家的忠诚常常带有非理性的、感性的或自然主义的因素。<br /></li><li>要增强公司的实力，提高公司的效率，就必须保证实现<strong>社会的基本信仰和承诺</strong>，否则，美国的工业社会将无法有效运作。<br /></li><li>套用艾德蒙·伯克的观点，要推翻一个社会，仅仅证明他不完善是不够的，还必须证明新的社会或机构可以做的更好。<br /></li><li>近代历史是一个由身份地位向契约合同转变的过程；它巧妙的概括了19世纪的信念，即社会地位和社会职责只能是经济地位上升的产物。<br /></li><li>通常只有那些终日居住在都市的人们才会把农村生活奉为一种理想状态，用来对比“单调”的现在工业社会。<br /></li><li><strong>任何一个正常人都需要大量有规律的生活来维持健康的心态。</strong><br /></li><li>如果一个人只求谋生，只是为工作而工作，不理解工作的意义，那么他就不是也称不上一个真正的公民。<br /></li><li>我们希望工会最终将由反社会的机构发展成服务社会的机构，肩负起引导工人成为真正的公民，融入工业社会的艰难重任。<br /></li><li><strong>单位工资的生产量。</strong><br /></li><li>如何在工人和消费者之间分配由效率提高带来的好处，也就是如何在增加工资和降低价格之间分配由效率带来的好处。<br /></li><li><strong>生产的实现必须结合三种要素：劳动力、原材料和资本设备。但是，最简单的工业操作也还要求第四种要素：有管理的组织。</strong><br /></li><li>人们虽然能理解公司的生存需求与其社会意义的一致性，但是直到大萧条期间，他们才真切的感受到了这一点。<br /></li><li>在现代工业条件下，有限的不是供给而是需求；在大规模生产的条件下，供给就其定义而言，实际上是没有极限的。（个人注：工业社会的生产过剩。）<br /></li><li><strong>由分部管理层负责解决目前的问题，由核心管理层着眼于长远。</strong><br /></li><li><strong>普通存在于小企业管理层的恐惧：培养一个能干的下属意味着促使自己失业。（中国古话：教会徒弟，饿死师傅。）</strong><br /></li><li>人类天性中存在一种追求权力的欲望。...社会是以人类追求权力和社会认同的天性为基础而建立起来的。<br /></li><li>市场即以个人经济决策为经济主宰的做法。...自由是一种信仰条文，并不是物理定律。<br /></li><li><strong>充分就业成了经济体系的试金石和我们经济政策的焦点问题。</strong><br /></li><li>风险是不可能完全避开的，除非我们连机遇也一同放弃。完全的稳定就意味着完全的僵化与停滞。<br /></li><li>只要生产资料的生产得以维持，萧条就不会发生。（个人注：例如金融危机时候，中国国家投资到铁公基上）<br /></li><li>经济生活的时间单位不是自然年，而是7-15年之久的商业周期。<br /></li><li>我的工作以及我为什么热爱它。<br /></li><li><strong>工作中最让我感到高兴的就是上司希望我能够告诉他如何把事情做得更好。</strong><br /></li><li>没有责任的权利就是专制，而没有权利的责任就是无能了。<br /></li><li>个人注：公司的主要领导人的工作，其实就是在处理关系。<br /></li><li><strong>所有的批评者不过是些大声叫嚷却并不咬人的狗。</strong></li></ul><p><img src="/img/公司的概念.png" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 管理 </tag>
            
            <tag> 德鲁克 </tag>
            
            <tag> 通用 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《智能时代--大数据与智能革命重新定义未来》读书心得</title>
      <link href="/2017/04/17/%E3%80%8A%E6%99%BA%E8%83%BD%E6%97%B6%E4%BB%A3--%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B8%8E%E6%99%BA%E8%83%BD%E9%9D%A9%E5%91%BD%E9%87%8D%E6%96%B0%E5%AE%9A%E4%B9%89%E6%9C%AA%E6%9D%A5%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2017/04/17/%E3%80%8A%E6%99%BA%E8%83%BD%E6%97%B6%E4%BB%A3--%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B8%8E%E6%99%BA%E8%83%BD%E9%9D%A9%E5%91%BD%E9%87%8D%E6%96%B0%E5%AE%9A%E4%B9%89%E6%9C%AA%E6%9D%A5%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>几千年来，我们都是以一种确定的方式来看待世界，我们努力找出上帝在表象之后设定的因果关系。令我们倍加鼓舞的是，我们还取得了不少的突破。从伽利略到牛顿到爱因斯坦，一批批的科学家用一些简单的公式、定理、数学模型，定义了这个世界的行为，这些让我们人类的自信得到了极大的提高。<span id="more"></span><br />但是，人类社会在发展过程中，在我们的知识领域越来越大时，我们发现自己的未知领域反而更大了。这个世界在我们所理解的确定性之外，有着更多的不确定性。这个世界展现出来的一种混沌的表象，我们无法用确定的模型去模拟，即使想通过模型模拟，其中的参数数量也已经远远超出了人类所能控制的范围。例如天气预报，我们做到的只能是通过简化参数来预测（所以并不能做到100%准确）；甚至很多现象我们还找不出相关的模型和公式去模拟它，例如地震等。进一步说，量子力学里面的测不准原理直接定义了这个世界就是不可以直接测量的，更谈不上用模型去模拟了。<br />人类科学经过工业革命、信息革命的快速发展，到了现代，想像以前那样找出一个优美的数学模型去模拟现实的世界，已经越来越困难了。通俗点说，就是肉都给吃光了，现在每个研究领域都是在啃硬骨头。所以，想获得更大的科学突破，<strong>科学家们开始改变思维：能找到现象背后的因果模型更好，因为这毕竟是一劳永逸的事情，很好的体现了上帝的意志。但是，在这个每个突破的都需要漫长的时间里，我们可以通过大数据思维，在基于不确定的前提下，借助数据量的突破性增长、借助计算机能力的突破性发展，以及人类在人工智能方面的飞跃（Alphago战胜了李世石），通过数据的相关性来掌握事物的规律，然后基于机器学习，不断优化模型。这就是大数据思维的核心。</strong><br />就像以前托密勒利用很多圆嵌套在一起，这个在地心说基础上的简单模型很好地描述了行星的运行规律，其准确程度比哥白尼基于日心说的模型还准确。同样，人们发现，在机器学习方面，多个简单模型的组合，然后通过大数据的机器训练，得出的参数结果，比复杂模型、小数据训练的效果更好。所以，现在数据量的增长成了解决问题的主要驱动力，在模型效果改进方面，模型的优化只占了20%，其它都是数据的增长带来的结果，例如机器的语音识别、机器翻译、图像识别等。<br />大数据给人们思维带来全新的改变，但在大数据阶段，相关技术的发展和支持也是关键。大数据体现出大数据量、多维度、完备性等特征，对数据的收集、存储、传输、处理带来很大的挑战。不过随着计算机技术的发展，这些问题已经慢慢得到了解决。<br />水到渠成，现在的机器智能在大数据的推动下，已经展现出了跳跃性的发展，使得人类开始正视人工智能对人类带来的好处和挑战。<br />人工智能能让我们这个社会更智能，例如智能交通系统、实时路况；能让我们这个社会更精细化，例如区块链（Block Chain）通过使用RFID技术跟踪产品的整个流通和交易环节；能让我们这个社会更个性化，例如新闻定制、疾病的个性化诊断等。大数据已经在农业灌溉（滴灌技术）、体育（数据分析、传感器反馈动作）、汽车制造、医药研究等等都发挥了重大的作用。<br />但是，人工智能（大数据）就像之前的工业革命一样，也会冲击现在人类的生活。包括从数据的安全（数据泄露的风险）、隐私的安全，还有就是冲击现在的职业体系。很多人在预测，人工智能之后，很多职业会在将来消失，甚至包括一些我们现在看来需要很高智能和经验的职业。例如律师，人工智能能更快速识别历史的档案文书、找出其中的有效信息；记者编辑，机器人能从各自媒体中，寻找热点问题，并写出不逊于人类的稿子；甚至将来的疾病也能通过人工智能进行诊断。<br />不管你愿不愿意，人工智能时代总要到来，那时将是一个最好的时代，也是一个最坏的时代（狄更斯《双城记》）。在这个时代到来的之前，我们需要做好什么准备，让这个时代成为我们最好的时代呢？<br /><img src="/img/数学建模.jpg" /><br /><img src="/img/数据使用流程.jpg" /><br /><img src="/img/机器学习.jpg" /><br /><img src="/img/机器问答.jpg" /><br /><img src="/img/不确定性思维.jpg" /><br /><img src="/img/信息论.jpg" /><br /><img src="/img/大数据处理.jpg" /><br /><img src="/img/服务.jpg" /></p><h1 id="文章摘要">文章摘要</h1><ul><li>全球数据量按每年平均40%的速度增长。<br /></li><li>大数据与机器智能相伴而生，促进物联网从感知到认知并智能决策的升华。<br /></li><li><strong>用不确定的眼光看待世界，再用信息来消除这种不确定性，是大数据解决智能问题的本质。</strong><br /></li><li>几千年来，我们人类的只是都是建立在归纳法之上，归纳法隐含的假设就是『未来将继续和过去一样』，换句话说应该叫连续性假设。<br /></li><li><strong>每一次大的技术革命都会带来阵痛，但同时诞生的，还有更多新的机会。</strong><br /></li><li>数据是文明的基石，人类对它的认识也反映了文明的程度。<br /></li><li>数据中隐藏的信息和知识是客观存在的。...数据中挖掘出信息，对其进行处理和抽象后就是知识。<br /></li><li>很多时候，我们无法直接获得信息，但是我们可以将相关联的信息量化，然后通过数学模型，间接地得到所要的信息。而各种数学模型的基础都离不开概率论和统计学。<br /></li><li>回到数学模型上，其实只要数据量足够，就可以用若干个简单的模型取代一个复杂的模型。这种方法成为数据驱动方法。它是大数据的基础，也是智能革命的核心，更重要的是，它带来一种新的思维方式。<br /></li><li>大量数据的使用，最大的意义在于它能让计算机完成一些过去只能有人类才能做到的事情，这最终将带来一场智能革命。<br /></li><li><strong>今天几乎所有的科学家都不坚持『机器要像人一样思考才能获得智能』，但是很多门外汉在谈到人工智能时依然想象着『机器在像我们那样思考。』...机器智能最重要的是能够解决人脑所能解决的问题，而不是在于是否需要采用和人一样的方法。</strong><br /></li><li><strong>笛卡尔的贡献在于提出了科学的方法论，即大胆假设，小心求证。</strong>这个方法论在我们今天的工作中还在使用。<br /></li><li>牛顿找到了开启工业革命大门的钥匙，而瓦特拿着这个钥匙开启了工业革命的大门。<br /></li><li>相比工业革命，任何王侯将相所谓的丰功伟绩都显得微不足道。<br /></li><li><strong>信息论的作用远不止在科学和工程上——它也是一种全新的方法论。与机械思维是建立在一种确定性的基础上所截然不同的是，信息论完全是建立在不确定性基础上，而要消除这种不确定性，就要引入信息。至于要引入多少信息，则要看系统中不确定性有多大。这种思路成为信息时代做事情的根本方法。</strong><br /></li><li>熵这个词，成了信息论和不确定性的代名词。<br /></li><li>而数据之间的相关性在某种程度上可以取代原来的因果关系，帮助我们得到我们想知道的答案，这便是大数据思维的核心。大数据思维和原有机械思维并非完全对立，它更多的是对后者的补充。<br /></li><li>亚马逊：由商品直接推荐商品（Item to Item）<br /></li><li>对Google自动驾驶汽车的各种报道通常会忽视一个事实，那就是它只能去Google『扫过街』的地方。<br /></li><li>新技术 + 原有产业 = 新产业<br /></li><li>中学为体，西学为用。<br /></li><li>安迪-比尔定律：比尔要拿走安迪所给的。（What Andy gives，Bill takes away） 安迪：intel的CEO，比尔：比尔.盖茨<br /></li><li>主动的一方不是各种看得见摸得着的工业品生产商，而是提供软件和服务的一方。<br /></li><li>通常人们在方便性和安全性方面优先考虑方便性，这是人的天性使然。<br /></li><li><strong>计算机系统的设计和高楼设计很大的不同是，前者实现并不考虑安全的隐患，而后者在每一个环节都要考虑安全的问题，这就是我们面临的现实。</strong><br /></li><li>工业时代的一个特征，就是一切标准化。<br /></li><li>隐私就像自由，只有当人们失去它的时候，才知道它的可贵。<br /></li><li>人类总体来讲是过分自信的，趋利而忽视危害，这一点研究幸福学和心理学的学者早就有了定论。<br /></li><li><strong>智能革命所要替代的是人类最值得自豪的部分----大脑。</strong><br /></li><li>其实社会公平只能反映在机会平等上，而不是结果平等。<br /></li><li>在每一个重大的技术革命开始的时候，真正勇敢地投身到技术革命大潮中的人毕竟是少数，受益者更少，大部分人则会犹豫和观望。<br /></li><li>使用大数据就像在一堆沙子里淘金，要从里面挖掘出有价值的东西。</li></ul><p><img src="/img/智能时代.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
            <tag> 大数据 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《富兰克林传》读书心得</title>
      <link href="/2017/04/10/%E3%80%8A%E5%AF%8C%E5%85%B0%E5%85%8B%E6%9E%97%E4%BC%A0%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2017/04/10/%E3%80%8A%E5%AF%8C%E5%85%B0%E5%85%8B%E6%9E%97%E4%BC%A0%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>富兰克林本人就是美国梦，或者说美国价值观的代表。<br />他从一个贫困的学徒成长成为一个成功的商人、哲学家、政治家、外交家，成为了国王的座上宾。<br />富兰克林一生，在科学和政治方面的建树颇多。科学上最著名就是那个闪电实验，并且发明了避雷针，在电学领域可谓是开山鼻祖。政治上，他在美国独立过程中发挥了重大的作用，包括作为美国独立宣言的起草人之一（执笔人主要是杰斐逊）；在他作为驻法代表的时候，运筹帷幄，分别签订了美英、美法和平条约；独立战争之后，在决定美国未来的制宪会议上，利用他的名望和长者身份，促进了协议的签订，为美国的统一发展打下了坚实的基础。<span id="more"></span><br />富兰克林的一生，他的哲学观、宗教观、政治观都是以实用主义出发。<br />他倡导一种勤奋、节俭的生活观，倡导一种服务他人、善待他人就是信仰上帝的德行，这后来发展成为了中产阶级的价值观。富兰克林给自己制定了一套道德体系，然后践行之，虽然不能说十全十美，但是富兰克林的一生确实是给自己道德要求树立了一个很高的标准。<br />在宗教方面，富兰克林是宗教包容的先驱，他认为关注教义区别会引发分歧，试图确定上帝的意志又超出了人类的智力范畴，这样的行为对社会无益。<strong>宗教的目的应该在于推动人和社会的进步</strong>，任何涉及这一点的教义对他来说都是好的。他认为宗教实践很有意义，因为他们鼓励善举，有助于推动社会道德的进步。<br />在外交上、政治上，在确保利益、确保目标的前提下，富兰克林能做出妥协，所以在他的努力下，达成了很多决定美国未来的协议和制度。<br />但是，再伟大如富兰克林者，在他的一生中，也遭遇到很多挫折和困难，特别是在伦敦的时候，那时的他还是一个骄傲、忠诚的英国人，他希望加强大英帝国的力量，而不是寻求北美殖民地的独立。富兰克林尽最大的努力想争取提升美殖民地地位和权利，但是，在英国那些高傲的大臣那里折翼而归。<br />在富兰克林感到深深挫败感的同时，也孕育了他以后反对英国、争取独立的思想的种子，而这也正成就了富兰克林名垂青史的一个转折点。</p><h1 id="文章摘要">文章摘要</h1><ul><li>他是活生生的人，而不是先贤祠中冰冷的大理石雕像。<br /></li><li>对于美国的“浪漫”传说之一就是人们为了自由，尤其是宗教自由才来到北美殖民地。<br /></li><li>每个人，乃至整个人类，都在实践中获取知识和智慧，不断前行，自我完善。<br /></li><li><strong>激发大家做有意义的事情。</strong><br /></li><li><strong>苏格拉底辩论法：苏格拉底通过温和的质疑说服别人的辩论法。通过提出一些温和的问题，逐步引导他人赞同自己的观点。这是一种温和的、间接的，而不是粗鲁的对抗。<br />先对对方某种方面表示赞同，然后委婉地提出自己的异议。...这种柔和、间接的辩论方式，使得富兰克林在某些人眼里就像圣人一眼个，尽管善于说服和控制别人。</strong><br /></li><li>当一名男子开始追求异性的时候，他就会变得比一生中任何时候都要愚蠢可笑。<br /></li><li>没有思想自由就不会有思想，没有言论自由就不会有公众自由。<br /></li><li>国家内部最危险的伪君子是那些拿着福音书又宣扬法律的家伙。一个以福音书和法律为幌子的人很可能会用他的宗教信仰欺骗人民，并以法律的名义糟蹋国家。<br /></li><li>富兰克林就是一个意志坚定、自我实现的人，他一生中都在有计划的为特定目标而努力。<br /></li><li>关于人性的课：如果你能够不让他人心生妒忌，他们就更容易赞赏你的工作。<br /></li><li><strong>两个判断力差不多的人在一起赌的时候，对钱更在乎的人往往会输，他对成功的渴望往往会冲昏了自己的头脑。...如果一个人太在乎输赢，往往会采取守势，因此就失去了主动权。</strong><br /></li><li>知识只能通过耳朵来获得，而不是通过嘴巴，因此在共读社内，富兰克林开始练习沉默和温和的交流。<br /></li><li>讨论的目的不是驳倒对方。<br /></li><li>那些对他人错误保持沉默的人，当他们自己犯错误的时候，将会得到更多的宽容。<br /></li><li>相信当真理和谬误公平较量时，前者总会压倒后者。<br /></li><li>八卦还有助于道德的提升，很多人行事的顾虑是担心公众舆论而非出于对道德准则的遵从。<br /></li><li>其自传的主题之一就是不断犯错误，又不断地弥补。<br /></li><li><strong>婚前要睁大双眼，婚后要半睁半闭。（难得糊涂）</strong><br /></li><li><strong>上帝赐予我们眼泪，却带走生命让它落下。</strong><br /></li><li>大多数自然神论者相信“总体的福佑”，就是上帝通过设定自然规律来体现自己的意志而不是插手人间的每件事。<br /></li><li>富兰克林给自己设置了一系列的美德，然后逼着自己践行，就像一个人非要带着枷锁走路一样。<br /></li><li>他的布道很少讲枯燥的教义，而是主张进行道德的实践，因此令我感到愉悦。<br /></li><li><strong>信仰是产生道德的手段</strong><br /></li><li>那些曾经帮助过你的人愿意再次帮你，他们比那些你帮助过的人更愿意如此。<br /></li><li><strong>咖啡馆阶层 中产阶级</strong><br /></li><li>如果女人可以保持冷静，那么男人们也会很快冷静下来。<br /></li><li>自食其力的人起码是自由的。<br /></li><li>他认为不问青红皂白，随便找一个人要他承担其族人所犯下的罪过是不道德的。“假如一个脸上有雀斑、红头发的人杀了我的妻子或孩子，难道从此我就可以杀死在任何地方遇到的所有长雀斑的红发男人、妇女和儿童作为报复吗？”<br /></li><li>新兴的中产阶级：既敌视无知的暴民，又与顽固的权贵对抗。<br />典型的中产阶级生活，既节俭，又放纵。<br />中产阶级吸纳了精英们的高雅举止和工人阶级的勤劳本质。<br /></li><li>这些都是诬陷，却说得理直气壮。<br /></li><li>远亲好相处。<br /></li><li>现款交易<br /></li><li>英国人攻击奴隶制是对殖民地人民要求自由的莫大讽刺。<br /></li><li><strong>沉默一直是富兰克林最有力的武器，使其看起来睿智、温和、安宁。</strong><br /></li><li>美国在国际事务中的力量将来自一种独一无二的现实主义和理想主义的融合。<br /></li><li>在美国，无所事事是种罪孽，但在法国，忙忙碌碌则显得粗俗。<br /></li><li><strong>要抑制自己想要战胜对手的欲望，即使自己输了也要保持微笑。即使你的对手想悔棋，你也要保持冷静，欣然接受-- 也许你可能会因此输掉这盘棋，但你将赢得对手的尊重。</strong><br /></li><li>战争无益处，和平总无害。（宁为太平狗，不做乱世人）<br /></li><li>如果这位先生和凡尔赛花园中的神像都被提名担任大使，我肯定会毫不犹豫的投雕像一票，起码它不会对国家造成伤害。<br /></li><li>只要积极、热情，别人自然而然地会对你产生好感。<br /></li><li><strong>富兰克林清楚地意识到与会代表之所以能够成功，并不是他们的自信，而是因为他们愿意承认自己可能会失败。</strong><br /></li><li>支持妥协并不是英雄主义、道德或永久确定性的必要元素，但这是民主进程的本质。<br /></li><li>也许你会发现即使没有宗教的帮助，也可以很容易地过上一种道德高尚的生活，但是想一想人类中有很大一部分人是虚弱无知的男女，还有很大一部分是未经世事、思虑不周的青少年，这些人需要宗教的帮助来使自己远离邪恶。<br /></li><li>我们发现其所有的观点均来自心灵的伟大。<br /></li><li>夏天的微风让人感到凉爽本不在于风本身，而是因为其加快了人汗水蒸发的过程。<br /></li><li>爱默生:每一种制度都可以被看作是一些伟人影子的延伸。<br /></li><li>善于讲故事时富兰克林的本质特征。借虚构的人物表达自己的观点。<br /></li><li><strong>团体出发，不是个人功劳。<br />他发现人们不愿意去支持“一项公益事业的倡导者，因为他们认为他可能是在沽名钓誉”。所以富兰克林尽量“隐身幕后”，将这一提议说成是大家共同努力的结果。这个方法的确有效，“所以我以后屡试不爽”。富兰克林发现，只要不把功劳揽到自己身上，人们是愿意伸出援手的。</strong><br /></li><li>响尾蛇，别践踏我。<br /></li><li>不议论别人的短处<br /></li><li>自嘲会让自己变得更有魅力。</li></ul><p><img src="/img/富兰克林传.png" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 读书心得 </tag>
            
            <tag> 富兰克林 </tag>
            
            <tag> 传记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>读书记录</title>
      <link href="/2017/04/07/%E8%AF%BB%E4%B9%A6%E8%AE%B0%E5%BD%95/"/>
      <url>/2017/04/07/%E8%AF%BB%E4%B9%A6%E8%AE%B0%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<p>下文是读书过程中，碰到的中文生词、英文语句、小知识的整理。</p><span id="more"></span><h1 id="一中文">一、中文</h1><ul><li>窠臼<br /></li><li>狗苟蝇营 gǒu gǒu yíng yíng<br />成语解释 苟：苟且；营：谋求。像狗那样苟且求活，像苍蝇那样营营往来。比喻不顾廉耻，到处钻营。<br /></li><li>趋炎附势<br />拼 音：qū yán fù shì<br />释 义：趋：奔走。炎：热，比喻权势。 奉承和依附有权有势的人。<br /></li><li>眼睑 jiǎn<br /></li><li>洗尽铅华 从低俗中脱离出来，一种世俗的改变，铅，古代用于化妆，华，外边的华丽。意思是洗掉伪装世俗的外表。<br /></li><li>色厉内荏<br />发音 sè lì nèi rěn<br />释义 色：神色，样子；厉：凶猛；荏：软弱。外表强更，内心虚弱。<br />出处 《论语·阳货》：“色厉而内荏，譬诸小人，其穿窬之盗也与。”<br /></li><li>奉为圭臬 fèng wéi guī niè ：奉：信奉；圭：意为土圭，测日影器；臬：意为水臬，古时测量水平的仪器；圭臬：比喻事物的准则。比喻把某些言论或事当成自己的准则。<br /></li><li>南橘北枳 nán jú běi zhǐ 枳：落叶灌木，味苦酸，球形。也叫枸橘。南方之橘移植淮河之北就会变成枳。比喻同一物种因环境条件不同而发生变异。**<br /></li><li>以邻为壑 hè：拿邻国当做大水坑，把本国的洪水排泄到那里去。比喻只图自己一方的利益，把困难或祸害转嫁给别人。《孟子·告子下》：“是故禹以四海为壑。今吾子以邻国为壑。”<br /></li><li>未雨绸缪：chóu móu 绸缪：紧密缠缚。天还没有下雨，先把门窗绑牢。比喻事先做好准备工作。<br /></li><li>僭 jiàn： 超越本分，古代指地位在下的冒用在上的名义或礼仪、器物：僭越。僭妄。僭伪（封建王朝称割据对立的王朝）。僭盗。<br /></li><li>鸿一瞥（piē）：鸿，即鸿雁，也叫大雁。惊鸿：轻捷飞起的鸿雁。“惊鸿”一词多形容女性轻盈如雁之身姿，惊鸿一瞥意思是人只是匆匆看了一眼，却给人留下极深的印象。<br /></li><li>胼手胝足pián shǒu zhī zú 【解释】胼、胝：老茧。皮肤等的异常变硬和增厚,一般是指长期从事体力劳动者，手脚生茧。形容十分辛勤劳动。<br /></li><li>汲汲营营jí jí yíng yíng：汲汲，勤求不休止的样子。营营，追逐求取。汲汲营营形容人急切求取名利的样子。<br /></li><li>觥筹交错<br /></li><li>边陲<br /></li><li>“孟不离焦”，或者“焦不离孟”出自《杨家将》，焦、孟指的是杨延昭（杨六郎）部下的两员大将焦赞和孟良，二人是结义弟兄，常形影不离。后用于比喻两人关系非常铁，感情深厚。<br /></li><li>穷兵黩武：qióng bīng dú wǔ 穷：竭尽；黩：随便，任意。随意使用武力，不断发动侵略战争。形容极其好战。<br /></li><li>夙夜匪懈：夙夜：早晚，朝夕；匪：不；懈：懈怠。形容日夜谨慎工作，勤奋不懈。<br /></li><li>噤若寒蝉：噤：闭口不作声。象深秋的蝉那样一声不吭。比喻因害怕有所顾虑而不敢说话。《后汉书·杜密传》：“刘胜位为大夫，见礼上宾，而知善不荐，闻恶无言，隐情惜己，自同寒蝉，此罪人也。”<br /></li><li>血脉偾张：fèn<br /></li><li>嬗变：shàn，变迁，更替，古同“禅”。<br /></li><li>付梓（zǐ）：把稿件交付排印。<br /></li><li>慰藉（jiè）<br /></li><li>饿殍(piǎo)遍野:殍：人饿死后的尸体。到处是饿死的人。形容老百姓因饥饿而大量死亡的悲惨景象。先秦·孟轲《孟子·梁惠王上》：“庖有肥肉，厩有肥马，民有饥色，野有饿莩，此率兽而食人也。”<br /></li><li>虚与委蛇（yí）：虚：假；委蛇：随便应顺。指对人虚情假意，敷衍应酬。《庄子·应帝王》：“乡吾示之以未始出吾宗，吾与之虚而委蛇。”<br /></li><li>连中三元：乡试：解（jiè）元； 会试：会元； 殿试：状元<br />解的几种读音<br />“解甲归田”的“解”“jiě”“解甲”，就是脱去束在身上的铠甲，故“解”当读jiě。<br />读为“jiè”的“解”字，主要含义有：1：押送，如“解差”（旧时押送犯人者）、“解送”（押送财物或者犯人等）；2：“解元”，明清两代称考取乡试考取第一名的人为“解元”（因为唐朝举进士都是由地方官解送入试，故称）<br />“跑马卖解”的“解”“xiè”<br />读为“xiè”的“解”字，旧指杂技表演的各种技艺，如“跑马卖解”（特指骑在在马上表演的技艺）、“解数”（本指战术驾驶，后泛指手段、本领）等。还有，姓也读此音，如：歌手“解晓东”的姓就是这个读音。</li></ul><h1 id="二英文">二、英文</h1><ul><li>在欧洲，St.Nicholas 和 Santa Claus（圣诞老人）是一回事。<br /></li><li>个例：Case by case<br /></li><li>善意：Goodwill<br /></li><li>联排别墅：Town house<br /></li><li>a blind Venetian 威尼斯盲人<br />a Venetian blind 软百叶窗帘（活动百叶窗）<br /></li><li>最佳实践 Best Practice<br />简单是美 Simple is Beauty！<br />化繁入简 Heavy to light.<br /></li><li>mentor 导师<br />check 支票<br />magic number （魔数）意指直接写在程序里面的具体的数值。<br /></li><li>佛罗伦萨马基雅维利的墓志铭：如此伟人无以铭之（to so great a name no praise in equal）。<br /></li><li>佛陀将开悟简单定义为“受苦的终结（the end of suffering)”<br />笛卡尔：我思故我在。I think, therefore I am。<br /></li><li>愤青： young contrarian<br /></li><li>英国的巨石阵：Stonehenge<br /></li><li>Beagle 小猎犬<br /></li><li>解铃还须系铃人：You build it,you break it.</li></ul><h1 id="三小知识">三、小知识</h1><ul><li>上有所好，下必甚（更厉害）焉<br /></li><li>覆巢之下，安有完卵<br /></li><li>轻徭薄赋、悲天悯人<br /></li><li>王八吃秤砣——铁了心<br /></li><li>覆巢之下，安得完卵。</li><li>仓廪（lǐn）实而知礼节，衣食足而知荣辱。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 词语 </tag>
            
            <tag> 英语 </tag>
            
            <tag> 小知识 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《卓有成效的管理者》读书笔记</title>
      <link href="/2017/03/28/%E3%80%8A%E5%8D%93%E6%9C%89%E6%88%90%E6%95%88%E7%9A%84%E7%AE%A1%E7%90%86%E8%80%85%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
      <url>/2017/03/28/%E3%80%8A%E5%8D%93%E6%9C%89%E6%88%90%E6%95%88%E7%9A%84%E7%AE%A1%E7%90%86%E8%80%85%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<p>看过序最多的一本书，一共有六个推荐序. :-）<br />作为管理学大师，作为第一位把管理进行科学化的思想家，德鲁克的管理学书籍通过简单易懂的描述，阐述了管理的精髓。<br />现在社会以知识工作者为主，不同于以往的体力工作者，没有一套可量化的、固定的检查、考核体系能对知识工作者进行过程考核。所以，为了提高组织的绩效，管理者（大部分的知识工作者）必须提高自我的绩效，进行有效的自我管理才能达到组织的目的。<br />本书围绕着，如何让自己成为一个有效的管理者。作者明确指出，卓有成效不是一种天赋，而是一种可以通过后天努力和实践学会的能力。所以，作者通过有效的时间管理、重视贡献、发挥人的长处、要事优先（其实也是时间管理的一部分）、有效决策等方面进行展开，说明了如何通过这些手段，让自己成为一个卓有成效的管理者。<br />作者的很多观点，现在看来都耳目能详、天经地义，但是放回50年前（本书1966年出版），作者能在二战后不久，就提出这样的管理理念，确实是让人叹为观止。<br />就像吴晓波在悼念德鲁克（2005年11月11日，彼得·德鲁克在酣睡中悄然去世）的文章所说的：“<strong>他走了之后，下一个该轮到谁来替我们思考管理？</strong>”<br /><span id="more"></span></p><figure><img src="/img/卓有成效.jpg" title="卓有成效的管理全书纲要" alt="卓有成效的管理全书纲要" /><figcaption aria-hidden="true">卓有成效的管理全书纲要</figcaption></figure><p>下面是本书的一些读书摘要</p><h1 id="一卓有成效是可以学会的">一、卓有成效是可以学会的</h1><p>把事情做对（<strong>to do things right</strong>）的能力，而不是“做对的事情”(<strong>to get the right things done</strong>)的能力<br />的确，谁也不知道一位知识工作者在想些什么。然而，思考却是他的本分，他既然是在思考，他就是在工作。<strong>知识工作者并不产生本身具有效用的产品，他生产的是知识、创意和信息。知识工作者是一项特殊的“生产要素”。 一个专业人员的产出必须与其他人的产出结合在一起，才能产生成果。</strong><br />知识工作不能用数量来衡量，也不能用成本来衡量。衡量知识工作者主要看其成果。<br /><strong>在组织内部不会有成果出现，一切成果都是发生在组织外部。</strong><br /><strong>对于外部的情况，真正重要的不是趋势，而是趋势的转变。趋势的转变才是决定一个机构及其努力的成败关键。对这种转变，必须有所察觉，转变是无法估量，无法界定，无法分类的。</strong></p><h1 id="二掌握自己的时间">二、掌握自己的时间</h1><p>在大型组织中，如果知识工作者的绩效表现不错，往往是因为该组织的高级主管能定期抽出时间来和他们进行交流。例如交流：</p><ul><li>你认为我们组织的领导，对你的工作应该了解些什么？<br /></li><li>你对我们这个组织有什么看法？<br /></li><li>你觉得我们还有哪些尚未开拓的机会？<br /></li><li>你觉得我们有哪些尚未觉察的危机？<br /></li><li>你希望从我这里知道些什么？</li></ul><p>只要遇到人事问题，决策总是很慢，并且需要经过多次考虑，才能最终定案。<br />关于用人，通用总裁斯隆说过：“我没有秘诀。我只是有这样的感觉，我第一念就想到的人选，往往不会是最适当的人选。我总要反复再三，才能做最后决定。”</p><h2 id="如何诊断自己的时间">1、如何诊断自己的时间</h2><p><strong>必须在处理某一工作的“当时”立即加以记录，而不能凭记忆补记。</strong><br />每月定期拿出来检讨。</p><ul><li>找出什么事根本不必做<br />逐项问：这件事如果不做，还有什么后果？如果认为不会有任何影响，那么这件事便该立即取消。（审视一下，这类事情对组织有无贡献，对于他本人有无贡献，或是对于对方的组织有无贡献。如果都没有，只要谢绝就得了。）<br /></li><li>时间记录上的哪些活动可以有别人代为参加而又不影响效果<br /></li><li>是否管理者在浪费别人的时间。</li></ul><h2 id="消除浪费时间的活动">2、消除浪费时间的活动</h2><ul><li>找出缺乏制度或远见而产生时间浪费的因素<br />例如，一项重复出现的危机应该是可预见的，因此，这类危机可以预先防止，或可以设计成一种例行作业，是每个人都能处理。<br /></li><li>人员过多，也常造成时间浪费<br />用一个人，应该是每天的工作都需要用到他。<br /></li><li>组织不健全。<br />其表现就是会议太多。会议是组织缺陷的一种补救措施。一个结构设计臻于理想的组织，应该没有任何会议。<br /></li><li>信息功能不健全。</li></ul><h2 id="统一安排可以自由支配的时间">3、统一安排可以自由支配的时间</h2><p>以90分钟为一单元。根据研究发现，一个普通人“超过90分钟”精力就难以集中，而“不够90分钟”则难以处理好一件事情</p><h1 id="三我能贡献什么">三、我能贡献什么</h1><p>管理者要由技术进入到观念；有机械性工作进入到分析性方法；由效率进入到成果；<br /><strong>管理者要自省：为什么组织聘他为管理者？他应该对组织由什么影响？</strong><br />管理者如果能着眼于贡献，那么他所重视的应当不仅是“方法”，而是“目标”和“结果”。</p><h2 id="管理者的承诺">1、管理者的承诺</h2><p>重视贡献、关注整体绩效、关心组织目标。<br />贡献的体现：</p><ul><li>直接成果；<br /></li><li>树立新的价值观及对这些价值观的重新确认；<br /></li><li>培养与开发明天所需要的人才；</li></ul><p>但是，职位的变化会使上面三种绩效之间的相对比重发生变化。所以，职位变化的时候，需要对自己提出新的挑战，需要看到努力的方向。<br />一个管理这的职位越高，他在对外方面所需的贡献也越大，因为一个组织里，通常只有职位最高的管理者，才能在对外方面自由活动。</p><h2 id="如何使专业人员的工作卓有成效">2、如何使专业人员的工作卓有成效</h2><p>对别人提出的问题：为便于你为机构做出贡献，你需要我做些什么贡献？需要我在什么时候，以哪种形式，用什么方式来提供这些共享。</p><h2 id="正确的人际关系">3、正确的人际关系</h2><p>着眼于贡献</p><ul><li>互相沟通<br /></li><li>团队合作<br /></li><li>自我发展<br /><strong>对自己设定目标。</strong><br /></li><li>培养他人</li></ul><h2 id="有效的会议">4、有效的会议</h2><p><strong>会议是管理者每日使用的管理工具。</strong><br />规则：你可以主持会议，听取重要的发言，也可以是与大家共同讨论。但你不能即主持会议，又高谈阔论。</p><h1 id="四发挥人的长处">四、发挥人的长处</h1><p><strong>不仅要发挥下属的长处，我们还要发挥自己的长处，甚至要帮助上级发挥他的长处。</strong><br />充分发挥人的长处，才是组织存在的唯一目的。（用人之长、容人之短）<br />一位管理者，如果仅能见人之短而不能识人之长，因而刻意避其所短，而非着眼于发挥其所长，则这位管理者本身就是一位弱者。他会觉得别人的才干可能构成对他本身的威胁。但是，世界上从来没有发生过下属的才干反而害了主管的事。美国的钢铁工业之父卡内基的墓志铭说得最为透彻：“这里躺着的人，知道选用比自己能力更强的人来为他工作。”<br />充分发挥人的长处，这是对人的尊重；尊重自己，也尊重他人。这是管理者的价值观在行为上的体现。<br /><strong>真正“苛求的上司”（实际上懂得用人的上司大部分都是“苛求”的上司），总是先发掘一个人最擅长做些什么，再来“苛求”他做些什么。</strong><br /><strong>通过组织来克服人的短处，就像一个球队、一个游戏的卡牌组合，都是充分利用个体的长处，个体的短处通过其它个体来弥补，达到最好的配合效果。</strong><br />因人设事的结果，是必将产生恩怨派系，组织绝对不能出现这种情况。<br />我们常常可以听到这样的说法：能建立起第一流经营体制的管理者，通常不会与周围的同事及下属保持过分亲密的关系。（会造成跟进个人好恶来挑选人才）</p><h2 id="用人的四个原则">1、用人的四个原则</h2><ul><li>卓有成效的管理者不会将职位设计成只有上帝才能胜任；<br />只有让平凡的人都做出不平凡的事的组织，才是好的组织。<br /></li><li>职位的要求严格，而涵盖要广；<br /><strong>一位知识工作者在初任某一职位时，其职位的标准，应能作为他日后发展的引导，应能成为他衡量自己、评估贡献的依据。</strong><br />某人在某一组织能有什么贡献，他本身的指示和技能是一个因素，组织的价值观和目标也是同样重要的因素。<br /></li><li>卓有成效的管理者在用人时，会先考虑某人能做些什么，而不是先考虑职位的要求是什么。<br />面谈考评是整个考评制度的重心所在。今天大多数组织制定的考评方法，其实是脱胎于一般医生对病人的评估。医生的目的在于治病，医生重视的是病人的毛病，而不是病人的优点。<br /></li><li>卓有成效的管理者知道在用人所长的同时，必须容忍人之所短。</li></ul><figure><img src="/img/少不了他.jpg" title="如何看待组织少不了谁的问题？" alt="如何看待组织少不了谁的问题？" /><figcaption aria-hidden="true">如何看待组织少不了谁的问题？</figcaption></figure><h2 id="如何管理上司">2、如何管理上司</h2><p>人大致可以分为两种类型：“读者型”和“听者型”。<br />协助上司发挥其所长，是促使管理者有效的最好方法。</p><h2 id="充分发挥自己的长处">3、充分发挥自己的长处</h2><p>所谓“别人不让我干”，恐怕是惰性和没有勇气的借口吧。就算是客观条件真有限制（事实上任何人做任何事均免不了有限制），也一定仍然可以做出许多有意义的重要工作来。<br /><strong>难点是在于如何发现一个人的长处！ 思考自己的长处！他会问“此人能做些什么？” 他会问：“哪一类工作别人多起来要费九牛二虎之力，而我做起来确实轻而易举？”</strong></p><h1 id="五要事优先">五、要事优先</h1><p>first things first.<br />do one thing at a time.</p><h2 id="摆脱昨天">1、摆脱昨天</h2><p>尤其重要的是：有效管理者打算做一项新的业务，一定先删除一项原有的业务。这对控制组织的“膨胀”是非常必要的。（推陈出新）<br />社<br />按压力来决定优先，说到压力，往往总是为了昨天。</p><figure><img src="/img/决策.jpg" title="压力下优先次序的选择" alt="压力下优先次序的选择" /><figcaption aria-hidden="true">压力下优先次序的选择</figcaption></figure><p> <font color="#4590a3" size = "3px"><strong>确定优先次序的重要原则：</strong></font></p><ul><li>重将来而不重过去；<br /></li><li>重视机会，不能只看到困难；<br /></li><li>选择自己的方向，而不盲从；<br /></li><li>目标要高，要有新意，不能只求安全和方便。</li></ul><p> <font color="#4590a3" size = "3px"><strong>所谓“领导力”，并不是指智慧和天赋，而是指人人皆可达成的专心、决心和目标。</strong></font></p><h1 id="六决策的要素">六、决策的要素</h1><p>防守政策将麻醉管理处的创造力。<br />一个垄断性的企业虽然没有对手，但是应该以将来作为对手。<br />决策，不是为了适应当时的临时需要，而是战略性的考虑。</p><h2 id="决策的五个要素">决策的五个要素</h2><ul><li>要确定了解问题的性质，如果问题是经常性的，那就只能通过建立规则或原则的决策才能解决；<br /><strong>以为有效的决策者碰到问题，总是先假设该问题为“经常性质”。他总是先假定该问题是一种表面现象，另有根本性的问题存在。</strong><br /></li><li>要确实找出解决问题时必须满足的界限，换言之，应找出问题的“边界条件”；<br />解决某一问题应有什么最低需要。<br /></li><li>仔细思考解决问题的正确方案是什么，以及这些方案必须满足哪些条件，然后再考虑必要的妥协、适应及让步事项，以期该决策能被接受；<br />研究“正确”的决策是什么，而不是研究“能为人接受”的决策是什么。人总有采取折中方法的倾向。<br /></li><li>决策方案要同时兼顾执行做事，让决策变成可以被贯彻的行动；<br />决策应该匹配当前的管理水平、人员水平，需要考虑这一决策应该有谁来执行？他们能做什么？同时设计出验收标准。<br /></li><li>在执行的过程中重视反馈，以印证决策的正确性及有效性。<br />军队中最重要的反馈是亲子视察，报告或沟通不一定靠得住。<br /></li><li>2000年前罗马律法就曾说过：“行政长官不宜考虑鸡毛蒜皮之类的事情。”</li></ul><h1 id="七有效决策">七、有效决策</h1><h2 id="人见解和决策的关系">1、人见解和决策的关系</h2><p>决策的原则：除非有不同的见解，否则就不可能有决策；</p><h2 id="反面意见的运用">2、反面意见的运用</h2><p>换位思考的能力，决策需遵循以下原则：</p><ul><li>如果利益远大于成本及风险，就该行动；<br /></li><li>行动或不行动；切忌只做一半或折中。</li></ul><h2 id="决策与电脑">3、决策与电脑</h2><p>电脑的决策是“硬性原则”的决策<br />相对电脑，人的逻辑性虽然不是特别强，但是人能够洞察，这正是人的有点所在。</p><h1 id="文章摘要">文章摘要</h1><ul><li>德鲁克先生主张以创新这类演进的方式解决发展的问题。<br /></li><li>从理解全局或整体出发，寻找不同事物之间的内在联系性，达到把握和解决个别问题的目的。<br /></li><li><strong>管理得好的工厂，总是单调乏味，没有任何激动人心的事件发生。 个人注：类似运维</strong><br /></li><li>有效管理就是要防患于未然，将例外管理变成例行管理。 个人注：类似风险管理。<br /></li><li>在组织而言，需要个人提供其贡献，在个人而言，需要组织作为达到个人目的的工具。<br /></li><li>总有人单独作战，无一部属，然而仍不失为管理者。<br /></li><li>德鲁克是推动管理学发展成为一门严肃科学的先驱。<br /></li><li>知识工作者的成果通常要与其他人的成果结合起来才能产生效益，因而管理者的作用日益凸显。<br /></li><li>由于只是工作者难以监督，因而组织效率将取决于组织成员能够对自身进行有效的管理。<br /></li><li>真正有创造力的企业要使组织内部每个员工具有企业家精神。<br /></li><li>管理有效性的关键，不在于有效的管理别人，而在于有效地管理自己。<br /></li><li>事实上，只要暂缓，就不会启动，也许永远不会启动。<br /></li><li>形成“高层次概念性认识”，从高层次观念入手，寻求解决问题的系统方案。<br /></li><li>管理最为重要的作用，就是把人们联系在一起共同实现目标的工作过程。<br /></li><li>把脚踩在大地上，踏踏实实在工作中实践。<br /></li><li>管理是一门实践的学科，管理不单在于“知”，更在于“行”<br /></li><li>知识权威、职位权威。<br /></li><li><strong>对管理者的有效性而言，最重要的人物，往往不是管理者直接控制的下属，而是其他部门的人，即所谓的“旁系人士”，或是管理者本人的上司。一位管理者如果不能与这些人主动接触，不能是这些人利用他的贡献，他本身就没有有效性可言。</strong><br /></li><li>所谓事实，应该是已经认定，已做分类，并且已确知其关联性。在我们对其量化之前，必须掌握一个概念，那就是：必须从无数现象中抽象出某一具体的特性，并对其命名，然后才能进行计算。<br /></li><li>社会生活及政治生活中最显著的一项事实是：暂时性的事物往往具有永久性。<br /></li><li>**世事洞明皆学问，人情练达即文章。<br /></li><li><strong>奉为圭臬 fèng wéi guī niè </strong>：奉：信奉；圭：意为土圭，测日影器；臬：意为水臬，古时测量水平的仪器；圭臬：比喻事物的准则。比喻把某些言论或事当成自己的准则。<br /><strong>南橘北枳</strong> nán jú běi zhǐ 枳：落叶灌木，味苦酸，球形。也叫枸橘。南方之橘移植淮河之北就会变成枳。比喻同一物种因环境条件不同而发生变异。**<br /></li><li><strong>a blind Venetian 威尼斯盲人<br />a Venetian blind 软百叶窗帘（活动百叶窗）</strong></li></ul><p><img src="/img/卓有成效的管理者.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 管理 </tag>
            
            <tag> 德鲁克 </tag>
            
            <tag> 卓有成效 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《Struts2技术内幕：深入解析Struts架构设计与实现原理》读书心得</title>
      <link href="/2017/03/20/%E3%80%8AStruts2%E6%8A%80%E6%9C%AF%E5%86%85%E5%B9%95%EF%BC%9A%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90Struts%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2017/03/20/%E3%80%8AStruts2%E6%8A%80%E6%9C%AF%E5%86%85%E5%B9%95%EF%BC%9A%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90Struts%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>网上查阅开发框架资料时，知乎推荐《Struts2技术内幕：深入解析Struts架构设计与实现原理》这本书。本书2013年出版，在亚马逊上只有Kindle电子版，后来在孔夫子旧书网上下单买了一本旧的全新书。<br />购买本书时也犹豫过，一是，这是本号称技术内幕的书，担心作者会陷入技术的细枝末节；二是，对中国人写关于技术的书，大多在堆砌不同资料的套路中来回，结果都是大杂烩，对本质的东西反而讲不透。<br />但是当开始看这本书的时候，就真正被吸引了，有一种爱不释手的感觉。<br />作者的逻辑是，我们编程过程碰到什么问题？解决方案是什么样？具体实现方式以及背后的思想是什么？围绕这三个环节深入浅出的把Struts框架进行讲解，让人读后有一种醍醐灌顶的感觉，让读者真正做到知其然并知其所以然。<br />总体来说是一本难得的好书。<br />Struts2框架是表示层的框架，MVC又是表示层最经典的设计模式（最佳实践），所以本书最核心的部分是围绕着MVC模式在展开，讲述了为了满足MVC的实现，在数据流、控制流方面，整个框架是如何设计、各层是如何交互的。<br />虽然目前Spring MVC异军突起，已经有开始取代Struts2的局面，但是从框架原理和思想角度来说，其实是相通的。关键点不是具体技术的细节，而是技术后面的方法论，技术背后的思想和思考问题的方法。 <span id="more"></span><br />下面是本书的一些读书笔记整理。</p><h1 id="一struts2的架构">一、Struts2的架构</h1><p><img src="/img/Java应用的金字塔结构.jpg" /><br /><img src="/img/Web容器的黑盒模型.jpg" /><br /><img src="/img/Struts2入口程序执行示意图.jpg" /><br /><img src="/img/XWork元素的调用关系图.jpg" /><br /><img src="/img/XWork控制比喻.jpg" /><br /><img src="/img/插件模式的齿轮运转模型.jpg" /></p><h1 id="二框架及设计模式">二、框架及设计模式</h1><ul><li>struts2<br />是表示层框架的框架 因为其最为核心的内容就是和Web容器打交道，帮助我们处理Http请求。Struts2通过扩展实现Servlet标准来处理Http请求。<br /></li><li>Spring<br />是业务层的框架<br /></li><li>Hibernate<br />是持久层的框架</li></ul><p>MVC是表示层的最佳实践（设计模式） M其实是请求--响应的数据模型（数据流）<br /><img src="/img/分层开发模式.jpg" /><br /><img src="/img/MVC模型图.jpg" /></p><p><strong>本书的关于表示层的问题及解答</strong>：<a href="/2017/03/18/%E8%A1%A8%E7%A4%BA%E5%B1%82%E7%9A%84%E5%9B%B0%E6%83%91/" title="表示层的困惑">表示层的困惑</a></p><h1 id="三容器及依赖注入">三、容器及依赖注入</h1><p>容器（Container），不仅支撑起一个框架的所有对象，同时也称为框架运行过程中最为重要的一个辅助元素，也是整个框架得以运行的核心基础。<br />为了更好的管理对象的生命周期，解决如下两个问题：</p><ul><li>1、在程序运行期间，应如何创建我们所需要的对象？<br /></li><li>2、当创建一个新的对象时，如何保证与这个对象所关联的依赖关系（其关联对象）也能够被正确的创建出来。</li></ul><p>当我们需要寻求容器帮助时，只要再恰当的地方加入一个标识符Annotation，容器在进行依赖注入操作时，就能够知晓并接管整个过程了。在这里，我们看到两个过程共同构成了XWork容器进行对象依赖注入操作的步骤：</p><ul><li>1、没某个对象的方法、构造函数、内部实例变量、方法参数变量加入@Inject的Annotation；<br /></li><li>2、调用容器的inject方法，完成被加入Annotation的那些对象的依赖注入。</li></ul><p>因为，我们这里顺利解决了容器定义中所提出的一个核心问题：如何建立起系统到容器或者容器托管对象的沟通桥梁--通过@Inject声明来完成。</p><p>在容器内部进行缓存的是对象实例的构建方法，而不是对象实例本身。这就让容器看起来像一个工厂集合，能够根据不同的要求，制造出不同种类的对象实例。</p><h1 id="四配置元素">四、配置元素</h1><p>配置元素分类：<br />从节点所表达的逻辑含义和节点在程序中所起的作用对配置元素进行分类。</p><ul><li>1、容器配置元素<br />Bean节点：构成程序运行时的对象<br />Constant节点：程序运行的执行参数<br /></li><li>2、事件映射关系<br />Package节点：定义了一种事件请求响应的映射关系，反映的是Strut2对于外部事件情就是如何进行响应的处理序列。</li></ul><h1 id="五servlet及线程安全">五、Servlet及线程安全</h1><p>是J2EEden重要标准之一，规定了Java如何响应Http请求的规范。通过httpServletRequest和HttpServletResponse对象，我们能够轻松地与Web容器交互。<br /><strong>线程安全，指的是在多线程环境下，一个类在执行某个方法时，对类的内部实例变量的访问时安全的。</strong><br />Servlet兑现故事一个无状态的单例对象（singleton），所以可能这个实例在不同的线程执行的时候，导致线程的实例变量被修改，因而不是线程安全的。<br />ThreadLocal模式（以空间换时间）和Synchronized关键字（以时间换空间）都是用于处理多线程并发访问变量的问题。<br />ThreadLocal模式两个步骤：</p><ul><li>1、建立一个类，并在其中封装一个静态的ThreadLocal变量，使其成为一个共享数据环境；<br /></li><li>2、在类中实现访问静态Trespassing变量的静态方法（设值和取值）</li></ul><p>使用ThreadLocal模式，可以对执行逻辑与执行数据进行有效地解耦。这一点是ThreadLocal模式给我们带来的最为核心的一个影响。因为在一般情况下，Java对象之间的写作关系，主要是通过参数和返回值进行消息传递，这也是对象协作之间的一个重要依赖。而ThreadLocal模式彻底打破了这种依赖关系，通过线程安全的共享对象来进行数据共享，可以有效避免在编程层次之间形成数据依赖。这也称为XWork事件处理体系核心的设计。<br /><img src="/img/Thread执行示意图.jpg" /></p><h1 id="六表达式引擎ognl">六、表达式引擎OGNL</h1><p>表达式引擎 OGNL（Object Graph Navigation Language）： 使用某些符合特定规则的字符串表达式来对Java的对象进行读和写的操作。 解决了Web应用与Java世界之间的沟通问题。既然要使用Java来开发Web应用，就必须使Java的变成要素能够与Web浏览器之间在数据层面保持良好的沟通，而这种沟通就是通过表达式引擎来完成的。<br />不同编程层次之间进行数据沟通的重要桥梁。<br />数据在不同的MVC层次上，扮演的角色和表现形式不同。这是由于Http协议与Java面向对象之间的不匹配造成的。如果我们要数据在View层（页面）和Java世界中互相流转传递，就会在“字符串”与“对象树”之间存在不匹配。这就需要一个翻译的角色来解决这种不匹配。这个角色，就是我们所说的表达式引擎。</p><h1 id="七请求---响应模式">七、请求 - 响应模式</h1><ul><li>1、参数 - 返回值（Param-Return）模式<br />对象的方法成为请求 - 响应模式在Java世界中的一种直观抽象；<br /></li><li>2、参数 - 参数（Param-Param）模式<br />参数 - 参数模式是一种最为基础的请求 - 响应实现机制，也是底层规范不得不采用的一种实现机制。<br /></li><li>3、POJO模式<br />POJO相对于某一次的响应是有状态响应。因为响应的处理流程、处理机制和处理结果，与当前POJO实例的内部属性的状态有关。<br />POJO模式直接从概念上突破了Servlet的对象的限制，将每一个请求的处理映射到一个县城安全的响应对象中去执行。因而从模式上讲，POJO模式是对传统的Servlet模式的而一个重大改进，是一种崭新的请求 - 响应模式的实现。</li></ul><p><img src="/img/请求-响应模式的职责示意图.jpg" /><br /><img src="/img/请求响应模式1.jpg" /><br /><img src="/img/请求响应模式.jpg" /></p><h1 id="文章摘录">文章摘录</h1><ul><li>轻量级的Web应用服务器是Jetty，无需安装、速度快，成为众多程序员进行Web开发调试的首选。<br /></li><li>调试源码是本书最为推荐的一种源码级别学习方法<br /></li><li>因为只有了解了为什么，我们才能知道怎么做，知道如何才能做得更好。<br /></li><li>当我们加载一个JAR包到CLASSPATH时，实际上是获得了JAR中所有对JDK的额外支持。<br /></li><li><strong>框架只是一个JAR包而已，其本质是对JDF的功能扩展</strong><br /></li><li>最佳实践 Best Practice<br />简单是美 Simple is Beauty！<br />化繁入简 Heavy to light.<br /></li><li>可读性、可维护性和可扩展性。<br /></li><li>配置就像是程序的影子，与程序总是如影随形。<br /></li><li>HTML语言是一种静态语言，它自身缺乏数据沟通的能力，也就是说，HTML语言需要另外一种机制的帮助才能完成与服务器端程序的沟通和逻辑控制。<br /></li><li>JSP（Java Server Page）允许在构成Page的HTML语言之上，嵌入Java的语法片段，从而加强其与Server的交互能力。达到，页面视图的构建、与服务器端进行数据沟通。所有JSP在运行期间都被编译成Servlet在Web容器中运行。<br /></li><li>HttpServletRequest兑现规划组要用于处理<strong>整个Http生命周期中</strong>的数据。<br /></li><li>视图的本质是Web容器对象HttpServletRequest（负责数据处理）和HttpServletResponse（负责内容呈现）对浏览器行为的控制。<br /></li><li>AJAX技术所带来的对视图便是的最重要 影响谬事通过JavaScript操作HTML的DOM节点来进行视图输出控制。它也逐渐成为视图表现的一个重要选择方向。不过我们可以发现，<strong>使用JavaScript来进行视图输出的控制实际上是把视图呈现的职责转移到了客户端变成的范畴，这是一种“职责转移”。</strong><br /></li><li>模块化实际上只一种“分而治之”的思想。</li></ul><p><img src="/img/Struts2技术内幕.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Struts2 </tag>
            
            <tag> 框架 </tag>
            
            <tag> MVC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《敏捷软件开发(原则模式与实践)》读书心得</title>
      <link href="/2017/03/15/%E3%80%8A%E6%95%8F%E6%8D%B7%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91(%E5%8E%9F%E5%88%99%E6%A8%A1%E5%BC%8F%E4%B8%8E%E5%AE%9E%E8%B7%B5)%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2017/03/15/%E3%80%8A%E6%95%8F%E6%8D%B7%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91(%E5%8E%9F%E5%88%99%E6%A8%A1%E5%BC%8F%E4%B8%8E%E5%AE%9E%E8%B7%B5)%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>刚看完瀑布模型过程代表作--《软件是这样炼成的》，书中介绍了一种庞大的、重型的过程方法。为了比较，又看了这本关于敏捷开发的书。<br />关于敏捷开发，之前陆陆续续有些了解，《敏捷软件开发(原则模式与实践)》的作者作为敏捷开发的创始者之一，在书中系统的说明了敏捷开发的思想以及相关实践。<br /><strong>敏捷开发（Agile Development），是一种面临迅速变化的需求快速开发软件的能力。为了获取这种敏捷性，我们需要使用一些可以提供必要的纪律和反馈的实践。我们需要使用一些可以保持我们的软件灵活、可维护的设计原则，并且我们需要知道一些已经被证明针对特定的问题可以平衡这些原则的设计模式。</strong></p><span id="more"></span><p>本书其实就是按照上面这一段话进行编排，把敏捷开发的思想、实践（XP）、设计原则、设计模式进行了讲述。<br />作为一种开发思想，敏捷开发只是提供了一些原则性的、指导性的思想，而XP（eXtreme Programming 极限编程）是敏捷开发的实践之一。<br /><strong>敏捷开发强调尽早地、经常的进行交付</strong>，然后接受业务反馈进行迭代开发，这是一种轻量过程的开发模式。敏捷开发基于<strong>一个最重要的事实是：需求总是在变化。</strong>用户的需求是一直会变的，另外，用户不可能一下子提出所有的需求，直到他们看到了产品（或原型）。敏捷开发能通过业务的逐步反馈、逐步沟通，达到需求不断优化、功能不断完善来满足业务的变化。<br />和瀑布模型不同的是，敏捷开发缺少了明确的设计阶段，并且敏捷开发的进度衡量是可工作软件（可工作的功能占比），而不是传统意义上的文档。这就解决了在瀑布模型中，需要分析、设计、编码之间的相关文档冻结，最后导致交付的成品不能符合业务当下的要求，一种可能是需求分析的不完善，或者是当前的业务已经发生了变化。<br />作为敏捷思想最有名的实践，极限编程提出了符合敏捷开发的工作方式，客户作为团队成员、用户故事（索引卡片）、迭代、回顾会、测试驱动编程、持续集成（每日构建）、结对编程等等。这里面有很多理想的成分，在现实中很难落地，特别是在中国这个文化背景中。例如结对编程、客户一同办公，在中国的现状下是很难实现的。<br />所以在实际的实践中，往往是把敏捷思想和传统瀑布过程管理模型进行结合和剪裁。例如，客户不能参加到团队中，可以以需求分析师作为客户代表，随时反馈业务的需求；不能做到结对编程，但是可以在有问题的时候，按需派出专人进行指导。但是尽早经常交付、迭代等思想还是可以采用，虽然迭代、持续构建对测试团队的自动化测试能力、回归测试能力要求很高。 相关极限编程的应用现状参照：<a href="/2017/03/13/%E6%9E%81%E9%99%90%E7%BC%96%E7%A8%8B%E7%9A%84%E5%BA%94%E7%94%A8%E7%8E%B0%E7%8A%B6/" title="极限编程的应用现状">极限编程的应用现状</a><br />为了满足敏捷开发拥抱变化的思想，需要做到各次迭代之间软件结构的可维护性、灵活性，所以需要开发人员遵循相关面向对象的设计原则以及采用各种设计模式。作者对设计原则和设计模式讲述了很多，但是对比起《Head First 设计模式》一书，略显枯燥，所以本书这部分的章节基本略过了。<br />本书最后的“两个公司的讽刺小品”，把瀑布型和敏捷开发的过程通过故事的形式展现出来，看了让人忍俊不禁，在一笑之余，还是明白了敏捷型给开发团队带来优势。</p><h1 id="敏捷软件开发宣言"><font color="#4590a3" size = "3px">敏捷软件开发宣言</font></h1><ul><li>我们一直在实践中探寻更好的软件开发方法，<br /></li><li>身体力行的同时也帮助他人。由此我们建立了如下价值观：<br /></li><li>个体和互动 高于 流程和工具<br /></li><li>工作的软件 高于 详尽的文档<br /></li><li>客户合作 高于 合同谈判<br /></li><li>响应变化 高于 遵循计划</li></ul><p>也就是说，尽管右项有其价值，我们更重视左项的价值。</p><h1 id="敏捷宣言遵循的原则12条"><font color="#4590a3" size = "3px">敏捷宣言遵循的原则(12条)</font></h1><p>我们遵循以下原则：</p><ul><li>我们最重要的目标，是通过持续不断地及早交付有价值的软件使客户满意。<br /></li><li>欣然面对需求变化，即使在开发后期也一样。为了客户的竞争优势，敏捷过程掌控变化。<br /></li><li>经常地交付可工作的软件，相隔几星期或一两个月，倾向于采取较短的周期。<br /></li><li>业务人员和开发人员必须相互合作，项目中的每一天都不例外。<br /></li><li>激发个体的斗志，以他们为核心搭建项目。提供所需的环境和支援，辅以信任，从而达成目标。<br /></li><li>不论团队内外，传递信息效果最好效率也最高的方式是面对面的交谈。<br /></li><li>可工作的软件是进度的首要度量标准。<br /></li><li>敏捷过程倡导可持续开发。责任人、开发人员和用户要能够共同维持其步调稳定延续。<br /></li><li>坚持不懈地追求技术卓越和良好设计，敏捷能力由此增强。<br /></li><li>以简洁为本，它是极力减少不必要工作量的艺术。<br /></li><li>最好的架构、需求和设计出自自组织团队。<br /></li><li>团队定期地反思如何能提高成效，并依此调整自身的举止表现。</li></ul><h1 id="面向对象设计的11个原则"><font color="#4590a3" size = "3px">面向对象设计的11个原则</font></h1><ol type="1"><li>单一职责原则（SRP）<br />就一个类而言，应该仅有一个引起它变化的原因。<br /></li><li>开放－封闭原则（OCP）<br />软件实体（类、模块、函数等）应该是可以扩展的，但是不可修改的。<br /></li><li>Liskov替换原则（LSP）<br />子类型（subtype）必须能够替换掉它们的基类型（base type）。<br /></li><li>依赖倒置原则（DIP）<ul><li>高层模块不应该依赖于低层模块，两者都应该依赖于抽象。<br /></li><li>抽象不应该依赖于细节，细节应该依赖于抽象。<br /></li></ul></li><li>接口隔离原则（ISP）<br />不应该强迫客户依赖于它们不要的方法。接口属于客户，不属于它所在的类层次结构。<br /></li><li>发布重用等价原则（REP）<br />重用的粒度就是发布的粒度<br /></li><li>共同重用原则（CRP）<br />一个包中的所有类应该是共同重用的。如果重用了包中的一个类，那么就要重用包中所有其它类。<br /></li><li>共同封闭原则（CCP）<br />包中的所有类对于同一类性质的变化应该是共同封闭的。一个变化若对一个包产生影响，则将对该包中的所有类产生影响，而对于其他的包不造成任何影响。<br /></li><li>无环依赖原则（ADP）<br />在包的依赖关系图中不允许存在环。<br /></li><li>稳定依赖原则（SDP）<br />朝着稳定的方向进行依赖。<br /></li><li>稳定抽象原则（SAP）<br />包的抽象程度应该和其稳定程度一致。</li></ol><p><strong>其中：<br />1－5的原则关注所有软件实体（类、模块、函数等）的结构和耦合性，这些原则能够指导我们设计软件实体和确定软件实体的相互关系；<br />6－8的原则关注包的内聚性，这些原则能够指导我们对类组包；<br />9－11的原则关注包的耦合性，这些原则帮助我们确定包之间的相互关系。</strong></p><h1 id="极限编程实践"> <font color="#4590a3" size = "3px">极限编程实践</font></h1><ol type="1"><li>完整团队<br />XP项目的所有参与者（开发人员、客户、测试人员等）一起工作在一个开放的场所中，他们是同一个团队的成员。这个场所的墙壁上随意悬挂着大幅的、显著的图表以及其他一些显示他们进度的东西。<br /></li><li>计划游戏<br />计划是持续的、循序渐进的。每2周，开发人员就为下2周估算候选特性的成本，而客户则根据成本和商务价值来选择要实现的特性。<br /></li><li>客户测试<br />作为选择每个所期望的特性的一部分，客户可以根据脚本语言来定义出自动验收测试来表明该特性可以工作。<br /></li><li>简单设计<br />团队保持设计恰好和当前的系统功能相匹配。它通过了所有的测试，不包含任何重复，表达出了编写者想表达的所有东西，并且包含尽可能少的代码。<br /></li><li>结对编程<br />所有的产品软件都是由两个程序员、并排坐在一起在同一台机器上构建的。<br /></li><li>测试驱动开发<br />编写单元测试是一个验证行为，更是一个设计行为。同样，它更是一种编写文档的行为。编写单元测试避免了相当数量的反馈循环，尤其是功功能能验证方面的反馈循环。程序员以非常短的循环周期工作，他们先增加一个失败的测试，然后使之通过。<br /></li><li>改进设计<br />随时利用重构方法改进已经腐化的代码，保持代码尽可能的干净、具有表达力。<br /></li><li>持续集成<br />团队总是使系统完整地被集成。一个人拆入（Check in）后，其它所有人责任代码集成。<br /></li><li>集体代码所有权<br />任何结对的程序员都可以在任何时候改进任何代码。没有程序员对任何一个特定的模块或技术单独负责，每个人都可以参与任何其它方面的开发。<br /></li><li>编码标准<br />系统中所有的代码看起来就好像是一人单独编写的。<br /></li><li>隐喻<br />将整个系统联系在一起的全局视图的概念。它是系统的未来影像，是它使得所有单独模块的位置和外观变得明显直观。如果模块的外观与整个隐喻不符，那么你就知道该模块是错误的。<br /></li><li>可持续的速度<br />团队只有持久才有获胜的希望。他们以能够长期维持的速度努力工作，他们保存精力，把项目看作是马拉松长跑，而不是全速短跑。</li></ol><h1 id="文章摘要">文章摘要</h1><ul><li><strong>有些东西讲得简明扼要能够给人以智慧的启迪。</strong><br /></li><li>mentor 导师<br />check 支票<br />magic number （魔数）意指直接写在程序里面的具体的数值。<br /></li><li>人与人之间的交互是复杂的，并且其效果从来都难以预测，但却是工作中最为重要的方面。<br /></li><li>我们建议从使用小工具开始，先使用一个免费的系统直到能够证明该系统已经不再合适，再考虑大一点的系统。不要认为更大的、更好的工具可以自动的帮你做得更好。通常，它们造成的障碍要大于带来的帮助。<br /></li><li>如果文档和代码之间失去同步，那么文档就会变成庞大的、复杂的谎言，会造成重大的误导。<br /></li><li><strong>对于团队来说，编写并维护一份系统原理和结构方面的文档将总是一个好主意，但是那份文档应该是短小的（Short）并且主题突出的（salient）。“短小”的意思就是说，最多有一二十页。“主题突出的”意思是说，应该仅论述系统的高层结构和概括的设计原理。</strong><br />直到迫切需要并且意义重大时，才来编制文档。<br /></li><li>成功的项目需要有序、频繁的客户反馈。不是依赖于合同或者关于工作的描述，而是让软件的客户和开发团队密切的在一起工作，并尽量经常地提供反馈。<br /><strong>成功的关键在于和客户之间真诚的协作，并且合同知道了这种协作，而不是试图去规定项目范围的细节和固定成本下的进度。</strong><br /></li><li>计划不能考虑得过远...当团队增加了对于系统的认识，当客户增加了对需求的认识...计划将会遭到形态（shape）上的变化，而不仅仅是日期上的改变。<br /><strong>较好的策略是：为下两周做详细的计划，为下三个月做粗略的计划，再以后就极为粗糙的计划。我们应该清楚的知道下两周要完成的任务，粗略的了解一下以后三个月要实现的需求。至于系统一年后将要做什么，有一个模糊的想法就行了。</strong><br /></li><li>密歇根大学的一项研究表明，在“充满积极讨论的屋子（War room）”里工作，生产率非但不会降低，反而会成倍的提高。<br /></li><li>计划游戏（planning game）的本质是划分业务人员和开发人员之间的职责。业务人员（也就是客户）决定特性的重要性，开发人员决定实现一个特性所划分的代价。<br /></li><li>重构：就是在不改变代码行为的前提下，对其进行一系列的改造（Transformation），旨在改进系统结构的实践活动。<br />重构就好比用餐后对厨房的清理工作。<br /></li><li><strong>隐喻：它是将整个系统联系在一起的全局视图；它是系统的未来景象，是它使得所有单独模块的位置和外观变得明显直观。</strong><br /></li><li>即使没有完成所有的用户素材，迭代也要在先前指定的日期结束。可以把一些为完成的素材放到下一迭代，在保证本次迭代功能完整的前提下。<br /></li><li><strong>素材分解成开发任务，一个任务就是一个开发人员能够在4~16个小时之内能实现的一些功能。</strong><br /></li><li>烈火验真金，逆境磨意志。<br /></li><li>首先编写测试可以迫使我们使用不同的观察点。我们必须从程序调用者的有利视角去观察我们将要编写的程序。... 为了是程序成为易于调用和可测试的，必须和周边环境解耦。这样，首先编写测试迫使我们接触软件中的耦合（Forces us to decouple teh sofeware)<br /></li><li>测试就像一套范例，它帮助其他程序员了解如何使用代码。这份文档是可以编译的、可运行的。它保持最新，它不会撒谎。<br /></li><li>为了使验收测试无需通过用户界面就能够获得对于业务规则的访问，就必须要以满足这个目的的方式来接触用户界面和业务规则之间的耦合。...验收测试可以促使你在大的方面做出优良的系统结构决策。<br /></li><li><strong>测试最重要的好处就是它对于架构和设计的影响。为了使一个模块或者应用程序具有可测试性，必须要对它进行解耦合。越是具有可测试性，耦合关系就越弱。全面地考虑验收测试和单元测试的行为对于软件的结构具有深远的正面影响。</strong><br /></li><li>画一幅图来探究一个想法是没有错的。然而，画一幅图后，不应该假定该图就是相关任务的最好设计。你会发现最好的设计是在你首先编写测试，一小步一小步前进时形成的。<br /></li><li><strong>在按照我的理解方式审查了软件开发的生命周期后，我得出一个结论：实际上满足工程设计标准的唯一软件文档，就是源代码清单。... 源代码就是设计。</strong><br /></li><li>OCP是面向对象设计的核心所在，背后的主要机制是抽象和多态。<br />LSP是使OCP成为可能的主要原则之一。<br />DIP是面向对象设计的标志所在。<br /></li><li><strong>依赖倒置原则：接口所有权的倒置。我们通常会认为工具库应该拥有他们的接口。但是当应用了DIP时，我们发现往往是客户拥有抽象的接口，而他们的服务者则从这些抽象接口派生。<br />（客户定义了需要什么，服务提供者按照这个要求来提供服务）</strong><br /></li><li>事实上，包的依赖关系图和描绘应用程序的功能之间几乎没有关系。相反，它们是应用程序可构建性的映射图。...包的依赖关系结构是和系统的逻辑设计一起增长和演化的。<br /></li><li>用例：描述了用户所期望的系统行为。...用例图是用来进行人与人之间的交流的，主要是用于分析师和干系人之间的交流。它们有助于按照不同类型的系统来组织系统的功能。<br /></li><li><strong>Martin文档第一定律：知道迫切需要并且意义重大时，才来编制文档。</strong><br /></li><li><strong>领域模型（Domain）是一组图，这些图有助于定于出现在用例中的术语。这些图显示了问题中关键对象以及它们之间的关系。...领域模型是一种描述工具，用来帮助人们记录它们的决策以及相关之间的交流是非常重要的。领域模型中的对象未必对应面向对象的设计，这样的对应也没有多大的价值。...领域模型里面的类是问题领域中的概念元素，和软件类没有直接关系。...我们使用它们来和用户进行交流的，而不是为了说明软件结构。...两者是不同的<em>概念层次</em>。</strong><br /></li><li>通过创建一个领域模型，我们更好的理解了手边的问题。这种更好的理解有助于我们对用例进行改善和补充。这两者之间的这种迭代是自然的，也是必要的。<br /></li><li>架构指的是构成应用程序的骨架（Skeleton）的软件结构。<br /></li><li><strong>有些东西讲得简明扼要能够给人以智慧的启迪</strong></li></ul><p><img src="/img/敏捷开发.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 读书心得 </tag>
            
            <tag> 敏捷开发 </tag>
            
            <tag> Agile </tag>
            
            <tag> XP </tag>
            
            <tag> 极限开发 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《软件是这样炼成的--软件过程管理与软件测试》 读书心得</title>
      <link href="/2017/03/08/%E3%80%8A%E8%BD%AF%E4%BB%B6%E6%98%AF%E8%BF%99%E6%A0%B7%E7%82%BC%E6%88%90%E7%9A%84--%E8%BD%AF%E4%BB%B6%E8%BF%87%E7%A8%8B%E7%AE%A1%E7%90%86%E4%B8%8E%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95%E3%80%8B%20%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2017/03/08/%E3%80%8A%E8%BD%AF%E4%BB%B6%E6%98%AF%E8%BF%99%E6%A0%B7%E7%82%BC%E6%88%90%E7%9A%84--%E8%BD%AF%E4%BB%B6%E8%BF%87%E7%A8%8B%E7%AE%A1%E7%90%86%E4%B8%8E%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95%E3%80%8B%20%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p><a href="http://www.cnblogs.com/tiandi/p/3385053.html">配置库管理</a><br /><a href="http://www.51testing.com/html/13/n-98113.html">软件质量目标</a></p><p>这一册主要是简述了软件工程的改进，其实就是质量保证的方方面面过程域。<br />软件过程是一个为建造高质量软件所需完成的任务框架，软件过程改进包括下面四点：</p><ul><li>1、软件过程定义<br /></li><li>2、软件过程文档化<br />包括软件过程定义文档化和过程执行文档化两部分<br /></li><li>3、软件过程培训<br /></li><li>4、软件过程强制执行</li></ul><span id="more"></span><p><img src="/img/过程域.png" title="过程域脑图" /><br /><a href="/img/软件是这样炼成的--软件过程管理与软件测试（过程管理部分）.mmap">整理脑图--过程域（可下载）</a></p><p><img src="/img/质量保证.png" title="质量保证" /><br /><a href="/img/软件是这样炼成的--软件过程管理与软件测试（测试部分）.mmap">整理脑图--测试（可下载）</a></p><h1 id="文章摘录">文章摘录</h1><ul><li>木星（Jupiter）是太阳系八大行星中体积最大、自转最快的行星，从内向外的第五颗 ... 中文名: 木星; 外文名: Jupiter; 别 称: 朱庇特、岁星; 分 类: 行星; 发现者: 伽利略<br /></li><li>在配置管理系统中，基线就是配置项在其生命周期的不同时间点上通过评审而进入正式受控的一种状态，而这个过程被称为“基线化”。<br />每一个基线都是其下一步开发的基准。<br /></li><li>影响软件质量的本质问题不是技术问题，而是管理；管理方法也不是管理的核心，管理的核心是管理意识。<br /></li><li><strong>组织评审会是，必须有评审依据。(最好有评审检查表)</strong><br /></li><li>软件质量保证的核心工作是软件开发过程保证。质量保证的本质工作是软件过程执行状态的监督，质量保证该软件开发过程中的每个过程点按照既定的质量执行，并且按照不同的评审等级进行评审。<br />说白了就是制定标准（过程标准、模板等），然后组织评审。<br /></li><li>短视者亡，长视者兴。<br /></li><li>软件过程改进的4大要素是过程定义化、文档化、培训和强制执行。<br /></li><li>梳理工作流程。只有将流程梳理清楚，明确了各部门内部员工的工作职责，明确部门之间的协调关系和协调资源。<br /></li><li>企业组织设计的根本目的是为了实现企业战略任务和经营目标服务的。<br /></li><li>盈利合同、战略合同、战术合同。<br /></li><li><strong>个人注：合道也应该把做具体事情的考核，转化为业绩目标性的考核。</strong><br /></li><li>项目经理和部门经理侧重于工作态度和能力的考核，这些指标无法量化，只能采取评语考核的方式了，只有这样才能给予部门经理和项目经理充分的考核全力，有效管理自己的团队。量化的考核，可以是结合项目的情况进行奖励考核的时候进行！<br /></li><li>经营企业和居家过日子一样，花每一分钱都要精打细算，该花的钱一点也不少花，不该花的钱一点也不多花，无效的工作不干。<br /></li><li>风险识别的过程的目的是将事情的不确定性转变为明确的风险陈述，为风险分析提供依据。<br /></li><li>需求评审分为：全局评审（业务中高层，项目目标、核心功能），局部评审（业务中层及一线人员，业务流程），内部评审（开发人员、分析人员）<br />需求评审是项目评审中最高级别的评审。<br /></li><li><strong>要充分识别客户的需求和潜在需求，需求确认非常重要，需求双方都要务实，设计实现别让需求扩大化，严格规范需求变更控制流程，别忽视需求跟踪，别忽视需求跟踪...提炼客户需求的时候，采用'往前跨半步'的方式，满足客户现在以及最近的将来可能需要的需求以满足系统的灵活性，切忌追求更加抽象化、更加完美、盲目扩大需求范围。要知道，简单是美，适用的才是最好的。</strong><br /></li><li><strong>概要设计阶段的核心人物是确定类间关系、确定类的方法体、确定方法体的输入和输出、设计类的接口与规范，而详细设计要完成数据库的逻辑结构、物理结构和概念设计等相关内容...详细设计的设计单位是类，设计内容是方法体</strong><br /></li><li><strong>如果是小项目，概要设计和数据库设计合并成一个文档。（小项目一般没有详细设计）</strong><br /></li><li>要分析写的文档给谁看？写文档的意义和价值！<br /></li><li>软件开发过程，一般遵循以下几步执行：<br />1、确定软件生命周期<br />2、确定各里程碑<br />3、确定各里程碑目标<br />4、确定每个里程碑涉及哪些开发过程域<br />5、确定每个开发过程域要执行哪些活动（可剪裁）<br />6、确定这些活动遵循的标准、规程以及使用的模板<br />7、制定验收标准<br /></li><li>软件质量活动包括<br />1、软件质量保证过程域<br />2、软件配置管理过程域<br />3、软件测试过程域<br /></li><li>我认为敏捷开发适用Product而不太适用于Project。<br /></li><li>个人注：管理就是轨道和信号等。在你开车走的时候，轨道就规定了你开车的路线，相关的信号灯就是随时给你反馈信息和要求。<br />或者说，就是工程里面的工程监理。<br /></li><li>可以达不到目标，但是不能没有目标。<br /></li><li>每次变更都要考虑调整（进度、成本、质量）<br />1、项目计划<br />2、成本<br />3、质量保证、配置管理<br /></li><li>软件分层，不同层不同观点人开发，这样能保证不同层功能（服务）的可复用性...基于服务的架构，决定了编码的顺序是：创建数据库、实体Bean开发、会话Bean开发、业务逻辑开发和表示层开发，这是因为上一层是为下一层提供服务的。<br /></li><li>详细设计做得好的话，程序员仅仅是“码工”了。<br /></li><li>代码走查的核心任务是：规范性检查、代码BUG检查。<br /></li><li>测试普遍存在的问题：<br />1、测试人员配比不足，最好是1:1，至少要做到4~5:1<br />2、测试计划及进度受制于开发计划及进度<br />3、测试人员和开发人员的沟通不畅，处于对立的局面<br />4、测试工具的应用不好，特别是自动测试工具<br />5、测试人员的技术不足，深层次的代码逻辑和数据库缺陷发现不了<br />6、测试用例没有严格的科学依据，甚至没有用例，就是‘点点测试’<br /></li><li>软件测试过程的改进和优化：测试过程提升+技术提升。<br /></li><li>制定计划的时候，一定要考虑冗余时间，一般来说，项目成熟度较高的项目，冗余10%~30%不等。反之，成熟度较低的话，冗余时间要设定得高一点，以防延期带来的连锁的反应。<br /></li><li><strong>测试需求分析其实在某种程度上可以取代部分用例设计工作，也就是说我们如果进行了较为详细的测试需求分析工作，测试用例就可以不用那么详细的编写了。例如上面的业务场景分析部分，只要按照业务流程直接进行测试执行是没有问题的。（个人注：其实是测试用例编写的依据，有了这些，就像编码之前的详细设计一样，基本水到渠成了。）</strong><br /></li><li>测试需求分析+业务流程分析--&gt;用例设计</li></ul><h1 id="文中重要图片截图">文中重要图片截图</h1><p><img src="/img/矩阵式项目组.jpg" title="矩阵式项目组" /><br /><img src="/img/质量保证部职责说明表.jpg" title="质量保证部职责说明表" /><br /><img src="/img/软件开发组织模型图.jpg" title="软件开发组织模型图" /><br /><img src="/img/软件过程体系结构图.jpg" title="软件过程体系结构图" /><br /><img src="/img/文件命名规范.jpg" title="文件命名规范" /><br /><img src="/img/项目特征表.jpg" title="项目特征表" /><br /><img src="/img/临时会议.jpg" title="临时会议" /><br /><img src="/img/风险分析表.jpg" title="风险分析表" /><br /><img src="/img/软件架构中用例图与其它图的关系.jpg" title="软件架构中用例图与其它图的关系" /><br /><img src="/img/过程规范层次图.jpg" title="过程规范层次图" /><br /><img src="/img/项目组组织架构图（矩阵结构）.jpg" title="项目组组织架构图（矩阵结构）" /><br /><img src="/img/风险应对措施.jpg" title="风险应对措施" /><br /><img src="/img/工作进度报告.jpg" title="工作进度报告" /><br /><img src="/img/代码走查报告.jpg" title="代码走查报告" /><br /><img src="/img/软件测试W模型.jpg" title="软件测试W模型" /><br /><img src="/img/软件测试价值图.jpg" title="软件测试价值图" /><br /><img src="/img/软件价值的实施分析.jpg" title="软件价值的实施分析" /><br /><img src="/img/测试过程改进总体目标简要说明.jpg" title="测试过程改进总体目标简要说明" /><br /><img src="/img/测试内部存在问题及改进.jpg" title="测试内部存在问题及改进" /><br /><img src="/img/改进后的测试工作流程.jpg" title="改进后的测试工作流程" /><br /><img src="/img/软件测试分阶段改进工作.jpg" title="软件测试分阶段改进工作" /><br /><img src="/img/测试过程改进第一阶段工作改进流程.jpg" title="测试过程改进第一阶段工作改进流程" /><br />冒烟测试的用例由测试来提供，开发执行<br /><img src="/img/测试过程改进第二阶段工作改进流程.jpg" title="测试过程改进第二阶段工作改进流程" /><br /><img src="/img/测试过程改进第三阶段工作改进流程.jpg" title="测试过程改进第三阶段工作改进流程" /><br /><img src="/img/BUG定位与回放过程.jpg" title="BUG定位与回放过程" /><br /><img src="/img/验收测试功能、非功能内容.jpg" title="验收测试功能、非功能内容" /><br /><img src="/img/常见的测试标准.jpg" title="常见的测试标准" /><br /><img src="/img/业务测试分阶段图.jpg" title="业务测试分阶段图" /><br /><img src="/img/软件开发过程图.jpg" title="软件开发过程图" /><br /><img src="/img/独立功能测试需求分析过程图.jpg" title="独立功能测试需求分析过程图" /><br /><img src="/img/测试需求分析与测试用例设计方法的关系.jpg" title="测试需求分析与测试用例设计方法的关系" /><br /><strong>重点</strong><br /><img src="/img/功能测试用例.jpg" title="功能测试用" /><br /><img src="/img/非功能测试用例--性能测试用例.jpg" title="非功能测试用例--性能测试用" /><br /><img src="/img/非功能测试用例--易用性测试用例.jpg" title="非功能测试用例--易用性测试用例" /><br /><img src="/img/业务场景测试用例.jpg" title="业务场景测试用例" /><br /><img src="/img/单元测试覆盖率分析.jpg" title="单元测试覆盖率分析" /><br /><img src="/img/版本计划说明.jpg" title="版本计划说明" /></p><p><img src="/img/软件是这样炼成的中.png" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 软件工程 </tag>
            
            <tag> 软件过程 </tag>
            
            <tag> 测试 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《极简欧洲史》读书心得</title>
      <link href="/2017/02/28/%E3%80%8A%E6%9E%81%E7%AE%80%E6%AC%A7%E6%B4%B2%E5%8F%B2%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2017/02/28/%E3%80%8A%E6%9E%81%E7%AE%80%E6%AC%A7%E6%B4%B2%E5%8F%B2%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>一本宏观的简要的介绍欧洲历史的书，在看的过程中，也针对欧洲和中国的历史对应，结合网上的资料，整理了中西方历史对照表。<br /><a href="/img/中西方历史对照表.xlsx">中西方历史对照表</a></p><span id="more"></span><p><img src="/img/中西方历史对照图1.png" /><br /><img src="/img/中西方历史对照图2.png" /><br /><img src="/img/中西方历史对照图3.png" /><br /><img src="/img/中西方历史对照图4.png" /><br />按照本书的介绍，贯彻作者极简史的思想，其实从以下几张图基本看出本书的脉络和内容了。<br /><img src="/img/年代划分.jpg" /><br /><img src="/img/文明演变.jpg" /><br /><img src="/img/侵略.jpg" /><br /><img src="/img/罗马帝国年份.jpg" /><br /><img src="/img/田地制度.jpg" /></p><h1 id="文章摘要">文章摘要</h1><ul><li>文艺复兴和宗教改革都是向过去看齐的运动。<br /></li><li>中国文明始终没有中断，但是却是由于中国文明没有中断，因而没有反思的机会，也就没有文艺复兴、宗教改革、科学革命的那种再生的动力。<br /></li><li>全球化其实一直在进行中，只不过从前很慢，现在很快。<br /></li><li>大格局看历史。<br /></li><li>欧洲文明发端之初，组成的元素有三：<br />1、古希腊和罗马文化；<br />2、基督教；<br />3、对罗马帝国进行侵略的日耳曼蛮族的战士文化；<br /></li><li>欧洲是海洋文明，中国是大河文明。<br /></li><li>唯有答案简单，才可能近乎正确。<br /></li><li>凯撒的归凯撒，上帝的归上帝。<br /></li><li>耶稣把犹太人的道德教训转化成了宇宙大爱。<br /></li><li>有人尊保罗为基督教的鼻祖，因为耶稣死的时候，这个信仰还只是犹太人的家务事。保罗确凿的指出这是所有人的宗教，自此以后，基督教就成了一种世界性的宗教。<br /></li><li>罗马人要求的一点是除了自己的宗教之外，还必须对皇帝敬拜。但基督徒跟犹太人一样，说自己只能崇拜唯一的真神，因为无论如何都不肯把君王当做神一般看待。<br /></li><li>早期的基督徒拒绝服兵役，罗马人对他们心生疑忌，这就是原因之一。<br /></li><li><strong>君士坦丁大帝公元313年，赋予基督教合法地位。</strong><br /></li><li>并非所有东西都归国王所有，是欧洲政府思维的基石。个人注：中国是，普天之下莫非王土！<br /></li><li>文艺复兴：是重新找回古希腊罗马学术！摆脱教会教条的束缚，回归古典时期古希腊、罗马的学术及文化。传递出古典的东西是无与伦比的。<br /></li><li>人体是完美的，这个观念是希腊的发明之一。裸体像和裸露的身体是有分别的。裸体像本身展现的是丰富的力与美，它是一种恰到好处的状态；裸露的身体就只是没穿衣服而已，而且因为没穿衣服显得自曝其短。<br /></li><li>文艺复兴时期的米开朗琪罗所雕刻的大卫像是公认的完美人类形貌。<br /></li><li>保罗说，你只要相信耶稣基督就能得救。马丁·路德就是从这句话中得到启发，进行了宗教改革。圣经是唯一的权威。<br /></li><li>法国启蒙运动-- 理性+教育。社会有两股非理性的强大势力：教会和国王。启蒙运动一位推动者如此归结该运动的诉求：我希望看到最后一个国王被最后一个神父的肠子给绞死。<br /></li><li>法国启蒙运动的伟大成果，是汇整出一部百科全书。它的根本不同在于将理性用于一切事物，让知识领域里没有层级之分。上帝的条目是在D（神）和R（宗教）字首的条目下。<br /></li><li>浪漫主义运动崇尚感受、情绪以及所有强烈的情感。<br /></li><li>希腊的三大哲人：苏格拉底、柏拉图、亚里士多德，在哲学方面举足轻重。有人说过，整个西方的思想传统无非是柏拉图的注脚。<br /></li><li>苏格拉底认为一般人的意见并不具备理性基础，他们对雅典的直接民主提出严重的质疑。他指出，认识善变和无常的、优柔寡断的、浅薄无知的、容易被操弄的，而政治是一种精细的艺术，需要智慧和良好判断，这不是每个公民都拥有的特质。 （代议式民主应该好点）<br /></li><li><strong>苏格拉底的问答法；亚里士多德三段论。（大前提、小前提、结论）</strong><br /></li><li><strong>对一切事物充满质疑，人会迷失方向；我们不能光靠理性过日子，一定要靠风俗、习惯和宗教对个人指点迷津，才可能成就一个社会。</strong><br /></li><li>法国也起源与日耳曼民族。<br /></li><li>穆斯林也承认耶稣和耶稣之前的先知，但深信穆罕默德是世上最后一位先知，能指引大家走向唯一真神安拉的怀抱。<br /></li><li>欧洲人改采用海路，部分原因是路上的东通之路已经完全落在穆斯林的手里。<br /></li><li>高举手臂的罗马共和国的致敬礼很像纳粹的行礼动作。<br /></li><li><strong>哲学家约翰·洛克的著作《政府论》，初版于1690年，提出成立政府已与签订一纸商业契约无异。</strong><br /></li><li>权力的分立未能得到确立的宪法根本不能称为宪法。<br /></li><li><strong>屋顶呈圆拱状的罗马式建筑。</strong><br /></li><li><strong>最排他的民族主义，世称“法西斯主义”</strong><br /></li><li>到了1850年，英国已经有半数人都居住在城市里。 个人注：2015年，中国城镇化率达到56.10%！<br /></li><li><strong>苏联的国旗：榔头+镰刀，分别代表城市里的劳工和乡村劳工。</strong><br /></li><li><strong>1480年，中国明朝皇帝下令停止所有的海外探险和贸易。 而1492年，哥伦布初次航行到美洲。</strong>  </li></ul><p><img src="/img/极简欧洲史.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 欧洲史 </tag>
            
            <tag> 心得 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《软件是这样炼成的--从软件需求分析到软件架构设计》 读书笔记</title>
      <link href="/2017/02/20/%E3%80%8A%E8%BD%AF%E4%BB%B6%E6%98%AF%E8%BF%99%E6%A0%B7%E7%82%BC%E6%88%90%E7%9A%84--%E4%BB%8E%E8%BD%AF%E4%BB%B6%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90%E5%88%B0%E8%BD%AF%E4%BB%B6%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%E3%80%8B%20%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
      <url>/2017/02/20/%E3%80%8A%E8%BD%AF%E4%BB%B6%E6%98%AF%E8%BF%99%E6%A0%B7%E7%82%BC%E6%88%90%E7%9A%84--%E4%BB%8E%E8%BD%AF%E4%BB%B6%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90%E5%88%B0%E8%BD%AF%E4%BB%B6%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%E3%80%8B%20%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<p>一块大部头，是关于软件工程的一本理论联系实际的指导性质的书，本书最特别的地方是从头到尾都是以一个项目来作为案例，让读者容易理解各个环节之间的来龙去脉。还是不错的一本书，适合有一定基础的读者，相当于从实际案例出发，梳理了一遍整个知识体系。<br />按照作者的说法，<strong>需求开发告诉客户想干什么，架构设计阶段从软件的视角出发，架构了软件对象（或类之间的关系），详细架构设计阶段是根据某个类设计了其具体的设计实现，比如说：每个类的方法体、输入和输出、数据结构和算法、对数据表的操作以及状态改变。</strong><br /><a href="/img/软件是这样炼成的.mmap">读书笔记脑图下载</a><br /><span id="more"></span><br /><img src="/img/读书笔记脑图1.png" /><br /><img src="/img/读书笔记脑图2.png" /></p><h1 id="一需求调研">一、需求调研</h1><ul><li>需求关注的是客户的业务流程以及客户处理的数据信息。<br /></li><li>调研方式：问卷调查、面谈（到现场调研，不要开大会；收集业务资料；考虑异常）、组织讨论（跨职能部门的流程的确认）<br />资料：组织机构、岗位职责、流程、表单（授权）、报表。<br /></li><li>静态结构：组织架构、部门职责、岗位职责<br />动态结构：流程、运营节点描述、数据表单（如果是合计的项，需要说明数据项的处理逻辑）</li></ul><h1 id="二需求分析">二、需求分析</h1><p><img src="/img/需求分析总图.jpg" /><br /><img src="/img/用例事件流描述1.jpg" /></p><p><img src="/img/用例事件流图.jpg" /></p><p><img src="/img/用例之间关系.jpg" /><br /><img src="/img/参与者关系分析.jpg" /><br /><img src="/img/用例图.jpg" /><br /><img src="/img/类初选.jpg" /><br /><img src="/img/类操作.jpg" /><br /><img src="/img/领域图.jpg" /><br /><img src="/img/领域图1.jpg" /><br /><img src="/img/领域类图确认表.jpg" /><br /><img src="/img/用例领域类数据集数据项关系.jpg" /><br /><img src="/img/命名规范.jpg" /><br /><img src="/img/易用性分析.jpg" /></p><ul><li>需求分析：动态--用例，考虑人机交互；静态--领域分析，考虑用例操作的对象实体<br /></li><li>需求告诉我们做什么，至于怎么做事由架构决定的。<br /></li><li>业务流程图作为系统分析需求功能和程序流程的基础，业务对象作为编写领域类图和数据字典的基础；客户调研报告中的组织结构作为子系统划分的依据；岗位职责描述部分作为系统中参与者的设计依据。<br /></li><li><strong>用例图，使得客户在大脑中形成了一个系统的运行蓝图，让客户明白能够在将来的系统中扮演的角色以及在这个角色中要承当的责任和客户将运行哪些行为动作完成自己的工作。</strong><br /></li><li>用例：业务流程图（分顶层业务流程图、底层业务流程图）中的节点，（运行节点的描述）。<br /></li><li>对于复杂的用例，可以考虑适当的使用用例运行流程图补充描述。（基本事件描述流）<br /></li><li>用例补充：系统自动处理的用例、系统维护方面的功能。<br /></li><li>用例参与者关系分析；分类后作为确认的资料依据；录入的部分一般都是用例的角色。<br /></li><li>用例关系的确认最好不在业务报告中获取，最好的方法是在用例描述中获得。<br /></li><li>用例的参与者：<br />1、谁使用该系统的主要功能；<br />2、谁将需要该系统支持以完成其工作；<br />3、谁讲维护、管理该系统以及保持该系统处于工作状态；<br />4、系统需要哪些硬件设备才能有效运行；<br />5、与该系统交互的系统有哪些；<br />6、谁或什么系统对本系统产生的结果感兴趣；<br /></li><li>用例图描述的是系统该有哪些功能。一般抽取业务调研报告中的动词或者动词词组。<br />领域模型来自业务描述中的名词以及对名词的抽象。领域模型是一个分析模型。领域模型只考虑业务描述中涉及的实体以及实体之间的关系。反映的是系统结构的关系图！<br /></li><li>领域类图依赖：<br />1、原始数据表格；<br />2、用例分析报告中用例用到的对象。<br />然后归纳整理。<br /></li><li>用例描述的粒度把我是所要描述的用例至少有一个实体对象存在。领域类图描述是，所有的描述语言都采取中文描述法，数据属性采取的是中文描述方法，不允许使用计算机专业的语言描述数据属性，是用户能够方便理解的中文描述法。<br /></li><li>用例图和类图是两个无法独立的UML图，用例描述提供什么样的服务，而服务的核心（载体）是以领域类的形式表现的。<br /></li><li>数据字典的作用：保持一致性、共享性；例如用户和需求分析师、需求分析师与设计人员、不同的系统之间。<br />*****面向对象的分析方法中，我们在需求分析阶段没有明确说明要定义数据字典，大部分用领域图来替代。</li></ul><h1 id="三概要设计">三、概要设计</h1><ul><li><strong>建模就是从不同的视角来反映系统的不同侧面，各种视图是互补，以达到完整体现软件系统的目的。</strong>UML可以通过不同的视角完成软件的建模工作！<br /></li><li>架构是蓝图，蓝图采用4+1模型来分不同角度体现，用UML工具来生成对应的视图。</li></ul><h2 id="软件架构">1、软件架构</h2><p><img src="/img/软件架构总图.jpg" /><br /><img src="/img/软件架构总图1.jpg" /><br /><img src="/img/体系结构.jpg" /><br /><img src="/img/时序图筛选.jpg" /><br /><img src="/img/时序图.jpg" /><br /><img src="/img/状态图1.jpg" /><br /><img src="/img/组件图1.jpg" /></p><ul><li><strong>软件体系结构本质上讨论的是系统中不同的独立构建存放的位置问题，以及这些构建之间的通信方式。软件体系结构中一般包括通信构件、处理构件和数据构件。UML时序图、活动图、状态图属于逻辑视图，描述的是对象关系，但是，这些UML视图的目标是辅助代码的实现。代码最终以构件的形式存在于具体的物理设备上，这就是我们所说的体系结构。</strong><br /></li><li>软件设计系统风格是描述某一特定应用领域中系统的组织方式的惯用模式。<br /></li><li><strong>设计模式关注的是类与类之间的关系，是类关系层面上的设计。</strong><br />设计模式是一种思想，有些框架是建立在某种模式之上的，是为了完成设计模式而开发的中间件，可以降低开发成本提高开发效率。例如，MVC是一种设计模式，而Struts则是为了实现MVC模式而开发的框架。<br /></li><li>Struts关注点在表示层，Spring关注点在业务逻辑部分，而Hibernate关注点是数据存储部分。<br /></li><li>体系结构比分层设计更为高层一些。<br /></li><li>分层是表示将功能进行有序的分组。通过分层，可以限制之系统间的依赖关系，使系统可以更松散的方式耦合，从而更易于维护。<br /></li><li>层数越多，可以将每层分布在不同的机器上。<br /></li><li>蓝图-- 概要设计；施工图--详细设计<br /></li><li><strong>设计模式的核心原则是“开闭原则”，对扩展是开放的，对修改是关闭的。一个好的系统是在不修改源代码的情况下，可以扩展功能。实现开闭原则的关键就是抽象化，在开闭原则中，不允许修改的是抽象的类或接口，允许扩展的是具体的实现类，抽象类和接口在开闭原则中扮演着极其重要的角色。即要预知可能变化的需求，有预见所有可能已知的扩展。<br />针对接口编程，而不是针对实现编程。</strong><br /></li><li>一般狭义上的业务逻辑不包括数据持久化。<br /></li><li><strong>框架：就是半成品，开发人员做填空题。</strong><br /></li><li><strong>时序图针对某一用例中对象之间的活动顺序关系，但是有时时序图自身特点决定了系统分析的不完整性，特别是无法满足条件转换、分支和分叉等，活动图刚刚好满足了这样的要求。</strong><br /></li><li>活动图能够在类设计层面起到指导编码就足够了。<br /></li><li>包可以直接理解为命名空间、文件夹。<br /></li><li>组件图应该是用户手册的一部分，知道系统实施人员完成系统的软件部署。<br /></li><li><strong>在我们现实开发过程中，如果做到概要设计这详细程度已经足够了。</strong><br /></li><li><strong>按照正常情况，程序员应该将70%左右的时间花在程序异常处理中。一个没有考虑异常的系统，就如同没有消防通道的大楼一样。</strong><br /></li><li>程序异常包括：<br />1、编译错误；<br />2、运行时发生错误（可以预料，但不能避免）；<br /><strong>3、业务逻辑错误（例如出生日期不能大于当前日期）</strong><br /></li><li>异常的处理很大程度上揭示了其所基于架构的强度。<br /></li><li>输出数据不完整：对于某些系统来说，数据不完整可能比系统停止运行带来更大的损失。较为理想的方法是向输出设备写一些信息，声明数据的不完整性；另一种方法是先缓冲要输出的数据，准备好全部数据之后再一次性输出。<br /></li><li>所有异常都未必是用一个简单的弹出窗口或者一个小动作来处理，而是需要比较复杂的流程来完成，这是，就出现了另外一个概念“其它事件流”</li></ul><h2 id="数据架构">2、数据架构</h2><p><img src="/img/数据库设计与需求分析关系.jpg" /><br /><img src="/img/数据库设计总图1.jpg" /><br /><img src="/img/数据库设计总图.jpg" /></p><ul><li>实体关系建模是在解读需求分析报告中领域类图和数据集以及数据字典的基础上，分析并确定实体名称和具体内容，剔除冗余实体、冗余数据项、冗余元组、冗余属性和冗余属性值。结合数据库之设计理念，创建实体卡片，分析其实体关系，绘制实体关系图（ER图）。<br />领域类图的元素中包括了对这个领域类的操作，而实体关系图中不包含操作。实体关系图中的对象是建立在领域类的基础上，但是要要对领域类中的数据属性等进行优化，尽量做到减少数据冗余。领域类图是架构设计中时序图、活动图、状态设计图的基础。而实体关系图是数据库设计的基础。<br /></li><li>冗余分析：<br />1、表冗余<br />2、记录冗余<br />3、属性冗余<br />4、属性值冗余<br /></li><li><strong>实体关系图（E-R图）编制一般分两步走，第一步只考虑实体以及他们之间的联系；第二步考虑给定实体的属性，不同时考虑两者是为了让设计工作变得单纯一点。</strong><br /></li><li>用建筑设计来比喻的话，实体关系图其实就是建筑材料的挑选过程，也就是说根据用户需求，我们决定需要哪些材料以及这些材料配比关系，争取做到一点都不浪费不冗余。在实体关系建模过程中，我们其实就是对领域类和数据集以及数据项的优化过程，并且确定了他们之间的关系。<br /></li><li>在逻辑设计阶段，需要考虑主键、外键、考虑是否为空、是否符合业务规则，数据库范式的分析。<br />1、创建表<br />映射E-R实体为表、表关系描述、数据完整性（实体完整性--主键约束、参照完整性--外键、用户定义完整性--业务规则）<br />2、范式检查表结构<br />3、是否满足所有的业务<br />4、检查按业务规则<br /></li><li><strong>派生数据的设计，在修改源数据的时候很容易导致不一致的情况出现；<br />提取表（统计表）：修改源数据的时候，需要考虑冲账的方式，把修改的差异体现在下一个统计周期中！</strong><br /></li><li><strong>存储过程出发点：少量输入参数、大量输出数据的情况下，减少数据库服务器为其它应用程序提供服务而带来的网络负荷，提高数据库操作性能。</strong><br /></li><li>数据库安全：<br />1、管理制度<br />2、数据授权<br />3、数据库技术：存储过程、视图、加密等。</li></ul><h1 id="四详细设计">四、详细设计</h1><p><img src="/img/程序系统的结构.jpg" /><br /><img src="/img/界面.jpg" /><br /><img src="/img/界面事件.jpg" /><br /><img src="/img/实现类.jpg" /></p><ul><li><strong>领域类图，用例图都是从系统功能和结构的视角来分析系统。但是，这些视图都是围绕着系统的外部结构进行描述的。那么，要使得这个系统能够做好设计，能够为系统开发者提供足够的指导性设计，就必须从系统的本质开始描述，从一个用例的内部结构开始描述，时序图所描述的就是用例内部的对象之间的关系。</strong><br /></li><li>用户与系统的交互点是界面。<br /></li><li>详细的时序图能够全面表达用例对象交互的顺序额关系，还能够反映出参与者与用例之间的交互过程，而这些过程都反映数据元素的变化过程，从业务的角度考虑，数据元素表现最为完整的部分应该是时序图。<br /></li><li>数据结构：一个数据结构是由数据元素依据某种逻辑联系组织起来的。讨论一个数据结构必须同时讨论该类数据上执行的运算才有意义。通常，确定了数据结构之后，算法就容易实现了。<br /></li><li><strong>数据，而不是算法，是系统构造的关键因素。这种洞见导致了许多种软件设计方法和程序设计语言的出现，面向对象的程序设计语言就是其中之一。</strong><br /></li><li>算法主要的应用场景分为：算法密集型（搜索）、业务逻辑密集型（ERP）、体验密集型（游戏）<br /></li><li>优化：<br />1、多表之间的关联关系通过视图的方式生成单表视图，这样避免将大量无用数据读到内存中，占用内存资源。<br />2、数据库排序后再加载到内存<br />3、索引<br />4、尽量减少从数据库中提取数据记录数量，为了降低空间复杂度，在数据检索过程中，做到对数据提取的准确定位。<br />5、对于方法体返回的数组严格按照实际列宽数据元素数量长度进行定义，保证了开辟内存一点都没有浪费。<br /></li><li>详细设计文档要设计到每个类的具体方法！设计单位是类。设计内容是方法体。<br /></li><li>衡量索引效率的 95/5 规则：如果查询的结果返回的行数少于表中所有行的5%，则索引是检索数据的最快方法，如果查询的结果超过5%，那么通常使用索引就不是最快的方式。</li></ul><h1 id="文章摘录">文章摘录</h1><ul><li>小软件企业的通病：工期延误，成本增加，质量无保证，员工斗志下降，不断加班却没有效率，员工怨声载道，客户叫苦连天。<br /></li><li>QS：质量保证，就是软件过程的控制、管理规范及执行监控。<br /></li><li>一个公司需要考虑<strong>管理架构+技术架构</strong>；<strong>经营+财务</strong>；<br /></li><li>UML：类似AutoCAD，一种软件设计、建模的工具。<br /></li><li>世界的本质其实和类相似，就是对象，对象本身的属性、动作；以及之间的相关关系。<br /></li><li><strong>软件设计思想：变与不变分离（可重用），分层思想（降低耦合），抽象（降低复杂度）</strong><br /></li><li><strong>面向对象的方法是以认识论为基础，用对象来理解和分析问题空间，并设计和开发出由对象构成的软件系统（解空间）的方法。由于问题空间和解空间都是由对象组成的，这样可以消除由于问题空间和解空间结构上的不一致带来的问题。</strong><br /></li><li>聋哑对话<br />老虎吃天 ---- 无从下口<br /></li><li><strong>继承的主要目的是为了抽象而不是重用和功能扩展</strong></li></ul><p><img src="/img/软件是这样炼成的上.png" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 软件工程 </tag>
            
            <tag> 需求分析 </tag>
            
            <tag> 软件架构 </tag>
            
            <tag> UML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《君主论》读书心得</title>
      <link href="/2017/02/07/%E3%80%8A%E5%90%9B%E4%B8%BB%E8%AE%BA%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2017/02/07/%E3%80%8A%E5%90%9B%E4%B8%BB%E8%AE%BA%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>这本书在购书中心买的，在众多版本中，因为这本的纸质特别好，并且其中的插图很不错，所以下手了;-)<br />《君主论》就像是意大利版的厚黑学，作者的主要目的是把本书作为君主镜鉴（类似中国古代的《资治通鉴》），探究君主如果统治和维持自己的国家。<br />“君主必须武装起来，因为有武装的人服从没有武装的人是非常不合理的。”简单的一句话，颠覆了古典政治学中<strong>智者统治</strong>这一根本原则。这句话，和毛主席说的，<strong>枪杆子里出政权</strong>是一个道理，侧面说明了军队，或者说实力是一个君主的基础，而仁政并不是，就像三国最终获胜的是魏国而不是蜀国一样。<br />在历史上，总是宣扬德政的君主，但是，作者指出“作为君主，关于<strong>道德的确切地位</strong>是一个沉重的话题”，俗话说（增广贤文）：慈不掌兵，义不行贾。所以，对于君主来说，为了维持自己的国家，道德往往居于次要的地位。政治是险恶的，所以一个“想要在一切事情上都发誓许愿以良善自持”的人，终将在恶人环伺中走向灭亡。<strong>君主必须学会能够为恶，</strong>并依据必然性使用或者不使用这种能力。只要达到目标，总的来说成功者是不应该受责难的。就像作者说的“必须理解：一位君主，尤其是一位新君主，不可能遵守所有那些被认为是良善之人应该拥有的品性；因为为了维持他的国家，迫于必然性，他常常不得不背信弃义、毫无仁慈、不讲人道、违反神道。”<br />关于君主的品性问题，马基雅维利认为，君主最好是既受人爱戴又被人畏惧，但是如果为必然性所迫有所选择的话，<strong>还是被畏惧好一些</strong>。因为人们是否爱戴君主是自己做主，而他们是否畏惧君主则是君主做主。再说关于人类，一般的可以说：他们是忘恩负义、容易变心的。但是同时，<strong>君主还是要避免自己受人憎恨</strong>，所以君主必须做到不能剥夺他人的财产，就像作者说的“因为人们忘记父亲之死比忘记遗产的丧失来得还要快”（断人财路如杀人父母），所以，君主能够仅凭不祸害臣民而非给予他们利益便能赢得他们的忠诚。<span id="more"></span><br />关于君主守信的问题，作者认为，那些完成了伟大事业的君主们都很少考虑守信，都深谙如果以他们的诡诈把人们搞得晕头转向，并最终征服了那些立足于诚实守信的人们。因此，作为君主必须懂得，世界上有两种斗争方式：一种是运用法律，另一种是运用武力。第一种方式为人类特有，第二种方式则为野兽所持有；但是，因为前者常常有所不足，所以必须求助于后者...君主<strong>应当同时效法狐狸与狮子。狐狸能识别陷阱，狮子能使财狼惊骇。</strong>但是，为了达到更好的治国目的，君主必须深知如何掩饰这种兽性，必须做一个伟大的伪装者和假好人。<br />以上的两点，马基雅维利认为是作为君主最重要的两个品性。<br />同时，作者也提到在治理国家中，面临的各种问题的处理，简要摘录如下：</p><h1 id="关于军队">1、关于军队</h1><p>作者提出，君主应该有他自己的武装力量，不依赖雇佣军或援军...并且君主应当亲自出马，担任统帅之职。如果需要委派将领，通过法律加以制约，并且做到纪律严明、奖惩分明、公正。<br />在作者看来，世上最虚弱、最不牢靠的东西，莫不过于不以自己的力量为基础的关于权力的名望了。所以一位君主除了战争及其规章制度和训练之外，不应该有其他的目标、其他任何的想法，也不应该把其他任何事情作为他的技艺，因为这是发号施令者应当关心的唯一技艺...他永远不要让自己的事项离开军事训练问题，并且在和平时期他应该比在战争时期更加关注。就如：养兵千日，用兵一时。</p><h1 id="君主处事">2、君主处事</h1><ul><li>能给一个新进崛起的人带来巨大荣誉的，莫过于由他发现的新的法律和新的制度。<br /></li><li>当一位新君主上台，他应当感激那些曾经帮助他掌权的人并依赖他们吗？实际上不需要。一位新君主在他的朋友和同盟那里只有“半心半意的拥护者”，因为他们指望从他那里得到好处；正如我们已经看到的，安抚他先前的敌人----那些害怕失去一切的人----其实更好。<br /></li><li>只要可能的话，还是不要背离良善之道；但如果为必然性所迫，就要懂得如何走上为非作恶之途。<br /></li><li>我们可以得出另外一个值得注意的结论：<strong>君主应当把担待责任的事情委诸他人，而把施恩布惠的事情留给自己。</strong><br /></li><li>君主需要处理好大人物、人民、士兵的平衡关系。有时候这三者的利益是冲突的。<br />存在两种对立的脾性：渴望不受大人物支配和压迫的人民、渴望支配和压迫人民的大人物。...君主应当与人民结盟反对贵族。（个人注：借由人民的力量来限制、打击贵族）君主需要通过打压大人物来讨得人民欢心。（个人注：有点类似中国现在的反腐的效果。）<br /></li><li><strong>当一位君主是真正的朋友或者敌人时，也就是说，他毫不犹豫地公开表示自己支持某个人而反对另一个人，他也会受到尊敬。这种方法总是比保持中立更有用。</strong>因为胜利者不需要在逆境中没有援助自己的可疑朋友；失败者也不会庇护你，因为你不愿意拿起武器来分担他的机运。<br /></li><li>一位君主必须注意，绝不要为了进攻他人而同某个比自己强大的人结盟（会沦为棋子），除非如上所说，迫于必然性。<br /></li><li>如何面对阿谀奉承：当每个人都能对你讲真话的时候，他们就会缺乏对你的尊敬。因此，一个谨慎的君主必须采取第三种方法：在他的国家里选择一些明智的人，单独让他们享有对他讲真话的自由，但只就那些他询问的事情，而不是其他事情。<strong>君主可以开始在一定范围内征求意见，但是必须自己决策，决策之后坚定不移的执行。</strong><br /></li><li>尽管一个人应该学着既大胆果敢又小心谨慎，<strong>但总体上，还是应该大胆果敢一些。</strong>机运之神是一个女人，她“宁愿让果敢的人而不是冷漠行事的人征服”；因此，她是年轻人的朋友。但是果敢与鲁莽往往也只是一线之隔，成功了就是果敢，失败了就是鲁莽。<br /></li><li>关于机运：某些机会使这些人走了运，但是是他们卓越的德能使他们能够洞察到这种机会（机会只会垂青那些有准备的人）...并且能够察微知著的人只是少数人，所谓的识时务者为俊杰，难就难在识时务、洞察时势，例如诸葛亮在隆中就洞察到三分天下的大局。所以在机运之外，德能才是更为关键的，因为上帝并不包办一切，这样就不至于把我们的自由意志和属于我们的那部分荣耀夺去。<br /><img src="/img/机运与德能.jpg" /></li></ul><h1 id="文章摘录">文章摘录</h1><ul><li>政府的分类取决于获取的方式，而不是像柏拉图和亚里士多德设想的那样，取决于政府的目的或者结构。<br /></li><li>任何人的祖国都是由一次原始的占有和征服来界定，因此也总是可以做同样的重新界定。<br /></li><li><strong>佛罗伦萨马基雅维利的墓志铭：如此伟人无以铭之（to so great a name no praise in equal）。</strong><br /></li><li>词语：<br /><strong>僭 jiàn： 超越本分</strong>，古代指地位在下的冒用在上的名义或礼仪、器物：僭越。僭妄。僭伪（封建王朝称割据对立的王朝）。僭盗。<br /><strong>以邻为壑</strong> hè：拿邻国当做大水坑，把本国的洪水排泄到那里去。比喻只图自己一方的利益，把困难或祸害转嫁给别人。《孟子·告子下》：“是故禹以四海为壑。今吾子以邻国为壑。”<br /><strong>未雨绸缪：chóu móu 绸缪：紧密缠缚。天还没有下雨，先把门窗绑牢。比喻事先做好准备工作。</strong><br /><strong>覆巢之下，安有完卵</strong><br /></li><li>上有所好，下必甚（更厉害）焉<br /></li><li>在和平时期大胆果敢的人不能取得成功，在暴乱时期小心谨慎的人不能取得成功。<br /></li><li>深刻认识人民性质的人应该是君主，而深刻认识君主的人应该属于人民。<br /></li><li>对人们要么加以安抚，要么加以剪除；因为他们可以报复收到的轻微的伤害，却无力报复收到的沉重的侵害；所以，对一个人的侵害应当时无需害怕他实施报复的那种伤害。（个人注：避免走中间路线）。<br /></li><li><strong>罗马人在他们夺得的那些地区，很好的遵循了这些策略：他们派遣移民，安抚弱国但不让其势力增长，镇压强大的势力，不让强大的外国势力在那里赢得声誉。</strong><br /></li><li><strong>在患病之初，是治疗容易而诊断困难；但随着时间的流逝...它就会变成诊断容易而治疗困难了。</strong><br /></li><li>大自然创造了人类，使其能够欲求每个事物，却不能得到每个事物，如此一来，由于欲求总是大于获取的能力，结果就是对现在所拥有的不满意。（个人注：有多大的头戴多大的帽）<br /></li><li>人们绝对不应当为了避免一场战争而听任混乱继续，因为那非但不能避免战争，反而只能拖延战争而对你不利。（个人注：战略不能让位于战术）<br /></li><li>我们可以得出一条永远没错或者罕有错误的一般性规律：谁是促使他人强大的原因，谁就是自取灭亡。<br /></li><li>如果那些被征服的国家，如前所说，习惯于生活在他们自己的法律之下、生活在自由之中，那么想要保有这种国家有三种方式：其一是毁灭它们，其二是亲自前往生活在那里，其三是允许他们生活在自己的法律之下，要求他们进贡并在那里建立一个对你友好的寡头国家。<br /></li><li>人们几乎总是走在他人走过的道路上，效法他人的行动，尽管并不能完全沿着别人的道路或者或得你所效法的那些人的德能；然而，一个审慎的人总是应该追随伟大人物的足迹，效法那些最卓越的人，因此，即使他自己的德能到不到那样的程度，但至少有几分相像。（个人注：站在巨人的肩膀上。）<br /></li><li><strong>摩西是《旧约》中希伯来的先知和主法者，居鲁士是波斯帝国的奠基人，罗穆卢斯（Romulus）是传说中罗马的奠基人和第一位国王，提修斯是传说中雅典的国王和雅典国家的奠基人。</strong><br /></li><li>发现新的方式和制度总是与寻找未知的水源和土地一样危险。（个人注:变更的阻力）。<br /></li><li>如果他们依靠自己并且能够使用武力，那么他们就罕有危险。由此观之，所有武装的先知都获得了胜利，而没有武装的先知都毁灭了。<br /></li><li>人民的天性是容易变化的（个人注：见风使舵是人民的本性）。<br /></li><li>这些人完全依赖他人给予他们一个国家的意愿和机运，而这两者都是变化无常、极不稳定的。（个人注：<strong>靠山山会倒，靠人人会跑</strong>）<br /></li><li>任何人如果相信给予新的恩惠会使大人物忘却往日的损害，他就是自欺欺人。<br /></li><li><strong>如何使用残酷：“妥善的使用”是指，出于维护自身安全的必然性，一次性地使用残酷手段；其后，除非为臣民谋取更大可能的利益，绝不继续使用；“恶劣的使用”是指，尽管一开始很少使用残酷手段，但其后与日俱增，而非日渐减少。</strong><br /></li><li><strong>所有的损害必须一下干完，所以，他们感受的越少，积怨就会越少；而恩惠应该点滴赐予，以便他们能够更好的品尝承恩受惠的滋味。（降低预期）</strong><br /></li><li>对于那些完全依赖你机运的人，只要不是贪得无厌，你就应该赐予名誉并加以宠爱。<br /></li><li>人们如果从原来他们相信要受到他损害的那个人那里得到好处的话，他们一定会更加感激他们的施惠者。<br /></li><li><strong>当远离死亡之境的时候，每个人都准备为他赴死（个人补充：但面对死亡的时候，个个都做鸟兽散）</strong><br /></li><li>既要让他的臣民产生祸患不会长久的希望，也要让他们对敌人的残酷感到恐惧。<br /></li><li>这位教皇（西克斯图斯四世）在向世人展示了一个教皇究竟能肆意妄为到何种程度方面可谓前无古人。<br /></li><li>西方领导科学认为领导力的形成依赖三大要素：一曰恐惧，二曰利益，三曰信仰。恐惧迫使人们服从，利益引导人们服从，信仰则产生发自内心的服从。</li><li>后来亚历山大六世继位，在历代教皇中，他最充分地说明了一位教皇可以利用金钱与武力使自己的气焰达到何等程度之盛。<br /></li><li><strong>“创造亚当”是米开朗琪罗创作的西斯廷教堂天顶画“创世纪”的一部分。</strong><br /></li><li>人们进行判断，一般依靠他们的眼睛甚于他们的双手，因为每个人都能看到你，却很少有人能接触到你。<br /></li><li>这里有伟大的正义：“对于那些迫不得已进行战争的人来说，战争是正义的；当除了拿起武器之外就毫无希望的时候，武器是神圣的”。</li></ul><p><img src="/img/君主论.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 历史 </tag>
            
            <tag> 君主 </tag>
            
            <tag> 马基雅维利 </tag>
            
            <tag> 古罗马 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《当下的力量》读书心得</title>
      <link href="/2017/02/04/%E3%80%8A%E5%BD%93%E4%B8%8B%E7%9A%84%E5%8A%9B%E9%87%8F%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2017/02/04/%E3%80%8A%E5%BD%93%E4%B8%8B%E7%9A%84%E5%8A%9B%E9%87%8F%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>一本灵修方面的书，主体思想有点像佛教。告诉人们通过活在当下，来避免痛苦和恐惧。<br />这本书好的地方是在阐述的同时，间或插进一些提问进行回答，这些问题往往是读者在读书过程中也会碰到或者想到的问题。<br />概括这本书来说，就是阐述了以下三点：<br /><strong>1、活在当下！</strong><br />关于时间，作者认为，时间是过去的痛苦、未来的恐惧之载体。接纳当下，活在当下，就是忘记过去的不幸，摒弃对未来的恐惧，从时间中跳出来，摆脱时间的束缚。<br />就像作者说的，时刻提醒自己，<strong>注意，此时此地。注意，此时此地！</strong><br /><strong>2、停止思维！</strong><br />思维和对应体现出来的情绪是一个小我，但是我们真正的本体是意识。在作者观念中，<strong>思维是有罪的</strong>，所以要把这两者分离，让意识占主导地位，而不是受思维的控制。<br /><strong>3、知足常乐！</strong><br />喜悦是你内在宁静状态的关键部分，我们要学会接受万物无常的本质，只有这样才能找到宁静。<br /><span id="more"></span><br />下面是具体各章节的摘录。</p><h1 id="一痛苦的来源">一、痛苦的来源</h1><p>活在当下：人无远虑必有近忧，活在当下只能是一种心态，摒弃过去的痛苦，放下对未来的恐惧，针对当下做理性的长远的思考。<br />作者认为，我们身上有两个我：‘我’和‘自己’。其中只有一个是真实的。我们的大脑就是其中的一个小我，大脑通过思维来控制这我们，而另外一个我就是我们的本体，是我们的意思，是观察者。这两者需要分离，需要我们控制大脑的思考、控制情绪，回归到我们的本体。<br />小我是一个虚假的自我，它是我们无意识地认同于思维而产生的。对于小我来说，当下的时刻几乎不存在，只有过去和未来才是最重要的。<br />思维只是意识的一小部分。<br />创造性的突破来自于无念状态。（灵感就是在思维停止的那一刻获取力量。）<br />情绪在思维和身体的相遇处产生，它是身体对思维的反应。<br />现在人们看书往往是想从书中找到成功的钥匙而不是真理，这就是舍本逐末。<br />欢乐总是衍生于你之外的事物，而喜悦是由内而生的。<br />佛陀说，人类的痛苦源于欲望或贪婪，如果你要摆脱痛苦，你就必须摆脱欲望。<br />求不得是痛苦的根源！</p><h2 id="二摆脱痛苦的途径">二、摆脱痛苦的途径</h2><p>知足常乐，把你的生活重心完全放到当下这一刻。<br />接纳，然后才去行动。不管当下时刻的情况怎样，心甘情愿的接受它，就像它是你的选择一样。<br />有能力改变能改变的，有勇气接受不能改变的，有智慧区分两者。<br />恐惧其实和任何具体的、真正迫在眉睫的危险无关。心理上的恐惧总是源于“可能会发生的事件”，而非“当下正在发生的事件”。</p><h1 id="三深深地进入当下">三、深深地进入当下</h1><p>时间造就了过去和未来，而过去是痛苦的源泉、未来是恐惧的依赖。<br />从思维中去除时间，思维就会停止。<br />当下才真正的珍贵，它是唯一真正存在的东西。<br />有些人喜欢参加冒险性的活动，如爬山、赛车等，原因是这些活动迫使他们进入当下时刻，在这些高度紧张的时刻里，他们能从时间、从问题、从思维中解放出来。<br />苦难只有在时间中才能存在，在当下他无法存活。<br />把意识（本体）激发出来，作为观察者，导师般的存在。俗话说，当局者迷旁观者清，思维情绪就是当局者，而临在就是旁观者了。<br />当你的脑子里充满问题时，新的事物或问题的解决方案就无法进入你的大脑。<br />船到桥头自然直，所以，当下只需要考虑当下能处理的问题，问题转化成你当下需要处理的事情，而不是背负问题的压力，问题其实是事情而已。<br />在真正的紧急情况下，思维停止了，你完全临在与当下，被一种更为有利的东西接管了。这就是许多普通人突然能够做出令人难以置信的事的原因。<br />集中在做事的过程而不是结果。当你的注意力转下当下的那一刻，你会感觉到临在，宁静而平和。<br />身体形式有生有死，但是你意识到了处于该形式之下的永恒的东西。<br />人生意义、目的：心灵的平静、愉悦，这些完全可以通过精神层面获得，而不是寄托于物质。<strong>例如颜回，一箪食，一瓢饮，在陋巷。人不堪其忧，回也不改其乐</strong></p><h1 id="四思维逃避当下的策略">四、思维逃避当下的策略</h1><p><strong>衡量你意识水平的最好指标是：你如何应付生活中的挑战。其实也就是情绪管理，停止思维，活在当下，就相当于管理、控制了自己的情绪。</strong><br />当你在抱怨时，你就使自己变成了一个受害者。<br />由<strong>深刻观察而采取行动</strong>比消极心态引发的行动更为有效。<br />树立目标，享受过程，而不是老想着达到目标。<br />直面现在的问题，然后行动。问问你自己，你此刻有什么问题，而不是明年、明天或5分钟后，而是现在这一刻，你有什么问题？你可以应付当下发生的事，但是你却无法应付未来还没有发生的事情----也没有这个必要。在应付当下发生的事情时，你需要的答案、力量、正确的行动或者资源都会在那里，不是在现在之前或之后。</p><h1 id="五临在状态">五、临在状态</h1><p>本体、意识和生命是同义词。意识是上帝的本质通过生命形式的显现。当意识从身体和心理形式的认同中解放出来时，它就变成了我们所谓的纯意识或受过精神启蒙的意识。<br />当然，永恒不是指无止境的时间，而是指无时间。因此，耶稣成了救世主，一个纯意识的工具。</p><h1 id="六内在身体">六、内在身体</h1><p>被称为身体的这个密集的物质结构，受限于生、老、病、死，但这不是最终的真理----这不是真正的你。这是对你本质的误解，你的本质是超越生和死的。<br />就像太阳永远比烛光光明一样，本体中的智慧远比你的大脑来得丰富。<br />通过消除负面情绪来阻断它们对你能量场的破坏。</p><h1 id="七进入未显化状态的大门">七、进入未显化状态的大门</h1><h1 id="八开悟的爱情关系">八、开悟的爱情关系</h1><p><strong>真正的拯救就是了解到，你是那个滋生万物的无时间、无形式的至一生命的不可分割的而一部分。</strong><br />人是宇宙全体的一部分，就像某一个细胞在身体中的地位一样。只是说后来这个细胞有了独立意识。<br />爱情最伟大的催化剂就是完全接受你伴侣的一切，而不是去批判或以任何方式改变他或她。<br />你不是与黑暗作战，而是将光亮带进黑暗之中。不是对幻像做出反应，而是在发现幻像的同时洞察它。<br />跟一个开悟的人相处并不容易。跟开悟的伴侣在一起，为开悟的那一方的思维会深深受挫，因为没有东西来抵抗它们，也就是说它们会变得脆弱，并且还有全部瓦解的风险，从而导致了小我的丧失。</p><h1 id="九超越幸福和不幸">九、超越幸福和不幸</h1><p>生长通常被看成是积极的，但是没有东西会永远生长。如果任何形式的生长不断向前发展，最终都会变成怪物或变得具有毁灭性。有衰退才会有新的成长。生长与衰老两者相互依赖。<br />对于灵性开悟来说，向下的周期是绝对关键的。你必须遭受一定深度的痛苦或损失才能被灵性世界吸引。<br />起伏是常态，失败是成功之母。<br />喜悦是你内在宁静状态的关键部分。学会了接受万物无常的本质，因为找到了宁静。<br /><strong>鸭子也教过我重要的心灵课程，你会发现他们是何等宁静，何等安逸，何等完全地进入当下时刻。他们是如此的完美，这只有无思维的生物才能做到。</strong></p><h1 id="十臣服的意义">十、臣服的意义</h1><p>在臣服的状态中，你会清楚地看到你需要做什么，然后才去行动，一次只做一件事，一次将注意力集中在一件事上。<br /><strong>东方武术中深藏的智慧：因势利导，以柔克刚</strong></p><h1 id="文章摘录">文章摘录</h1><ul><li><strong>ABC理论，A是引发你情绪的事件，B是你的信念或你对事情的诠释，C就是结果，即你的负面情绪...B是你唯一可以完全航空和改变的因素。</strong><br /></li><li>佛陀将开悟简单定义为“受苦的终结（the end of suffering)”<br /></li><li><strong>笛卡尔：我思故我在。I think, therefore I am。</strong><br /></li><li>顿悟（satori)这个词描述短暂的开悟或短暂的无思维、完全临在的状态。<br /></li><li>你是否倾听过，真正地倾听过森林中的山泉的声音？或者你是否真正地倾听过在寂静的夏夜鸟儿的歌唱？当你的思维宁静时，你才会关注到这些。<br /></li><li>教学相长<br /></li><li>字句本身并不是真理，它们只是指向真理...蜂蜜这个词并不是蜂蜜，除非你尝试过蜂蜜，你才会知道它的味道。在你尝过它之后，蜂蜜这个词就变得对你不再重要了。（个人注：字句变成了沟通、交流的工具而已）<br /></li><li><strong>惊鸿一瞥（piē）：鸿，即鸿雁，也叫大雁。惊鸿：轻捷飞起的鸿雁。“惊鸿”一词多形容女性轻盈如雁之身姿，惊鸿一瞥意思是人只是匆匆看了一眼，却给人留下极深的印象。</strong><br /></li><li><strong>马克思说过，宗教是人民的精神鸦片。</strong><br /></li><li>个人注：哲学就是教会人们如何去面对生死及对未来的恐惧。<br /></li><li><strong>色即是空，空即是色。色：指一切有形的物质。</strong><br /></li><li>即使天空乌云密布，太阳也不会消失，它仍然在云层的另一边。<br /></li><li><strong>耶稣说：去爱你的敌人！</strong></li></ul><p><img src="/img/当下的力量.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 灵修 </tag>
            
            <tag> 活在当下 </tag>
            
            <tag> 修行 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《基于全生命周期的主数据管理MDM详解与实践》读书心得</title>
      <link href="/2017/01/20/%E3%80%8A%E5%9F%BA%E4%BA%8E%E5%85%A8%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%E7%9A%84%E4%B8%BB%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86MDM%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E8%B7%B5%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2017/01/20/%E3%80%8A%E5%9F%BA%E4%BA%8E%E5%85%A8%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%E7%9A%84%E4%B8%BB%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86MDM%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E8%B7%B5%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>一本纯粹堆积概念的书，没什么干货，就不具体写心得了。<br />下面简单的就主数据进行一些整理。</p><h1 id="产生的背景">1、产生的背景</h1><p>其实主数据因为是IT系统在发展过程当中，缺乏整体的规划（包括业务和技术），导致各部门自产自销，只关心本部门本系统的数据，导致各系统中重用的关键的基础信息编码及口径都不一致。</p><h1 id="处理流程">2、处理流程</h1><p>所以目前主数据管理首先要做的是各源系统的主数据合并和清理问题。主要的流程是“合并-&gt;清洗-&gt;审核-&gt;分发”；然后在实际运行过程当中，对新增、变更的部分进行管理，其中新增和变更可以在各个源系统维护，然后提交到MDM系统审核、分发，也可以在MDM系统统一维护，然后分发。</p><h1 id="分发技术">3、分发技术</h1><p>分发可以采用<strong>同步WebService接口</strong>，也可以采用<strong>JMS消息的异步机制</strong>。</p><h1 id="特殊说明">4、特殊说明</h1><p>有一些主数据，例如组织架构，虽然是主数据，但是变化频率还是很高。针对组织架构、项目的架构（期区、单位工程、楼栋）经常发生变化的情况，关键是业务数据必须绑定到原子层面上，如果在原子层面上没有变化，那么业务数据不需要重新绑定。但是，如果原子层面都发生了变化，那就需要调整了。如果原子层面的变化，前后是有映射关系，也可以通过系统来处理。<br /><span id="more"></span><br />下面是SAP针对组织架构数据的单独分类。</p><blockquote><p>数据类型的定义：<br />企业应用中较为通用的数据划分方式，是将数据定义为三种类型：系统数据 system data、主数据 master data、业务数据 biz data。<strong>而SAP在其系统上，将组织机构的数据(Organization data)分离出来，做为一个较为独立的概念</strong>，是不无道理的。而业务数据，也提出业务主数据的概念，进行中也区别对待。</p></blockquote><p><strong>系统数据</strong><br />系统数据是指用于系统自身正常运行所需要的数据，缺少这部分的数据、或者这部分数据的不完整都将造成系统不能运行、或运行不正常，并且这部分数据在开发的最初阶段就被基本确定下来，在以后的阶段中基本不会变化。（R/3中定义为 System data:It is the data which R/3 system needs for itself.）最为常见的就是系统的配置文件（例如系统菜单结构的数据，一般认为流程的配置也属于系统数据），对于Java、dotNet平台，通常倾向于使用的xml数据做为系统数据。<br />系统数据与业务在逻辑上，没有必然的关联性，是完全分离的。在很多业务系统中，容易混淆的是将人员数据、权限数据，还有所谓的“代码数据”（如：亲友的类型配置），认为是系统数据。</p><p><strong>组织机构数据</strong><br />组织机构数据，从字面上就可以完全理解了。它是指业务系统中，组织机构的配置的数据；它是在系统配置阶段，同时由客户进行定义而进入系统的，进入系统后，这部分数据就很少发生改变。( It is a customizing data which is entered in the system when the system is configured and is then rarely changed. )<br />从系统数据中划分出组织机构数据，有非常重要的意义。<strong>组织机构从本源上来说，同时具有主数据和系统数据的性质。但是组织机构数据有别于一般的主数据之处在于，“极少变化”组织机构数据，一旦发生变化（特别是层次关系的变化），对于整个系统中的业务关系，业务角色的职责分配，就会产生相应的变化。其变动的影响范围的大小，因一个系统的规模的大小，即系统应用到的业务范围的大小（是单独的物料系统，还是有关联的MIS，还是业务更紧密的ERP）的不同而不同。其中常见的影响较大的，将会是人员、角色、权限、流程的数据，而反映到系统业务层。在当前很多系统中流程与核心业务的掺杂的模式，还有权限与核心业务胶浊的情况下，这种影响都将是毁灭性的。</strong></p><p><img src="/img/基于全生命周期的主数据管理.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MDM </tag>
            
            <tag> 主数据 </tag>
            
            <tag> 组织机构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《大型网站技术架构--核心原理与案例分析》读书心得</title>
      <link href="/2017/01/12/%E3%80%8A%E5%A4%A7%E5%9E%8B%E7%BD%91%E7%AB%99%E6%8A%80%E6%9C%AF%E6%9E%B6%E6%9E%84--%E6%A0%B8%E5%BF%83%E5%8E%9F%E7%90%86%E4%B8%8E%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2017/01/12/%E3%80%8A%E5%A4%A7%E5%9E%8B%E7%BD%91%E7%AB%99%E6%8A%80%E6%9C%AF%E6%9E%B6%E6%9E%84--%E6%A0%B8%E5%BF%83%E5%8E%9F%E7%90%86%E4%B8%8E%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<h1 id="一大型网站架构演化">一、大型网站架构演化</h1><p>初始阶段的网站架构（单台服务器）-&gt; 应用服务器和数据服务分离（数据库和文件服务器都单独成一个服务器） -&gt; 使用缓存改善网站性能（本地缓存、分布式缓存） -&gt; 使用应用服务器集群改善网站的并发处理能力 -&gt;　数据库读写分离 -&gt; 使用反向代理（网站中心机房）和CDN（网络运营商那里）加速网站响应（缓存静态、热点内容）-&gt;使用分布式文件系统和分布式数据库系统（更常用的是业务分库） -&gt;使用NoSQL和搜索引擎 -&gt; 业务拆分 -&gt;分布式服务<br /><img src="/img/技术架构2.jpg" /><br /><span id="more"></span></p><h1 id="二大型网站架构模式">二、大型网站架构模式</h1><h2 id="分层">1、分层</h2><p>软件在横向方面进行切分<br /><img src="/img/技术架构3.jpg" /></p><h2 id="分割">2、分割</h2><p>软件在纵向方面进行分割。<br />业务拆分：大型网站为了应对日益复杂的业务场景，通过使用分而治之的手段将整个网站业务分成不同的产品线，分归不同的业务团队负责。<br />具体到技术上，也会根据产品线划分，将一个网站拆分成许多不同的应用，每个应用独立部署。应用之间可以通过一个超链接建立关系，也可以通过消息队列进行数据分发，<strong>当然最多的还是通过访问同一个数据存储系统来构成一个关联的完整系统。</strong></p><h2 id="分布式">3、分布式</h2><p>分层和分割的一个主要目的就是为了切分后的模块便于分布式部署，即将不同模块部署在不同的服务器上，通过远程调用协同工作。<br />1） 分布式应用和服务<br />将分层和分割后的应用和服务模块分布式部署。<br />2）分布式静态资源<br />3）分布式数据和存储<br />4）分布式计算<br />Haddoop的MapReduce分布式计算框架。</p><h2 id="集群">4、集群</h2><h2 id="缓存">5、缓存</h2><p>1）CDN：Content Distribute Network，内容分发网络。<br />2）反向代理<br />3）本地缓存<br />使用缓存的前提：一是数据访问热点不均衡；二是数据在某个时间段内有效，不会很快过期。<br />4）分布式缓存<br />网站数据库几乎都是按照有缓存的前提进行负载能力设计的，所以缓存的可用性至关重要。避免<strong>缓存雪崩</strong>这种故障</p><h2 id="异步">6、异步</h2><p>在分布式系统中，多个服务器集群通过分布式消息队列实现异步，分布式消息队列可以看做内存队列的分布式部署。<br />异步架构的典型是生产者消费者模式，两者不存在直接调用，<strong>只要保持数据结构不变</strong>，彼此功能实现可以随意变化而不受影响。<br />特性：提高系统可用性；加快网站响应速度；消除并发访问高峰。</p><h2 id="冗余">7、冗余</h2><p>冗余包括冷备份、热备份、灾备数据中心，另外其实集群也简介实现了应用的冗余。</p><h2 id="自动化">8、自动化</h2><p>发布过程自动化：自动化代码管理、自动化测试、自动化安全检查、自动化部署。<br />自动化监控：自动化报警、自动化失效转移、自动化失效恢复、自动化降级、自动化分配资源。</p><h2 id="安全">9、安全</h2><p>通过密码和手机校验码进行身份验证；Https对网络通讯进行加密；对存储的敏感数据加密（例如用户密码）；对敏感信息、垃圾信息进行过滤；对交易转账等重要操作进行风险控制。</p><h1 id="三大型网站核心架构要素">三、大型网站核心架构要素</h1><p>架构：最高层次的规划，难以改变的决定。这些规划和决定奠定了事物未来发展的方向和最终的蓝图。<br />软件架构：有关软件整体结构和组件的抽象描述，用于指导大型软件系统各个方面的设计。<br />性能、可用性、伸缩性、扩展性、安全是网站架构最核心的五个要素。</p><h2 id="性能">1、性能</h2><p>衡量性能的指标：<br />响应时间、TPS、系统性能计数器等。<br />性能优化的方法：</p><ul><li>浏览器端：可以通过浏览器缓存、页面压缩、合理布局页面、较少Cookie传输等手段优化；CDN、反向代理服务器等缓存热点文件；<br /></li><li>应用服务器端：本地缓存、分布式缓存；异步操作；集群等；<br /></li><li>代码层面：通过多线程、内存管理等手段优化；<br /></li><li>数据库服务器端：索引、缓存、SQL优化等。而NoSQL通过优化数据模型、存储结构、伸缩性等更有优势。</li></ul><h2 id="可用性">2、可用性</h2><p>网站高可用的主要手段是冗余，所以衡量一个系统架构设计是否满足高可用的目标，就是假设系统中任何一台或多台服务器宕机时，以及出现各种不可预期的问题时，系统整体是否依然可用。</p><h2 id="伸缩性">3、伸缩性</h2><p>所谓伸缩性是指通过不断向集群中加入服务器的手段来缓解不断上升的用户并发访问压力和不断增长的数据存储需求。</p><h2 id="扩展性">4、扩展性</h2><p>网站的扩展性架构直接关注网站的功能性需求，快速响应需求变化。核心是不同产品之间要解耦。<br />网站可扩展架构的主要手段是事件驱动架构（主要实现是消息队列）和分布式服务（业务和可复用服务分离开，通过分布式服务框架调用）。</p><h2 id="安全性">5、安全性</h2><h1 id="四瞬时响应网站的高性能架构">四、瞬时响应：网站的高性能架构</h1><h2 id="性能测试">1、性能测试</h2><p>网站性能测试的主要指标：响应时间、并发数、吞吐量、性能计数器等。<br /><img src="/img/技术架构4.jpg" /><br />性能测试是一个总称，具体可以细分为：性能测试、负载测试、压力测试、稳定性测试<br /><strong>性能测试</strong>：<strong>以系统初期规划的性能指标为预期目标</strong>，对系统不断施加压力，验证系统在资源可接受范围内，是否能达到性能预期。<br /><strong>稳定性测试</strong>：被测试系统在特定硬件、软件、网络环境下，给系统加载一定业务压力，是系统运行一段较长时间，以此检测系统是否稳定。<br /><img src="/img/技术架构5.jpg" /><br /><img src="/img/技术架构6.jpg" /><br /><img src="/img/技术架构7.jpg" /></p><h2 id="性能优化">2、性能优化</h2><h3 id="性能分析">1）性能分析</h3><ul><li>检查日志：检查请求处理的各个环节的日志，分析哪个环节响应时间不合理、超过预期；<br /></li><li>检查监控数据：检查监控数据，分析影响性能的主要要素是内存、磁盘、网络还是CPU，是代码问题还是架构设计不合理，或者系统资源确实不足。</li></ul><h3 id="性能优化-1">2）性能优化</h3><h4 id="web前端性能优化">（1） Web前端性能优化</h4><p><strong>浏览器访问优化</strong></p><ul><li>减少http请求：http是无状态的应用层协议，减少http请求数目可有效提高访问性能。可以通过合并CSS、合并JavaScript、合并图片来减少http请求。<br /></li><li>使用浏览器缓存：将CSS、JavaScript、Logo、图标这些更新频率较低的静态资源缓存在浏览器中，可以极好的改善性能。为了及时更新，可以修改该文件名字，然后在更新HTML中的引用。<br /></li><li>启用压缩：会对服务器和浏览器产生一定的压力。<br /></li><li>CSS放在页面最上面、JavaScript放在页面最下面：CSS放上面，方便页面渲染的时候，CSS能提前下载，JavaScript要看是否影响页面的解析情况而定。<br /></li><li>减少Cookie传输：太大的Cookie会影响数据传输。</li></ul><p><strong>CDN加速</strong><br />CDN本质仍然是一个缓存，而且将数据缓存在离用户最近的地方。所以CDN是部署在网络运营商的机房。CDN一般缓存一些静态资源：图片、文件、CSS、Script脚本、静态网页等。</p><p><strong>反向代理</strong></p><h4 id="应用服务器性能优化">（2）应用服务器性能优化</h4><p><strong>分布式缓存</strong><br /> <font color="MidnightBlue" size = "3px">网站优化第一定律：优先考虑使用缓存优化性能。</font><br />缓存主要存放那些读写比很高、变化很少的数据。一般第缓存设置时效时间，但会导致一定时间的数据不一致。<br />缓存本质是一个内存Hash表，网站应用中，数据缓存以一对Key、Value的形式存储在内存Hash表中。Hash表的数据读写时间复杂度是O(1)。Hash表是软件开发中常用的一种数据结构，期设计思想在很多场景下都可以应用。<br /><img src="/img/技术架构8.jpg" /><br />分布式缓存案例1--JBoss Cache：通常将应用程序和缓存部署在同一台服务器上，并且分布的缓存之间同步更新。<br /><strong>分布式缓存案例2--Memcached：</strong>优点是分布缓存之间互不通信（结合<strong>一致性Hash算法</strong>），使用简单的通信协议（TCP或UDP）和通信序列化协议（XML或JSON）。内存管理采用LRU算法。<br /><img src="/img/技术架构9.jpg" /></p><p><strong>异步操作</strong><br />使用消息队列将调用异步化。消息队列服务器处理速度远快于数据库，并且比数据库有更好的伸缩性。<br /> <font color="MidnightBlue" size = "3px">高并发时，任何可以晚点做的事情都应该晚点再做！</font></p><p><strong>使用集群</strong></p><p><strong>代码优化</strong></p><ul><li>多线程：充分利用CPU在磁盘IO或网络IO堵塞时候的效率。<br /></li><li>资源复用：包括单例（Singleton）和对象池（Object Pool），例如数据库的连接池、Web请求的线程池（Thread Pool）;<br /></li><li>数据结构：采用Hash表。字符串Hash散列算法有Time33算法。<br /></li><li>垃圾回收：</li></ul><p><strong>存储性能优化</strong></p><ul><li>机械硬盘 VS. 固态硬盘<br /></li><li>B+树 VS. LSM树：数据的索引存储结构；关系型数据库主要使用B+树，NoSQL采用LSM树。<br /></li><li>RAID VS. HDFS：RAID技术在传统关系型数据库及文件系统中使用比较广发，但在大型网站采用的NoSQL及分布式文件系统中，相当于间接实现了RAID的相关功能。HDFS以块（Block）为单位管理文件内容，默认是64M（操作系统默认是512字节）。</li></ul><h1 id="五万无一失网站的高可用架构">五、万无一失：网站的高可用架构</h1><h2 id="网站可用性度量与考核">1、网站可用性度量与考核</h2><p><strong>1) 网站不可访问原因</strong><br />DNS会被劫持、CDN服务器可能会挂掉、网站服务器可能会宕机、网络交换机可能会失效、硬盘会损坏、网卡会松掉、甚至机房会停电、空调会失灵、程序会有BUG、黑客会攻击、促销会引来大量访问、第三方合作伙伴的服务会不可用......</p><p><strong>2)网站可用量度量</strong><br />4个9 99.99%可用 一年大约最多53分钟不可用；<br />3个9 99.9%可用 一年大约最多9个小时不可用；<br />2个9 99%可用 一年大约最多88个小时不可用；</p><p><strong>3)网站可用性考核</strong><br />可用性指标对外是承诺，对内是考核。<br /><img src="/img/技术架构11.jpg" /></p><h2 id="高可用的网站架构">2、高可用的网站架构</h2><p>主要手段是数据和服务的冗余备份及失效转移。<br /><img src="/img/技术架构12.jpg" /></p><h2 id="高可用的应用">3、高可用的应用</h2><ul><li><strong>通过负载均衡进行无状态服务的失效转移</strong><br /></li><li><strong>应用服务器集群的Session管理</strong><br />Web应用中将这些多次请求修改使用的上下文对象称作为会话（Session）<br />有Session复制、Session绑定（通过Hash算法）、利用Cookie记录Session、<strong>Session服务器</strong>（实际上是将应用服务器的状态分离）。</li></ul><p><img src="/img/技术架构13.jpg" /></p><h2 id="高可用的服务">4、高可用的服务</h2><p>可服用的服务模块为业务产品提供基础公共服务。高可用的服务策略：</p><ul><li>分级管理<br /></li><li>超时设置<br /></li><li>异步调用<br /></li><li>服务降级<br /></li><li><strong>幂等性设计</strong><br />必须在服务层保证服务重复调用和调用一次的结果相同，即服务具有幂等性。<br />有些服务天然具有幂等性，例如将性别设置为男，不管设置多少次，结果都一样。但是对于转账交易等操作，问题就比较复杂，需要通过交易编号信息进行服务调用有效性校验，只有有效的操作才能继续进行。</li></ul><h2 id="高可用的数据">5、高可用的数据</h2><p>CAP原理，数据的一致性可以分为如下几点：</p><ul><li>数据强一致<br /></li><li>数据用户一致：在各个副本可能不一样，但是终端用户访问时，通过纠错和校验机制，可以确定一个一致且正确的数据返回给用户。<br /></li><li>数据最终一致：系统经过一段时间（通常是一个比较短的是时间段）的自我恢复和修正，数据最终会达到一致。</li></ul><p><strong>数据备份</strong>：数据异步热备（就是通常所说的Master-Slave同步机制、读写分离）<br /><img src="/img/技术架构14.jpg" /></p><p><strong>失效转移</strong>：如果存储不对等，那么就需要重新计算路由，选择存储服务器。</p><h2 id="高可用网站的软件质量保证">6、高可用网站的软件质量保证</h2><p><strong>1）代码控制</strong><br />分支开发、主干发布：目前在开源技术社区，Git作为版本控制工具，正逐步取代SVN的地位。<br /><img src="/img/技术架构15.jpg" /><br /><strong>2）预发布验证</strong><br />预发布服务器是一种特殊用途的服务器，它和线上的正式服务器唯一的不同就是没有配置在负载均衡服务器上，外部用户无法访问。单预发布验证操作的是真实的数据，可能会出现不可预测的问题。<br /><img src="/img/技术架构16.jpg" /><br /><strong>3）灰度发布</strong><br />每天只发布一部分服务器，观察运行稳定没有故障，再逐步发布完毕。这种手段也被称为<strong>AB测试</strong><br /><img src="/img/技术架构17.jpg" /></p><h2 id="网站运行监控">7、网站运行监控</h2><p> <font color="MidnightBlue" size = "3px">不允许没有监控的系统上线</font>，运维没有监控的网站，犹如驾驶没有仪表的飞机。</p><h3 id="监控数据采集">1）监控数据采集</h3><ul><li>用户行为日志收集<ul><li>服务器端日志收集<br /></li><li>客户端浏览器日志收集<br /></li></ul></li><li>服务器性能监控<br /></li><li>运行数据报告</li></ul><h3 id="监控管理">2）监控管理</h3><ul><li>系统报警<br /></li><li>失效转移<br /></li><li>自动优雅降级</li></ul><h1 id="六永无止境网站的伸缩性架构">六、永无止境：网站的伸缩性架构</h1><h2 id="网站架构的伸缩性设计">1、网站架构的伸缩性设计</h2><p>网站的伸缩性设计可分为两类，一类是根据功能进行物理分离实现伸缩，一类是单一功能通过集群实现伸缩。前者是不同的服务器部署不同的服务，提供不同的功能；后者是集群内的多台服务器部署相同的服务，提供相同的功能。</p><h3 id="不同功能进行物理分离实现伸缩">1）不同功能进行物理分离实现伸缩</h3><p>纵向分离（分层后分离）<br />横向分离（业务分割后分离）</p><h3 id="单一功能通过集群规模实现伸缩">2）单一功能通过集群规模实现伸缩</h3><p> <font color="MidnightBlue" size = "3px">当一头牛拉不动的时候，不要去寻找一头更强壮的牛，而是用两头牛来拉。</font></p><h2 id="应用服务器集群的伸缩性设计">2、应用服务器集群的伸缩性设计</h2><ul><li>HTTP重定向负载均衡<br /></li><li><strong>DNS域名解析负载均衡</strong>：许多DNS支持基于地理位置的域名解析，可将域名解析成距离用户地理最近的一个服务器地址。但因为DNS是多级解析，变化生效的时间较长。<br /></li><li>反向代理负载均衡<br /></li><li>IP负载均衡：在网络层通过修改请求目标地址进行负载均衡。<br /></li><li><strong>数据链路层负载均衡</strong>：在通信协议的数据链路层修改mac地址进行负载均衡。也称直接路由方式（DR）。集群素有服务器虚拟IP地址都和负载均衡服务器的IP相同。在Linux平台上最好的链路层负载均衡开源产品是LVS（Linux Virtual Server）</li></ul><p>负载均衡算法：轮询、加权轮询、随机、最少连接、源地址散列。</p><h2 id="分布式缓存集群的伸缩性设计">3、分布式缓存集群的伸缩性设计</h2><p>Memcached分布式缓存集群。<br /> <font color="MidnightBlue" size = "3px">计算机的任何问题都可以通过增加一个虚拟层来解决。</font>计算机网络的7层协议，每一层协议都可以看做是下一层协议的虚拟层；计算机操作系统可以看做是计算机硬件的虚拟层；Java虚拟机可以看做是操作系统的虚拟层；分层的计算机软件架构事实上也是利用虚拟层的概念。</p><h2 id="数据存储服务器集群的伸缩性设计">4、数据存储服务器集群的伸缩性设计</h2><p><strong>缓存+读写分离+业务分库</strong></p><h3 id="关系数据库集群的伸缩性设计">1） 关系数据库集群的伸缩性设计</h3><p><img src="/img/技术架构18.jpg" /><br />除了数据库主从读写分离，前面提到的业务分割模式也可以用在数据库，不同业务数据表部署在不同的数据库集群上，即俗称的<strong>数据分库</strong>。这种方式的制约条件是跨库的表不能进行Join操作。<br />数据分片：将一张表拆分分别存储到多个数据库中。主要的工具有Amoeba和Cobar。</p><h3 id="nosql数据库的伸缩性设计">2）NoSQL数据库的伸缩性设计</h3><p>NoSQL放弃了关系数据库的两大重要基础：以关系代数为基础的结构化查询语言（SQL）和事务一致性保证（ACID）。而强化其它一些大型网站更关注的特性：高可用和可伸缩性。主要的产品有Apache HBase。</p><h1 id="七随需应变网站的可扩展架构">七、随需应变：网站的可扩展架构</h1><p>减低软件系统耦合性是最关键的思考方式。<br /> <font color="MidnightBlue" size = "3px">软件架构师最大的价值不在于掌握了多少先进技术，而在于具有将一个大系统分成N个低耦合的子模块的能力，这些子模块包含横向的业务模块，也包含纵向的基础技术模块。这种能力一部分源自专业的技术和经验，还有一部分源自架构师对业务场景的理解、对人性的把握、甚至对世界的认知。</font><br />模块分布式部署以后具体<strong>聚合方式主要有分布式消息队列和分布式服务</strong>。</p><h2 id="利用分布式消息队列降低系统耦合性">1、利用分布式消息队列降低系统耦合性</h2><p>事件驱动架构：通过在低耦合的模块之间传输时间消息，以保持模块的松散耦合，并借助时间消息的通信完成模块间的合作。生产者消费者模式，分布式消息队列等。开源的分布式消息队列产品有<strong>Apache ActiveMQ</strong></p><h2 id="利用分布式服务打造可服用的业务平台">2、利用分布式服务打造可服用的业务平台</h2><p>纵向拆分相对比较简单，通过梳理业务，将较少相关的业务剥离，使其成为独立的Web应用。而对于横向拆分，不但需要识别可复用的业务，设计服务接口，规范服务依赖关系，还需要一个完善的分布式服务管理框架。</p><h3 id="web-service与企业级分布式服务">1）Web Service与企业级分布式服务</h3><p>Web Service 用以整合异构系统及构建分布式系统，通过SOAP（Simple Object Access Protocol，简单对象访问协议）和服务提供者通信，使用相关的服务。<br /><img src="/img/技术架构19.jpg" /></p><h3 id="分布式服务框架设计">2）分布式服务框架设计</h3><p>目前国内的开源分布服务框架是<strong>阿里巴巴的Dubbo</strong><br /><img src="/img/技术架构20.jpg" /></p><h2 id="可扩展的数据结构">3、可扩展的数据结构</h2><p>NoSQL的ColumnFamily（列族）</p><h1 id="八固若金汤网站的安全架构">八、固若金汤：网站的安全架构</h1><p>网站安全主要包括各种Web攻击和信息泄露（<strong>被黑客拖库</strong>）。</p><ul><li>全球大约70%的Web应用攻击都来自XSS攻击和SQL注入攻击。<br /></li><li>Referer Check：HTTP请求头的Referer域中记录着请求来源，可以通过检查来源，验证其是否合法。<strong>很多网站使用这个功能实现图片防盗链（如果图片访问的页面不是来自自己的网页就拒绝）</strong><br /></li><li>Web应用防火墙： ModSecurity<br /></li><li>信息加密：单向散列加密（MD5、SHA）、对称加密（DES、RC算法）、非对称加密（RSA算法）<br /></li><li>信息过滤和反垃圾：文本匹配、分类算法（贝叶斯分类算法）黑名单<br /></li><li>电子商务风险控制：规则引擎、统计模型<br />统计模型：目前主要使用，根据历史交易中的欺诈交易信息训练分类算法，然后将经过采集加工后的交易信息输入分类算法，即可得到交易风险分值。<strong>由于统计模型采用模糊识别，并不精确匹配欺诈类型规则，因为对新出现的交易欺诈有一定的预测性。</strong><br /><img src="/img/技术架构21.jpg" /></li></ul><h1 id="九案例分析">九、案例分析</h1><p>秒杀系统架构图<br /><img src="/img/技术架构22.jpg" /><br />为了保证系统的安全，保持适度的公平公正即可。即使系统出了故障，也不应该给用户显示出错页面，而是显示秒杀活动结束页面，避免不必要的困扰。</p><h1 id="十典型故障案例分析">十、典型故障案例分析</h1><p>软件设计有两种风格，一种是将软件设计得很复杂，以使其缺陷没有那么明显；一种是将软件设计得很简单，以使其没有明显的缺陷。</p><ul><li>写日志也会引发故障：硬盘空间的问题<br /></li><li>高并发访问数据库引发的故障：使用率高的查询数据进行缓存<br /></li><li>高并发情况下锁引发的故障：使用锁要谨慎；<br /></li><li>缓存引发故障：对缓存的管理要严格<br /></li><li>应用启动不同步引发故障：后台服务都要准备好，前台应用才能启动。<br /></li><li>大文件读写独占磁盘引发的故障：按照不同文件类型和用途进行管理和存放。<br /></li><li>滥用生产环境引发的故障：访问线上生产环境要规范。<br /></li><li>不规范流程引发的故障：代码提交前要用Diff命令进行代码比较（Code review）<br /></li><li>不好的编程习惯引发的故障：对Null进行检查。</li></ul><h1 id="文章摘录">文章摘录</h1><ul><li>我们永远无法像传统行业一样，去精确估算，并按预先精确设计好的图纸去完成我们的产品。<br /></li><li>传统的企业应用系统主要面对的技术挑战是处理复杂凌乱、千变万化的所谓业务逻辑，而大型网站主要面对的技术挑战是处理超大量的而用户访问和海量的数据处理；前者的挑战来自功能性需求，后者的挑战来自非功能性的需求。<br /></li><li>过度承诺<br /></li><li>NoSQL 和搜索引擎都是源自互联网的技术手段，对可伸缩的分布式特性具有更好的支持。<strong>应用服务器通过一个统一数据访问模块访问各种数据，减轻应用程序管理诸多数据源的麻烦。</strong><br /></li><li>LAMP技术：Linux+Apache+MySQL+PHP。<br /></li><li>是业务成就了技术，是事业成就了人，而不会相反。<br /></li><li>不能企图用技术解决所有问题，有时候需要从业务优化的角度来考虑。业务退后一小步，技术前进一大步。<br /></li><li>模式：“每一个模式描述了一个我们周围不断重复发生的问题及该问题解决方案的核心。这样，你就能一次又一次地使用该方案而不必做重复工作。”模式的关键在于模式的可重复性，问题与场景的可重复性带来解决方案的可重复使用。<br /></li><li>产品设计之初就需要一个明确的定位：什么是产品要实现的功能，什么不是产品提供的特征。在漫长的生命周期中，会有形形色色的困难和诱惑来改变产品的发展方向，左右摇摆、什么都想做的产品，最后有可能成为一个失去生命力的四不像。<strong>个人注：人生也是一样，对自己必须有正确的定位，有所为，有所不为。</strong><br /></li><li><strong>通信要考虑两方面，一是通信协议，是选择TCP还是UDP，抑或Http；二是通信序列化协议，数据传输的两段，必须使用彼此可识别的数据序列化方式才能使通信得以完成，例如XML、JSON等文本序列化协议。<br />类比：不同国家的人通信，意思要选择某种语言（例如英语），二是要选择载体，是通过电话（音频），还是书信（文本），还是视频（视频）等。</strong><br /></li><li><strong>Lucence:由Apache出品，Java开发的开源全文搜索引擎。</strong><br /></li><li>一定要坚信：一群优秀的人做一件他们热爱的事，一定能取得成功。<br /></li><li>寻找一个值得共同奋斗的目标，营造一个让大家都能最大限度发挥自我价值的工作氛围。<br /></li><li>大多数人，都比自己以为的更优秀，有些优秀需要在合适的环境中才能被激发出来，比如做一些有挑战的事，和更优秀的人合作，抑或拥有了超越自我的勇气。<br /></li><li><strong>蓝图应该写在软件架构设计文档的扉页、写在邮件的签名档，写在内部即时通信的公告上。</strong><br /></li><li>对于技术细节的争论应该立即验证而不是继续讨论。<br /></li><li>要想成就自己，就必须首先成就他人。<br /></li><li><strong>“鱼是最后一个看见水的”，天天面对这些问题，反而不觉得有什么问题。</strong><br /></li><li><strong>对于大多数应用来说，开源的MySQL数据库已经绰绰有余了，而我们还在使用昂贵的Oracle。</strong><br /></li><li>如何提出问题：<ul><li>把“我的问题”表述成“我们的问题”<br /></li><li>给上司提封闭式问题，给下属提开放式问题；<br /></li><li>指出问题而不是批评人；<br /></li><li><strong>用赞同的方式提出问题：我非常赞同你的方案，不过我有一个小小的建议......</strong></li></ul></li></ul><p><img src="/img/技术架构1.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 架构 </tag>
            
            <tag> 高性能 </tag>
            
            <tag> 高可用 </tag>
            
            <tag> 易伸缩 </tag>
            
            <tag> 可扩展 </tag>
            
            <tag> 安全 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo文章置顶</title>
      <link href="/2017/01/06/Hexo%E6%96%87%E7%AB%A0%E7%BD%AE%E9%A1%B6/"/>
      <url>/2017/01/06/Hexo%E6%96%87%E7%AB%A0%E7%BD%AE%E9%A1%B6/</url>
      
        <content type="html"><![CDATA[<p>在hexo github 的issue里找到了解决办法,解决Hexo置顶问题，只需两步：</p><ol type="1"><li>用文章中的js代码替换node_modules/hexo-generator-index/lib/generator.js (见下文代码段)</li><li>在需要置顶的文章的front-matter中添加top值，值越大越置顶。</li></ol><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">title: 某某文章</span><br><span class="line">date: </span><br><span class="line">tags:</span><br><span class="line">categories: </span><br><span class="line">top: 1000</span><br></pre></td></tr></table></figure><span id="more"></span><p>以下是最终的node_modules/hexo-generator-index/lib/generator.js <figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&#x27;use strict&#x27;</span>;</span><br><span class="line"><span class="keyword">var</span> pagination = <span class="built_in">require</span>(<span class="string">&#x27;hexo-pagination&#x27;</span>);</span><br><span class="line"><span class="variable language_">module</span>.<span class="property">exports</span> = <span class="keyword">function</span>(<span class="params">locals</span>)&#123;</span><br><span class="line">  <span class="keyword">var</span> config = <span class="variable language_">this</span>.<span class="property">config</span>;</span><br><span class="line">  <span class="keyword">var</span> posts = locals.<span class="property">posts</span>;</span><br><span class="line">    posts.<span class="property">data</span> = posts.<span class="property">data</span>.<span class="title function_">sort</span>(<span class="keyword">function</span>(<span class="params">a, b</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span>(a.<span class="property">top</span> &amp;&amp; b.<span class="property">top</span>) &#123; <span class="comment">// 两篇文章top都有定义</span></span><br><span class="line">            <span class="keyword">if</span>(a.<span class="property">top</span> == b.<span class="property">top</span>) <span class="keyword">return</span> b.<span class="property">date</span> - a.<span class="property">date</span>; <span class="comment">// 若top值一样则按照文章日期降序排</span></span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">return</span> b.<span class="property">top</span> - a.<span class="property">top</span>; <span class="comment">// 否则按照top值降序排</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span>(a.<span class="property">top</span> &amp;&amp; !b.<span class="property">top</span>) &#123; <span class="comment">// 以下是只有一篇文章top有定义，那么将有top的排在前面（这里用异或操作居然不行233）</span></span><br><span class="line">            <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span>(!a.<span class="property">top</span> &amp;&amp; b.<span class="property">top</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">return</span> b.<span class="property">date</span> - a.<span class="property">date</span>; <span class="comment">// 都没定义按照文章日期降序排</span></span><br><span class="line">    &#125;);</span><br><span class="line">  <span class="keyword">var</span> paginationDir = config.<span class="property">pagination_dir</span> || <span class="string">&#x27;page&#x27;</span>;</span><br><span class="line">  <span class="keyword">return</span> <span class="title function_">pagination</span>(<span class="string">&#x27;&#x27;</span>, posts, &#123;</span><br><span class="line">    <span class="attr">perPage</span>: config.<span class="property">index_generator</span>.<span class="property">per_page</span>,</span><br><span class="line">    <span class="attr">layout</span>: [<span class="string">&#x27;index&#x27;</span>, <span class="string">&#x27;archive&#x27;</span>],</span><br><span class="line">    <span class="attr">format</span>: paginationDir + <span class="string">&#x27;/%d/&#x27;</span>,</span><br><span class="line">    <span class="attr">data</span>: &#123;</span><br><span class="line">      <span class="attr">__index</span>: <span class="literal">true</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;);</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> 技术相关 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
            <tag> 置顶 </tag>
            
            <tag> top </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《长尾理论：为什么商业的未来是小众市场》读书心得</title>
      <link href="/2017/01/03/%E3%80%8A%E9%95%BF%E5%B0%BE%E7%90%86%E8%AE%BA%E4%B8%BA%E4%BB%80%E4%B9%88%E5%95%86%E4%B8%9A%E7%9A%84%E6%9C%AA%E6%9D%A5%E6%98%AF%E5%B0%8F%E4%BC%97%E5%B8%82%E5%9C%BA%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2017/01/03/%E3%80%8A%E9%95%BF%E5%B0%BE%E7%90%86%E8%AE%BA%E4%B8%BA%E4%BB%80%E4%B9%88%E5%95%86%E4%B8%9A%E7%9A%84%E6%9C%AA%E6%9D%A5%E6%98%AF%E5%B0%8F%E4%BC%97%E5%B8%82%E5%9C%BA%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<h1 id="一长尾市场的产生">一、长尾市场的产生</h1><p>在传统经济学中，80/20原则占了主导地位，那是因为现实世界中的货架是有限的，所以，能进到消费者视角的仅仅是销量最好的那20%的产品。<br />在工业化及互联网时代，在一个无限货架空间限制和其它供应商瓶颈的时代，面向特定小群体的产品和服务可以和主流热点具有同样的经济吸引力。这时候，传统经济学中两个重要的稀缺性函数--边际生产成本、边际销售成本，正在趋近于零，为长尾市场奠定了基础。从更广的角度来看，我们明显可见，长尾理论阐释的实际上是<strong>丰饶经济学</strong>。当我们文化中的供需瓶颈开始消失，所有产品都能被人取得的时候，长尾便会自然发生。<br />按照数学集合论的原理：一个极大的数（长尾中的产品）乘以一个相对较小的数（每一种长尾产品的销量），仍然等于一个极大极大的数。并且，通过互联网手段，企业可以方便的将大规模市场转化成无数的利基市场。从而，利基市场也能给企业带来大的利润，并而打破了传统经济学中的80/20原则。<br /><span id="more"></span><br /><img src="/img/长尾市场8.jpg" /><br />现在，长尾市场有有形产品，也有数字产品。有形产品只是把集中化的供给和分散化的需求联络在一起。目前有形产品到数字产品（也是<strong>从原子到字节的过程</strong>）的范围还比较小，但纯数字的零售，边际成本和销售成本几乎为零，这个的发展是无可限量的。不过随着现在3D打印技术的进步，有形产品也会慢慢的转化到无形产品来的。<br /><img src="/img/长尾市场5.jpg" /><br />同时，在长尾市场，因为产品的无限，所以，帮助用户找到自己需要的货品就是一个很重要的工作。而网络上天然的集合器功能，加上目前发展迅猛的搜索及推荐系统，让这一切成了可能。</p><p>所以说，<strong>长尾市场</strong>，概括为两句话：</p><ol type="1"><li>提供所有产品；</li><li>帮我找到它</li></ol><h1 id="二长尾市场的三要素">二、长尾市场的三要素</h1><p><img src="/img/长尾市场4.jpg" /></p><h2 id="生产普及内容">1、生产普及（内容）</h2><p><img src="/img/长尾市场1.jpg" /><br />根据幂律分布图，幂律曲线的值域无限接近但永远不会下降至零，它也被称为“长尾”曲线。在统计学中，这种形状的曲线被称为“长尾分布”，因为相对头部来说，它的尾巴特别长。我们把注意力集中在这条长尾上，把它转化成一个专有名词，就是“长尾理论”的来历。</p><h2 id="传播普及平台">2、传播普及（平台）</h2><p><img src="/img/长尾市场2.jpg" /><br />产品的无限话，需要有一个相对集中的展示地方，让消费者知道去哪里找到这些产品，需要让消费者了解到这些产品及相关的产品信息。<br />成功长尾集合器的法则：<br />存货：虚拟存货（提供平台，货物在其它合作伙伴那里）例如亚马逊、淘宝，数字存货：iTune<br />总包（Crowdsoucing）：企业原本需要花钱雇人做的事，用户却很高兴免费去做。维基百科，豆瓣的电影评论<br />传播途径：跨时空，多种方式<br />多产品：产品不同的组合、定制<br />多价格体系：不同的套餐、消费方式。<br />分享信息：产品详实的信息。<br />完整的内容：提供完整版，例如网络上不限于电影院的版本。<br />市场反馈：评测，形成正反馈<br />免费的力量：首先用免费或者低价吸引大批用户，然后说服其中的某些人升级为付费的高级用户。</p><h2 id="供需相连工具">3、供需相连（工具）</h2><p><img src="/img/长尾市场3.jpg" /><br />在一个无限选择的时代，统治一切的不是内容，而是寻找内容的方式。<br />为什么过滤器对长尾来说如此重要？原因很简单：如果没有过滤器，长尾有可能只是一个恼人的噪声源而已。搜索引擎应运而生，帮助人们从噪声中分离出来了一些有价值的信号。在网上，存货是“非排他性”的，换句话说，噪声依然存在，但Google允许你有效地忽略噪声。</p><h2 id="集合器">1） 集合器</h2><p>目前的集合器，从之前的统一化集合器，慢慢也发展出很多主体化集合器（垂直搜索）<br /><img src="/img/长尾市场6.jpg" /></p><h2 id="过滤器">2） 过滤器</h2><p>传统的商业采用的是事前过滤器，通过人工预测判断那些产品畅销，然后进行订购，但这些往往不可靠，所以商店里面出现了滞销品，占用了宝贵的货架空间。<br />而信息化时代是事后过滤器，只负责统计，例如搜索、推荐系统等，反映的是实际市场的心声，但他们也会疏导和放大消费者的行为。会出现反馈环，出现马太效应，赢者通吃的局面。<br /><img src="/img/长尾市场7.jpg" /></p><h1 id="三长尾市场的特点">三、长尾市场的特点</h1><h2 id="头部尾部的关系">1、头部尾部的关系</h2><p>虽然尾部很重要，但是，头部产品一般是作为尾部的起点，如果消费者找不到他们熟悉的条目，自然也就找不到搜索长尾的起点了。</p><h2 id="价格的考虑">2、价格的考虑</h2><p>思考：畅销品、滞销品谁的折扣多点？<br />1）对于需求市场<br />实体店应该是滞销品折扣多，因为占用货架、有存货成本，所以需要尽快卖出。畅销品因为买的人多，物以稀为贵，所以涨价。<br />但是在网上，因为存货成本几乎为0，所以不需要清理滞销产品，反而这些产品是利基市场，需要的人，往往对价格不敏感，所以也不需要降价。而畅销品，因为生产力是无限的，从走货的角度来说，可以通过折扣，来走量，获得更大的利益。例如亚马逊的策略就是如此。<br />2）对于愿望市场<br />需求越大，价格就越高。<br />例如老歌，许多唱片公司开始尝试着用折扣的方式推广一些老歌和不知名的新作。</p><h2 id="营销方式发生变化">3、营销方式发生变化</h2><p>在网络时代，评价体系发生变化：在网上，人们利用口头传播效应来创造需求，这种力量正在取代传统的营销方式。（口碑营销、熟人营销）<br />对于新一代消费者来说，一个公司怎样宣传它的品牌并不重要，重要的是Google搜索出来的条目怎么评价它。（例如豆瓣上的评价也一样）<br />所以，我们也需要适应这种变化，而不是仅仅通过公式的广告来推广，也需要重视网络的评价。</p><h1 id="文章摘录">文章摘录</h1><ul><li>大一统的文化只是例外，不是规则。<br /></li><li><strong>经济学的任务：力求用简单易懂的框架来描述真实世界的现象。</strong><br /><strong>经济学：一门社会科学，研究的是稀缺资源下的选择。亚当·斯密就是用时间（或便捷性）和金钱的权衡理论开创现代经济学的。从此，有限蛋糕的分割方法就成了经济学的核心问题。</strong><br /></li><li>“饮水机效应”指的是办公室里围绕某个大众文化事件的热烈讨论。<br /></li><li>下水道的最高排放量通常是在超级碗大赛（Super Bowl）的中场休息时测量到的。<br /></li><li>知道大热门的密码公式，把魅力四射的年轻男人卖给年轻的女人。成功的要点无非就是帅气的外表和打造出的个性，音乐本身几乎成了无关紧要的事。<br /></li><li>黄金唱片：销量超过50万张；<br />白金唱片：销量超过100万张；<br />超白金唱片：销量200万~1000万张；<br />钻石唱片：1000万张以上。<br /></li><li>电影启用大牌演员和导演，只不过是给一件不可预见的事情带来一点点可预见性。<br /></li><li>AC尼尔森公司（ACNielsen），又名：AGB尼尔森，是一家总部位于美国纽约市国际市场调查研究公司，北美地区总部坐落于伊利诺斯州绍姆堡。主要研究包括消费品市场的情况和动态、解决市场和销售问题，以及确定市场发展机会。在该行业是全球最大和最有名气的公司。<br /></li><li>反馈环：<br />分为正反馈环和负反馈环。正反馈环是指市场中的投资者受到分析师乐观的收益预测、通货膨胀回落及“货币幻觉”的影响等等，导致了投机性价格上升，最初的投资者取得了成功，从而吸引了公众的注意力，导致了价格的进一步上涨，因为通过投资者需求的上升，最初的价格上涨又反馈到了更高的价格中；负反馈环则与之相反。<br />反馈环理论不单适用于投资行为，其背后的原理更多凸显的是心理因素，<strong>受心理预期的影响，某种行为会在群体间得到放大，结果便是好的愈好，差的愈差</strong>，到了临界点，泡沫破灭，又会恢复原始状态，周而复始。<br /></li><li>邮购分类目录是互联网购物的前身。<br /></li><li><strong>宽客（Quant）宽客是现代金融市场的基础——金融衍生品的创造者。</strong> 另一种定义称：指一群靠数学模型分析金融市场的物理学家和数学家。他们相信数学的精确性是分析最复杂的人类活动的基础，还曾用分析神经系统的数学技巧来赚钱。<br />还有一种定义：指数量金融师，他们受过严格的科学训练，主要的工作是建立工作金融交易模型。<br /></li><li>马克思在《德意志意识形态》一书中提出，劳动--也就是被迫、非自发的有偿工作，将被主观积极性取代。物质生产为每一个人留下了从事其它活动的剩余时间。<br />（个人注：其实就像8小时之外的时间，是每个人自己支配的。）<br /></li><li>维基（Wiki）：夏威夷语单词，意思是快捷或迅速。<br />wikipedia上创建了一个堪比亚历山大大帝的古代图书馆相媲美的知识宝库。之前创建权威知识库一直就是学者们的事情，但wiki颠覆了这点，采用了一种开放式的集思广益。wikiepedia从混乱中创造出了秩序。<br />wiki有时候也只一种多人协作的模式。<br />例如：wikipedia：是维基百科； wikiLeaks：维基揭秘。两者没什么关系，只是两者都使用了协作的模式，都使用了Wiki的字眼而已。<br />维基百科的优点：<ul><li>时时更新的能力；</li><li>篇幅的无限性和视觉辅助（比如图片和图表）；</li><li>大量链接到其他资源的外部链接；</li><li>维基百科或许还能更好的显示不同的观点和争议之处。<br /></li></ul></li><li>P2P：peer-to-peer 对等，点对点；<br /></li><li>自己亲自发现的东西通常会给他们带来更大的满足感。<strong>默多克说过：年轻人不会等待某个神圣的数据来告诉他们什么东西是重要的，他们想控制他们的媒体而不是被媒体控制。</strong><br /></li><li>信息理论：这么学问研究的主要是一个信息采集的问题，从随机性的电子噪声中分离出连贯有序的信号。（信号-噪声比）<br /></li><li><strong>马尔萨斯就是零和游戏的支持者，他认为人口会以几何级数增长，而农业产出只能以算术技术增长。根据马尔萨斯的观点，食物匮乏最终会扼杀经济增长。</strong><br /></li><li>许多非商业书籍只是营销工具而已，目的是提高作者的学术声望，推销他们的顾问服务，为他们赚取演讲费，或者只是将他们的名号留在世上。从这个角度来看，自我出版并不是赚钱的一种方式，而是对其他人传达信息的一种方式。<br /></li><li><strong>自助出版网站Lulu.com，但在中国因为各种因素，是无法开展这个业务的。具体见阮一峰的文章《自助出版网站Lulu.com》</strong><br /></li><li>黑天鹅效应<br />《黑天鹅效应：如何及早发现最不可能发生但总是发生的事》（The Black Swan）是一本由纳西姆·尼可拉斯·塔雷伯（Nassim Nicholas Taleb）所著<strong>关于随机和不确定性</strong>的书。其典故来自当欧洲人首次接触到黑天鹅所引发的冲击而命名。<br />在18世纪欧洲人发现澳洲之前，由于他们所见过的天鹅都是白色的，所以在当时欧洲人眼中，天鹅只有白色的品种。直到欧洲人发现了澳洲，看到当地的黑天鹅后，人们认识天鹅的视野才打开，只需一个黑天鹅的观察结果就能使从无数次对白天鹅的观察中推理出的一般结论失效，引起了人们对认知的反思——以往认为对的不等于以后总是对的。<strong>“黑天鹅”隐喻那些意外事件：它们极为罕见，在通常的预期之外，在发生前，没有任何前例可以证明，但一旦发生，就会产生极端的影响。</strong><br /></li><li>帕累托原则（80/20法则）也是一种重要少数原则（Law of the Vital Few）<br /></li><li>电子商务：便利、便宜<br />实体店：体验好、社交需要（人是群居动物）<br />在物理世界中移动的是消费者，而不是产品，互联网相反。<br /></li><li>城市中各公司聚集在一起，也是为了利用密集人口所带来的生产率的优势、规模经济和知识外溢效应。<br /></li><li>今天的零售货架就是人类与产品供应链的互动界面。<br />超市中，中部神奇货架的销售力是底部货架的5倍还多。<br /></li><li>博客可以专注于某些特殊的主题，这样的专业化程度，媒体企业的记者是没有几个能做到的。因为记者越专业，公司要雇用得记者就越多，这样他们才能覆盖所有的领域。<br /><strong>自媒体</strong>的力量。<br /></li><li><strong>愤青： young contrarian</strong><br /></li><li>60年代我们学会了质疑权威，但并没有提供质疑权威的工具。现在我们掌握了这些工具-- 互联网<br /></li><li><strong>戴维·福斯特·华莱士：电视又粗俗、又下流、又愚蠢，并不是因为电视观众又粗俗、又下流、又愚蠢。电视之所以是这副样子，只是因为人们在那些粗俗、下流、愚蠢的兴趣爱好上极端的相似，但在那些优雅、美好、高尚的兴趣爱好上却又大相径庭。</strong><br /></li><li>eBay最初只是作为一个实验项目而创立的：试试看网上出售旧货是否强于现场兜售。<br /></li><li>浏览器同时扮演了统一用户界面和操作系统防护盾的角色。<br /></li><li><strong>Google的广告：自我服务模式、可衡量的效果、低进入成本和不断改进广告内容的能力。</strong><br /></li><li>他们既没有陶醉于过去所取得的成绩，也没有忘记它们必须遵循的传统。<br /></li><li><strong>以客户为中心</strong> 倾听消费者的声音。<br /></li><li>最大的机构，往往不愿意摒弃那些传统的、组织严密的，有指挥有控制的信息传递方式。<br /></li><li>公关的职责从对外关系转向了对内关系，从沟通交流转移到了培训员工如何有效地做好自我的宣传。<br /></li><li>超新星：<br />超新星是一颗爆炸的恒星，而且它的光度会短暂的超越整个星系，辐射出的能量如同太阳或普通恒星一生所辐射的总量。在它的光度衰退前，可以用裸眼看见几个星期或数月。这极端的亮度爆发的辐射会驱逐这颗恒星大部分或全部的物质，并以30,000 km/s（光速的10%）的速度驱动着激波进入周遭的星际物质。这个激波会清扫出一个膨胀的气体壳层，称为超新星遗迹的外壳。超新星是星系引力波潜在的强大来源。初级宇宙射线有很大的比例来自超新星。<br />恒星演化中的作用<br />超新星爆发后的遗迹包括一个中央的致密星体和因激波而快速向外扩散的物质。这些物质在快速膨胀的状态下扫过周围的星际物质，这种状态能够持续长达两个世纪。其后它们将经历一个绝热膨胀的过程，进而再用一万年左右的时间逐渐冷却并与周围的星际物质混合。<br />根据天文学中的标准理论，大爆炸产生了氢和氦，可能还有少量锂；而其他所有元素都是在恒星和超新星中合成的。超新星爆发令它周围的星际物质充满了金属（对于天文学家来说，金属就是比氦重的所有元素，与化学中的概念不同）。这些合成的金属丰富了形成恒星的分子云的元素构成[126]，所以每一代的恒星（及行星系）的组成成分都有所不同，由纯氢、氦组成到充满金属的组成。超新星是宇宙间将恒星核聚变中生成的较重元素重新分布的主要机制，不同元素的所有的分量对于一颗恒星的生命，以至围绕它的行星的存在性都有很大的影响。<br />膨胀中的超新星遗迹的动能能够压缩凝聚附近的分子云，从而启动一颗恒星的形成。如果气体云无法释掉过多的能量，增大的湍流压也能阻止恒星形成[10]。<br />在太阳系附近的一颗超新星爆发中，借助其中半衰期较短的放射性同位素的衰变产物所提供的证据能够了解四十五亿年前太阳系的元素组成，这些证据甚至显示太阳系的形成也有可能是由这颗超新星爆发而启动的。由超新星产生的重元素经过了和天文数字一样长的时间后，这些化学成分最终使地球上生命的诞生成为可能。</li></ul><p><img src="/img/长尾市场.png" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 管理 </tag>
            
            <tag> 市场 </tag>
            
            <tag> 长尾理论 </tag>
            
            <tag> 小众立场 </tag>
            
            <tag> 利基市场 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《一本书读懂财报》读书心得</title>
      <link href="/2016/12/26/%E3%80%8A%E4%B8%80%E6%9C%AC%E4%B9%A6%E8%AF%BB%E6%87%82%E8%B4%A2%E6%8A%A5%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2016/12/26/%E3%80%8A%E4%B8%80%E6%9C%AC%E4%B9%A6%E8%AF%BB%E6%87%82%E8%B4%A2%E6%8A%A5%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>3月份就在Kindle看过这本书，在Kindle看过的书，感觉效果不很好，最近接触到财务报表，又决定买一本纸质书来看了。<br />资产负债表、利润表、现金流量表是企业三张财务报表，这三张报表经常听到、看到，但是平时只知道皮毛，现在重看了一遍，很多事情就豁然开朗了。<br />资产负债表和利润表告诉我们公司的运营情况，现金流量表告诉我们公司的风险状况。通俗点概况：如果资产负债表和利润表说这个公司好，说明他赚钱；现金流量表说这个公司好，说明它有钱。<br />现金流量表既涵盖了企业经营活动的现金流，也包括了企业投资和融资活动的现金流，也就是说，他把企业的经营、投资、融资活动又重新描述了一遍。现金流是风险的视角；资产负债表是家底的视角；利润表是收益的视角。他们构成了一个有机的整体，完整的、不多不少地描述了一家公司的所有经济活动。<br />三张报表分别构建了两个体系，一个体系由现金流量表构成，另一个体系由资产负债表和利润表共同构成，这两者都记录了企业所有现金的流入和流出。但在现金流量表上，我们不需要考虑这个支出跟未来有没有关系；<strong>而资产负债表和利润表构成的体系当中，我们则以经济活动是否与未来有关未标准，将他们各自反应在两张报表上。</strong>实际上，两个体系描述了同样的经济活动，但他们各自描述经济活动的方式是不一样的。<br /><strong>资产和费用之间有一个很大的共同之处，就是都得花钱。但如果这笔钱可以换来一个对将来有用的东西，那它就是资产。如果这笔钱花完就完了，那它就是费用。</strong>在很多情况下，资产和费用只存在时间概念的不同，今天说的资产其实就是明天的费用（例如折旧费（或者成本），长期待摊费用等）。最初是一笔资产，但随着企业的正常运营和时间的流逝，最终变成了一笔费用，这就叫待摊费用。比如预付的房租、预付的广告费、开办费等（一般指超过一年的）。<br />书中提到一句话很好：会计是一个谨慎的行业！所以，在会计的过程中，从谨慎的角度去思考总是对的。<br />以上就是对三张报表的粗略理解，下面就是看书过程的一些记录。 <span id="more"></span></p><h1 id="一资产负债表">一、资产负债表</h1><p><strong>资产负债表：企业大单反相机（时点的概念）</strong><br />企业一辈子其实只做了三件事：经营、投资、融资；<br />与历史成本相对的，用当前的市场价格来计价的，会计们会把它称为“公允价值”。如果一个东西的市场价格全世界都看得到，没有任何争议，它就可以用公允价值来计价。除了金融资产和房地产（投资性房地产）之外，所有的大多数资产都是按照历史成本来计价。<br />历史成本的两层含义：<br />1、只有花了的钱才能记在账上；<br />2、<strong>增加资产价值的唯一途径就是发生一个新的交易；</strong><br />股东权益：公司中总资产扣除负债后所余下的部分，也称为净资产。<br />在中国，股本必须等于注册资本。当股东实际投入的资金比注册资本多，那么多出来的这块，就是所谓的资本公积。（上市公司一定有资本公积）<br />股本金额就等于公司的注册资本，这就意味着，股本的总额体现了这个公司对外承担法律责任的上限，而股本的组成则确定了多个股东之间权利义务的关系。因此股本具有非常重要的法律意义。<br />资产负债表：由资产、负债、股东权益三部分组成；<br />资产负债表的左边告诉我们，钱被拿去做什么了；右边告诉我们，企业的钱是从哪里来的；让公司的股东了解自己的家底，股东必须了解自己投入的资金都去了哪里，又欠了谁的钱。<br /><strong>资产等于负债加上股东权益就是资产负债表上最基础的逻辑关系，也是整个会计学当中最基础的逻辑关系，我们称之为“会计恒等式”。</strong></p><h1 id="二利润表">二、利润表</h1><p>利润表主要用来描述企业的经营活动。企业在经营过程中支付的各种费用、支付的所得税以及研发支出等。这些经济活动只与经营有关，所以利润表是主要描述经营活动的一种财务报表。<br /><strong>利润表就是给这家公司的盈利状况录了一段视频。</strong><br />所得税：是企业赚到钱才需要交纳；<br />流转税：无论是否赚钱，只要有业务的企业都得交纳，常见的流转税有营业税和增值税。<br />营业税：价内税；会在利润表中出现<br />增值税：价外税；一种销售税，属于消费者承担的税费，属累退税。不会在利润表中出现，增值税要交给税局的，所以体现在资产负债表中的应交税金这一项目中。<br />营业税及附加：营业税的附加税费包括城市维护建设费、教育附加等。<br /><strong>期间费用：营业费用、管理费用、财务费用</strong><br />营业费用：在生产和销售过程中产生的费用就是营业费用。例如运费、仓储费、广告费、<strong>销售人员的工资</strong>、门店的租金或折旧等。<br />管理费用：与企业的管理环节的有关的一切费用。例如<strong>管理人员的工资</strong>、行政办公费用、办公楼的折旧等。<br />（房地产中工程人员的工资属于成本）<br /><strong>毛利=营业收入-营业成本</strong><br />税法规定：当公司的广告支出超出了营业收入的15%之后，超过的部分就不能在（所得）税前列支了。<br />固定成本：厂房设备的折旧是一种固定成本。</p><h1 id="三现金流量表">三、现金流量表</h1><p>现金流量表本来就是一张流水账单。<br />现金流量表向我们展示了资产负债表上货币资金增减变化的原因。<br /><img src="/img/财报1.jpg" /></p><h1 id="四表间逻辑">四、表间逻辑</h1><p><strong>利润表和资产负债表最直接、最表面的联系：利润表中的一部分利润有可能被归入资产负债表中的未分配利润这一项，未分配利润将两张表联系在一起。</strong><br /><strong>通过设计不同的组织架构，来实现不同的会计结果：</strong>例如：研发支出应该被记录在管理费用中，但是，外购技术（说明该技术的商业价值得到了认可）的支出是记在无形资产中。所以，可以把研发部门作为独立的法人，母公司采购研发公司的技术，这时候，就可以记录在无形资产中了。<br />在经营活动中，应收账款、应付账款等项目，他们会被列在资产负债表中。<br />但是，资产负债表虽然与经营活动有那么一点点的关系，但和投资、融资才算是真正的血亲。<br /><strong>任何时间利润和现金流的差异都正好是非现金资产和负债的变化，同时，这也是三张报表最为重要的内在联系。</strong>，例如，购买50万的资产，现金流量表有50万的现金流出，同时，在固定资产一项中有记录50万的固定资产；但是在利润表中是没有体现的。这样，现金流量表和利润表的差额就多出50万。<br />然后，设备使用第一年，产生5万的折旧（入成本或者费用），这时，利润表多了5万的费用。这时，利润表和现金流量表又不一样了。而两者的差异正好是企业资产负债表上减少的那部分非现金资产。</p><p>当一个企业遭遇巨大风险的时候，关注风险是第一的，现金流对他来说更重要；当企业经营搞活动风险在相对可控的范围时，利润就显得更为就重要。经济形势不好的时候，很多企业会特别关注现金流；而经济形势好的时候，他们又会格外关注利润。<br /><strong>不同人关注三张的角度：</strong></p><ul><li>银行：关注现金流量表<br /></li><li>投资人：关注利润表；<br /></li><li>收购方：关注资产负债表</li></ul><h1 id="五财务报表的分析方法">五、财务报表的分析方法</h1><ul><li>1）同型分析<br />各项占比，让企业更加了解财务报表的结构；同时，可以进行趋势分析、比较分析（同行）；<br /><img src="/img/财报2.jpg" /><br /></li><li>2）比率分析<br />利润率：毛利率、净利润率<br />总资产周转率：收入÷总资产<br />效益：净利润率=净利润÷收入<br />效率：总资产周转率=收入÷总资产<br />总资产回报率（ROA）=效率*效益=净利润÷总资产<br />企业的短期偿债能力：流动比率=企业流动资产÷流动负债；或者更谨慎的方法，速动比率=（流动资金- 存货）÷流动负债<br />企业的长期偿债能力：资产负债率=企业负债总额÷企业资产总额<br /></li><li>3）现金流分析</li></ul><p><img src="/img/财报3.jpg" /><br /><img src="/img/财报4.jpg" /></p><h1 id="六影响财务数据的因素">六、影响财务数据的因素</h1><p>1、外部环境存在差异<br />2、企业的战略定位<br />分为：1）成本领先战略型。体现在毛利率低；毛利只受企业战略定位的影响，净利润还收管理水平、营销模式等因素的影响。2）差异化战略型<br />3、战略执行能力</p><h1 id="七什么样的企业才是好企业">七、什么样的企业才是好企业</h1><p>好企业应该是赚钱的，赚钱是企业存在的终极目标。<br />机会成本：为了得到某种东西而要放弃另一些东西的最大价值；也可以理解成在面临多方案择一决策时，被舍弃的选项中最高价值者是本次决策的机会成本。<br /><strong>行业平均盈利水平是衡量企业用股东钱的成本（而不是银行利息），就是所谓的加权平均资本成本。</strong><br />不过需要考虑“利息的税盾作用”<br /><strong>一家公司的净利润大于0，但是经济利润小于0，那么这并不是一家赚钱的公司。</strong><br />贴现：那些钱相当于现在的多少钱，这个过程就叫做“贴现”。也叫现值。用现值-成本就是净现值。<br /><img src="/img/财报5.jpg" /></p><p><img src="/img/一本书读懂财报.png" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 读书心得 </tag>
            
            <tag> 三张 </tag>
            
            <tag> 财务报表 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《人类简史-从动物到上帝》读书心得</title>
      <link href="/2016/12/19/%E3%80%8A%E4%BA%BA%E7%B1%BB%E7%AE%80%E5%8F%B2-%E4%BB%8E%E5%8A%A8%E7%89%A9%E5%88%B0%E4%B8%8A%E5%B8%9D%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2016/12/19/%E3%80%8A%E4%BA%BA%E7%B1%BB%E7%AE%80%E5%8F%B2-%E4%BB%8E%E5%8A%A8%E7%89%A9%E5%88%B0%E4%B8%8A%E5%B8%9D%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>作者说得好“我们之所以研究历史，不是为了要知道未来，而是要拓展视野，要了解现在的种种绝非‘自然’，也并非无可避免。未来的可能性远超出我们的想象。......历史总是这样从一个岔路走到另一个岔路，选择走某条路而非另一条的原因总是神秘而不可知。”<br />就像书中所说，人类从一种食物链中端的生物，发展到现在类似上帝的存在，其中的每个关键步骤都是超乎当时人们的想象的。</p><p>本书从智人的认知革命说到了现代科学革命。智人开始发展出新的思维和沟通方式（新的语言），这种认知的革命，开始让智人站上了生物链的最顶端。认知，让智人能想象出现实世界中不存在的事物以及虚构的故事、让智人能够组织更大规模的合作和创新，就这样，智人扩展到了整个地球并主宰了地球。<br /><span id="more"></span><br />农业革命的到来，人类第一次不再让自然牵着鼻子走，人类能在小范围内做到了人定胜天。人们种植农作物、饲养家禽，人们丰年储存粮食、食物以备来年之用。在这过程中，衍生出初级的人类分工，人类在组织层面达到了更高的水平，人类从部落的形式一步步进化到了城邦、国家。</p><p>工业革命的到来，让人类发出了上帝已死的最强音。人类慢慢站到了上帝的视角环视我们的地球。并且，我们的视角再也不仅局限于地球，而是投向了浩瀚的星空。<br /><strong>工业革命的核心，其实就是能量转换的革命。</strong>工业革命，让人类对能量的控制达到了前所未有的水平。从单纯的能量利用，变成了能量转换、存储再利用等，人类能力得到飞跃的发展，从而跳出了自然力量的控制，人类第一次真正体验到了人定胜天的滋味。</p><p>工业革命之后，资本、科学、帝国，三者相互作用，让人类的科学水平、文明水平、生活水平得到最大程度的提升。人类不再满足于自身的自然定位，而是努力克服人类生物性的局限，医学水平的发展使人类对抗疾病、对抗自然衰老的能力大大增强；科学水平的发展，使人类借助科学的力量突破了空间及时间的影响；尤其是现代AI技术的发展，让人们开始思考，将来的世界是否会出现超越人类的智能生命出现？这个奇点出现之后的任何事情，都超出我们现在所能想象的。到时人类如何定位、如何应对都是一个未知的、细思极恐的话题！那时候，作为创造智能生命的人类，是作为神一般的存在还是搬起石头砸自己的脚？</p><p>回顾人类的历史，从一个部落的合作、到一个地区的合作、到一个国家的合作，直到现在的地球村。最终人类不仅有了改变历史进程的能力，更有了结束历史进程的能力。而未来的故事就像作者所说的，可能性会远远超出我们的想象。</p><p><img src="/img/认知革命.jpg" /></p><h1 id="文章摘录">文章摘录</h1><ul><li>所谓同一物种，就是他们会彼此交配，能够产生出下一代；<br /></li><li>对智人来说，大脑只占身体总量的2%-3%，但是身体休息而不活动时，大脑的能量消耗却占了25%；<br /></li><li><strong>“第四权力”</strong>是西方社会的一种<strong>关于新闻传播媒体在社会中地位的比喻</strong>。它所表达的一种社会力量：新闻传播媒体总体上构成了与立法、行政、司法并立的一种社会力量，对这三种政治权力起制衡作用。这实际是一种认识理念，以这种简单比喻的形式，19世纪以来普及于西方主要工业国家，但又经常受到人们的质疑。<br /></li><li>150人是一个组织自然管理的门槛。<br /></li><li>任何大规模人类合作的根基，都在于某种只存在于集体想象中的虚构的故事。<br /></li><li><strong>难点不在于讲故事，而在于要让人相信。 个人注：例如现在的商业模式，好多时候都是在讲故事。关键是讲故事的人一定要自己都要相信。</strong><br /></li><li>演化心理学，认为现在人类的各种社会和心理特征是从农业时代之前就已经开始形成。现在社会逼迫所有人类都采用一夫一妻的核心家庭，这其实是与我们的生物本能背道而驰的。<br /></li><li>农业和工业社会的传染病（例如天花、麻疹、肺结核）多半是来自家禽家畜。<br /></li><li>最早海平面较低，西伯利亚东部还有陆地和阿拉斯加的西北相连。<br /></li><li><strong>生存和繁衍是最基本的演化标准，根据这个标准，小麦可以说是地球史上最成功的植物。</strong><br /></li><li><strong>【小知识】</strong><br /><strong>英国的巨石阵：Stonehenge</strong><br />Beagle 小猎犬<br /><strong>胼手胝足pián shǒu zhī zú</strong> 【解释】胼、胝：老茧。皮肤等的异常变硬和增厚,一般是指长期从事体力劳动者，手脚生茧。形容十分辛勤劳动。<br /><strong>汲汲营营jí jí yíng yíng</strong>：汲汲，勤求不休止的样子。营营，追逐求取。汲汲营营形容人急切求取名利的样子。<br /><strong>觥筹交错</strong><br />边陲<br /><strong>“孟不离焦”，</strong>或者“焦不离孟”出自《杨家将》，焦、孟指的是杨延昭（杨六郎）部下的两员大将焦赞和孟良，二人是结义弟兄，常形影不离。后用于比喻两人关系非常铁，感情深厚。<br /><strong>穷兵黩武：</strong>qióng bīng dú wǔ 穷：竭尽；黩：随便，任意。随意使用武力，不断发动侵略战争。形容极其好战。<br /><strong>夙夜匪懈</strong>：夙夜：早晚，朝夕；匪：不；懈：懈怠。形容日夜谨慎工作，勤奋不懈。<br /></li><li>生物学没有创造，而是演化。而演化就没有平等的概念，而是物竞天择、适者生存。演化的基础是差异，而不是平等。<br /></li><li>想象所构建出来的秩序总是有一夕崩溃的风险，因为这些秩序背后靠的是虚构的故事。例如制度、主义、宗教等等都是虚构出来的。<br /></li><li>苏美尔、古埃及、中国和银价帝国的文化中，发展出力能够将文字记录予以归档、编目和检索。这是一种文字的核心要求。<br /></li><li>文书和会计的想法就是没有人性，像个文件柜一样。过去的自由链接、整体思考，已经转变为分割思考、官僚制度。<br /></li><li>《风俗通》（应劭著）<br />原文：俗说天地开辟，未有人民，女娲（传说是人面蛇身，创造了人类）抟黄土做人。剧务（工作繁忙），力不暇供（没有多余的力量来供应需要），乃引（牵、拉）绳于泥中，举以为人。故富贵者，黄土人；贫贱者，引縆（绳）人也。<br />译文：民间传说，天地开辟之初，大地上并没有人类，是女娲把黄土捏成缉鸡光课叱酒癸旬含莫团造了人。她干得又忙又累，竭尽全力干还赶不上供应。于是她就拿了绳子把它投入泥浆中，举起绳子一甩，泥浆洒落在地上，就变成了一个个人。后人说，富贵的人是女娲亲手抟黄土造的，而贫贱的人只是女娲用绳沾泥浆，把泥浆洒落在地上变成的。<br /></li><li>洪水后，诺亚成为一个农夫，还种植了一个葡萄园。他喝了园中的酒便醉了，在帐棚里赤裸身子。这时含（含有四个儿子，古实、埃及、弗和迦南，其中圣经曾两次记载含是“迦南的父亲”。含看过喝醉了的诺亚的下体，而让迦南受诺亚的咒诅，要作兄弟奴隶的奴隶。）看见他父亲赤身，就到外边告诉他两个弟兄。于是闪和雅弗拿了一件衣服，倒退著进去，盖在诺亚身上；他们背着诺亚，以免看到父亲的赤身。诺亚醒了酒，知道小儿子向他所作的事，就说：“迦南当受咒诅，必给他弟兄作奴仆的奴仆；又说：耶和华闪的神是应当称颂的！愿迦南作闪的奴仆。愿神使雅弗扩张，使他住在闪的帐棚里；又愿迦南作他的奴仆。”<br /></li><li>三K党（英语：Ku Klux Klan, KKK），指美国历史上和现代三个不同时期奉行白人至上主义运动和基督教恐怖主义的民间仇恨团体，也是美国种族主义的代表性组织。该组织常使用恐怖主义方式来达成自己的目的。<br /></li><li><strong>教育带来进一步的教育，而无知只会造成进一步的无知。</strong><br /></li><li>人类几乎从出生到死亡都被种种虚构的故事和概念围绕，让他们以特定的方式思考，以特定的标准形式，想要特定的东西，也遵守特定的规范。就是这样，让数以百万计的陌生人能遵照这种人造而非天生的直觉，合作无间。这种人造的直觉就是“文化”。<br /></li><li>金钱并不是物质上的现实，而只是心理上的想象。所以，金钱的运作就是要把前者转变为后者。<strong>金钱正是有史以来最有效的互信系统</strong>。金钱货币史上真正的突破，就是人类终于开始相信某种货币形式，虽然他们本身没有什么固定价值，但却能方便储存和运送。<br /></li><li>宗教信仰的重点是自己相信，但金钱信仰的重点是别人相信。<br /></li><li>历史就是没有正义。 个人注：只有力量，只有利益。<br /></li><li>帝国的定义就在于文化的多元性和疆界灵活性两项。<br /></li><li><strong>中国的汉族，以公元前206年到公元220年的汉朝为名。</strong><br /></li><li>在金钱和帝国之外，宗教正是第三种让人类统一的力量。<br /></li><li>很多古代神话其实就是一种法律契约，人类承诺要永远崇敬某些神灵，换取人类对其他动植物的控制权。<br /></li><li>六畜兴旺六畜：牛、马、羊、猪、鸡、狗。指各种牲畜、家禽繁衍兴旺。<br /></li><li>耶稣号称自己就是旧约里面所说的来解救以色列人的弥赛亚。 福音就是耶稣做的好事。<br /></li><li>二元论宗教信奉着善与恶这两种对立力量的存在。<br /></li><li>基督徒大致上信奉一神论的上帝，相信二元论的魔鬼，崇拜多神论的圣人，还相信泛神论的鬼魂。<br /></li><li>佛教，崇拜的这个秩序是自然法则，而不是神圣的意志，虽然他们也相信有神祗的存在，但认为这些神祗和人类、动物和植物一样会受自然法则的限制。<br /></li><li>生命就像毫无意义的追寻。<br /></li><li>释伽牟尼思索人类苦痛的本质、原因和解决方式。欲望，让人心永远不满、永远不安。无欲则无苦。<br /></li><li>涅槃：梵文的原意就是熄灭。佛陀：觉悟者。<br /></li><li><strong>佛陀的教诲：痛苦来自欲望；要从痛苦中解脱，就要放下欲望；而放下欲望，就必须训练心智，体验事物的本质。</strong><br /></li><li>宗教：信念+仪式<br /></li><li>演化人文主义，以纳粹为最著名的代表。<br /></li><li>历史的铁则就是：事后看来无可避免的事，在当时看来总是毫不明显。<br /></li><li><strong>一级混沌指的是，不会因为预测而变化，例如天气就属于一级混沌系统；二级混沌系统，指的是，会受到预测的影响而改变，因此就永远无法准确预测；例如市场或者股票就是。</strong><br /></li><li>宗教总是假设世上所有重要的事情都已经为神所知。<br /></li><li>概率计算是精算学的基础。<br /></li><li>培根：知识就是力量。科学就是一种知识，科技工具就成了人们的力量来源。<br /></li><li>古罗马军队的优点在于：有效率的组织；铁一般的纪律；庞大的后勤力量。<br /></li><li>坏血病：缺乏维生素C，所以以前的船员经常有这个病，要多吃水果蔬菜。<br /></li><li>事物都有好坏正反两方面。<br /></li><li>耶稣说过：骆驼穿过针的眼，比财主进神的国还容易。<br /></li><li>1776年，苏格兰经济学家亚当·斯密出版了《国富论》，这可以说是史上最重要的经济学著作。书中指出，人类全体财富的基础，就在于希望增加个人利润的自私心理。这一点可以说是人类历史上最重要的革命性的概念，而且还不只是从经济的角度，也包括道德和政治的角度。他其实告诉我们，贪婪是好的，而且我们让自己过得好的时候，不只是自己得到，还能让他人收益。利己就是利他。亚当·斯密推翻了传统上认为财富与道德彼此对立的概念，这些天堂的大门也会为富人而敞开，而有钱也就有了道德。<br /></li><li>现在资本主义的一大重点，就在于出现了一种新的道德标准：应该把利润拿出来，继续投资生产。资本（Capital）与财富（wealth）有所不同。资本指的是投入生产的各种金钱、物品和资源。而财富指的则是那些埋在地下或者浪费在非生产性活动的金钱、物品或资源。<br /></li><li>独裁国家不愿保障个人和其财产，于是资本也就一点一滴离开，流向那些原意遵守法制、保护私有财产的国家。保护私产+司法独立是资本主义发展的重要前提。<br /></li><li>自由市场资本主义美中不足的地方就是，它无法保证利润会以公平的方式取得或者以公平的方式分配。<br /></li><li><strong>在水煮沸的那一刻，水壶或锅的盖子会开始跳上跳下，这是热能转换为动能。也是蒸汽机发明的来源。</strong><br /></li><li>在地心引力下将一颗小苹果抬升一米，所需的能量就是一焦耳。<br /></li><li><strong>工业革命最重要的一点，其实就在于它就是第二次农业革命。</strong><br /></li><li>秩序及隐含着稳定和连续的意义。<br /></li><li>狄更斯在《双城记》写到：<br />It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way--in short, the period was so. far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only.<br />翻译如下：<br />那是最美好的时代，那是最糟糕的时代；那是智慧的年头，那是愚昧的年头；那是信仰的时期，那是怀疑的时期；那是光明的季节，那是黑暗的季节；那是希望的春天，那是失望的冬天；我们全都在直奔天堂，我们全都在直奔相反的方向--简而言之，那时跟现在非常相象，某些最喧嚣的权威坚持要用形容词的最高级来形容它。说它好，是最高级的；说它不好，也是最高级的。<br /></li><li>我们只要回顾过往就发现，自己对于过去历史的看法总是受到近几年事件的左右。<br /></li><li><strong>快乐是一种主观感受。而佛教认为，快乐既不是主观感受到愉悦，也不是主观感受到生活有意义，反而是在于放下追求主观感受这件事。佛教更重要也更深刻的见解在于，真正的快乐不在于我们的主观感受。我们如果越强调主观感受，反而就月感到苦。佛教给我们的建议是，除了别再追求外在的成就之外，同时也别再追求那些感觉良好的心里感受了。</strong><br /></li><li>啮齿目：niè chǐ 啮齿目是哺乳动物中的一目，其特征为上颌和下颌各两颗会持续生长的门牙，啮齿目动物必须通过啃咬来不断磨短这两对门牙。</li></ul><p><img src="/img/人类简史.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 历史 </tag>
            
            <tag> 读书心得 </tag>
            
            <tag> 人类史 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《知行合一王阳明》读书心得</title>
      <link href="/2016/11/30/%E3%80%8A%E7%9F%A5%E8%A1%8C%E5%90%88%E4%B8%80%E7%8E%8B%E9%98%B3%E6%98%8E%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2016/11/30/%E3%80%8A%E7%9F%A5%E8%A1%8C%E5%90%88%E4%B8%80%E7%8E%8B%E9%98%B3%E6%98%8E%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>王阳明一生虽然立下奇功，但在仕途上也属于郁郁不得志之列。<br />王阳明在被贬贵州修文县时，在龙场悟道，创立了心学，从此，王阳明走向了人生的巅峰。<br />王阳明的心学强调“知行合一”，遵循内心的良知，便能达到宁静于内、无敌于外的境界。知行是合为一体的，知是行的开始，行是知的结果，由此，心学否定了程朱理学格物致知理论关于知行两者割裂开的思想。也正因为这样，王阳明一直受到了儒家弟子的攻击和反对。<br />王阳明提出，人的一生就是要达到致良知，根据自己的良知去行事。真正的自信就是，相信自己的良知，并按良知做事。<br />王阳明提倡的“良知”，除了关于道德的善恶之心外，还有关于智慧的是非之心。他认为，这些良知就在人心中，不需要再向外界寻求，很多人只是良知给蒙蔽了。但是，王阳明的心学和佛学不同的是，心学是一种入世的学问，强调行的重要性，要在行中磨炼自己。<br />看了整本书，最大的感觉就是，王阳明才是真正的心理学大师，剿匪的时候，充分利用了人的心理，虚虚实实，或招安、或强攻，基本是以最小的代价获得了最大的胜利。</p><span id="more"></span><blockquote><p><strong>格物致知</strong><br />格物致知是中国古代儒家思想中的一个重要概念，源于《礼记·大学》八目──格物、致知、诚意、正心、修身、齐家、治国、平天下─所论述的“欲诚其意者，先致其知；致知在格物。物格而后知至，知至而后意诚”此段。但《大学》文中只有此段提及“格物致知”，却未在其后作出任何解释[1]，也未有任何先秦古籍使用过“格物”与“致知”这两个词汇而可供参照意涵，遂使“格物致知”的真正意义成为儒学思想的难解之谜。<br />东汉郑玄最早为“格物致知”作出注解，而自从宋儒将《大学》由《礼记》独立出来成为《四书》的一部后，“格物致知”的意义也就逐渐成为后世儒者争论不休的热点议题，以至于今。现在社会上关于“格物致知”的流行诠释是根据南宋朱熹学说的部分观点，认为“格物致知”就是研究事物而获得知识、道理。《现代汉语词典》将其解释为：“穷究事物的原理法则而总结为理性知识”。现今流行观点和朱熹观点的差异，乃是在关于“致知”的解释。朱熹所谓的“知”是知性─包含了智慧与知识─而现代流行观点的“知”只是指知识，这种观念变异可能是由于现今社会流行唯物论观点所产生的影响。<br />虽然朱熹乃是儒学史上承先启后的一代大儒，但他对于“格物致知”的观点之所以在后世成为主流，并非是因为获得后世儒家学者的普遍赞同。事实上，朱熹学说在南宋当时还因政治党争而被斥为“伪学”，而后世的许多儒家学者也更大力批判朱熹对于“格物致知”的学说观点。但因为朱熹的《四书集注》在元朝中叶就被官方采用为科举取士的应试准则，而自从明太祖开始独尊朱熹学说为《四书》上的唯一官方思想权威以后，朱熹学说更是成为明清两代历时五百余年在科举应试上的官方教条观点。因而朱熹在“格物致知”上的观点也就在数百年的官方教条权威下，成为后世社会上的普遍流行观点。所以在清末的洋务学堂中，就把物理、化学等学科称为“格致”[2]，即“格物致知”的简称。上海等地还成立科学技术学校，称为格致书院。</p></blockquote><blockquote><p><strong>横渠四句</strong><br />即“为天地立心，为生民立命，为往圣继绝学，为万世开太平”，为北宋儒学家<strong>张载</strong>的名言。当代哲学家冯友兰将其称作“横渠四句”。由于其言简意宏，一直被人们传颂不衰。其意思是读书人其心当为天下而立，其命当为万民而立，当继承发扬往圣之绝学，当为万世开创太平基业，说出了读书人应当有的志向和追求：天下、万民、圣贤之道、太平基业。</p></blockquote><blockquote><p><strong>宋真宗赵恒的《劝学诗》</strong><br />富家不用买良田，书中自有千锺粟；<br />安居不用架高堂，<strong>书中自有黄金屋；</strong><br />出门莫恨无人随，书中车马多如簇；<br />娶妻莫恨无良媒，<strong>书中自有颜如玉；</strong><br />男儿若遂平生志。六经勤向窗前读。</p></blockquote><blockquote><p><strong>仁义礼智信</strong><br />“仁义礼智信”为儒家“五常”，孔子提出“仁、义、礼”，孟子延伸为“仁、义、礼、智”，董仲舒扩充为“仁、义、礼、智、信”，后称“五常”。这“五常”贯穿于中华伦理的发展中</p></blockquote><blockquote><p><strong>胶柱鼓瑟</strong><br />源于故事“齐人跟赵人学习瑟这种乐器。他不去刻苦钻研演奏瑟的技术，却依照赵人预先调弄好的音调，将瑟上调音的短柱用胶粘固起来，就高高兴兴地回到了家乡。齐人回家后，摆弄了多年，总是弹不出一支曲子。他还觉得奇怪呢!后来，有人从赵国来，了解到是怎么回事，觉得这个齐人的举动是多么愚蠢啊!”比喻固执拘泥，不知变通。</p></blockquote><h1 id="文章摘要">文章摘要</h1><ul><li>孟子说人性本善，荀子说人性本恶，告子则说，人性可善可恶。<br /></li><li>儒学只有在大一统时代才有力量。 个人注：儒学在大一统的时代是用来麻痹人民的精神鸦片。<br /></li><li>董仲舒曾用“天人感应”的方式来限制皇权，“天人感应”认为，国君做了坏事，老天就发怒；国君做了好事，老天就高兴。<br /></li><li>能勇敢向前是勇气，能转身是智慧，智勇兼备，才可成大事。 个人注：知进退！<br /></li><li>儒家知识分子最大的追求就是把皇上塑造成德高望重的圣贤。 个人注：所谓的圣君良相。（说难听点儒学就是向上靠拍皇帝马屁，向下用来愚民的一种学说）<br /></li><li><strong>噤若寒蝉</strong>：噤：闭口不作声。象深秋的蝉那样一声不吭。比喻因害怕有所顾虑而不敢说话。《后汉书·杜密传》：“刘胜位为大夫，见礼上宾，而知善不荐，闻恶无言，隐情惜己，自同寒蝉，此罪人也。”<br /></li><li><strong>轻徭薄赋、悲天悯人</strong><br /></li><li>王八吃秤砣----铁了心<br /></li><li>人生一切所谓困难，都是比较而言。<br /></li><li>人的力量永远来自心灵。当你的心灵产生力量后，外界的环境看上去也就没想象中的险恶了。<br /></li><li><strong>逆境让人成长，让人成熟。摩西被放逐渺无人迹的沙漠，才有了《摩西十诫》；耶稣在颠沛流离的传道中悟得大道；穆罕穆德在放逐地创建了伊斯兰教；释迦摩尼放弃了王子养尊处优的生活，到深山老林中度过了艰苦的岁月，创建佛教。</strong><br /></li><li>孝顺父母的终极目的就是让他们心上安宁，物质条件还在其次。<br /></li><li>所谓政治力，无非就是一个人处理各种关系的能力，主要就是人际关系。<br /></li><li><strong>学术辩论是要明理，而不是要分胜败。</strong><br /></li><li>古人云，攻我短者是吾师。（但人的天性是防御性的心态，忠言逆耳）<br /></li><li>心学和禅学的区别就是在于实践。（个人注：一个入世，一个出世）<br /></li><li>佛教以超脱生死来劝人信奉，道教以长生不老劝人信奉。<br /></li><li>一个内心强大的人，肯定是做事的人。<br /></li><li><strong>一个人是否成熟，要看他在面对事情时的态度。</strong><br /></li><li>不要迷信自己的经验。世间一切瞬息万变，拿从前的经验对待新出现的事物是<strong>胶柱鼓瑟</strong>。<br /></li><li>靠山山倒，靠河河枯。<br /></li><li><strong>莫道君行早，更有早行人。</strong><br /></li><li>政治无是非、无亲情，利害即是非。<br /></li><li>人因性格、人生阅历和生活环境的不同，看待事物时的态度就会迥然不同。<br /></li><li><strong>汤武和吕伊：汤是商汤，武是周武，伊是伊尹，吕是吕尚，也就是姜子牙。</strong><br /></li><li><strong>兵法说，围五攻十。 包围敌人要用五倍于敌人的士兵，攻击敌人就要用十倍于敌人的士兵。</strong><br /></li><li><strong>大家智慧都相差无几，胜负之决只在此心动与不动。</strong><br /></li><li>恶人也不是天不怕地不怕的，他们最怕的就是丧失利益。对付他们，只需要给他们摆清利害关系，他们就会知难而退。<br /></li><li>将心比心，永远都不会过时，必能产生奇效。<br /></li><li><strong>结交皇帝身边的红人是一个政治家变通的智慧</strong>，多年以后的张居正让半死不活的明帝国重获生命力，靠的就是和宫中的大太监冯保的友谊。<br /></li><li>真正服膺王阳明心学的人，都没有登上权力之巅。（个人注：政治还是要靠权谋）<br /></li><li>只要他能给你带来心灵上的安宁，它就是好学问。</li></ul><p><img src="/img/知行合一王阳明.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 历史 </tag>
            
            <tag> 哲学 </tag>
            
            <tag> 王阳明 </tag>
            
            <tag> 知行合一 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《策略思维》读书笔记</title>
      <link href="/2016/11/18/%E3%80%8A%E7%AD%96%E7%95%A5%E6%80%9D%E7%BB%B4%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
      <url>/2016/11/18/%E3%80%8A%E7%AD%96%E7%95%A5%E6%80%9D%E7%BB%B4%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<p>博弈论类似中国古代的纵横家，他们通过各种类似博弈的故事，建功立业。<br />而博弈论是在西方科学体系基础上的一种模型，把商场、生活、工作上的各种博弈，转化为数学模型，借鉴一些数学的方法，运筹帷幄，决胜千里。<br />不过，数学、物理模型的要义就是抛开次要因素，抓住核心要素来建立。但是，实际上，一些次要的因素的变化，往往会导致较大的差异结果。例如蝴蝶效应，所以实际上的博弈论并不能一概而论。再说，博弈论最难的地方就是在一个案例（事件）中抽象出对应的数学模型，还要找到核心的要素，保留核心的要素，剔除次要的因素，这要靠经验、水平了。<br />在实际生活中，有时候，经验比这些所谓的科学手段还更有效。一个人能根据经验，对一些形势、人性做出的判断，比通过博弈工具更直达事件本质。<br />再说，<strong>博弈总感觉是在术层面上的技巧（雕虫小技），而仁义、大智若愚这些大道层面上的思维，其实其境界更高</strong>。<br /><span id="more"></span></p><h1 id="一策略思维的简要说明">一、策略思维的简要说明</h1><h2 id="定义">1、定义</h2><p>策略思维是关于了解对手打算如何战胜你，然后战而胜之的<strong>艺术</strong>。关于策略思维的科学称为博弈论。在社会科学中，研究策略性决策制定过程的分支称为博弈论。</p><h2 id="策略思维">2、策略思维</h2><p>策略思维认为人都是理性的。<br /><strong>换位思维</strong>：从别人的角度来思考问题、来观察问题。不过，从别人的角度观察这个世界做起来并不容易，我们总喜欢把别人看做和我们一样的人，而不是不同的类型。博弈论要求你设身处地，仔细分析自己若处于对方的境地，思路会有什么变化，哪怕你完全不能同意他们的见解。<br />在你做决定的时候，必须将冲突考虑在内，同时注意发挥合作的效力。类似的互动决定就具有策略性，与之相适应的行动计划称为一个策略。</p><ul><li>其它人的行动向我们展示了他们究竟知道什么，我们应该利用这些信息指导我们自己的行动。<br /></li><li>在博弈论的过程当中，我们不能忽略自尊和失去理性的这两种要素。<br /></li><li>先行者处于不利地位：在博弈游戏里，抢占先机、率先出手并不总是好事。因为这么做会暴露你的行动，其他参与者可以利用这一点来占你的便宜。（类似高手不先出招一样:))。</li></ul><h2 id="博弈分类">3、博弈分类</h2><p>博弈分为同时发生、相继发生。</p><ul><li>冲突的博弈<br />零和博弈<br /></li><li>合作的博弈<br /></li><li>冲突+合作的博弈<br />存在博弈均衡（或者没有）。</li></ul><h2 id="博弈均衡">4、博弈均衡</h2><p>我们已经找到了一个策略组合，其中，各方的行动就是<strong>针对对方行动而确定的最佳对策</strong>。一旦知道对方在做什么，就没人愿意改变自己的做法。博弈论学者把这么一个结果称为 <strong>“均衡”</strong>。这个概念是由普林斯顿大学数学家约翰·纳什（John Nash）提出的。<br />有些博弈存在好几个均衡，有些博弈却一个均衡都没有。</p><h2 id="视觉辅助工具">5、视觉辅助工具</h2><h3 id="树状图决策树博弈树-相继行动">1）树状图（决策树、博弈树）—— 相继行动</h3><p><img src="/img/决策树.jpg" title="决策树" /><br /><img src="/img/博弈树.jpg" title="博弈树" /></p><h3 id="博弈表-同时行动">2）博弈表 —— 同时行动</h3><p>显示所有可能想象得到的策略组合将会相应产生什么结果。<br /><img src="/img/博弈表.jpg" title="博弈表" /></p><h3 id="博弈均衡图">3）博弈均衡图</h3><p><img src="/img/博弈均衡点.jpg" title="博弈均衡点" /></p><h1 id="二博弈四大法则">二、博弈四大法则</h1><p><strong>1、法则1（相继行动）：向前展望，倒后推理。</strong><br />向前展望，倒后推理：应该首先明确自己最后希望达到什么目标，然后从这个结果倒后研究，直到找出自己现在应该选择哪条道路，这样才能保证以后可以达到那个目标。这种策略，<strong>一般从未来某个固定点开始考察</strong>。<br /><strong>2、法则2（同时行动）：假如你有一个优势策略，请照办。</strong><br /><strong>3、法则3（同时行动）：剔除所有劣势策略，不予考虑，如此一步一步做下去。</strong><br /><strong>4、法则4（同时行动）：寻找这个博弈的均衡，即一对策略，按照这对策略做，各个参与者的行动都是对对方行动的最佳回应。</strong><br /><strong>5、混合策略（同时行动）</strong><br />如果以上法则2、3、4都不行，这时候你需要将你的策略混合运用，“混合策略!”</p><h1 id="三策略行动">三、策略行动</h1><p><strong>针对相继行动的博弈。</strong><br /><strong>策略行动</strong>：一个策略行动的设计意图在于改变对方的看法和行动，使之变得对自己有利。其突出特征是刻意限制你的行动自由。例如声称（威胁），如果对方怎么样，你就一定会怎么样之类的。</p><h2 id="以牙还牙法则">1、以牙还牙法则</h2><p>体现了任何一个性质有效的策略应该符合的四个原则：清晰、善意、刺激性和宽容性（保持一定小概率的容忍，防止误判）。<br />适度原则：不过最小限度、可行（可信）而又达到目的的威胁是最佳的。<br />一个成功的威胁是那种完全不必实现的威胁。不战而屈人之兵。</p><h2 id="随机策略">2、随机策略</h2><p><strong>随机性可以通过看你的手表的秒针，如果是单数怎么样、双数怎么样，或者在哪个期间怎么样。</strong><br />随机策略的最广泛的用途在于以较低的监管成本促使人们遵守规则。规则在于，预期的惩罚应该与罪行相称，而这种心理预期应该将被逮住的概率考虑在内。</p><h2 id="混合策略">3、混合策略</h2><p><strong>只适用：两个选手各有两个策略的零和博弈。</strong><br /><img src="/img/混合策略图.jpg" title="混合策略图" /><br /><img src="/img/混合策略表.jpg" title="混合策略表" /><br /><strong>选择这个点的混合策略，会形成一个博弈上的均衡，双方都没有方法改善自己的地位。</strong><br /><img src="/img/混合策略图2.jpg" title="混合策略图" /><br /><img src="/img/混合策略图3.jpg" title="混合策略图" /></p><h2 id="跟随策略">4、跟随策略</h2><p>追踪而来的新公司总是倾向于采用更加具有创新性的策略，而龙头老大们则愿意模仿跟在自己后面的公司（例如腾讯:-))。</p><h1 id="四其它的一些策略">四、其它的一些策略</h1><h2 id="边缘政策">１、边缘政策</h2><p>边缘政策则是你愿意在事实发生前创造这个风险，却在时机来临时不愿意将这个风险付诸实践。<br />核心是一种风险管控机制，发出一种双方都不能承受的结果的风险提示（但不是确定的提示）。完全成功的边缘政策仍是<strong>一门艺术</strong>和<strong>一种冒险</strong>。</p><h2 id="不稳定的均衡点">2、不稳定的均衡点</h2><p>开始严格执法，达到一个正向均衡之后，后期就能自动的达到良性循环。有点类似自适应了。</p><p><img src="/img/不稳定的均衡点.jpg" /></p><h2 id="投票的策略">3、投票的策略</h2><p>投票表决的次序就有可能对最后的结果产生重大影响。<br /><strong>第230页“法庭的秩序”这一章的关于法庭运转方式对囚犯判决结果有严重影响的例子很有趣！</strong></p><figure><img src="/img2/策略思维new.jpg" title="法庭的秩序" alt="法庭的秩序" /><figcaption aria-hidden="true">法庭的秩序</figcaption></figure><h2 id="讨价还价">4、讨价还价</h2><p>讨价还价的本质在于提出折中的方案解决分歧。<br />要看谁有相对优势，能支撑更多时间（等待成本较低），一般就能获取到比中间值更高的收获。<br />谈判者应该将所有有关共同利益的问题放在一起进行讨价还价，利用各方对这些问题的重视程度不同，达成对大家来说都更好的结果。</p><h1 id="文章摘要">文章摘要</h1><ul><li>谈判有一个策略：在最后期限做出一定的让步，同时，提出自己最核心的要求，这时候，一般大家都想到尽快结束谈判，从而做出让步。<br /></li><li><strong>[数学定理要思考其应用，而不再纠结与其严格的证明！]</strong><br /></li><li>有一个重要的结论：博弈的结果在很大程度上取决于参与者的人数。参与者越多越好。<br /></li><li>假如你不得不冒一点风险，通常是越早冒险越好。<br /></li><li>寻找优势策略是每一个人首要的任务。<br /><strong>名句：爱过之后失去总比从来没有爱过好。</strong>换而言之，爱是一种优势策略。<br /><strong>优势策略的优势是指你这个策略对你的其它策略占有优势，而不是对你的对手的策略占有优势。</strong><br /></li><li>只要你有行动的自由，你就有让步的自由。<br /></li><li><strong>“围师遗阙”</strong>是指包围敌人，要留缺口；出处：《孙子兵法》《军争篇》：“归师勿遏，围师遗阙，穷寇勿迫，此用兵之法也。”<br />给对方一条路，不然兔子急了还咬人呢，或者说狗急跳墙。<br /></li><li>对于耐用品，一个垄断者实际上是在跟以后的自己竞争，从而使市场变得富有竞争性。<br /></li><li><strong>但见新人笑，那闻旧人哭。《佳人》杜甫</strong><br /></li><li>扑克玩家应该隐藏在自相矛盾的面具后面。<br /></li><li><strong>我们的晚餐并不是来自屠夫、啤酒酿造者或点心师的善心，而是来源于他们对自身利益的考虑......[每个人]只关心他自己的安全、他自己的得益。他由一只看不见的手引导着，去提升他原来本没有想过的另一目标。他通过追求自己的利益，结果也提升了社会的利益，比他一心要提升社会利益还有效。《国富论》亚当·史密斯（Adam Smith）</strong><br /></li><li>差额选举是指候选人数多于应选人数的不等额选举。<br /></li><li><strong>马太效应：圣经中“马太福音”的故事：凡有的，还要加给他，叫他有余；凡没有的，连他所有的也要夺去。</strong><br /></li><li><strong>招标：一个简单的机制就是将合同判给开价最低者，但付给她开价第二底者的价码。</strong><br />【个人注】但是在中国这种围标厉害的国家可能行不通;-)<br /><strong>拍卖其实也是相当于将物品授予开价最高者（心理价位），而价码等于次高开价（超过这个价，次高开价人就退出了）。</strong><br /></li><li>杀鸡儆猴。“杀鸡儆猴”说的是杀掉鸡来吓唬猴子，比喻惩戒一个以警戒其余。<br /></li><li>三方对决<br />话说有三个仇家，分别叫做拉里（Larry）、莫（Mo）和卷毛( Curly），他们决定来一场三方对决。总共有两个回合：第一回合，每人得到一次射击机会，射击次序分别为拉里、莫和卷毛；第一回合过后，幸存者得到第二次射击机会，射击次序还是拉里、莫和卷毛。对于每一个参与对决的人，最佳结果都是成为惟一幸存者；次佳结果则是成为两个幸存者之一；排在第三位的结果，是无人死亡；最差的结果当然是自己被对方打死。<br />拉里的枪法很糟糕，瞄准10次只有3次能够打中目标。莫的水平高一点，精确度有80％。卷毛是神枪手，百发百中。<br />那么，拉里在第一回合的最优策略应该是什么？在这个问题里，谁有最大的机会幸存下来？<br />案例的意义：弱者可以通过放弃自己的第一个成功机会来取得更好的结果。<br /></li><li>歌利亚是《圣经》故事中被牧羊人大卫杀死的巨人。<br /></li><li><strong>部队的训练</strong><br /><strong>1、无条件服从</strong><br />例如日常的折被子，就是训练士兵不问原因，无条件服从，形成条件反射。训练出不问是非的战斗机器。<br /><strong>2、自豪感</strong><br />灌输为祖国自豪、为当兵自豪。自豪感通常是一种精英主义的情感。<br /></li><li>有些案例没有标准的答案，不过，这也是人生的一大特点。许多时候，并不存在完全正确的答案，只能用并不完美的方法处理遇到的问题。（见招拆招、船到桥头自然直）</li></ul><p><img src="/img/策略思维.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 管理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>文章发表时间显示了两遍</title>
      <link href="/2016/11/15/%E6%96%87%E7%AB%A0%E5%8F%91%E8%A1%A8%E6%97%B6%E9%97%B4%E6%98%BE%E7%A4%BA%E4%BA%86%E4%B8%A4%E9%81%8D/"/>
      <url>/2016/11/15/%E6%96%87%E7%AB%A0%E5%8F%91%E8%A1%A8%E6%97%B6%E9%97%B4%E6%98%BE%E7%A4%BA%E4%BA%86%E4%B8%A4%E9%81%8D/</url>
      
        <content type="html"><![CDATA[<p>更新了 NexT主题之后，发表文章那里显示了两次时间（之前的版本是只有一个时间的）：<br />NexT Informations<br />Master： 5.1.0<br />NexT Scheme:<br />Pisces</p><p>其實是對的， Posted on 2016-11-04 | 2016-11-04 第一個日期， 是你create， 第二個日期是你有沒有modify 過你的文章。</p><p>如何屏蔽modify的选项：<br />在主题NexT下面的目录：layout/_macro 裏面的post.swig，<br />去掉如下一段：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">time</span> <span class="attr">title</span>=<span class="string">&quot;&#123;&#123; __(&#x27;post.modified&#x27;) &#125;&#125;&quot;</span> <span class="attr">itemprop</span>=<span class="string">&quot;dateModified&quot;</span> <span class="attr">datetime</span>=<span class="string">&quot;&#123;&#123; moment(post.updated).format() &#125;&#125;&quot;</span>&gt;</span>  </span><br><span class="line">    &#123;&#123; date(post.updated, config.date_format) &#125;&#125;  </span><br><span class="line"><span class="tag">&lt;/<span class="name">time</span>&gt;</span></span><br><span class="line">```  </span><br><span class="line">  </span><br><span class="line">还有上面的这个竖：  </span><br><span class="line"></span><br><span class="line">``` html  </span><br><span class="line">            <span class="symbol">&amp;nbsp;</span>|<span class="symbol">&amp;nbsp;</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 技术相关 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 互联网 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《Hadoop技术内幕》读书心得</title>
      <link href="/2016/11/01/%E3%80%8AHadoop%E6%8A%80%E6%9C%AF%E5%86%85%E5%B9%95%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2016/11/01/%E3%80%8AHadoop%E6%8A%80%E6%9C%AF%E5%86%85%E5%B9%95%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>一本比较偏重技术的书，介绍了很多的技术细节，所以只是涉猎的看了一下。</p><h1 id="一hadoop的基本原理">一、Hadoop的基本原理</h1><p>Hadoop现在已经被看成大数据分析的“神器”。Hadoop有三大重要的模块，即基础公共库、<strong>HDFS（分布式文件系统）</strong>实现和<strong>MapReduce实现（分布式计算框架）</strong>；<br /><strong>MapReduce由两个阶段组成：Map和Reduce。</strong><br /><strong>通常而言，用户需要处理的数据均以文件形式存储到HDFS上！而非机构化的数据。</strong><br /><span id="more"></span></p><h2 id="一个例子">1、一个例子</h2><p><img src="/img/MapReduce例子.jpg" /></p><ul><li>split的多少决定了Map Task的数目，因为每个split会交由一个Map Task处理。<br /></li><li>Partitioner的作用是对Mapper产生的中间结果进行分片，以便将同一分组的数据交给同一Reduce处理，它直接影响Reduce阶段的负载均衡。</li></ul><h2 id="编程步骤">2、编程步骤</h2><p><strong>MapReduce能够解决的问题有一个共同的特点：</strong>任务可以被分解为多个子问题，且这些子问题相对独立，彼此间不会有牵制，待并行处理完这些子问题后，任务便被解决。<br />MapReduce编程模型给出了其分布式编程方法，共5个步骤：<br />1）迭代(iteration)。遍历输入数据，并将之解析成Key/value对；<br />2）将输入Key/value映射（Map）成另外一些Key/value对；<br />3）依据Key对中间数据进行分组（Grouping）；<br />4）以组为单位对数据进行规约（Reduce）；<br />5）迭代。将最终产生的Key/value对保存到输出文件中。</p><h2 id="mapreduce运行过程">3、MapReduce运行过程</h2><p>Map Task分解成Read、Map、Collect、Spill和Combine五个阶段，讲Reduce Task分解成Shuffle、Merge、Sort、Reduce和Write五个阶段。<br /><img src="/img/MapReduce运行过程.jpg" /></p><h2 id="hadoop原理">4、Hadoop原理</h2><p>在Hadoop MapReduce中，不同组件的通信协议均是基于RPC的，它们就像系统的“骨架”，支撑起整个MapReduce系统。</p><ul><li>RPC： Remot Procedure Call，是一种常用的分布式网络通讯协议。JDK中的RMI（Remote Method Invocation）也是一种RPC框架。<br /></li><li>Stub程序：客户端和服务器端均包含Stub程序，可将之看做代理程序。它使得远程函数调用表现的跟本地调用一样，对用户程序完全透明。<br /></li><li>RPC 通用架构<br /><img src="/img/RPC通用架构.jpg" /><br /></li><li>IDL：开源RPC框架提供了一套接口描述语言（Interfae Description Language,IDL)。它提供了一套通用的数据类型，并以这些数据类型来定义更为复杂的数据类型和对外服务接口。一旦用户按照IDL定义的语法编写完成接口文件后，即可根据实际应用需要生成特定的编程语言（例如Java，C++，Python等）的客户端和服务器端代码。</li></ul><p><strong>任务推测执行原理：</strong>Hadoop采用了推测执行（Speculative Execution)机制。它根据一定的法则推测出“拖后腿”的任务，并为这样的任务启动一个备份任务，让该任务与原始任务同时处理一份数据，并最终选用最先成功完成任务的计算结果作为最终结果。</p><h1 id="二下一代mapreduce框架">二、下一代MapReduce框架</h1><p><strong>下一代MapReduce框架的基本设计思想是蒋JobTracker的两个主要功能，即资源管理和作业控制（包括作业监控、容错等），分拆成两个独立的进程。</strong><br />随着互联网的告诉发展，基于数据密集型应用的计算框架不断出现。从支持离线处理的MapReduce，到支持在线处理的Storm，从迭代式计算框架Spark到流式处理框架S4，各种框架诞生与不同的公司或者实验室。一种可能的技术方案如下：网页建索引采用MapReduce框架，自然语言处理/数据挖掘算法用MPI邓。考虑到资源利用率、运维成本、数据共享等因素，公司一般希望将所有这些框架部署到一个公共的集群中，让他们共享集群的资源，并对资源进行统一使用，这样，便诞生了资源统一管理与调度平台。<br />YRAN是Apache的下一代MapReduce框架；Corona是Facebook于2012年11月开源的下一代MapReduce框架；<br />YARN与Corona比较：<br /><img src="/img/YARN与Corona比较.jpg" /></p><h1 id="文章摘录">文章摘录</h1><ul><li>Apache软件基金会：支持开源软件项目而办的一个非营利性组织。<br /></li><li>Rack：架子、机架；<br /></li><li>DAG：Directed Acqlic Graph，作业，工作流，有向无环图。<br /><img src="/img/Hadoop技术内幕.jpg" /></li></ul>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 开发 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《数学桥--对高等数学的一次观赏之旅》读书心得之一</title>
      <link href="/2016/10/30/%E3%80%8A%E6%95%B0%E5%AD%A6%E6%A1%A5--%E5%AF%B9%E9%AB%98%E7%AD%89%E6%95%B0%E5%AD%A6%E7%9A%84%E4%B8%80%E6%AC%A1%E8%A7%82%E8%B5%8F%E4%B9%8B%E6%97%85%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97%E4%B9%8B%E4%B8%80/"/>
      <url>/2016/10/30/%E3%80%8A%E6%95%B0%E5%AD%A6%E6%A1%A5--%E5%AF%B9%E9%AB%98%E7%AD%89%E6%95%B0%E5%AD%A6%E7%9A%84%E4%B8%80%E6%AC%A1%E8%A7%82%E8%B5%8F%E4%B9%8B%E6%97%85%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97%E4%B9%8B%E4%B8%80/</url>
      
        <content type="html"><![CDATA[<p>一本非常烧脑的书！光是看书就花了四周，做笔记又花了一周。<br />看完《无言的宇宙--隐藏在24个数学公式背后的故事》之后，想深入了解那些数学的定理，就决定重捡起这本书看看（书是之前买的，但因为比较深奥，当时就没看）。看的过程，仿佛就是重温了一遍大学的高等数学。<br />不过本书重点是体现数学的思想，对一些深奥的、细节的问题倒是一带而过，所以看起来还是能把高等数学中核心的部分及思想体会一遍。<br />本书最大的特点是，根据人类在认识自然、解决生活中实际问题的要求，自然而然带出数学的相关领域。例如从自然数到整数，从整数到分数，从分数到有理数、到无理数，从实数到复数等等。这些数的出现，不是数学家凭空想出来的，而是人们结合需要一步步的扩展来的。在数学发展的过程中，集合慢慢替代了数成为了数学的基本要素，集合的概念在数学中发挥了重大的作用。数学的不同领域，其实就是定义满足一定公理、定义、运算要求的一些符合条件元素的集合，然后在这些公理、定义上，用形式化的符合来对这个集合进行研究和推理，形成了一个完整的领域。例如欧式几何、罗氏几何、黎曼几何，例如群论，例如高维数系等等。<br /><strong>数学的思想</strong>：</p><ul><li>1）实际需求（问题） ；<br /></li><li>2）定义形式 ；<br /></li><li>3）对形式进行条件约束 ；<br /></li><li>4）证明这些约束满足实际的需求（问题）；<br /></li><li>5）抽象出定理并推广到一般的情况。</li></ul><p>类似：1）要把一个东西快速运到另外一个地方； 2）定义一个叫车的东西； 3）车必须满足，能开动、速度要达到多少、装载容量、安全等等； 4）证明这个车符合实际的需求； 5）推广到运输工具（包括运人等）。</p><span id="more"></span><h1 id="一数">一、数</h1><p><img src="/img/从1到无穷.jpg" /><br /><img src="/img/代数基本定理.jpg" /><br /><img src="/img/高维数系.jpg" /></p><ul><li><strong>代数数</strong>：就是一个整数系多项式方程的实数根；例如<span class="math inline">\(\sqrt {2}\)</span><br /><strong>超越数</strong>：就是任何不是代数数的实根；例如π。<br /></li><li><strong>哥德巴赫猜想</strong>：所有大于2的偶数都可以写成两个素数的和。<br /></li><li><strong>拓扑</strong>：研究与任何特定距离结构无关的形状的性质。</li></ul><h2 id="求两个数的最大公因数">1、求两个数的最大公因数</h2><p><strong>长除法。</strong><br /><img src="/img/因数分解.jpg" /><br /><img src="/img/长除法1.jpg" /><br /><img src="/img/长除法2.jpg" /><br /><img src="/img/长除法3.jpg" /></p><h2 id="连分数">2、连分数</h2><p><img src="/img/长除法3.jpg" /><br /><img src="/img/连分数.jpg" /></p><h2 id="rsa密码">3、RSA密码</h2><p><img src="/img/RSA密码.jpg" /></p><p>下一篇：《数学桥--对高等数学的一次观赏之旅》读书心得之二</p><p><img src="/img/数学桥.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 科技 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《数学桥--对高等数学的一次观赏之旅》读书心得之二</title>
      <link href="/2016/10/30/%E3%80%8A%E6%95%B0%E5%AD%A6%E6%A1%A5--%E5%AF%B9%E9%AB%98%E7%AD%89%E6%95%B0%E5%AD%A6%E7%9A%84%E4%B8%80%E6%AC%A1%E8%A7%82%E8%B5%8F%E4%B9%8B%E6%97%85%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97%E4%B9%8B%E4%BA%8C/"/>
      <url>/2016/10/30/%E3%80%8A%E6%95%B0%E5%AD%A6%E6%A1%A5--%E5%AF%B9%E9%AB%98%E7%AD%89%E6%95%B0%E5%AD%A6%E7%9A%84%E4%B8%80%E6%AC%A1%E8%A7%82%E8%B5%8F%E4%B9%8B%E6%97%85%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97%E4%B9%8B%E4%BA%8C/</url>
      
        <content type="html"><![CDATA[<p>接上篇：《数学桥--对高等数学的一次观赏之旅》读书心得之一</p><h1 id="二分析">二、分析</h1><p>分析是一门处理无穷的学科，例如微积分。</p><h2 id="二项式定理">1、二项式定理</h2><p>在初等代数中，二项式定理（英语：Binomial theorem）描述了二项式的幂的代数展开。根据该定理，可以将两个数之和的整数次幂诸如<span class="math inline">\((x + y)^n\)</span> 展开为类似 <span class="math inline">\(ax^by^c\)</span> 项之和的恒等式，其中b、c均为非负整数且b + c = n。系数a是依赖于n和b的正整数。当某项的指数为0时，通常略去不写。例如：<span class="math display">\[\left( x+y\right) ^{4}=x^{4}+4x^{3}y+6x^{2}y^{2}+4xy^{3}+y^{4}\]</span></p><p>其中n次方的系数满足杨辉三角的排列。 <span id="more"></span> <img src="/img/杨辉三角.gif" /></p><h2 id="方程迭代解法">2、方程迭代解法</h2><p>f(x)=0的解法：牛顿-拉弗森方法。 <img src="/img/方程迭代解法.jpg" /> <img src="/img/方程迭代解法1.jpg" /> 迭代法，要考虑分形的曲线，有些序列不收敛，对初始值的选择非常敏感，这些就是混沌。</p><h2 id="级数">3、级数</h2><p>序列的各项之和称为级数。</p><h3 id="几何级数"><strong>1）几何级数</strong></h3><p><img src="/img/几何级数.jpg" /></p><h3 id="调和级数"><strong>2）调和级数</strong></h3><p><strong>调和级数</strong>（英语：Harmonic series）是一个<font color="#4590a3" size = "4px"><strong>发散</strong></font>的无穷级数，表达式为： <span class="math display">\[\sum _{n=1}^{\infty }\dfrac {1} {n}=1+\dfrac {1} {2}+\dfrac {1} {3}+\dfrac {1} {4}+\ldots\]</span></p><p><strong>交错调和级数</strong> <span class="math display">\[\sum _{n=1}^{\infty }\dfrac {\left( -1\right) ^{n+1}} {n}=1-\dfrac {1} {2}+\dfrac {1} {3}-\dfrac {1} {4}+\ldots\]</span> 这个级数可经交错级数判别法证明<font color="#4590a3" size = "4px"><strong>收敛</strong></font>。特别地，这个级数的和等于2的自然对数： <span class="math display">\[1\,-\,{\frac  {1}{2}}\,+\,{\frac  {1}{3}}\,-\,{\frac  {1}{4}}\,+\,{\frac  {1}{5}}\,-\,\cdots \;=\;\ln 2.\]</span></p><p><strong>广义化：P-级数</strong> 调和级数广义化的其中一个结果是p-级数，定义如下： <span class="math display">\[\sum _{n=1}^{\infty }\dfrac {1} {k^{p}}\]</span> 其中P是任意正实数。当p=1，p级数即调和级数。由积分判别法或柯西并项判别法（en:Cauchy condensation test（英文））可知p-级数在p&gt;1时收敛（此时级数又叫过调和级数（over-harmonic series）），而在p ≤ 1时发散。 当p&gt;1时，p-级数的和即ζ(p)，也就是黎曼ζ函数在p的值。</p><h3 id="幂级数">3）幂级数</h3><p><span class="math display">\[\begin{align*}f(x)&amp;=\sum _{n=0}^{\infty }a_{n}\left(x-c\right)^{n}\\&amp;=a_{0}+a_{1}(x-c)^{1}+a_{2}(x-c)^{2}+a_{3}(x-c)^{3}+\cdots\end{align*} \]</span> 其中的c和<span class="math inline">\(a_{i}\)</span> 都是常数。幂级数中的每一项都是一个幂函数，幂次为非负整数。幂级数的形式很像多项式，在很多方面有类似的性质，可以被看成是“无穷次的多项式”。 如果把 (x-c) 看成一项，那么幂级数可以化简为 <span class="math display">\[\sum _{n=0}^{\infty }a_{n}x^{n}\]</span> 后者被称为幂级数的标准形式。一个标准形式的幂级数完全由它的系数来决定。 幂级数可以确定收敛半径，将一个函数写成幂级数： <span class="math display">\[\sum _{n=0}^{\infty }a_{n}(x-c)^{n}\]</span> 的形式称为将函数在c处展开成幂级数。<strong>不是每个函数都可以展开成幂级数。</strong> 多项式可以看做系数从某一项开始全是零的幂级数，例如多项式<span class="math display">\[f(x)=x^{2}+2x+3\]</span> 可以写成标准形式的幂级数： <span class="math display">\[f(x)=3+2x+1x^{2}+0x^{3}+0x^{4}+\cdots\]</span> 也可以写成（c=1）： <span class="math display">\[f(x)=6+4(x-1)+1(x-1)^{2}+0(x-1)^{3}+0(x-1)^{4}+\cdots\]</span> 实际上，多项式可以写成在任意c附近展开的幂级数。就这个意义上说，幂级数是多项式的推广。 等比级数的公式给出了对|x|&lt;1，有 <span class="math display">\[ \dfrac  {1}{1-x}=\sum _{n=0}^\infty x^{n}=1+x+x^{2}+x^{3}+\cdots \]</span> 是幂级数中基本而又重要的一类。同样重要的还有指数的幂级数展开： <span class="math display">\[e^{x}=\sum _{n=0}^\infty \frac  {x^{n}}{n!}=1+x+\frac  {x^{2}}{2!}+\frac  {x^{3}}{3!}+\cdots \]</span> 以及正弦函数（对所有实数x 成立）： <span class="math display">\[\sin \left( x\right) =\sum _{n=0}^{\infty }\dfrac {\left( -1\right) ^{n}x^{2n+1}} {\left( 2n+1\right) !}=x-\dfrac {x^{3}} {3!}+\dfrac {x^{5}} {5!}-\dfrac {x^{7}} {7!}+\ldots \]</span> 这些幂级数都属于泰勒级数。</p><ul><li><strong>ln（1+x）的展开式当且仅当-1&lt;x&lt;=1的时候收敛</strong></li><li><strong>expx的展开式对任意的x都收敛</strong></li></ul><h2 id="收敛的判别方法">4、收敛的判别方法</h2><ul><li>比率判别法</li><li>交错级数判别法</li><li>绝对收敛</li><li>比率判别法</li></ul><h2 id="函数极限">5、函数极限</h2><p>序列的极限是考虑离散的情况（考虑n趋于∞），需要把极限的概念推广到以实数为自变量的函数（考虑x趋于某个数a，f(x)趋于一个极限l）。实数极限的最为重要的应用是微积分理论。 <font color="#4590a3" size = "4px">注意：在极限的工程当中，我们不需要知道f(x)在极限的点x=a的函数值。函数值在a点可以没有定义！</font></p><h2 id="连续函数">6、连续函数</h2><p>在取极限点处取极限值为函数值的函数，称为在这个点是连续的。这种函数的图像可以笔不离纸的画出来。连续函数的函数仍然是连续函数。 连续函数的重要定理是<strong>介值定理</strong>。例如知道函数值在某些点取负数、某些点取正数，那一定存在函数值取0的点（函数与x轴相交）。 连续是可微的必要但非充分条件。</p><h2 id="微分">7、微分</h2><p>可微是研究函数的光滑性概念，<strong>光滑意味着函数的图像中没有转折点</strong>。 函数的瞬时变化率就是微分（导数），如果导数存在，也是改点的唯一的切线的斜率。</p><p><strong>1）基本函数的导数</strong>.<br />所谓基本函数是指一些形式简单并且容易求出导数的函数。这些基本函数的导函数可以通过定义直接求出。 常见的多项式函数就是基本函数之一。如果 <span class="math inline">\(\displaystyle f(x)=x^{r}\)</span> ，其中r是非零实数，那么导函数 <span class="math display">\[\displaystyle f&#39;(x)=rx^{r-1}\,\]</span><br />函数f的定义域可以是整个实数域，但导函数的定义域则不一定与之相同。例如当 <span class="math inline">\(\displaystyle r={\frac {1}{2}}\)</span> 时： <span class="math display">\[\displaystyle f&#39;(x)={\frac {1}{2}}x^{-{\tfrac {1}{2}}}\,\]</span> 导函数的定义域只限所有正实数而不包括0。需要注意的是，不会有多项式函数的导数为 <span class="math inline">\(\displaystyle x^{-1}\)</span> 。当 r = 0 时，常函数的导数是0。</p><p>底数为e的指数函数 <span class="math inline">\(\displaystyle y=e^{x}\)</span> 的导数还是自身： <span class="math display">\[\displaystyle {\frac {\mathrm {d} }{\mathrm {d} x}}e^{x}=e^{x}.\]</span></p><p>而一般的指数函数 <span class="math inline">\(\displaystyle y=a^{x}\)</span> 的导数还需要乘以一个系数： <span class="math display">\[\displaystyle {\frac {\mathrm {d} }{\mathrm {d} x}}a^{x}=\ln(a)a^{x}.\]</span></p><p>自然对数函数的导数则是<span class="math display">\[{\displaystyle  {\frac {\mathrm {d} }{\mathrm {d} x}}\ln(x)={\frac {1}{x}},\qquad x&gt;0.} \]</span></p><p>同样的，一般的对数函数导数则还需要乘以一个系数：<span class="math display">\[ {\displaystyle  {\frac {\mathrm {d} }{\mathrm {d} x}}\log _{a}(x)={\frac {1}{x\ln(a)}}}  \]</span></p><p>三角函数的导数仍然是三角函数，或者由三角函数构成: <span class="math display">\[\displaystyle {\frac {\mathrm {d} }{\mathrm {d} x}}\sin(x)=\cos(x)\;\qquad \qquad \qquad {\frac {\mathrm {d} }{\mathrm {d} x}}\tan(x)=\sec ^{2}(x)={\frac {1}{\cos ^{2}(x)}}.\]</span> <span class="math display">\[\displaystyle {\frac {\mathrm {d} }{\mathrm {d} x}}\cos(x)=-\sin(x)\qquad \qquad \qquad {\frac {\mathrm {d} }{\mathrm {d} x}}\cot(x)=-\csc ^{2}(x)=-{\frac {1}{\sin ^{2}(x)}}.\]</span> 反三角函数的导数则是无理分式: <span class="math display">\[{\displaystyle {\frac {\mathrm {d} }{\mathrm {d} x}}\arcsin(x)={\frac {1}{\sqrt {1-x^{2}}}},\qquad {\frac {\mathrm {d} }{\mathrm {d} x}}\arccos(x)=-{\frac {1}{\sqrt {1-x^{2}}}},\qquad {\frac {\mathrm {d} }{\mathrm {d} x}}\arctan(x)={\frac {1}{1+x^{2}}}.}  \]</span></p><p><strong>2）微分中值定理</strong></p><p>在实分析中，中值定理（mean value theorem）描述了连续光滑曲线在两点之间的光滑性： 令 <span class="math inline">\({\displaystyle f(x)}\)</span> 为连续且光滑，任取其上两点 <span class="math inline">\({\displaystyle (a,f(a))}\)</span> 与 <span class="math inline">\({\displaystyle (b,f(b))}\)</span> ， a &lt; b，那么在这两端点之间必定存在一点 <span class="math inline">\({\displaystyle (c,f(c)),a&lt;c&lt;b}\)</span> ，使得过c的切线斜率等于该二端点割线的斜率，即 <span class="math display">\[{\displaystyle f&#39;(c)={\frac {f(b)-f(a)}{b-a}}}\]</span> 连续函数任二点之间的连续性，则由介值定理来描述。<br />中值定理包括微分中值定理和积分中值定理。</p><p><strong>3）洛必达法则</strong></p><p><strong>0/0型不定式极限</strong><br />若函数f(x)和g(x)满足下列条件：<br />⑴ <span class="math inline">\({\displaystyle \lim _{x\to c}{f(x)}=\lim _{x\to c}{g(x)}=0}\)</span><br />⑵ 在点C的某去心邻域内两者都可导，且<span class="math inline">\({\displaystyle g&#39;(x)\neq 0}\)</span> ；<br />⑶ <span class="math inline">\(\lim _{x\to c}{\frac {f&#39;(x)}{g&#39;(x)}}=A\)</span> （A可为实数，也可为 ±∞ ），<br />则: <span class="math display">\[{\displaystyle \lim _{x\to c}{\frac {f(x)}{g(x)}}=\lim _{x\to c}{\frac {f&#39;(x)}{g&#39;(x)}}} =A\]</span></p><p>例如：x/sin(x)的在0点的极限是1/cos(0)=1</p><p><strong>4）求导链式法则</strong><br />链式法则（chain rule），是求复合函数导数的一个法则。设 f和g为两个关于 x可导函数，则复合函数<span class="math inline">\({\displaystyle (f\circ g)(x)}\)</span> 的导数 <span class="math inline">\({\displaystyle (f\circ g)&#39;(x)}\)</span> 为： <span class="math display">\[{\displaystyle (f\circ g)&#39;(x)=f&#39;(g(x))g&#39;(x).} \]</span></p><p><strong>5）导数的四则运算法则</strong><br />①（u±v)'=u'±v'<br />②（uv)'=u'v+uv'<br />③（u/v)'=(u'v-uv')/v<span class="math inline">\(^2\)</span></p><h2 id="积分">8、积分</h2><p><strong>用面积来定义积分！</strong><br />如果一个函数f(x)在实轴的一个区间I的最大下和和最小上和等于同一个数，那么我们说这个函数在I上是可积的。那个数称为曲线下放区域的面积，记做： <span class="math display">\[\int_a^b f(x)\,\mathrm{d}x \]</span> <strong>求导和积分是互逆的过程。</strong><br /><strong>微积分基本定理：</strong> <img src="/img/求导和积分.jpg" /> <strong>积分的另外一个本质就是求和。</strong><br /><strong>定积分与不定积分的区别：</strong></p><ul><li>不定积分计算的是原函数（得出的结果是一个式子）</li><li>定积分计算的是具体的数值（得出的借给是一个具体的数字）</li><li>一个实变函数在区间[a,b]上定积分是一个实数。它等于该函数的一个原函数在b的值减去在a的值。</li></ul><p><strong>定积分：</strong> <img src="/img/定积分.png" /> <strong>不定积分：</strong><br />在微积分中，一个函数 <span class="math inline">\({\displaystyle {\begin{smallmatrix}f\end{smallmatrix}}}\)</span> 的不定积分，也称为原函数或反导数，是一个导数等于 <span class="math inline">\({\displaystyle {\begin{smallmatrix}f\end{smallmatrix}}}\)</span> 的函数 <span class="math inline">\({\displaystyle {\begin{smallmatrix}F\end{smallmatrix}}}\)</span> ，即 <span class="math inline">\({\displaystyle {\begin{smallmatrix}F&#39;=f\end{smallmatrix}}}\)</span>。不定积分和定积分间的关系由微积分基本定理确定。 <span class="math display">\[{\displaystyle {\begin{matrix}\int f(x)dx=F(x)+C\end{matrix}}}\]</span> 其中 <span class="math inline">\({\displaystyle {\begin{smallmatrix}F\end{smallmatrix}}}\)</span> 是 <span class="math inline">\({\displaystyle {\begin{smallmatrix}f\end{smallmatrix}}}\)</span> 的不定积分。这样，许多函数的定积分的计算就可以简便地通过求不定积分来进行。</p><p><strong>高斯积分（概率积分）：</strong><br />在正态分布中用到！<br />高斯积分（Gaussian integral），有时也被称为概率积分，是高斯函数<span class="math inline">\({\displaystyle f(x)=e^{-x^{2}}}\)</span> 的积分。它是依德国数学家兼物理学家卡尔·弗里德里希·高斯之姓氏所命名。 <span class="math display">\[{\displaystyle \int _{-\infty }^{\infty }e^{-x^{2}}dx={\sqrt {\pi }}} \]</span> 高斯积分在概率论和连续傅里叶变换等的统一化等计算中有广泛的应用。在误差函数的定义中它也出现。虽然误差函数没有初等函数，但是高斯积分可以通过微积分学的手段解析求解。</p><p><strong>分部积分:</strong><br />在传统的微积分教材里分部积分法通常写成不定积分形式： <span class="math display">\[{\displaystyle \int f(x)g&#39;(x)\,dx=f(x)g(x)-\int f&#39;(x)g(x)\,dx,}\]</span> 如果更简单些，令 <span class="math inline">\({\displaystyle u=f(x)}、 {\displaystyle v=g(x)}\)</span> ，微分 <span class="math inline">\({\displaystyle {\rm {d}}u=f&#39;(x){\rm {d}}x}\)</span> 和<span class="math inline">\({\displaystyle {\rm {d}}v=g&#39;(x){\rm {d}}x}\)</span> ，就可以得到更常见到的形式： <span class="math display">\[{\displaystyle \int u\,dv=uv-\int v\,du} \]</span> 注意，上面的原式中含有g的导数；在使用这个规则时必须先找到不定积分g，并且积分<span class="math inline">\({\displaystyle \int gf&#39;{\rm {d}}x}\)</span> 必须是可积的。 在级数的离散分析中也可以用到类似的公式表达，称为分部求和。</p><h2 id="对数函数">9、对数函数</h2><p>对数函数的引入 <img src="/img/对数函数.jpg" /> <img src="/img/对数函数1.jpg" /> ln1=0； 因为在0点函数是发散的，所以限制自变量仅考虑为正数的情况。<span class="math display">\[ \]</span> 性质: <span class="math display">\[\begin{align*}&amp;{\displaystyle \ln(1)=\int _{1}^{1}{\frac {1}{t}}\,dt=0\,}  \\ \\&amp;{\displaystyle \ln(-1)=i\pi \,} \\ \\&amp;{\displaystyle \ln(x)&lt;\ln(y)\quad {\rm {for}}\quad 0&lt;x&lt;y\,} \\ \\&amp;{\displaystyle \lim _{x\to 0}{\frac {\ln(1+x)}{x}}=1\,} \\ \\&amp;{\displaystyle \ln(x^{y})=y\,\ln(x)\,} \\ \\&amp;{\displaystyle {\frac {x-1}{x}}\leq \ln(x)\leq x-1\quad {\rm {for}}\quad x&gt;0\,} \\ \\&amp;{\displaystyle \ln {(1+x^{\alpha })}\leq \alpha x\quad {\rm {for}}\quad x\geq 0,\alpha \geq 1\,} \end{align*}\]</span> 自然对数的导数性质导致了ln(1 + x)在0处的泰勒级数，也叫做麦卡托级数： <span class="math display">\[{\displaystyle \ln(1+x)=\sum _{n=1}^{\infty }{\frac {(-1)^{n+1}}{n}}x^{n}=x-{\frac {x^{2}}{2}}+{\frac {x^{3}}{3}}-\cdots } \]</span> 对于所有 <span class="math inline">\({\displaystyle \left|x\right|\leq 1,}\)</span> 但不包括x = -1.</p><h2 id="指数函数">10、指数函数</h2><p>expx函数的定义是对数函数的反函数！ <img src="/img/指数函数图.jpg" /> <strong>指数函数的导数是它本身</strong>。<br /><em>e</em>，作为数学常数，是自然对数函数的底数。有时被称为欧拉数（Euler's number），以瑞士数学家欧拉命名；<em>e</em> = 2.71828182845904523536...<br /><em>e</em> 是无理数和超越数,这是第一个获证为超越数的数，而非故意构造的。就像圆周率 <span class="math inline">\({\displaystyle {\begin{smallmatrix}\pi\end{smallmatrix}}}\)</span> 和虚数单位 <em>i</em>，<em>e</em> 是数学中最重要的常数之一。它有几种等价定义，下面列出一部分。<br /><strong>最常见的四种 <em>e</em> 的定义如下</strong></p><ol type="1"><li>定义 <em>e</em> 为下列极限值： <span class="math display">\[{\displaystyle e =\lim _{n\to \infty }\left(1+{\frac {1}{n}}\right)^{n}} \]</span></li><li>定义 <em>e</em> 为下列无穷级数之和： <span class="math display">\[{\displaystyle e =\sum _{n=0}^{\infty }{1 \over n!}={1 \over 0!}+{1 \over 1!}+{1 \over 2!}+{1 \over 3!}+{1 \over 4!}+\cdots } \]</span></li><li>定义 <em>e</em> 为唯一的正数 <em>x</em> 使得 <span class="math display">\[{\displaystyle \int _{1}^{x}{\frac {1}{t}}\,dt={1}} \]</span></li><li>定义 <em>e</em> 为唯一的实数 <em>x</em> 使得 <span class="math display">\[{\displaystyle \lim _{h\to 0}{\frac {x^{h}-1}{h}}=1} \]</span></li></ol><p>这些定义可证明是等价的，请参见文章指数函数的特征描述。</p><p><strong>性质</strong><br />很多增长或衰减过程都可以用指数函数模拟。指数函数<span class="math inline">\({\displaystyle e^{x}}\)</span> 的重要性，在于它是唯一的函数（零多项式函数除外）与自身导数相等（乘以常数，最一般的函数形式为<span class="math inline">\({\displaystyle ke^{x}}\)</span> ，k为任意常数）。即: <span class="math display">\[{\displaystyle {\frac {d}{dx}}e^{x}=e^{x}}\]</span> 指数函数<span class="math inline">\({\displaystyle e^{x}}\)</span> 的泰勒级数为 <span class="math display">\[{\displaystyle e^{x}=\sum _{n=0}^{\infty }{\frac {x^{n}}{n!}}=1+x+{\frac {x^{2}}{2!}}+{\frac {x^{3}}{3!}}+...  \quad  \forall x}\]</span> x为复数时依然成立，因此根据<span class="math inline">\({\displaystyle \sin x}\)</span> 及 <span class="math inline">\({\displaystyle \cos x}\)</span> 的泰勒级数，得出在数学中一条称为欧拉公式的重要等式： <span class="math display">\[{\displaystyle e^{\mathrm {i} x}=\cos x+{\rm {i}}\sin x\,\!}\]</span> 当 <span class="math inline">\({\displaystyle x=\pi }\)</span> 的特例是欧拉恒等式：</p><p><span class="math display">\[{\displaystyle e^{\mathrm {i} \pi }+1=0\,\!}\]</span></p><p>此式被理查德·费曼称为“欧拉的宝石”。</p><h2 id="泰勒级数">11、泰勒级数</h2><p><strong>把函数和幂级数关联起来。</strong> <img src="/img/泰勒级数.jpg" /> 几个常用的泰勒级数：</p><p>下面我们给出了几个重要的泰勒级数。参数x 为复数时它们依然成立。</p><p>几何级数： <span class="math display">\[{\displaystyle {\frac {1}{1-x}}=\sum _{n=0}^{\infty }x^{n}\quad \forall x:\left|x\right|&lt;1}\]</span> 二项式定理： <span class="math display">\[(1+x)^{\alpha }=\sum _{n=0}^{\infty }C(\alpha ,n)x^{n}\quad \forall x:\left|x\right|&lt;1,\forall \alpha \in \mathbb {C} \]</span> 二项式展开中的C(α,n)是二项式系数。 指数函数和自然对数： <span class="math display">\[e^{x}=\sum _{n=0}^{\infty }{\frac {x^{n}}{n!}}\quad \forall x\]</span> <span class="math display">\[{\displaystyle \ln(1+x)=\sum _{n=1}^{\infty }{\frac {(-1)^{n+1}}{n}}x^{n}\quad \forall x\in (-1,1]} \]</span> 三角函数： <span class="math display">\[{\displaystyle \sin x=\sum _{n=0}^{\infty }{\frac {(-1)^{n}}{(2n+1)!}}x^{2n+1}\quad \forall x}\]</span> <span class="math display">\[{\displaystyle \cos x=\sum _{n=0}^{\infty }{\frac {(-1)^{n}}{(2n)!}}x^{2n}\quad \forall x} \]</span></p><h2 id="π与分析学观点下的三角学">12、π与分析学观点下的三角学</h2><p><img src="/img/π的分析学定义1.jpg" /> <img src="/img/π的分析学定义2.jpg" /> <img src="/img/三角函数的引入.jpg" /> π的表达式： <span class="math display">\[\begin{align*}\frac{\pi}{4}&amp;=1- \frac{1}{3} + \frac{1}{5} - \frac{1}{7} + \frac{1}{9} - \cdots  (Leibniz定理) \\\frac{\pi^2}{6}&amp;= \frac{1}{1^2} + \frac{1}{2^2} + \frac{1}{3^2} + \frac{1}{4^2} + \cdots \end{align*}\]</span></p><h2 id="傅里叶级数">13、傅里叶级数</h2><p><img src="/img/傅里叶级数.jpg" /></p><p>下一篇：《数学桥--对高等数学的一次观赏之旅》读书心得之三</p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 科技 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《数学桥--对高等数学的一次观赏之旅》读书心得之三</title>
      <link href="/2016/10/30/%E3%80%8A%E6%95%B0%E5%AD%A6%E6%A1%A5--%E5%AF%B9%E9%AB%98%E7%AD%89%E6%95%B0%E5%AD%A6%E7%9A%84%E4%B8%80%E6%AC%A1%E8%A7%82%E8%B5%8F%E4%B9%8B%E6%97%85%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97%E4%B9%8B%E4%B8%89/"/>
      <url>/2016/10/30/%E3%80%8A%E6%95%B0%E5%AD%A6%E6%A1%A5--%E5%AF%B9%E9%AB%98%E7%AD%89%E6%95%B0%E5%AD%A6%E7%9A%84%E4%B8%80%E6%AC%A1%E8%A7%82%E8%B5%8F%E4%B9%8B%E6%97%85%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97%E4%B9%8B%E4%B8%89/</url>
      
        <content type="html"><![CDATA[<p>接上篇：《数学桥--对高等数学的一次观赏之旅》读书心得之二</p><h1 id="三代数">三、代数</h1><h2 id="线性方程">1、线性方程</h2><h2 id="向量及向量空间">2、向量及向量空间</h2><p>一个向量空间完全由它的维数和用来与向量做标量乘法的数系所刻画。</p><h2 id="矩阵">3、矩阵</h2><p>矩阵用来表示向量函数。</p><h2 id="非齐次线性方程">4、非齐次线性方程</h2><p>解线性方程的本质实际上就是求矩阵的逆阵！<br />矩阵A可逆的充要条件是A中列向量的任何一个线性组合都不为0；例如：不能一列全是0，或者两列完全相同！反过来，矩阵任何一个列向量线性组合都不可能等于零最明显的情况是这个矩阵是单位矩阵的一个纯量倍数。</p><span id="more"></span><h2 id="行列式">5、行列式</h2><p>行列式：detA表示A的行列式；一个方阵A有一个逆阵的充要条件是detA!=0。<br />对于简单的2阶和3阶的矩阵，行列式的表达式相对简单，而且恰好是每条主对角线（左上至右下）元素乘积之和减去每条副对角线（右上至左下）元素乘积之和（见图中红线和蓝线）。<br />2阶矩阵的行列式：<br /><span class="math display">\[{\displaystyle {\begin{vmatrix}a_{1,1}&amp;a_{1,2}\\a_{2,1}&amp;a_{2,2}\end{vmatrix}}=a_{1,1}a_{2,2}-a_{1,2}a_{2,1}}\]</span><br />3阶矩阵的行列式：<br /><span class="math display">\[{\displaystyle \displaystyle {\begin{vmatrix}a_{1,1}&amp;a_{1,2}&amp;a_{1,3}\\a_{2,1}&amp;a_{2,2}&amp;a_{2,3}\\a_{3,1}&amp;a_{3,2}&amp;a_{3,3}\end{vmatrix}}=a_{1,1}a_{2,2}a_{3,3}+a_{1,2}a_{2,3}a_{3,1}+a_{1,3}a_{2,1}a_{3,2}-a_{1,3}a_{2,2}a_{3,1}-a_{1,1}a_{2,3}a_{3,2}-a_{1,2}a_{2,1}a_{3,3}}\]</span><br /><img src="/img/行列式的计算.png" /><br />几何意义：二维和三维欧氏空间中的例子<br />行列式的一个自然的源起是n维平行体的体积。行列式的定义和n维平行体的体积有着本质上的关联。<br />二维向量组的行列式<br /><img src="/img/行列式的几何意义.png" /><br />在一个二维平面上，两个向量X = (a, c)和X' = (b, d)的行列式是：<br /><span class="math display">\[{\displaystyle \det(X,X&#39;)={\begin{vmatrix}a&amp;b\\c&amp;d\end{vmatrix}}=ad-bc} \]</span><br />比如说，两个向量X = (2, 1)和X' = (3, 4)的行列式是：<br /><span class="math display">\[{\displaystyle \det(X,X&#39;)={\begin{vmatrix}2&amp;3\\1&amp;4\end{vmatrix}}=2\cdot 4-3\cdot 1=5} \]</span><br />经计算可知，当系数是实数时，行列式表示的是向量X和X'形成的平行四边形的有向面积，并有如下性质：<br />行列式为零当且仅当两个向量共线（线性相关），这时平行四边形退化成一条直线[8]。<br />如果以逆时针方向为正向的话，有向面积的意义是：平行四边形面积为正当且仅当以原点为不动点将X逆时针“转到”X'处时，扫过的地方在平行四边形里，否则的话面积就是负的。如右图中，X和X'所构成的平行四边形的面积就是正的。<br />行列式是一个双线性映射。也就是说，<span class="math display">\[{\displaystyle \det(\lambda X+\mu Y,X&#39;)=\lambda \det(X,X&#39;)+\mu \det(Y,X&#39;)\;} \]</span><br />并且<br /><span class="math display">\[{\displaystyle \det(X,\lambda X&#39;+\mu Y&#39;)=\lambda \det(X,X&#39;)+\mu \det(X,Y&#39;)\;} \]</span></p><h2 id="最优化">6、最优化</h2><p><strong>1）线性约束</strong><br />给定一些约束方程的条件下求得某个量的一个最优值或最大值。<br /><strong>函数的最大和最小值都出现在区域的顶点上</strong>。<br /><font color="#4590a3" size = "4px"><strong>凸区域：区域中任何两点都可以被一条不与边界相交的直线段连接起来。</strong></font><br /><img src="/img/线性约束.jpg" /><br /><strong>2）单纯形法</strong><br /><img src="/img/单纯形法.jpg" /><br /><img src="/img/单纯形法1.jpg" /></p><h2 id="距离长度角度">7、距离、长度、角度</h2><p>从一个向量到一个标量的运算：纯量积（点积），符号是：·<br />例如x·y表示向量x与向量y的距离。<br />两个向量<span class="math display">\[{\displaystyle {\vec {a}}}  = [a_{1}, a_{2}, …, a_{n}] \]</span>和<span class="math display">\[{\displaystyle {\vec {b}}} = [b_{1}, b_{2}, …, b_{n}] \]</span>的点积定义为:<br /><span class="math display">\[{\displaystyle {\vec {a}}\cdot {\vec {b}}=\sum _{i=1}^{n}a_{i}b_{i}=a_{1}b_{1}+a_{2}b_{2}+\cdots +a_{n}b_{n}} \]</span><br />定义向量的长度<br /><span class="math display">\[{|x|=\sqrt {x\cdot x}}\]</span></p><p>角度及点积的几何意义：<br /><strong>几何定义</strong>:在欧几里得空间中，点积可以直观地定义为<br /><span class="math display">\[{\displaystyle {\vec {a}}\cdot {\vec {b}}=|{\vec {a}}|\,|{\vec {b}}|\cos \theta \;}  \]</span> 这里 |<span class="math inline">\({\displaystyle {\vec {x}}}\)</span>| 表示 <span class="math inline">\({\displaystyle {\vec {x}}}\)</span> 的模（长度），θ表示两个向量之间的角度。<br />注意：点积的形式定义和这个定义不同；在形式定义中，<span class="math inline">\({\displaystyle {\vec {a}}}\)</span> 和 <span class="math inline">\({\displaystyle {\vec {b}}}\)</span> 的夹角是通过上述等式定义的。<br />这样，两个互相垂直的向量的点积总是零。若<span class="math inline">\({\displaystyle {\vec {a}}}\)</span> 和<span class="math inline">\({\displaystyle {\vec {b}}}\)</span> 都是单位向量（长度为1），它们的点积就是它们的夹角的余弦。那么，给定两个向量，它们之间的夹角可以通过下列公式得到：<br /><span class="math display">\[{\displaystyle \cos {\theta }={\frac {\mathbf {a\cdot b} }{|{\vec {a}}|\,|{\vec {b}}|}}}  \]</span> 这个运算可以简单地理解为：在点积运算中，第一个向量投影到第二个向量上（这里，向量的顺序是不重要的，点积运算是可交换的），然后通过除以它们的标量长度来“标准化”。这样，这个分数一定是小于等于1的，可以简单地转化成一个角度值。<br /><img src="/img/纯量积几何意义.jpg" /></p><h2 id="二维空间的二次型">8、二维空间的二次型</h2><p>ax²+bx+cy²+exy=f<br />通过配平方把x,y的线性项去掉。变成形式Ax²+Bxy+Cy²=D；利用二次方程求解公式可求。<br />这些解有六种情况：圆周、椭圆、双曲线、抛物线、直线、点；<br />如下图：（<strong>用一个平面去切割一个圆锥</strong>）<br /><img src="/img/二次型.jpg" /></p><h2 id="特征向量和特征值">9、特征向量和特征值</h2><p><img src="/img/特征值.jpg" /><br /><strong>重点研究实对称矩阵的特殊性质：</strong></p><ul><li>实对称矩阵的特征值总是实数；<br /></li><li>对称矩阵的两个特征向量如果对应的特征值不同，那么他们相交；<br /></li><li>从任何一个n*n实对称矩阵M的特征值向量集合总，总可以为R^n选取一组标准正交基。</li></ul><p><strong>应用：特征脸。</strong><br />在图像处理中，脸部图像的处理可以看作分量为每个像素的灰度的向量。该向量空间的维数是像素的个数。一个标准化面部图形的一个大型数据集合的协方差矩阵的特征向量称为特征脸。它们对于将任何面部图像表达为它们的线性组合非常有用。特征脸提供了一种用于识别目的的数据压缩的方式。在这个应用中，一般只取那些最大特征值所对应的特征脸。</p><h2 id="对称">10、对称</h2><p><strong>对称群：简称群</strong>。任何满足群公理的系统均可被认为是一个由对称组成的相容集合。<br /><img src="/img/群公理.jpg" /><br /><img src="/img/群作用1.jpg" /><br /><img src="/img/群作用2.jpg" /></p><p>下一篇：《数学桥--对高等数学的一次观赏之旅》读书心得之四</p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 科技 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《数学桥--对高等数学的一次观赏之旅》读书心得之四</title>
      <link href="/2016/10/30/%E3%80%8A%E6%95%B0%E5%AD%A6%E6%A1%A5--%E5%AF%B9%E9%AB%98%E7%AD%89%E6%95%B0%E5%AD%A6%E7%9A%84%E4%B8%80%E6%AC%A1%E8%A7%82%E8%B5%8F%E4%B9%8B%E6%97%85%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97%E4%B9%8B%E5%9B%9B/"/>
      <url>/2016/10/30/%E3%80%8A%E6%95%B0%E5%AD%A6%E6%A1%A5--%E5%AF%B9%E9%AB%98%E7%AD%89%E6%95%B0%E5%AD%A6%E7%9A%84%E4%B8%80%E6%AC%A1%E8%A7%82%E8%B5%8F%E4%B9%8B%E6%97%85%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97%E4%B9%8B%E5%9B%9B/</url>
      
        <content type="html"><![CDATA[<p>接上篇：《数学桥--对高等数学的一次观赏之旅》读书心得之三</p><h1 id="四微积分与微分方程">四、微积分与微分方程</h1><h2 id="微积分">1、微积分</h2><p>积分的本质就是无限细分然后求和！<br />具体的求导、积分的公式见前文二、分析中对应微分和积分的部分！<br /><img src="/img/积分求和.jpg" /><br /><span id="more"></span><br /><strong>微积分基本定理：积分和求导是一对互逆的过程。</strong><br /><img src="/img/微积分基本定理.jpg" /></p><h2 id="线性常微分方程">2、线性常微分方程</h2><p>通常用像t，exp(t)，sin(x)，cos(x)，lnt这些常见函数的组合来猜测微分方程解的可能形式，尽管有时候借着几分运气！<br /><strong>1） 解齐次线性方程</strong></p><ul><li>猜测法<br />我们研究一些方法，这些方法让我们对一个完全非线性系统其实根本不需要去求得任何解就能得到关于这些解的有效信息！这些定性的方法是微分方程中混沌现象研究的基础！<br /></li><li>幂级数法<br />我们将要利用的事实是，如果一个函数在点t=0处的各阶导数形态良好，那么正如我们学习分析学时所知道的，它就是一个关于原点的幂级数展开式，先把一个微分方程化成一个关于幂级数的方程，这个方程就有可能解出。<br />化成幂级数之后，先代入方程，通过满足方程的解，求出幂级数的系数！<br /><strong>有些方程需要用到广义的幂级数来展开，例如贝塞尔函数。</strong></li></ul><p><strong>2）解非齐次方程</strong><br />解是非线性系统。例如：f(x+y)!=f(x+y)<br />求出一个特解，然后再求出对应齐次方程的一个解，然后加起来，就得出另外一个特解。</p><p><strong>个人理解：线性方程和线性系统是不一样的</strong>，线性方程代表，未知数没有幂次项；而线性系统，指方程的解满足线性系统要求，就是不同解的线性组合还是方程的解。例如齐次方程和非齐次方程可以都是线性方程，但是齐次方程的解是线性系统；而非齐次方程的解不是线性系统！</p><h2 id="偏微分方程">3、偏微分方程</h2><p>f(x,t)，偏袒一方的求f(x,t)关于x和t的导数，偏导数可以看做函数f(x,t)在另一个变量的值固定时的变化率。要对函数求偏导数，你只要暂时把另外一个变量看成常数，像一维情况那样求导数就行了。</p><p><strong>当要建立一个微分方程时，我们首先探究这个问题的一个离散化近似，然后取极限以确定精确解！</strong><br /><strong>物理或化学方程为什么往往是偏微分方程？因为这些运动或者变化，并且都会受多个变量的影响，例如，运动会随时间或者位置而变化，所以在实际过程当中，就先离散化，然后求极限来处理，所以基本都是用偏微分方程来表示</strong></p><p><strong>1）弦振动方程</strong><br /><strong>弦振动方程</strong><br /><img src="/img/弦振动方程.jpg" /><br /><strong>波动方程</strong><br /><span class="math display">\[{\partial ^{2}u(x,t) \over \partial t^{2}}={KL^{2} \over M}{\partial ^{2}u(x,t) \over \partial x^{2}}\]</span><br />在这个例子中，波速<span class="math display">\[{\displaystyle c={\sqrt {\frac {KL^{2}}{M}}}} \]</span><br />一般解<br />代数方法<br />一维标量形式波动方程的一般解是由达朗贝尔给出的。原方程可以写成如下的算子作用形式：<br /><span class="math display">\[{\displaystyle \left[{\frac {\partial }{\partial t}}-c{\frac {\partial }{\partial x}}\right]\left[{\frac {\partial }{\partial t}}+c{\frac {\partial }{\partial x}}\right]u=0.\,} \]</span><br />从上面的形式可以看出，若F和G为任意函数，那么它们以下形式的组合<br /><span class="math display">\[{\displaystyle u(x,t)=F(x-ct)+G(x+ct)\,}\]</span><br />必然满足原方程。上面两项分别对应两列行波（"行"与"行动"中同音）——F表示经过该点（x点）的右行波，G表示经过该点的左行波。为完全确定F和G的最终形式还需考虑如下初始条件：<br /><span class="math display">\[{\displaystyle u(x,0)=f(x)\,} \]</span> <span class="math display">\[{\displaystyle u_{t}(x,0)=g(x)\,} \]</span> 经带入运算，就得到了波动方程著名的达朗贝尔行波解，又称达朗贝尔公式：<br /><span class="math display">\[{\displaystyle u(x,t)={\frac {f(x-ct)+f(x+ct)}{2}}+{\frac {1}{2c}}\int _{x-ct}^{x+ct}g(s)ds} \]</span><br />在经典的意义下，如果 <span class="math inline">\({\displaystyle f(x)\in C^{k}}\)</span> 并且 <span class="math inline">\({\displaystyle g(x)\in C^{k-1}}\)</span> 则 <span class="math inline">\({\displaystyle u(t,x)\in C^{k}}\)</span>。但是，行波函数F和G也可以是广义函数，比如狄拉克δ函数。在这种情况下，行波解应被视作左行或右行的一个脉冲。<br />基本波动方程是一个线性微分方程，也就是说同时受到两列波作用的点的振幅就是两列波振幅的相加。这意味着可以通过把一列波分解成它的许求解中很有效。此外，可以通过将波分离出各个分量来分析，例如傅里叶变换可以把波分解成正弦分量。<br /><strong>弦乐器</strong><br /><img src="/img/弦乐器1.jpg" /><br /><img src="/img/弦乐器2.jpg" /></p><p><strong>2）扩散方程</strong><br /><img src="/img/扩散方程.jpg" /></p><h2 id="微积分与几何相遇">4、微积分与几何相遇</h2><p><strong>1） 切向量和法向量</strong><br /><img src="/img/切向量.jpg" /><br /><img src="/img/法向量.jpg" /><br /><strong>2） 梯度、散度和旋度</strong></p><p>下一篇：《数学桥--对高等数学的一次观赏之旅》读书心得之五</p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 科技 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《数学桥--对高等数学的一次观赏之旅》读书心得之五</title>
      <link href="/2016/10/30/%E3%80%8A%E6%95%B0%E5%AD%A6%E6%A1%A5--%E5%AF%B9%E9%AB%98%E7%AD%89%E6%95%B0%E5%AD%A6%E7%9A%84%E4%B8%80%E6%AC%A1%E8%A7%82%E8%B5%8F%E4%B9%8B%E6%97%85%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97%E4%B9%8B%E4%BA%94/"/>
      <url>/2016/10/30/%E3%80%8A%E6%95%B0%E5%AD%A6%E6%A1%A5--%E5%AF%B9%E9%AB%98%E7%AD%89%E6%95%B0%E5%AD%A6%E7%9A%84%E4%B8%80%E6%AC%A1%E8%A7%82%E8%B5%8F%E4%B9%8B%E6%97%85%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97%E4%B9%8B%E4%BA%94/</url>
      
        <content type="html"><![CDATA[<p>接上篇：《数学桥--对高等数学的一次观赏之旅》读书心得之四</p><h1 id="五概率">五、概率</h1><p>概率论就是用数学的确定性来描述随机的过程。<br /><strong>样本空间:</strong>Ω； <strong>某个结局</strong>:ω（在样本空间中）<br />概率论就是先要把问题转换成事件，然后通过概率学符号来处理。<br /><strong>事件：</strong>某些我们感兴趣事件（A）的概率P（A）。A是Ω样本空间中的某一种情况。<br />P(A)=使A中事件可以发生的等可能方式的数目/等可能的结局的总数。<br />等可能：相同可能，同概率随机事件。P(ω1)=P(ω2)=P(ω3)……<br /><span id="more"></span></p><h2 id="例1生日相同问题">例1、生日相同问题</h2><p>假设有n个人参加一个聚会，其中至少有两个人生日相同的概率是多少？<br /><strong>思路：</strong>求完全不相同的概率，再1减去该概率。<br /><strong>特征：</strong>当n大于或等于23的时候，两个人生日相同的可能性就过半。如果房间里有70个人，那么有两个人同生日的概率就达到了99.9%。<br /><img src="/img/同生日.jpg" /></p><h2 id="例2比赛中止问题">例2、比赛中止问题</h2><p>如果不是等可能是，必须非常小心。</p><h2 id="例3门和山羊的问题">例3、门和山羊的问题</h2><p>蒙提霍尔问题，亦称为蒙特霍问题或三门问题（英文：Monty Hall problem），是一个源自博弈论的数学游戏问题，大致出自美国的电视游戏节目Let's Make a Deal。问题的名字来自该节目的主持人蒙提·霍尔（Monty Hall）。<br />这个游戏的玩法是：参赛者会看见三扇关闭了的门，其中一扇的后面有一辆汽车或者是奖品，选中后面有车的那扇门就可以赢得该汽车或奖品，而另外两扇门后面则各藏有一只山羊或者是后面没有任何东西。当参赛者选定了一扇门，但未去开启它的时候，知道门后情形的节目主持人会开启剩下两扇门的其中一扇，露出其中一只山羊。主持人其后会问参赛者要不要换另一扇仍然关上的门。问题是：换另一扇门会否增加参赛者赢得汽车的机会率？如果严格按照上述的条件的话，答案是会。—换门的话，赢得汽车的概率是2/3。</p><h2 id="例4外套问题">例4、外套问题</h2><p>n位数学家参加一个聚会，他们脱下外套，放在一起，聚会结束，他们各人随机地取了一件，问题：至少有一个人取了自己外套的概率是多少？<br /><img src="/img/外套问题.jpg" /></p><h2 id="容斥公式">1、容斥公式</h2><p><img src="/img/韦恩图.jpg" /><br /><img src="/img/容斥公式.jpg" /></p><h2 id="条件概率">2、条件概率</h2><p>A在B已发生的条件下的概率。P(A|B)<br /><img src="/img/条件概率.jpg" /></p><h2 id="全概率定律和贝叶斯公式">3、全概率定律和贝叶斯公式</h2><p><img src="/img/全概率定律.jpg" /><br /><img src="/img/贝叶斯公式.jpg" /></p><h2 id="样本空间上的函数随机变量">4、样本空间上的函数：随机变量</h2><p><strong>我们感兴趣的往往不是一个特定试验的结局，而是这个结局的某种函数。</strong><br />随机变量：我们探究时间空间上的函数的性质，这样的一种<strong>函数</strong>称为随机变量！</p><p><strong>1）二项分布</strong><br />结局只有成功或失败两种！<br /><img src="/img/二项分布.jpg" /><br />二项分布的泊松近似：<br />我们感兴趣的是，实验进行了很多次，但其中不成功却发生得相当稀少的情况。（小概率事件）<br /><img src="/img/二项分布的泊松近似.jpg" /></p><p><strong>2）泊松分布</strong><br /><strong>泊松分布适合于描述单位时间内随机事件发生的次数的概率分布。</strong><br />Poisson分布（法语：loi de Poisson，英语：Poisson distribution），译名有泊松分布、普阿松分布、帕松分布、布瓦松分布、布阿松分布、波以松分布、卜氏分配等，又称泊松小数法则（Poisson law of small numbers），是一种统计与概率学里常见到的离散概率分布，由法国数学家西莫恩·德尼·泊松（Siméon-Denis Poisson）在1838年时发表。<br />泊松分布适合于描述单位时间内随机事件发生的次数的概率分布。如某一服务设施在一定时间内受到的服务请求的次数，电话交换机接到呼叫的次数、汽车站台的候客人数、机器出现的故障数、自然灾害发生的次数、DNA序列的变异数、放射性原子核的衰变数、激光的光子数分布等等。<br />泊松分布的概率质量函数为：<br /><span class="math display">\[{\displaystyle P(X=k)={\frac {e^{-\lambda }\lambda ^{k}}{k!}}} \]</span><br />泊松分布的参数λ是单位时间（或单位面积）内随机事件的平均发生率。<br /><img src="/img/泊松分布图.png" /></p><p><strong>3）连续性随机变量</strong><br />随机变量是作用在某个样本空间上而产生出实数输出的函数！<br />概率密度函数：<strong>概率是取某段期间的面积（积分）来代替，而不是像离散随机变量那样是一个点的值</strong><br /><img src="/img/概率密度函数.jpg" /></p><p><strong>4）正态分布</strong><br /><img src="/img/正态分布.jpg" /></p><p><strong>5）均匀分布</strong><br />蒲丰投针问题，用来估计π的值！</p><h2 id="平均化与期望">5、平均化与期望</h2><p><strong>需要关注的不是试验产生某个特定结果的概率，而是这个试验最有可能产生的结果范围。期望为我们给出了关于一个试验可能是什么结局的好想法！</strong><br />期望的定义（加权平均）：<br /><img src="/img/期望.jpg" /><br /><strong>期望是刻画整个概率分布的一个单独的数，它不一定是任何一个特定试验的结果。</strong><br />例如，抛硬币。E[x]=(1+2+3+4+5+6)/6=3.5<br /><strong>这种情况说明：因为掷骰子，掷一次，6个点数都是同样的可能！所以，这种情况下，由期望所预测出来的值基本上没有用处！</strong></p><h2 id="离散程度与方差">6、离散程度与方差</h2><p>方差，提供了分布值正在平均值周围离散程度的一种切实度量！方差大意味着随机变量有一个较广的分布，而方差小意味着一个较窄的分布。<br /><img src="/img/方差.jpg" /></p><h2 id="极限定理">7、极限定理</h2><p><strong>1）切比雪夫不等式</strong><br />切比雪夫不等式为我们提供了一种方法，这种方法利用方差准确地确定了随机变量与平均值的偏差至少为一给定值的最大概率。切比雪夫不等式给出了最好的界限。<br /><img src="/img/切比雪夫不等式.jpg" /><br />这个不等式以数量化这方式来描述，究竟“几乎所有”是多少，“接近”又有多接近：<br />与平均相差2个标准差以上的值，数目不多于1/4<br />与平均相差3个标准差以上的值，数目不多于1/9<br />与平均相差4个标准差以上的值，数目不多于1/16<br />……<br />与平均相差k个标准差以上的值，数目不多于1/k²<br />举例说，若一班有36个学生，而在一次考试中，平均分是80分，标准差是10分，我们便可得出结论：少于50分（与平均相差3个标准差以上）的人，数目不多于4个（=36*1/9）。<br /><img src="/img/标准差.jpg" /><br /><img src="/img/标准差分布图.png" /></p><p><strong>2）大数律</strong><br />假设我们把某个随机试验进行许多次，并记下我们每一次试验的结果。直觉告诉我们，经过足够多次的试验后，根据所谓的“平均律”，所记录结果的平均值会趋向与某个固定的极限。有了大数定律，我们可以确信，为求得一个随机变量的期望，我们只要把它测量许多次，然后取我们所得值的平均值就可以了。</p><p><strong>3）中心极限定理和正态分布</strong><br />中心极限定理是概率论中的一组定理。中央极限定理说明，大量相互独立的随机变量，其均值的分布以正态分布为极限。这组定理是数理统计学和误差分析的理论基础，指出了大量随机变量之和近似服从正态分布的条件。<br /><img src="/img/中心极限定理.png" /></p><p>完！</p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 科技 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>如何在Hexo中使用MathJax</title>
      <link href="/2016/10/24/%E5%A6%82%E4%BD%95%E5%9C%A8Hexo%E4%B8%AD%E4%BD%BF%E7%94%A8MathJax/"/>
      <url>/2016/10/24/%E5%A6%82%E4%BD%95%E5%9C%A8Hexo%E4%B8%AD%E4%BD%BF%E7%94%A8MathJax/</url>
      
        <content type="html"><![CDATA[<p>MathJax是一款运行在浏览器中的开源的数学符号渲染引擎，使用MathJax可以方便的在浏览器中显示数学公式，不需要使用图片。目前，MathJax可以解析Latex(wiki百科使用的)、MathML和ASCIIMathML的标记语言。 MathJax项目于2009年开始，发起人有American Mathematical Society, Design Science等，还有众多的支持者，个人感觉MathJax会成为今后数学符号渲染引擎中的主流，也许现在已经是了。<br /><strong>注：wiki拷贝过来的公式需要做一定的修改，才能正确显示。具体的差异可以通过一下网站生成一个公式的LaTeX代码比较一下！《黄金分割》这篇博客是第一次用MathJax书写数学公式。</strong></p><h1 id="一math的使用及安装">一、Math的使用及安装</h1><h2 id="在线手写数学工具生成器">1、在线手写数学工具生成器</h2><p>推荐一个在线手写公式转Tex格式的利器：<a href="https://webdemo.myscript.com/views/math.html#">Web Equation</a>。<br />通过手写公式，即可得到公式所对应的Tex格式，非常好用。</p><span id="more"></span><h2 id="语法">2、语法</h2><p><strong>请注意</strong>：当你需要在编辑器中插入数学公式时，可以使用两个美元符 $$ 包裹 TeX 或 LaTeX 格式的数学公式来实现。提交后，问答和文章页会根据需要加载 Mathjax 对数学公式进行渲染。</p><div class="sourceCode" id="cb1"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>$a + b^2$  </span><span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>$x = &#123;-b \pm \sqrt&#123;b^2-4ac&#125; \over 2a&#125;$  </span><span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>$$\dfrac &#123;a+b&#125; &#123;a&#125;=\dfrac &#123;a&#125; &#123;b&#125;=\phi$$  </span><span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>$$\Delta_A(\lambda)=\det(\lambda I-A)$$  </span></code></pre></div><p>显示如下：<br /><span class="math inline">\(a + b^2\)</span><br /><span class="math inline">\(x = {-b \pm \sqrt{b^2-4ac} \over 2a}\)</span><br /><span class="math inline">\(\dfrac {a+b} {a}=\dfrac {a} {b}=\phi\)</span><br /><span class="math inline">\(\Delta_A(\lambda)=\det(\lambda I-A)\)</span></p><h2 id="安装">3、安装</h2><div class="sourceCode" id="cb2"><pre class="sourceCode txt"><code class="sourceCode default"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>npm install hexo-math --save  </span></code></pre></div><p>安装之后重启hexo server</p><h1 id="二hexo主题与math的冲突解决">二、Hexo主题与Math的冲突解决</h1><h2 id="在hexo主题下的文件_config.yml中找到如下语句修改为true">1、在hexo主题下的文件_config.yml中找到如下语句，修改为true</h2><div class="sourceCode" id="cb3"><pre class="sourceCode tex"><code class="sourceCode latex"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a># ---------------------------------------------------------------  </span><span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a># Third Party Services Settings  </span><span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a># ---------------------------------------------------------------  </span><span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a># MathJax Support  </span><span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>mathjax:  </span><span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>  enable: true  </span><span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>  cdn: //cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML  </span></code></pre></div><!--more--><h2 id="如何处理hexo和mathjax的兼容问题">2、如何处理Hexo和MathJax的兼容问题</h2><p>简单来说，要让你的Hexo支持MathJax渲染公式，你只需要使用两条命令：<br />To fully support MathJax in your Hexo blog, you can simply use the following commands:</p><div class="sourceCode" id="cb4"><pre class="sourceCode javascript"><code class="sourceCode javascript"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>npm uninstall hexo<span class="op">-</span>renderer<span class="op">-</span>marked <span class="op">--</span>save  </span><span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>npm install hexo<span class="op">-</span>renderer<span class="op">-</span>kramed <span class="op">--</span>save  </span></code></pre></div><p>第一条命令用于卸载 hexo-renderer-marked（注意，如果你使用了其他的渲染插件，请卸载对应的插件），它是hexo自带的Markdown渲染引擎。<br />The first command uninstall Hexo’s default Markdown renderer.</p><p>第二条命令用于安装 hexo-renderer-kramed 插件，这个渲染插件针对MathJax支持进行了改进。安装完成后，重新生成博客就会惊喜地发现你的公式已经能够正常显示了。<br />The second command install new Markdown renderer which can support MathJax fully. After installation, you should regenerate your blog to see the changes.</p><p>安装完以后，先 hexo clean &amp;&amp; hexo g 重新生成静态网页，然后 hexo s 查看，这回公式能正常显示了：</p><p>P.S. 其实，这个问题就是因为Markdown渲染和MathJax渲染冲突造成的，除了换别的渲染器，直接修改渲染用的正则表达式也是一种解决思路，但是这个思路有一定风险，如果引起了别的bug而没有及时发现，自己又没有做好备份和记录，就需要浪费很多额外的时间来定位问题。</p>]]></content>
      
      
      <categories>
          
          <category> 技术相关 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MathJax </tag>
            
            <tag> 数学公式 </tag>
            
            <tag> Hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《无言的宇宙--隐藏在24个数学公式背后的故事》读书心得之一</title>
      <link href="/2016/10/19/%E3%80%8A%E6%97%A0%E8%A8%80%E7%9A%84%E5%AE%87%E5%AE%99--%E9%9A%90%E8%97%8F%E5%9C%A824%E4%B8%AA%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E8%83%8C%E5%90%8E%E7%9A%84%E6%95%85%E4%BA%8B%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97%E4%B9%8B%E4%B8%80/"/>
      <url>/2016/10/19/%E3%80%8A%E6%97%A0%E8%A8%80%E7%9A%84%E5%AE%87%E5%AE%99--%E9%9A%90%E8%97%8F%E5%9C%A824%E4%B8%AA%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E8%83%8C%E5%90%8E%E7%9A%84%E6%95%85%E4%BA%8B%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97%E4%B9%8B%E4%B8%80/</url>
      
        <content type="html"><![CDATA[<p>一本科普性质的数学书，作者摘选了24条在人类历史上最重要的数学公式（物理公式），介绍其背景及原理等。<br />像作者所说，<strong>数学是表达宇宙的一种语言。</strong>看了本书之后，这些公式的优美，但背后揭示出宇宙本质的深刻程度，让人叹为观止。<br />数学是一门科学，从毕达哥拉斯与柏拉图起希腊哲学家就对数学有着崇高的评价，他们将其视为纯理性的科学，认为它能穿透实际世界虚幻的表面，洞悉其实质；同时，数学也是一门艺术，它追求简单、优美。<br />下面，就让我们跟随作者的步伐，领略这些美妙绝伦的公式吧。<br /><span id="more"></span></p><h1 id="第一部分古代的定理">第一部分：古代的定理</h1><h2 id="我们为什么信赖算术世界上最简单的公式">1.我们为什么信赖算术：世界上最简单的公式</h2><p><img src="/img/公式1.jpg" /><br />阿拉伯数字然让数学民主化了。以前的数学都是精英阶层的专利。数学史的发展就是让数学普及开了，但是现在的高等数学，走向了抽象化的道路，又进入了精英阶层的专利了。<br />大部分数学家强烈的感受到，数字，以及我们研究的大量其他数学创造物，都代表了人超越了人类思维的客观现实。如果是这样，出现能证明1+1既等于2又等于3这类矛盾陈述的可能性就微乎其微。逻辑家们将之称为“柏拉图主义者”的观点。“典型的数学家在工作日是柏拉图主义者，而在星期天是形式主义者。”<br /><strong>与任何其它语言、宗教或信仰系统相比，在穿越文化与时间界限方面算术最为成功。的确，搜寻地外生物的科学家经常假定，我们能够解码的第一份来自地外世界的信息将以数学形式发送，因为数学是最为广泛接受的宇宙通用语言。</strong></p><h2 id="抗拒新概念零的发现">2.抗拒新概念：零的发现</h2><p><img src="/img/公式2.jpg" /></p><p>现代数学对于零的重要性的强调通常毫无过分之处。数学家们把它称为单位元素，因为把它加到任何数字上，都不会改变那个数字。单位元素对数学的重要性就相当于同义词对文学的重要性。</p><h2 id="斜边的平方毕达哥拉斯定理">3.斜边的平方：毕达哥拉斯定理</h2><p><img src="/img/公式3.jpg" /><br /><strong>毕达哥拉斯认为，世界万物都是由数字统治的。</strong><br />质数对现代密码学至关重要。现代密码学很大的一部分基于如下理念：对于一个很大的合数，比如一个有几百位数的合数，找到它的质因子十分困难。<br /><strong>归谬法，也称反证法，在早期的数学证明中是非常有用的一个工具。</strong></p><h2 id="圆的游戏π的发现">4.圆的游戏：π的发现</h2><p><img src="/img/公式4.jpg" /><br /><img src="/img/欧拉π公式.jpg" /><br />林德曼在1882年证明了关于π的另外一个更为微妙的事实：它是一个超越数（不能以任何系统为有理数多项式方程的解来表示，例如<span class="math inline">\(\sqrt{2}\)</span> 不是一个超越数，因为它是方程x²=2的解），一种加强版的无理数。</p><h2 id="从芝诺悖论谈起无穷的概念">5.从芝诺悖论谈起：无穷的概念</h2><p><img src="/img/公式5.jpg" /><br />古代希腊人都还对无穷大这一概念没有把握，是他们无法取极限。<br />几何级数的各项按某一常数比逐项递减。</p><h2 id="杠杆作用的重要性杠杆原理">6.杠杆作用的重要性：杠杆原理</h2><p><img src="/img/公式6.jpg" /><br /><strong>阿基米德他最引以为豪的成果是他证明了球体体积是它的外接圆柱体体积的三分之二，或可以等价的将此公式表达为：<span class="math inline">\(V=\dfrac{4}{3}πr³\)</span> 。他甚至要求人们在他的墓碑上刻此球体和它的外接圆柱体的图像。</strong></p><p>下一篇：《无言的宇宙--隐藏在24个数学公式背后的故事》读书心得之二</p><p><img src="/img/无言的宇宙.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 科技 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《无言的宇宙--隐藏在24个数学公式背后的故事》读书心得之二</title>
      <link href="/2016/10/19/%E3%80%8A%E6%97%A0%E8%A8%80%E7%9A%84%E5%AE%87%E5%AE%99--%E9%9A%90%E8%97%8F%E5%9C%A824%E4%B8%AA%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E8%83%8C%E5%90%8E%E7%9A%84%E6%95%85%E4%BA%8B%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97%E4%B9%8B%E4%BA%8C/"/>
      <url>/2016/10/19/%E3%80%8A%E6%97%A0%E8%A8%80%E7%9A%84%E5%AE%87%E5%AE%99--%E9%9A%90%E8%97%8F%E5%9C%A824%E4%B8%AA%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E8%83%8C%E5%90%8E%E7%9A%84%E6%95%85%E4%BA%8B%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97%E4%B9%8B%E4%BA%8C/</url>
      
        <content type="html"><![CDATA[<p>接上篇：《无言的宇宙--隐藏在24个数学公式背后的故事》读书心得之一</p><h1 id="第二部分探索时代的定理">第二部分：探索时代的定理</h1><h2 id="口吃者的秘密卡尔达诺公式">7.口吃者的秘密：卡尔达诺公式</h2><p><img src="/img/公式7.jpg" /><br /><span id="more"></span><br />卡尔达诺公式具有长期的影响，甚至超过了它所解决的问题本身的重要意义。例如，它是首次吸引人们在数学中使用虚数和复数的事物之一。虚数是平方为负数的数字，这是任何实数都不具有的性质。有了虚数我们就可以宣传：-1有两个平方根，他分别可以以<span class="math inline">\(i\)</span> 和<span class="math inline">\(-i\)</span> 记之。没有虚数，我们就只能说-1没有平方根。一旦有了虚数，我们就可以把复数定义为带有实数与虚数的两个部分的数，例如<span class="math inline">\(1+2i\)</span> 。<br /><strong>如果没有虚数，不但现代数学无法想象，就连现代物理也同样无法想象。</strong><br /><strong>在征服了三次与四次方程之后，1824年，挪威数学家终于证明，对已五次方程，不存在任何卡尔达诺式的求解公式。</strong></p><h2 id="九重天上的秩序开普勒的行星运行定律">8.九重天上的秩序：开普勒的行星运行定律</h2><p><img src="/img/公式8.jpg" /><br /><img src="/img/开普勒第一定律.jpg" /><br /><img src="/img/开普勒第二定律.jpg" /><br /><img src="/img/开普勒第三定律.jpg" /></p><h2 id="书写永恒费马最后定理">9.书写永恒：费马最后定理</h2><p><img src="/img/公式9.jpg" /></p><p>1993年，怀尔斯宣布他证明了费马大定理。</p><h2 id="一片未曾探索过的大陆微积分基本定理">10.一片未曾探索过的大陆：微积分基本定理</h2><p><img src="/img/公式10.jpg" /><br />在17世纪，数学家们确实发现了他们相当于“美洲新大陆”的发现，这是一块未经探索的数学“大陆”。这片大陆的名字叫做“微积分”。<br /><strong>莱布尼兹也有许多数学意外的兴趣。作为哲学家，可以举出他在著作中有关邪恶的例子；他认为，尽管有些邪恶是必需的，但上帝创造了“一切可能的世界中最美好的一个”。</strong><br />一条曲线的切线的斜率就是那个图形所代表的函数的变化率。<br />解决求切线问题的微分和解决求积问题的积分。<strong>微分和积分互为逆运算</strong>。<br /><img src="/img/微积分定理.jpg" /><br /><strong>为什么说这一发现开发了数学的新大陆呢？因为它最终让数学彻底掌握了连续变化的概念。在莱布尼兹和牛顿之前，数学家们一直被局限于静止的图像或者离散数量的桎梏(gù)之内，连续运动与连续变化的数量之世界与他们世界绝缘。但整个现代科学都是关于变化的科学。数学家们在微积分中找到了他们投身现代科学的必要工具。</strong></p><h2 id="关于苹果传说以及彗星牛顿定律">11.关于苹果、传说……以及彗星：牛顿定律</h2><p><img src="/img/公式11.jpg" /><br />牛顿的《自然哲学的数学原理》对物理学的重要意义等同于欧几里得的《几何原本》对几何学的意义。</p><ul><li>牛顿第一定律：运动物体将永远保持匀速直线运动，除非有外力将其停止或者改变其运动方向。<br /></li><li>牛顿第二定律：作用在物理上的力等于其动量的变化率。<span class="math inline">\(F=\dfrac{d\left( mv\right) }{dt}=ma\)</span><br /></li><li>牛顿第三定律：对于任何一个作用力，都存在一个与它大小相等方向相反的反作用力。</li></ul><p><strong>牛顿真正独树一帜的成就就是他运用微积分，把引力定律和它运动定律结合，从而建立并随之解决了描述行星轨道的方程的能力。</strong></p><h2 id="伟大的探索者欧拉定理">12.伟大的探索者：欧拉定理</h2><p><img src="/img/公式12.jpg" /><br /><img src="/img/欧拉公式.jpg" /><br /><strong>微积分中最重要的函数或许就是指数函数<span class="math inline">\(exp(x)\)</span> ，因为这是唯一一个导数和积分都是它本身的函数。<br /><span class="math inline">\(exp(ix)=cos(x)+isin(x)\)</span><br />这是欧拉本人认为最重要的公式，它把微积分中最终的<span class="math inline">\(exp、cos、sin\)</span> 三个函数联系在了一起。</strong><br />欧拉是一个光辉的特例，他发表了大量的著作，他以实际行动作为人们的榜样，带领他人前进，并对数学得以发展到今天的状况作出了贡献：今天的数学是一项职业，有关数学的信息不存在专利，而是公之于众，供大家分享。</p><p>下一篇：《无言的宇宙--隐藏在24个数学公式背后的故事》读书心得之三</p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 科技 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《无言的宇宙--隐藏在24个数学公式背后的故事》读书心得之三</title>
      <link href="/2016/10/19/%E3%80%8A%E6%97%A0%E8%A8%80%E7%9A%84%E5%AE%87%E5%AE%99--%E9%9A%90%E8%97%8F%E5%9C%A824%E4%B8%AA%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E8%83%8C%E5%90%8E%E7%9A%84%E6%95%85%E4%BA%8B%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97%E4%B9%8B%E4%B8%89/"/>
      <url>/2016/10/19/%E3%80%8A%E6%97%A0%E8%A8%80%E7%9A%84%E5%AE%87%E5%AE%99--%E9%9A%90%E8%97%8F%E5%9C%A824%E4%B8%AA%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E8%83%8C%E5%90%8E%E7%9A%84%E6%95%85%E4%BA%8B%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97%E4%B9%8B%E4%B8%89/</url>
      
        <content type="html"><![CDATA[<p>接上篇：《无言的宇宙--隐藏在24个数学公式背后的故事》读书心得之二</p><h1 id="第三部分普罗米修斯时代的定理">第三部分：普罗米修斯时代的定理</h1><p><strong>革命的爆发经常是有那些对自己正在做些什么一无所知的人点燃引线的。</strong></p><h2 id="新的代数汉密尔顿与四元数">13.新的代数：汉密尔顿与四元数</h2><p><img src="/img/公式13.jpg" /><br /><span id="more"></span><br />四元数完全出自一个人的想象之中，是新代数的第一个例子。这一步骤与其他数学家对于新集合和新函数的发现几乎同步，这些发现共同作用，把数学家从传统的结构与束缚中解放了出来。<strong>有史以来第一次，数学家们可以在物质世界之外探险，他们可以自由自在地发明了一个全新的世界。（个人注：抽象代数）</strong><br />现在的问题不再是哪些结构是可能的，而是哪些结构值得研究。一个新结构会有助于解决已经存在的问题吗？它会有深刻的、富有挑战性的、有其固有美感的理论吗？</p><h2 id="两颗流星群论">14.两颗流星：群论</h2><p><img src="/img/公式14.jpg" /><br />1858年，查尔斯·艾尔米特证明，任何五次多项式方程的解不能用有限目的的<span class="math inline">\(+ - x ÷ 和 \sqrt{x}\)</span> 来表达，但是可以用一种新型函数写出，这种函数名为椭圆函数，是阿贝尔发现的。<br /><strong>这是正常的人类对难题的反应：如果你无法用现有的工具克服困难，那就发明新工具好了。</strong><br /><strong>群的概念现在已经变成了数学家用以表达对称这一古老想法的主要工具。化学家在运用群论描述晶体对称性。物理学家运用群论描述粒子的对称性。</strong></p><h2 id="鲸鱼几何与蚂蚁几何非欧几何">15.鲸鱼几何与蚂蚁几何：非欧几何</h2><p><img src="/img/公式15.jpg" /><br /><strong>第五条公设说：同一平面内一条直线a和另外两条直线b.c相交，若在a某一侧的两个内角的和小于两直角，则b.c两直线经无限延长后在该侧相交。</strong><br />按几何特性（曲率），现存非欧几何的类型可以概括如下：</p><ul><li>坚持第五公设，引出<strong>欧几里得几何</strong>。<br /></li><li>以“可以引最少两条平行线”为新公设，引出<strong>罗氏几何（或称双曲面几何）</strong>。<br /></li><li>以“一条平行线也不能引”为新公设，引出<strong>黎曼几何（或称椭圆几何）</strong>。</li></ul><p>这三种几何学，都是常曲率空间中的几何学，分别对应曲率为0、负常数和正常数的情况。<br /><strong>三种几何中垂直于同一线段的两条直线的图象</strong><br /><img src="/img/非欧几何.png" title="左：双曲几何，即罗氏几何；中：欧几里德几何；右：球面几何，即黎曼几何" /></p><h2 id="我们信赖质数质数定理">16.我们信赖质数：质数定理</h2><p><img src="/img/公式16.jpg" /><br />当高斯1796年还在大学就读时，他便证明了正十七边形可以用尺规作图法画出。<br />正是通过所有质数的乘法运算，才组成了所有其它的一切数字。在这种意义上，它们就像化学中的元素那样基本。<br /><img src="/img/无限循环小数.jpg" /></p><h2 id="关于谱系的想法傅立叶级数">17.关于谱系的想法：傅立叶级数</h2><p><img src="/img/公式17.jpg" /></p><h2 id="上帝之眼中看到的光麦克斯韦方程">18.上帝之眼中看到的光：麦克斯韦方程</h2><p><img src="/img/公式18.jpg" /><br />磁场是由电流产生的。电场是由变化的磁场引发的。而且说到底，光只不过就是传播中的电磁波。<br /><strong>时光荏苒：rěn rǎn 荏苒：时间一点一点的流逝。指时间渐渐地过去了。</strong></p><p>下一篇：《无言的宇宙--隐藏在24个数学公式背后的故事》读书心得之四</p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 科技 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《无言的宇宙--隐藏在24个数学公式背后的故事》读书心得之四</title>
      <link href="/2016/10/19/%E3%80%8A%E6%97%A0%E8%A8%80%E7%9A%84%E5%AE%87%E5%AE%99--%E9%9A%90%E8%97%8F%E5%9C%A824%E4%B8%AA%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E8%83%8C%E5%90%8E%E7%9A%84%E6%95%85%E4%BA%8B%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97%E4%B9%8B%E5%9B%9B/"/>
      <url>/2016/10/19/%E3%80%8A%E6%97%A0%E8%A8%80%E7%9A%84%E5%AE%87%E5%AE%99--%E9%9A%90%E8%97%8F%E5%9C%A824%E4%B8%AA%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E8%83%8C%E5%90%8E%E7%9A%84%E6%95%85%E4%BA%8B%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97%E4%B9%8B%E5%9B%9B/</url>
      
        <content type="html"><![CDATA[<p>接上篇：《无言的宇宙--隐藏在24个数学公式背后的故事》读书心得之三</p><h1 id="第四部分我们这个时代的定理">第四部分：我们这个时代的定理</h1><h2 id="光电效应量子与相对论">19.光电效应：量子与相对论</h2><p><img src="/img/公式19.jpg" /><br /><span id="more"></span><br /><img src="/img/光1.jpg" /><br /><img src="/img/光2.jpg" /><br /><img src="/img/光3.jpg" /><br /><strong>质量就应该是物体所含能量的一种量度，光就应该具有质量。</strong><br />狭义相对论：从任何以匀速运动的参照系及惯性系统中观察，物理定律都是不变的。<br />爱因斯坦的看法：我们无法发现光速有任何变化的原因是：长度与实践都是相对的。他们取决于你的参照系。<br />广义相对论要解决的问题是，在这种理论中，无论观察者处于何种参照系，物理定律将都表达为同样的方式。<br />广义相对论的关键见解是：加速度与引力之间是不可分割的。在引力场内的自由落体运动和在不存在引力场的一部分空间内的匀速运动之间没有可以分辨的不同。<br /><strong>物质告诉时空如何弯曲，弯曲的空间告诉物质如何运动。正式广义相对论使得爱因斯坦预言了光线在引力场内的弯曲。</strong></p><h2 id="从劣质雪茄到威斯敏斯特westminster大教堂狄拉克公式">20.从劣质雪茄到威斯敏斯特（Westminster）大教堂：狄拉克公式</h2><p><img src="/img/公式20.jpg" /><br /><strong>该公式具有高度的物理审美观，狄拉克曾多次说过，物理公式必须是优美的，他认为平方根很丑陋。</strong><br />什么是量子物理？物理学家们所测量的能量、电荷、角动量等都是量子化的。它们并非无限可分的；能量、电荷等等全部存在一个最小单位。<br /><strong>单个量子的表现与我们在宏观世界上习惯的任何东西都有所不同。例如，爱因斯坦告诉我们，一个光子即是粒子又是波。这怎么可能？</strong><br /><strong>对于量子化的粒子来说，对角动量的任何观察都是一种“或者全部或者全不”的事件，看上去，好像粒子正在等待着你去测量，然后在测量的瞬间“决定”它是否绕你选择的轴自旋。</strong><br /><img src="/img/反物质1.jpg" /><br /><strong>玻色（shǎi）子</strong><br /><img src="/img/量子.jpg" /></p><h2 id="王国缔造者陈省身高斯博内公式">21.王国缔造者：陈省身—高斯—博内公式</h2><p><img src="/img/公式21.jpg" /><br />从爱因斯坦开始，物理学家常吃惊的发现，数学原来早已准备好了他们所需要的工具。反之亦然：数学家们也不断的意识到，是物理学上的问题和定理带来了最有趣、最深刻的数学发展。爱因斯坦的广义相对论要求弯曲空间，而这就要求一项可以有逐点不同曲率的非欧几何。<br /><img src="/img/物理数学1.jpg" /><br /><img src="/img/物理数学2.jpg" /></p><h2 id="有一点儿无限连续统假说">22.有一点儿无限：连续统假说</h2><p><img src="/img/公式22.jpg" /><br /><strong>在接近19世纪末的时候，数学家们开始形成了一个共识，认为集合，而非数字，才是建筑数学大厦的基本材料。（个人注：数字是原子？）</strong><br />1900年，希尔伯特表列了数学家应该在20世纪加以研究的二十三个最重要的问题，连续统假说名列榜首。<br /><img src="/img/连续统假说.jpg" /><br /><img src="/img/数学一致性1.jpg" /><br /><img src="/img/数学一致性2.jpg" /></p><h2 id="混沌理论洛伦兹方程">23.混沌理论：洛伦兹方程</h2><p><img src="/img/公式23.jpg" /><br />真实的世界是非线性的。<br /><strong>混沌的第一个标志性特征：对初始条件的敏感性。</strong><br /><img src="/img/混沌1.jpg" /><br /><img src="/img/混沌2.jpg" /></p><h2 id="驯虎布莱克斯科尔斯方程">24.驯虎：布莱克—斯科尔斯方程</h2><p><img src="/img/公式24.jpg" /><br />数学经济学，全新的一代交易家，数量分析专家，这些人通常有数学或物理背景，他们懂得偏微分方程。而且一切期权会根据数学模型有理性的定价：这是他们的信条。<strong>但也有些人认为，布莱克—斯科尔斯一类的模型永远不应该使用，因为它们低估了发生极端事件的可能性。一次极端事件就可能让你过往的努力付之一炬。</strong></p><h1 id="结论将来会如何">结论：将来会如何？</h1><p>人工智能+大数据。<br /><img src="/img/人工智能.jpg" /></p><p>完！</p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 科技 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《苦难辉煌》读书心得</title>
      <link href="/2016/09/22/%E3%80%8A%E8%8B%A6%E9%9A%BE%E8%BE%89%E7%85%8C%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2016/09/22/%E3%80%8A%E8%8B%A6%E9%9A%BE%E8%BE%89%E7%85%8C%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>“透过落地玻璃窗，看见13年前走掉的儿子被带到门前、宋美龄满面微笑地迎上去的时候，他坐在客厅的沙发上纹丝不动。这个在战场上东征西讨南攻北伐的战争枭雄，此刻却在用报纸挡住湿润的双眼。”这本惊涛骇浪的史书，难得有一幕这么温馨的时刻。<br />蒋介石一直耿耿于怀，如果不是西安事变（1936年12月12日），那离剿灭红军就差两个星期了。1936年11月的时候，因为红军为打通连通苏联方案的宁夏战役失利，中央已经做好了万一在陕北无法立足的这种最坏情况下的打算，那时候形势确实是岌岌可危。西安事变之后，随后国共达成了一致抗日的第二次国共合作。在抗日战争中，中国共产党在这场战争中得到了极大的发展。直到后来的国内战争，共产党已经非之前的那个吴下阿蒙了。<br /><span id="more"></span><br />孙中山说过：历史潮流浩浩荡荡，顺之者昌，逆之者亡。<br />所以，1911年革命党人发动辛亥革命，推翻了爱新觉罗王朝；所以，在1919年发起五四运动，开始了新民主主义革命；所以，第一次国共合作发动北伐，消灭了北洋军阀。<br />一切都顺应着历史的潮流，直到1927年蒋介石发起了四一二反革命政变。共产党在一片迷雾中，寻找着前进的道路。<br />而长征，就是共产党及红军图生存、求发展的唯一选择了。中共党史上最为重要的一步，莫过于出发长征。中国共产党人和中国工农红军最深重的困难与最耀眼的辉煌，皆出于此。<br />说是苦难，中国红军于1934年10月开始长征，在敌人的前堵后追之下，湘江之战时红军由出发时的8.6万人锐减为4万余人，损伤过半；四渡赤水到过金沙江时，红军人数已减到2万余人，又减了一半；1935年9月，中央红军到达吴起镇时，只剩下7200人。<br />说是辉煌，中国红军在长征途中找到了适合中国国情的革命路线和方向，找到了适合中国条件下的领导人。1935年1月15日遵义会议，毛泽东进入了核心决策层（五常委，张闻天、周恩来、毛泽东、博古、陈云），排名第三。二渡赤水后，3月12日，刚刚成立新的三人团确立毛泽东的实际领导地位。从此，中国的红军在毛主席的领导下，走向了辉煌。<br />尽管中途有四渡赤水毛主席指挥的土城及鲁班场两次败仗（毛主席一生有四次败仗），但这也是红军为了寻找根据地过程中，面对不可预知的前程、实力强大的敌人，也是不能避免的。<br />尽管中途有1935年9月9日，张国焘在卓木雕成立伪中央，使红军因内部分裂面临覆灭的可能。但在这过程中，也考验了真正共产党人的党性和纪律。<br />纵观整个长征，最令人惊叹的除了领袖人物的领导艺术和指挥技巧，还有中国工农红军空前顽强战胜死亡的决心和寻找胜利之意志。<br />四渡赤水、巧夺金沙江、飞夺泸定桥、过草地、爬雪山，中国工农红军的精神力量战胜了一切困难险阻。<br />中国的红色政权为什么能够存在？为什么能够在两万五千里的长征中坚持到了最后？当然，除了共产党人的因素之外，毛泽东指出其中“白色政权之间的战争”即军阀混战是根本的一条。<br />在国民党围剿过程中，中央军内部矛盾重重、地区军阀割据一方显露无疑。<br />湘军、桂军、黔军、川军都是防蒋重于防共，他们认为“有匪有我，无匪无我”，所以，对红军的围剿本着驱逐出境、保持实力为根本。而蒋介石也是想通过围剿红军的名义把势力范围扩大到西南几省，或者借机歼灭相关的军阀。所以，白色政权之间的勾心斗角成了共产党最好的生存缓冲空间。<br />当然，西安事变的发生，在全国抗日统一战线的压力下，国共实现了第二次合作，抵抗日寇侵略，共产党获得了更好的发展空间。<br />蒋介石可能还一直念念不忘那两个星期，但是，共产党人的无畏精神，不怕苦、不怕死的精神，为人民谋福利的主义，这也是蒋公可能忽略了的要素。</p><blockquote><p>纵横捭阖（zòng héng bǎi hé）：“纵横”即竖和横；“捭阖”是开和合，字面上理解成“自如地横竖开合”（达到操纵控制对方的目的）。不过，“纵横”有其特殊含义，是指战国时的“合纵”与“连横”的谋毠。战国时有七国争霸，齐、楚、燕、韩、赵、魏等六国采取了联合对抗强秦的作法谓之“合纵”；秦国则执行分化六国，使其服从秦国而个个击破谓“连横”。因此，这成语用“纵横”（合纵和连横）两大策略指称国际间错综复杂的政治和外交斗争。</p></blockquote><ul><li><strong>广雅书院校友：杨匏（páo）安</strong><br /></li><li>建立一个党，巩固一个党，发展一个党，需要理想，需要主义，也还需要经费。富于理想的中国共产党人，争论了很长时间才承认了这个现实。<br /></li><li>"打土豪、分田地"既是红色政权政治动员的基础，也是中国共产党人经济独立的基础。<br /></li><li><strong>斯大林：胜利者是不受指责的，这是一般公理。</strong><br /></li><li>甲午战争后中国士大夫阶层痛定思痛，终于认识到不是“器不如人”，而是“制不如人”。<br /></li><li>同盟会党人朱执信翻译了《共产党宣言》。 个人注：朱执信于1919年去世，广州的执信中学，就是孙中山为了纪念他而创立。<br /></li><li>马克思、恩格斯的著名论断：到目前为止的一切社会历史都是阶级斗争的历史。<br /></li><li><strong>英国文学家塞缪尔的那句名言：爱国心在不少场合，是被流氓当做隐身衣来使用的。</strong><br /></li><li><strong>自从人类被划分为阶级以后，阶级的核心就是政党。政党的核心是领袖。领袖的核心是什么？是意志和思想。 个人注：领导者也是这样。</strong><br /></li><li><strong>毛泽东说：革命不是请客吃饭，不是做文章，不是绘画绣花，不能那样雅致，那样从容不迫，文质彬彬，那样温良恭俭让。革命是暴动，是一个阶级推翻一个阶级的暴烈的行动。</strong><br /></li><li><strong>黄埔军校门口有一副铿锵作响的对联：升官发财，请往他处；贪生怕死，勿入斯门。</strong><br /></li><li>鲁迅说过：“革命被头挂退的事是很少的。”<br /></li><li>正统的历史，从来就不一定就是信史。<br /></li><li>“四二一”反革命事变前，严重辞职，将二十一师交给陈诚代管。蒋召见陈诚，问其对国内形势的基本态度。陈诚只一句话：<strong>绝对服从蒋总司令。</strong><br /></li><li>朱德的话语中已经包含两条政治纲领：共产主义必然胜利；革命必须自愿。这两条纲领后来成为人民军队政治宣传工作的基础。<br /></li><li><strong>西方领导科学认为领导力的形成依赖三大要素：一曰恐惧，二曰利益，三曰信仰。恐惧迫使人们服从，利益引导人们服从，信仰则产生发自内心的服从。</strong><br /></li><li><strong>陈毅：“一个真正的革命者，不仅经得起胜利的考验，能做胜利时的英雄，也要经得起失败的考验，做失败时的英雄。”</strong><br /></li><li>一句名言：人的一生虽然漫长，但关键时刻只有几步。<br /></li><li><strong>彭德怀是一团烈火。毛泽东一句“谁敢横刀立马，唯我彭大将军。”</strong><br /></li><li><strong>凡事预则立，不预则废。</strong><br /></li><li>那是一张白纸，好画最新最美的图画。<br /></li><li>希特勒的啤酒馆暴动。<br /></li><li>那是一个根本不考虑毛泽东讲话时间、地点的时代（文革期间），翻开就念，念完就用，而且主要是对别人而念而用。<br /></li><li>孙中山借助一部分军阀的力量打击另一部分军阀的做法不可靠。<br /></li><li>中国的热血青年推翻了爱新觉罗王朝，实现了1911年辛亥革命并在1919年的五四运动之后开始了新民主主义革命；<br /></li><li>工农红军中那个多次出人敢死队长的许世友。<br /></li><li>只有两条道路，中间道路是没有的。一切想在革命和反革命中间找出第三条出路的分子，必然遭到残酷的失败，而变为反革命进攻革命的辅助工具。<br /></li><li><strong>中共党史上最为重要的一步，莫过于出发长征。中国共产党人和中国工农红军最深重的困难与最耀眼的辉煌，皆出于此。</strong><br /></li><li>虔诚使领袖人物的个别结论变成普通真理。但共产党人的首要条件却不是虔诚。<br /></li><li>所谓决策，往往是面对十字路口的选择。<br /></li><li><strong>胜利从来不是鼓掌出来的，不管掌声有多么热烈。它也不是计划制定出来的，不管计划有多么翔实。</strong><br /></li><li><strong>中原大战之后，林蔚提出“高官少兵”原则，即对归降的西北军将领官可以给得很大，兵却编得很少。</strong><br /></li><li>中国有三个半军事家：蒋百里、杨杰、白崇禧、刘伯承（1916年3月，在攻打丰都时候，有一颗子弹射穿了一只眼睛，因为瞎了一只眼，算半个）<br /></li><li>给人以火星者，必怀火炬。<br /></li><li>佛家称世界从生成到毁灭的过程为一劫。万劫，言谓时间，之漫长。万劫不复，意为永远不能复活。<br /></li><li><strong>越在困难的时候，作为领导人越要冷静，要敢于负责。</strong><br /></li><li><strong>1935年12月，工农红军第一方面军长征结束，毛泽东说：长征是宣言书，长征是宣传队，长征是播种机。</strong><br /></li><li>失败孕育着胜利的种子，胜利也包含着失败的基因。<br /></li><li>真理在大多数时候，并不是一轮光芒四射的红日。更多的时候，它可能只是黑夜中一道闪电，甚至是遥远的前方一缕若明若暗的微光。发现真理，需要智慧。跟随真理，则需要勇气。<br /></li><li>四渡赤水在后人看是伟大的，但伟大从来以苦难为代价。<br /></li><li>我们总结自己的历史，辉煌是财富，教训也是财富。甚至是更值得珍惜的财富。<br /></li><li>以退为进，这是近代中国政治中屡见不鲜的手法。<br /></li><li><strong>毛泽东写的《纪念白求恩》，称赞白求恩是一个高尚的人，一个纯粹的人，一个有道德的人，一个脱离了低级趣味的人，一个有益于人民的人。</strong><br /></li><li><strong>1888年诞生的《国际歌》就唱到：从来没有什么救世主。</strong><br /></li><li>1975年4月5日，蒋介石逝世，享年89岁；<br />1938年张国焘脱离共产党，后来加入戴笠的军统，1979年12月在多伦多逝世。<br />2001年10月14日张学良在檀香山逝世，享年101岁。<br />2003年10月23日，宋庆龄在美国纽约逝世，享年106岁。<br /></li><li>师哲问，我们和王明的真正分歧在哪里？毛泽东沉默片刻，这样说了一句：他为别人（个人注：指苏联）考虑得太多了，为我们自己考虑得太少了。<br /></li><li>需要热血的时代，便只能是年轻人的时代。<br /></li><li>幸福是安宁。这块土地什么也不缺，恰恰是缺安宁。<br /></li><li>蒋经国（蒋与原配毛福梅的儿子），曾经在苏联莫斯科中山大学读书（与邓小平是同学），并加入共产党。年轻的时候，还在报纸撰文写信大骂其父，大有势不两立的意思。后来，还是回到蒋介石身边当起太子了。<br /></li><li>遵义会议是中共党史上一个生死攸关的转折点，同时也是中国革命与共产国际关系史上的一个意义重大的转折点。<br /></li><li>从1934年10月10日开始长征，到1935年9月27日的榜罗镇会议，终于确定最终的根据地为陕北，长征的战略目标终于最终选择完成。<br /></li><li>四渡赤水，并非指挥若定，神机妙算，基本上还是相机而动，一次次的寻找，又一次失去，就是因为失去根据地的红军，迫切需要找到新的落脚点而皆未实现，所有的目的都是一个，为了打破第二的围追堵截。并且，毛泽东一生中打了四次败仗（高兴圩、南雄、土城战役、茅台战役），有两次就发生在四渡赤水期间，包括一渡赤水之前的土城战役，三渡赤水前的茅台那次战役（鲁班场战役）。<br /></li><li>1935年6月25日，中央红军、红四面军在懋功会合。但之后，1935年9月9日，张国焘在卓木雕成立伪中央（1936年9月26日结束分裂），使红军因内部分裂面临覆灭的可能。<br /></li><li>1991年，91岁的张学良在美国纽约实事求是地回忆了当年的一幕：“是我们东北军自己选择不抵抗的。我当时的判断是日本人不会占领全中国，我没认清他们的侵略意图，所以尽量避免刺激日本人，不给他们扩大战事的借口。打不还手，骂不还手。是我下的指令。”<br /></li><li>阅读长征的故事将使人们再次认识到，人类的精神一旦唤起，其威力将是无穷无尽的。<br /></li><li>刘志丹等人创立的陕北根据地，使中共中央和中央红军在前后整整一年二万五千里的寻找中，终于找到了一个落脚点。<br /></li><li>大姑娘上轿--头一回<br /></li><li>覆巢之下，安得完卵。<br /></li><li>血脉偾张：fèn<br /></li><li>嬗变：shàn，变迁，更替，古同“禅”。</li></ul><p><img src="/img/红军长征图1.jpg" /><br /><img src="/img/红军长征图2.jpg" /><br /><img src="/img/红军长征图3.jpg" /></p><p><img src="/img/苦难辉煌.png" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 历史 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《建筑的永恒之道》读书心得</title>
      <link href="/2016/09/18/%E3%80%8A%E5%BB%BA%E7%AD%91%E7%9A%84%E6%B0%B8%E6%81%92%E4%B9%8B%E9%81%93%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97%20/"/>
      <url>/2016/09/18/%E3%80%8A%E5%BB%BA%E7%AD%91%E7%9A%84%E6%B0%B8%E6%81%92%E4%B9%8B%E9%81%93%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97%20/</url>
      
        <content type="html"><![CDATA[<p>有人说过，<strong>读书是获得第二人生的过程。</strong><br />《建筑的永恒之道》是一本很有哲学感的讲述建筑的书，作者试图探究建筑的永恒之道，探究什么样的建筑能给人们带来活力、带来生机。<br />作者认为，建筑是为人服务的，一个和谐的、自然的，能让你感觉到自己、感觉到生活气息、有活力的地方是建筑追求的目标。<br />本书先是介绍了永恒建筑应该具有的特质（作者命名为无名特质，因为无法确切的用某个词语来下定义），然后为了满足这些特质，作者联系我们的语言、自然的进化，提出了用一种模式语言来构建满足这种无名特质的建筑，从私人住宅、到街区、到城市，一切都能涵盖在这种模式语言中。<span id="more"></span><br />在这本书之后，作者还写了一本专门介绍建筑模式语言的书----《建筑模式语言（上下册）》，其中共记录了253个模式，具体描述了各种具体建筑的模式。<br />作者认为，一种建筑形式能抵消各种系统内的作用力（例如内在精神力、社会和心理作用力、经济作用力、结构作用力等等，见本书P195），那就是具有无名特质的形式。建筑也像一个生命体，各个环节都体现出生命的特质，系统系统的完整性。而在模式语言方面，也像生物界一样，有对应的遗传密码、变异。保证了模式语言的传承和发展。<br />作者提到理想话的模式语言设计，做到所有的建筑符合个性的的要求、符合心灵的要求，设计师不能做出任何妥协。但在实际生活中，特别是现在工业化的时代，效率、成本（时间、人工）的考虑，要求设计师做出更多模式化的、千篇一律的建筑产品，而不是艺术品。</p><blockquote><p>电影《英雄》中秦始皇说过剑有三重境界：<br />- <strong>第一重境界，手中有剑，心中亦有剑；</strong><br />- <strong>第二重境界，手中无剑，心中有剑；</strong><br />- <strong>第三重境界，手中无剑，心中亦无剑；</strong><br />独孤求败的木剑级就像秦始皇所说的第二重境界，讲求手中无剑，剑在心中虽赤手空拳，却能以剑气杀敌于百步之外，而剑法的最高境界，则是手中无剑，心中也无剑，是以大胸怀包容一切那便是不杀便是和平。</p></blockquote><ul><li>个人注：<strong>人最难的是克服对未来的恐惧，包括对死亡、财产、安全等的恐惧，当你放下这一切的时候，就能成为另外一个自己。</strong><br /></li><li>每个地方的特征是由不断发生在那里的事件（自然的事件+生活的事件？）的模式所赋予的。<br /><strong>（个人注：一个人的特质是由他经历过的事情所赋予的）</strong><br /></li><li>个人注：一个城市的特质也和你用哪一种眼光去审视这个城市有关，有人在广州找到了繁华的都市气质，有人在广州上下九找到了千年古都的简朴气质，有人在广州的城中村找到了广州的亲和感。<br /></li><li>空间中给的每一个模式都是有与之联系的事件模式。<br /></li><li>在原子尺度的规模上，他们是作为可统一的重复整体出现的。<br /></li><li>如果有一个有顶但却开敞的外廊或走廊的转换空间，这<strong>转换空间（个人注：如果的过渡空间）</strong>就是一个室内、室外间的心理上的过渡，它更自然地把你从室内带出而进入庭院。<br /></li><li>没有死的存在和对死的意识，自然的特征就不能出现。<br /></li><li>无名特质不能创造，只能由一个过程来产生。<br />（个人注：就像一个人的气质，腹有诗书气自华）<br /></li><li>每一个模式就是一个规则，它描述了产生它所限定的整体，你所必须要做的事情。<br />个人注：<strong>建筑设计 类比：1、胎儿（整体的展开） 2、语言 （要素+规则+创造力）</strong><br />从数学的观点看，最简单的语言是一个包括两个系列的系统：1.一系列要素或符号。2.组合这些符号的一系列规则。<br /></li><li>所有的好毛毯都遵从这一规则：不管两块并列的颜色再哪儿，它们之间总有第三种不同颜色的细线条。<br /></li><li>个人注：特别的特点，反而容易发现，普通的，反而容易让人熟视无睹。<br /></li><li>像<strong>两侧采光</strong>这种特定模式从人们的建筑知识中消失了。<br /></li><li><strong>圆的严格数学定义：各点同中心点等距。</strong><br /></li><li>为使设计有意义，需要遵从模式的精神，而不是字面的意义。<br /></li><li><strong>寻找一个名字是创造或发现一个模式的基本部分。</strong><br /></li><li><strong>不是建筑使这个小客栈趣味盎然的，而是发生在那儿的事情-- 是你遇见的人，你在那做的事情，人们入睡前讲给你的故事。</strong><br />个人注：建筑反映了当时、当地人们对某种生活方式的憧憬。<br /></li><li>庭院、城市的中心广场是人们交流聚会的地点。<br /></li><li>判断一个模式的好坏，我们总可以自问，一个模式给我们何种感觉，而且我们也总可以同样询问他人。一个人的感受是直接和明确的<br /></li><li>然后我们看到了，一个个平衡模式的概念深深扎根于感觉的概念。但尽管如此，感觉本身并非事物的实质。<br /></li><li>这一陈述是用<strong>宗教的热情</strong>来表现的。<br /></li><li>当一个小孩从父母或从周围的人那里“学习”语言，他没有学他们语言之中的所有的规则--因为他不能看到或者听到规则。他只听到他们说出的句子。而他所做的就是为自己创造规则的系统，规则完全是第一次由他创造的。他不断的改变这些规则，直到用这些规则，他能够产生出同他听到的语言相像的语言。在这一阶段，我们说，他“掌握”了语言。<br /></li><li>城市是个有生命的东西，其模式即是活动模式，有是空间模式。<br /></li><li>个人注：一切事物都是在发展的，管理也一样。不同的环境、不同的技术、不同的人群、不同的材料，都需要我们探究不同的管理方式和科学。但是管理也像生物体一样，有些本质的DNA是不变的，是可以传承的。<br /></li><li>设计常常被想成一个综合的过程，把东西放到一起的过程，结合的过程。但我预先的部分加起来不可能形成有自然特征的任何东西。<br />但是设计就像胚胎的发展一样，在发展的每个阶段，新的结构出现于已经出现的结构之上。发展的过程本质上是一个操作序列，每个操作分化了由以前的操作所作出的结构。<br />一个婴儿，从目前怀孕的第一天起，就作为一个整体，每天都是一个胎儿的整体直至诞生。它不是把部分加起来的一个顺序，而是一个本身的扩展、卷曲、分化着的整体。<br />（个人注：设计的步骤也大概如此：先总体后局部，整体比细节重要，因为他控制了整个设计，设计分层进行，每层的整体又控制了对应层的细节）。<br /></li><li>个人注：设计时先用心去想，你想象中的住宅是怎么样的，从功能的角度、从将来要发生的事件的角度来想，而不是具体的构件。<br /></li><li>一张画，甚至一个粗略的草图，都是非常刻板的---- 当设计还在胚胎状态的时候，它具体化了远远超出设计本身所要求的细节处理。<br /></li><li>人们常说，一组人不能够创造一件艺术作品，或任何完整的东西，因为不同的人各执己见，因而使最终的作品成了不具特色的折衷物。<br /></li><li>一段时间，人们认为城市必须由规划师做出规划或蓝图。据说，如果城市的秩序不是这样产生，城市将不会有秩序。于是，人们竟不顾所有传统社会建设的美丽的城市和村庄都没有规划图的明显事实而恪守这一信条，人们已经让自己放弃自由。<br />个人注：<strong>作者反对城市规划，但是其实古时美丽的城市并不代表没有规划，只是说以前的城市规模比较小，在心里想好就行了，不需要复杂的蓝图之类的，但我认识还是有规划的。规划，作为宏观的定位，类似大动脉的性质。细节，类似毛细血管，再由人们生活过程中自发修补完成，这也是一个城市发展的合理的途径。</strong><br /></li><li>我们只有在无我时，才能使建筑有活力。<br /></li><li>不畏死的人，是自由生活的人，因为他可以接受即将发生的事情，而不是总在通过努力控制它而扼杀他。<br /></li><li>译者注：痛楚的人生产生了强烈的忧患意识。十多年来我一直思考人生周围的一切。我拼命的读书，我不断的旅行，我思考人生的意义，我了解苦难的中国。渐渐的，我发现，<font color="#4590a3" size = "4px"><strong>一个人的思索只有转化为社会的思索才能真正的推动文化的前进。</strong></font></li></ul><p><img src="/img/建筑的永恒之道.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 哲学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《沉思录》读书笔记</title>
      <link href="/2016/09/13/%E3%80%8A%E6%B2%89%E6%80%9D%E5%BD%95%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
      <url>/2016/09/13/%E3%80%8A%E6%B2%89%E6%80%9D%E5%BD%95%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<p>奥勒留：古罗马皇帝+苦修哲学家，以一世英主而身兼苦修哲学家的人历史唯一一个。<br />本书是作者的随笔，在行军中记录人生的感悟，所以结构比较零散、内容重复，也没有分主题归门别类。<br />作者重点记录了如何为人处世、如何理性思考、如何秉承自然之道等等的感悟。<br />比较特别的是，作者在理性以及自然之道之上，讲述了人应该如何坦然面对死亡的问题，作者认为死亡也是自然变化的结果，死亡也是元素之间的转换，这是自然之道。<br />关于死亡，早在两千多年前，有人问孔老夫子死是怎么回事，孔子回答说，“未知生，焉知死。”；而法国哲学家蒙田说过：“学习哲学即是学习如何去死。”所以，学习哲学让人能更平静的面对死亡。<br />对于哲学，作者说到，<strong>哲学就像亲娘，在你需要慰藉的时候，可以投入它的怀里。</strong> 确实是，哲学能让人从更高的境界、更高的角度去思考和看待问题，能让人从日常的短视的观念中解脱出来。像作者一直强调的“你遭遇外界挫折而烦恼的时候，使你困扰的不是那件事情的本身，而是你自己对那件事情的判断。”<br />在做人方面，有一点作者讲得非常好，<font color="#4590a3" size = "4px"><strong>我们要追求人性中美好的部分，而不是成功。</strong></font><br /><span id="more"></span></p><blockquote><p>犬儒学派（希腊语：κυνισμός，英语：Cynicism）是古希腊一个哲学学派，由苏格拉底的学生安提西尼创立，其信奉者被称为犬儒（希腊语：Κυνικοί, 拉丁语：Cynici，英语：Cynics）。该学派否定社会与文明，提倡回归自然，清心寡欲，鄙弃俗世的荣华富贵；要求人克己无求，独善其身，近于中国的道家。最著名的犬儒学派人士是安提西尼的弟子狄奥根尼。<br />由于中文里本无现成的对应词汇，在中国大陆犬儒主义常被理解为“讥诮嘲讽、愤世嫉俗、玩世不恭”。</p></blockquote><h1 id="文章摘要">文章摘要</h1><ul><li>夙(sù)愿：夙：素有的，旧有的。<br />付梓（zǐ）：把稿件交付排印。<br /><strong>慰藉（jiè）</strong><br /></li><li>所谓一个国家，祭祀根据个人平等与言论自由以制定一套法律，适用于所有的人；所谓君主，其最高理想乃是人民的自由。<br /></li><li>主意打定之前仔细考虑，主意打定之后坚决不移。<br /></li><li><font color="#4590a3" size = "4px">每日清晨对你自己说：我将要遇到好管闲事的人、忘恩负义的人、狂妄无理的人、欺骗的人、骄傲的人。他们所以如此，乃是因为他们不能分辨善恶。</font><br /></li><li>你是个老人了，不要再做奴隶，不要再做被各种私欲所牵挂的傀儡，不要再令他怨恨现世的命运，并且恐惧未来的命运。<br /></li><li>如果根本没有神，或者神不管人间事，那么生存在一个没有神或没有神意的宇宙又有何益呢？<br /></li><li><strong>要记取，一切事物均取决于我们的看法。</strong><br /></li><li>人生即主观：There is nothing either good or bad, but thinking makes it so.<br /></li><li>没有一桩不幸的事，不可以由于勇敢承担而变成为幸事。<br /></li><li>有人对他无理挑剔，他一律予以容忍，绝不反唇相讥。<br /></li><li>训练你自己“细心听取别人讲话”，尽可能地深入他的内心。<br /></li><li>太阳之下没有新的事物。<br />《圣经.传道书》第一章第九节上的话，意思是：在阳光底下没有新鲜的东西，我们每天看到的事情都是以前做过的，已经有的事情今后还会有，历史是一种不断地循环，不断地重复。<br /></li><li>不要为将来担忧。如果你必须去到将来 ，你会带着同你到现在时一样的理由去的。<br /></li><li>对事发怒毫无用处。<br /></li><li>自然总不比艺术差，因为艺术乃是模仿自然者。<br /></li><li>伊菲索斯人的作品里记载着一则劝告，要经常怀想一位道德高尚的古人。</li></ul><p><img src="/img/沉思录.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 哲学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《数据仓库工具箱：维度建模权威指南》读书心得</title>
      <link href="/2016/09/07/%E3%80%8A%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%B7%A5%E5%85%B7%E7%AE%B1%EF%BC%9A%E7%BB%B4%E5%BA%A6%E5%BB%BA%E6%A8%A1%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2016/09/07/%E3%80%8A%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%B7%A5%E5%85%B7%E7%AE%B1%EF%BC%9A%E7%BB%B4%E5%BA%A6%E5%BB%BA%E6%A8%A1%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>一本介绍了数据仓库（DW）和商业智能（BI）的建模过程，不过因为主要的难度是在后端的ETL过程，所以本书理论上说重点还是介绍了DW的建模过程，BI更多指向的是前端的报表开发，技术上难度没有后端的ETL大。<br />本书最大的特点是，针对我们常见行业重点业务过程的建模进行了详细描述，让用户对数据建模有个直观的了解。<br />在启动BI项目之前，我们必须明白，<strong>BI的主要收益是，获得高质量的决策。</strong><br /><span id="more"></span></p><h1 id="数据仓库的核心架构">1、数据仓库的核心架构</h1><p><strong>ETL： 获取（Extract）、转换（Transform）、加载（Load）</strong><br />E:连接不同的数据源，通过任务调度获取数据源数据，并对数据源的数据做简单的检查；<br />T：进行数据质量检查及清理、数据转换、一致性维度的检查及转换；<br />L：把清理转换后的数据加载到目标表，供BI报表工具使用。在加载过程需要考虑相关的代理键生成、一致性检查等。</p><p>架构设计就像建房子的蓝图，因为没有架构的结构无法承受压力。架构在需求的基础上，重点关注的涉及时间、可用性和性能方面的需求。例如发布的频率，就涉及相关的同步的频率、时间要求。加载对服务器、带宽等方面的需求。<br /><img src="/img/数据仓库3.jpg" /></p><h1 id="kimball-dwbi生命周期">2、Kimball DW/BI生命周期</h1><p><img src="/img/数据仓库8.jpg" /></p><h1 id="步骤维度设计过程">3、4步骤维度设计过程</h1><ul><li>1）选择业务过程<br /></li><li>2）声明粒度<br /></li><li>3）确认维度<br /></li><li>4）确认事实</li></ul><h1 id="维度表和事实表">4、维度表和事实表</h1><p>事实表（Fact Table）+ 维度表（Dimensional Table）<br />维度属性支持报表过滤和标识，事实表支持报表中的数字数值。<br /><strong>事实表一般只包括：</strong></p><ul><li>主键<br /></li><li>退化维度<br /></li><li>外键<br /></li><li>事实值（性能度量）</li></ul><p><strong>事实表的分类：</strong></p><ul><li>事务事实表<br /></li><li>周期快照事实表<br /></li><li>累积快照事实表</li></ul><p><strong>维度表的缓慢变化技术：</strong></p><ul><li>技术0：不变化<br /></li><li>技术1：直接修改值<br /></li><li><strong>技术2：(最常用的技术)增加新行，增加：生效日期、失效日期、目前状态3个列</strong><br /></li><li>技术x：略<br /><img src="/img/数据仓库7.jpg" /></li></ul><p>事实表通常像下图一样，采用星型模型<br /><img src="/img/数据仓库1.jpg" /><br />事实表，一般是数值性，针对需要汇总的数据；<br /><img src="/img/数据仓库2.jpg" /></p><h1 id="数据集成和一致性维度">5、数据集成和一致性维度</h1><p><strong>数据集成：企业全景视图。</strong>一致性维度意味着跨不同数据库建立公共维度属性，只有这样才能使用这些属性构建横向钻取报表，才能是得KPI等关键指标能通过计算差异和比率来开展数学比较工作。（例如收入、支出的维度必须一致，才能计算利润）。维护一个总线矩阵来保证一致性维度。<br /><img src="/img/数据仓库6.jpg" /><br /><img src="/img/数据仓库4.jpg" /><br /><img src="/img/数据仓库5.jpg" /></p><h1 id="建模过程常见的原则及问题">6、建模过程常见的原则及问题</h1><h2 id="建表原则">1）建表原则</h2><ul><li>事实表的粒度应该一致<br /></li><li>事实表一般不包括文本字段<br /></li><li>建议采用最细粒度（灵活性+可扩展性），然后通过汇总聚集、通过总线矩阵保持一致性的维度（避免烟筒式的孤岛）。<br /></li><li>事实表一般采用代理键<br />（自然键作为退化维度）<br />主键最好是无意义的字段便于以后扩展，假设以标书编码为主键，以后标书编码填错需要改的时候，关联表都需要跟着改。如果是一个无意义的自增字段是主键就无此问题。<br /></li><li>维度表一般采取明确的文本说明，而不是一些代码和神秘值<br />报表的分组也是，最好不要利用键值某几个字节的隐含含义来分组，最好利用明确的属性。这样，在某些业务修改对应标签属性，但是键值没有修改的情况下，不会出错。<br /></li><li>维度表，需要约束查询或者分类分析的信息；<br /></li><li>多数商业模型的维度包括5-20个维度之间。（只要不改变事实表，维度能方便的增加）<br /></li><li>维度模型的设计应反映组织的主要业务过程事件，不应该被设计成仅能发布特定报表或回答特定问题。<br /></li><li>维度建模注重简单化、易用性、性能，所以严格抵制3NF(数据库建模的第3范式)<br />第3范式会导致查询复杂化、连接导致性能低效、影响用户的理解。规范化非常适合支持事务处理并保证参照的完整性，但在维度模型主要是用于支持分析处理，所以查询的性能、可理解性反而是最需看重的。<br /></li><li>建议采用最细粒度（灵活性+可扩展性）<br /></li><li>通过总线矩阵保持多过程的一致性维度。<br /></li><li>关系数据库中不能存在多对多关系。用来消除多对多关系的最常用方法是通过添加桥接表来创建两个一对多关系。<br /></li><li>每个事实表必须有一个指向日期维度的外键，主要用于上卷和过滤。<br /></li><li>最好将事务代码当成退化维度来处理。<br /></li><li>维度表通常不能随事实表同时变化的情况存在。一般维度表是相对稳定的。<br /></li><li>尽量避免使用雪花模式（应该使用星型模型），偶尔使用支架模型可以接受，但是要严格评估。</li></ul><h2 id="需要避免的常见维度建模错误">2）需要避免的常见维度建模错误：</h2><ul><li><strong>错误10：</strong>在事实表中放入文本属性。<br /></li><li><strong>错误9：</strong>限制使用冗长的描述符以节省空间。<br /></li><li><strong>错误8：</strong>将层次划分为多个维度<br /></li><li><strong>错误7：</strong>忽略对维度变化跟踪的需要。<br /></li><li><strong>错误6：</strong>使用更多的硬件解决所有的性能问题。（通过创建聚集、建立分区、建立索引等）<br /></li><li><strong>错误5：</strong>使用操作型键连接维度和事实。<br /></li><li><strong>错误4：</strong>忽视对事实粒度的声明并混淆事实粒度。<br /></li><li><strong>错误3：</strong>使用报表设计维度模型。<br /></li><li><strong>错误2：</strong>希望用户查询规范化的原子数据。<br /></li><li><strong>错误1：</strong>违反事实和维度的一致性要求</li></ul><h2 id="etl重要的瓶颈问题通过etl工作流监视器来监控性能">3）ETL重要的瓶颈问题：（通过ETL工作流监视器来监控性能）</h2><p>考虑ETL的瓶颈，有两个来源，一是从客户的反馈获取，二是通过监控的工具监控性能和容量的趋势。</p><ol type="1"><li>针对源系统或者中间表的低效索引查询；<br /></li><li>SQL语法导致优化器做出错误的选择；<br /></li><li>随机访问内存（RAM）不足导致的内存颠簸<br /></li><li>在RDBMS中进行的排序操作；<br /></li><li>缓慢的转换步骤；<br /></li><li>过多的I/O操作；<br /></li><li>不必要的读写；<br /></li><li>重新开始删除并重建聚集而不是增量式的执行这一操作；<br /></li><li>在流水线中过滤（改变数据获取）操作应用太迟；<br /></li><li>未利用并行化和流水线方式；<br /></li><li>不必要的事务日志，特别是在更新时存在的事务日志；<br /></li><li>网络通信及文件传输的开销。</li></ol><h2 id="项目成功评估要素">4）项目成功评估要素</h2><ul><li>1）强有力的业务责任人；<br /></li><li>2）项目强烈的动机；（获取竞争性资源）<br /></li><li>3）项目可行性（技术可行性、资源可行性、<strong>数据可行性</strong>）</li></ul><h1 id="本书重点内容图片摘选">本书重点内容图片摘选</h1><p><img src="/img/数据仓库9.jpg" /><br /><img src="/img/数据仓库10.jpg" /><br /><img src="/img/数据仓库11.jpg" /><br /><img src="/img/数据仓库12.jpg" /><br /><img src="/img/数据仓库13.jpg" /><br /><img src="/img/数据仓库14.jpg" /></p><p><img src="/img/数据仓库工具箱.png" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《忒修斯之船》读书心得</title>
      <link href="/2016/08/09/%E3%80%8A%E5%BF%92%E4%BF%AE%E6%96%AF%E4%B9%8B%E8%88%B9%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2016/08/09/%E3%80%8A%E5%BF%92%E4%BF%AE%E6%96%AF%E4%B9%8B%E8%88%B9%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>蛋蛋(00)后推荐的一本书。<br />拆封这本书时，不得不惊叹于其精致及特别，整本书散发着一股旧书的味道（musty smell），还附带着一堆设计特别、材质各异的明信片、海报、老照片等配件。<br />象出版社所宣称，这是一本推理小说加实体版解谜游戏的综合体（注意，是综合体，不是书），代表了一种新的阅读形式，让读者深入到书本的情节中去。<br />这个综合体总的来说有三条线。<br />第一条线：书本正文讲述的是失忆主人公在追寻自己身份的过程中，卷入了一系列反抗独裁军火商韦沃达家族的过程，书中有工人暴动、暗杀、背叛、爱情（这点有点模糊）等等。<br />第二条线：本书翻译（所谓的译者）柯岱拉那充满玄机的译注，让关于石察卡的真实身份、她和石察卡的感情更显得扑朔迷离。<br />第三条线：本书两位素不相识的读者Eric和Jen，在书中空白处互动批注了探索、研究作者石察卡真实身份的关键资料。当然，还少不了这两位恋人（后来成为恋人）在空白处玩的类似互递小纸条的爱情游戏。（此处想起《岛上书店》说的，人的一生最幸运的是能找到有共同阅读兴趣的人。）<br /><span id="more"></span><br />原书的结局是开放性的（类似诺兰最喜欢玩的把戏），S原来的身份是谁？韦沃达有给杀死吗？石察卡的真正身份是谁？正如本书的预告片最后一句台词“the book is just the beginning”，真正的探险旅程将从第二遍开始。<br />按照攻略，本书要至少要读五遍，还包括整理Eric和Jen的注解之类，才能真正把整个谜底解开。一个真正烧脑的历程，作为只是路人粉的我只看了一遍，还是放弃了。</p><p>回归原书的本身，书中的一切都在忒修斯之船的悖论下延伸。<br />书中反复出现的船，在一次次的毁坏中重修，还是原来的船吗？S组织的旧成员死去，新成员一个个加入，就如忒修斯之船一般，那还是原来的S组织吗？甚至，一个找不回身份的失忆人士还是原来的自己吗？</p><p>书中一直强调，人生就是这样，某一段时间你到达一个地方，以为到了终点。结果发现其实又回了起点，然后循环着。就像爬到了一个山头发现还有另一个山头。<br />同时，在个人意识和集体意识层面，S也一直在摇摆，对抗集权还是追求自己的缪斯？S一直在反思自己的选择，诘问自己当时的选择是否错了，或者是否另一种才会更好。但是，在集体无意识的控制下，在群体的推动下，S还是无意识的选择了集体意识。</p><p>不过话说回来，看完本书的最大帮助，就是教会了自己读书做笔记的N种自娱自乐的方式;-)。</p><p><strong>忒修斯之船</strong></p><blockquote><p>忒修斯之船亦称为忒修斯悖论，是一种同一性的悖论。假定某物体的构成要素被置换后，但它依旧是原来的物体吗？公元1世纪的时候普鲁塔克提出一个问题：如果忒修斯的船上的木头被逐渐替换，直到所有的木头都不是原来的木头，那这艘船还是原来的那艘船吗？因此这类问题现在被称作“忒修斯之船”的问题。有些哲学家认为是同一物体，有些哲学家认为不是。在普鲁塔克之前，赫拉克利特、苏格拉底、柏拉图都曾经讨论过相似的问题。近代霍布斯和洛克也讨论过该问题。这个问题的有许多变种，如“祖父的斧头”。</p></blockquote><p><strong>缪斯</strong></p><blockquote><p>缪斯（希腊语：Μουσαι、拉丁语：Muses）是希腊神话中主司艺术与科学的九位古老文艺女神的总称。她们代表了通过传统的音乐和舞蹈、即时代流传下来的诗歌所表达出来的神话传说。</p></blockquote><p><img src="/img/忒修斯之船第三人笔迹1.png" title="忒修斯之船第三人笔迹" /><br /><img src="/img/忒修斯之船第三人笔迹2.png" title="忒修斯之船第三人笔迹" /><br /><img src="/img/忒修斯之船附件清单.png" title="忒修斯之船附件清单" /><br /><img src="/img/忒修斯之船.png" /><br /><img src="/img/忒修斯之船侧面.png" /><br /><img src="/img/忒修斯之船封底.png" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 小说 </tag>
            
            <tag> 读书心得 </tag>
            
            <tag> 玄幻 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《人性的弱点》读书心得</title>
      <link href="/2016/08/08/%E3%80%8A%E4%BA%BA%E6%80%A7%E7%9A%84%E5%BC%B1%E7%82%B9%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2016/08/08/%E3%80%8A%E4%BA%BA%E6%80%A7%E7%9A%84%E5%BC%B1%E7%82%B9%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>本书英文名是《How to win friends an d influence people》，直译应该是《如果赢得朋友和影响别人》，书中讲的也是待人处世的道理。中文译名《人性的弱点》有误导读者之嫌，不过这倒不影响这本书的超高销量。<br />大学时就看过这本书，现在重看一遍，感悟却完全不一样，里面说的很多朴素的道理都能联系到实际生活、工作中的事情。<br />书中通过很多案例来说明如何待人处世的方方面面，其实也是一本提高情商、管理情绪方面的书。里面说的一些道理都是很常见、很通俗易懂的（有心灵鸡汤之嫌;-)），<strong>但是所谓的知易行难，如何让这些道理在现实生活中得到贯彻，才是这本书真正的意义。</strong> 所以，建议学习本书最好的方法应该是采用类似MBA的教学方法，结合案例，提出自己的观点和别人深入讨论，然后再与书中的观点、方法比较，这样就能找出自己思维的误区以及加深对本书所讲述道理的印象。**<br />题外话：感觉这本书还是很中庸的，不像美国这个民族的性格，反而和论语里面的很多论点相近。**<br />话说回来，本书讲了很多道理，但是个人认为本书的核心其实还是：<strong>“你希望别人怎么对待你，你就应该怎样去对待别人。”</strong>在你做任何事情、做任何决策之前，想想如果对方是你，你想对方怎么对你，这样，所有的事情都豁然开朗了。<br /><span id="more"></span><br /><img src="/img/人性的弱点脑图.png" title="人性弱点的知识脑图" /></p><h2 id="文章摘录">文章摘录</h2><ul><li>卡耐基对于演讲有自己的看法，他觉得每个人在情绪激动的时候都能够大声的表达自己的看法。假如一拳直接打倒街上看起来最软弱的人，他的愤怒和他说话的表情，也几乎和大演说家威利姆相差无几。卡耐基说：“每个人，只要他有热烈的情绪，都是可以在公众面前发表热烈的演讲的。”<br /></li><li>在讲到如何培养自信心时，他说：“只要你去做一件自己害怕做的事，并且取得了成功，那么，你的自信心也就建立起来你了。”<br /></li><li>99%的情况下，无论你犯了多么严重的错误，你都不会指责自己。批评的成效微乎其微，因为人们习惯性地在受到批评时，竭力替自己辩护，从而给自己建造一层防御壁垒。<br /></li><li>苛责和批评对于别人都是无济于事的。<br /></li><li><strong>罗斯福总统曾说，在他任职期间，但凡遇到解决不了的问题时，他就会抬头看写字台壁上的巨大的林肯画像，想象着如果是林肯遇到和自己一样的难题，他会怎么去做，他们解决这些问题。</strong><br /></li><li>我们在进行人际交往的时候，需要牢记一点，我们面对的是感性动物而不是理性的动物，所以，人情世故有时候逼逻辑道理强上一万倍。<br /></li><li>对他人随意指责、批评、抱怨是愚蠢的人的通病，而能够克制自己，以宽容、平和的心态与他人相处的，往往是具有人格魅力的人的高尚之处。<br /></li><li><strong>激发别人的热情的能力、发挥每个人才能的能力是一个管理者最重要的能力之一。</strong><br /></li><li>如果成功也是有方法的话，那么这个方法就是把握对方的需求，并照顾到对方的需求。 <strong>个人注：但是难点在于你要能找到对方的需求和立场，才能根据这个来思考问题。</strong><br /></li><li><strong>了解对方的立场，并从对方的立场想问题，把这个立场放到第一位。 个人注；就像下围棋也是同一个道理。</strong><br /></li><li>快乐是自己心里的内在状态，而不需要向外界求得的。<br /></li><li>恩特•卡耐基 是钢铁大王，和本书的作者戴尔•卡耐基不是同一个人。<br /></li><li>如果记住别人的姓名：没有特殊的技巧，方法其实很简单，如果他听得不清楚，他就说：“对不起，我没有听清楚，你能不能再说一遍？”如果是个很少见到的姓名，他就会问：“对不起，这个字怎么拼？”在谈话过程中，他会不厌其烦的把对方姓名记忆数次。同时，在他的脑海里，把UI放的姓名和他的脸孔、表情、外形连贯起来。如果这个人对他很重要，拿破仑三世就把这个人的名字写在纸上，仔细的看然后记住，接着把纸撕了。<br /></li><li>这个世界上最重要的一条铁律：你希望别人怎么对待你，你就应该怎样去对待别人。<br /></li><li><strong>赞同对方的观点，然后自然引出自己的观点。而不要通过否定对方的观点来说出自己的观点。</strong><br /></li><li><strong>释伽牟尼曾经这样说过：“恨永远无法终止恨，唯有爱可以止恨。” 个人注：耶稣应该也说过，爱你的敌人，如果你不爱你的敌人，和你的敌人也就没什么差别了。</strong><br /></li><li>与其跟一只狗一起走，不如让狗先走一步。如果你给狗咬了一口，你即使把这只狗打死，也无法治好你的伤口。<br /></li><li>你可以比别人聪明，但是你不能直接告诉他你比他聪明。 个人注：扮猪吃老虎 :-)<br /></li><li><strong>在基督生前2200年，埃及国王教训他的儿子说：“一定要用外交的手腕，这样才能帮助你达到你所希望的目的。”</strong><br /></li><li>别忘了这样一句话：“用争夺的方法，你永远都无法得到满足。可是当你谦让的时候，你所得到的比你所期望的更多。” 个人注：例如六尺巷也是这个道理。<br /></li><li>100年前的林肯说过：“一滴蜂蜜，比一加仑的胆汁，捕捉到的苍蝇更多。”<br /></li><li>苏格拉底辩论法：他提出问题，都是他的反对者愿意接受并且同意的。他不断的获得对方的承认，到最后，反对者在不知不觉中就接受了他在几分钟前还坚持否认的结论。<br /></li><li>个人注：通过迂回的战术来达到自己的目的，有点类似围棋里面的声东击西，东是对方所好，西是你的目的。<br /></li><li><strong>发过哲学家洛希夫克曾经这样说过：“如果你想获得更多仇人，你就战胜你的朋友，但是，如果想获得更多的朋友，那么就让你的朋友胜过你。”</strong><br /></li><li>所以，别让我们显得硕果累累，我们要处处低调、谦虚，那样就会有越来越多的人喜欢你，谁都愿意跟你相处。<br /></li><li>银行家摩根说过：“人做任何一件事，都存在两种理由，一种是好听的，一种是真实的。”他人会经常想到那个真实的理由，而我们则较喜欢那个好听的动机。所以要改变一个人的意念，你就需要激发出他高尚的动机。<br /></li><li>如果你想要某人改变他的某些缺点，那你就要表现出他拥有这些优点。就像莎士比亚说的----若是你缺乏某种特长，那就假设你有这个特长。</li></ul><p><img src="/img/人性的弱点.png" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 生活 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《Head First 设计模式》读后感</title>
      <link href="/2016/08/03/%E3%80%8AHead%20First%20%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E3%80%8B%E8%AF%BB%E5%90%8E%E6%84%9F/"/>
      <url>/2016/08/03/%E3%80%8AHead%20First%20%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E3%80%8B%E8%AF%BB%E5%90%8E%E6%84%9F/</url>
      
        <content type="html"><![CDATA[<p>这是一本连目录都很有趣的书。<br />作者以四人帮（Gof）的《Design Patterns: Elements of Reusable Object-Oriented Software》为基础，妙趣横生、深入浅出的介绍了相关的设计模式。<br />软件开发有两大难点：<br /><strong>1、复杂度</strong>：因为软件模拟整个现实的世界，而现实世界是复杂的、无序的；<br /><strong>2、变化</strong>：变化是唯一不变的真理。<br />所以软件管理就是管理软件的复杂度和变化。设计模式让我们从以往的代码重用，上升到经验的重用，让我们的思考架构提升到模式的层面，而不是仅仅停留在琐碎的对象上。<br />建立一种弹性的、可维护的、可以应对变化的软件，其核心原则就是“<strong>封装变化</strong>”，封装对象的创建、方法的调用、复杂的接口等等。<br />封装变化就是要把变的和不变的部分分开，概括来说有三点：<br />1、找出变化的部分；<br />2、分离变化的部分；<br />3、封装变化的部分。<br /><span id="more"></span><br />以下是根据本书整理了的一些知识点。<br /><img src="/img/设计模式脑图.png" title="设计模式脑图" /></p><p>说明：以下的类图摘录自：rc008-designpatterns_online.pdf文件</p><h1 id="设计模式1策略模式strategy-pattern">设计模式1：策略模式（Strategy Pattern）</h1><p>定义了算法族，分别封装起来，让它们之间可以互相替换，模式让算法的变化独立于使用算法的客户。<br /><img src="/img/策略模式.png" title="策略模式" /></p><ul><li><strong>设计原则1：封装变化</strong><br />找出应用中可能需要变化之处，把它们独立出来，不要和那些不需要变化的代码混在一起；<br />其实就是封装的概念，让系统变得更有弹性。<br /></li><li><strong>设计原则2：针对接口编程，而不是针对实现编程；</strong><br />其实就是通过多态的方式，在执行的时候改变相关的函数绑定。把经常变化的行为（可以视同是一种算法）单独封装成接口类，通过set方法，而已在执行的时候改变对应的行为。<br /></li><li><strong>设计原则3：多用组合，少用继承；</strong><br />同设计原则2,也是针对接口进行编程，然后母类组合了这些行为类（算法），在运行的时候动态的改变行为，提高了系统的弹性。</li></ul><h1 id="设计模式2观察者模式observer-pattern">设计模式2：观察者模式（Observer Pattern）</h1><p>定了了对象之间的一对多的依赖，这样依赖，当一个对象改变状态的时候，它的所有依赖者都会收到通知并自动更新。<br /><img src="/img/观察者模式.png" title="观察者模式" /><br />类似出版社+订阅者的关系，其中包括了了Subject与Observer接口的类。<br />观察者模式支持推（Push）和拉（Pull）的模式！<br />观察者模式的代表MVC。</p><ul><li><strong>设计原则4：为了交互对象之间的松耦合设计而努力。</strong></li></ul><h1 id="设计模式3装饰者模式decorator-pattern">设计模式3：装饰者模式（Decorator Pattern）</h1><p>动态地将责任附加到对象上。若要扩展功能，装饰者提供了比继承更有弹性的替代方案。<br /><img src="/img/装饰者模式.png" title="装饰者模式" /><br />装饰者和被装饰者必须是一样的类型，利用继承达到“类型匹配”。</p><ul><li><strong>设计原则5： 类应该对扩展开放，对修改关闭。</strong></li></ul><h1 id="设计模式4工厂模式factory-pattern">设计模式4：工厂模式（Factory Pattern）</h1><h2 id="工厂方法模式factory-method-pattern">工厂方法模式（Factory Method Pattern）</h2><p>定义了一个创建对象的接口，但由子类决定要实例化的类是哪一个。工厂方法把实例化推迟到了子类。<br /><img src="/img/工厂方法模式.png" title="工厂方法模式" /></p><h2 id="抽象工厂模式abstract-factory">抽象工厂模式(Abstract Factory)</h2><p>提供一个接口，用于创建相关或依赖对象的家族，而不需要明确指定具体类。<br /><img src="/img/抽象工厂模式.png" title="抽象工厂模式" /><br />工厂模式能将对象的创建封装起来，是应用程序解耦，并降低其对特定实现的依赖。<br />在子类中实例化，就可以横向扩展子类而不需要修改超类以及对超类调用的代码。<br />工厂方法将生产知识封装锦各个创建者，这样的做法也可以被视为是一个框架。框架的概念就是搭好框架之后，直接在里面加内容（产品）就可以了。<br />如果代码是针对接口而写，那么通过多态，它可以与任何新类实现该接口。</p><ul><li><strong>设计原则6： 要依赖抽象，不要依赖具体类。</strong></li></ul><h1 id="设计模式5单件模式singleton-pattern">设计模式5：单件模式（Singleton Pattern）</h1><p>确保一个类只有一个实例，并提供一个全局访问点。<br /><img src="/img/单件模式.png" title="单件模式" /><br />实现单件模式，需要私有的构造器、一个静态方法和一个静态变量。<br />可以采用延迟实例化的方式创建单件，同时，需要考虑解决多线程的问题，例如可以通过同步synchronized来处理方法。</p><h1 id="设计模式6命令模式command-pattern">设计模式6：命令模式（Command Pattern）</h1><p>将“请求”封装成对象，一遍使用不同的请求、队列或者日志来参数化其它对象。命令模式也支持可撤销的操作。<br /><img src="/img/命令模式.png" title="命令模式" /><br />命令对象包括：特定的接受者+一组动作（excute（）方法）<br />调用者包括：持有一个命令对象+setCommand（）方法+调用执行的方法<br /><strong>命令模式将发出请求你的对象和执行请求的对象解耦。</strong><br />调用者可以接受命令当做参数，甚至在运行时动态地进行。</p><h1 id="设计模式7适配器模式adapter-pattern-和-外观模式facade-pattern">设计模式7：适配器模式（Adapter Pattern） 和 外观模式（Facade Pattern）</h1><p>适配器模式是将一个类的接口，转换成客户期望的另一个接口。适配器让原本不兼容的类可以合作无间。<br /><img src="/img/适配器模式.png" title="适配器模式" /><br />外观模式是是提供了一个统一的接口，用来访问子系统中的一群接口。外观定了一个高层接口，让子系统更容易使用。</p><ul><li>适配器将一个对象包装起来以改变其接口；<br /></li><li>装饰者将一个对象包装起来以增加新的行为和责任；<br /></li><li>外观将一群对象“包装”起来以简化其接口<br /></li><li><strong>设计原则7：最少知识原则（Least Knowledge）：只和你的密友谈话。</strong><br />只使用和本类直接相关的对象的方法。</li></ul><h1 id="设计模式8模板方法模式template-method-pattern">设计模式8：模板方法模式（Template method Pattern）</h1><p>在一个方法中定义了一个算法的骨架，而将一些步骤延迟到子类中。模板方法使得子类可以在不改变算法结构的情况下，重新定义算法中的某些步骤。<br /><img src="/img/模板方法模式.png" title="模板方法模式" /><br />Hook（钩子）我们将“默认不做事的方法”称为钩子。类似，实际中的钩子，需要的时候就挂上，不需要的就放着不做事。钩子可以作为条件控制，影响抽象类中的算法流程。如果在在子类中，这个算法是可选的，就使用钩子。子类可以选择是否哦实现这个钩子方法。<br />模板方法模式和适合创建框架。固化算法结构（把方法申明为final），具体的某些步骤实现由子类动态处理。<br />模板方法的抽象类可以定义具体的方法、抽象方法和钩子。<br />工厂方法是模板方法的一种特殊版本。</p><ul><li><strong>设计原则8：好莱坞原则（Hollywood Principle）</strong><br />别调用（打电话给）我们，我们会调用（打电话给）你。<br />由超类主控一切，当他们需要的时候，自然会去调用子类，这就跟好莱坞一样。</li></ul><h1 id="设计模式9状态模式state-pattern">设计模式9：状态模式（State Pattern）</h1><p>允许对象在内部状态改变时改变它的行为，对象看起来好像修改了它的类。<br /><img src="/img/状态模式.png" title="状态模式" /><br />这个模式将状态封装成独立的类，并将动作委托到代表当前状态的对象，我们知道行为会随着内部状态而改变。<br />工作步骤：1、判断当前状态；2、执行对应状态的动作；3、动作之后对应的状态（可能变、也可能不变）；循环以上3步。</p><ul><li>策略模式：可以通过组合不同的对象来改变行为；<br /></li><li>状态模式：通过状态的改变来委托给不同的状态类来执行不同的行为；</li></ul><p>和程序状态机（PSM）不同，状态模式用类代表状态。</p><ul><li><strong>设计原则9：类应该只有一个改变的理由。</strong></li></ul><h1 id="设计模式10代理模式proxy-pattern">设计模式10：代理模式（Proxy Pattern）</h1><p>为另一个对象提供一个替身或者占位符以访问这个对象。<br /><img src="/img/代理模式.png" title="代理模式" /><br />包括：</p><ul><li>远程代理：控制访问远程对象；<br /></li><li>虚拟代理：控制访问创建开销大的资源；<br /></li><li>保护代理：基于权限控制对资源的访问，动态代理，是运行时才将Proxy类创建出来。</li></ul><p>装饰者模式为对象加上行为，而代理则是控制访问。</p><h1 id="设计模式11复合模式compound-pattern">设计模式11：复合模式（Compound Pattern）</h1><p>复合模式是结合两个或以上的模式，组成给一个解决方案，解决一再发生的一般性问题。<br />MVC（Model-View-Controller）<br />Model 2：Web开发人员也都在适配MVC，使它复合浏览器/服务器模型，并使用Servlet和JSP技术的结合，来达到MVC的分离效果。</p><h2 id="文章摘录">文章摘录</h2><ul><li>当两个对象之间松耦合，它们依然可以交互，但是不太清楚彼此的细节。<br /></li><li>改变主题或者观察者其中一方，并不会影响另一方。因为两者是松耦合的，所以<strong>只要它们之间的接口仍被遵守</strong>，我们就可以自由地改变它们。<br /></li><li>不管是在什么时候，只要我们有一大堆东西，很自然地就会想到要为他们分类，这可以帮助我们在更抽象的层次上思考这些东西。例如汽车的分类就是这样。<br /></li><li>其它书本的截图 <img src="/img/RMI1.png" title="RMI" /> <img src="/img/RMI2.png" title="RMI" /> <img src="/img/设计模式定义模板.png" title="设计模式定义模板" /> <img src="/img/设计模式定义.png" title="设计模式定义" /> <img src="/img/设计模式分类1.png" title="设计模式分类1" /> <img src="/img/设计模式分类2.png" title="设计模式分类2" /></li></ul><p><img src="/img/设计模式.png" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《零年：1945 现代世界诞生的时刻》读后感</title>
      <link href="/2016/07/23/%E3%80%8A%E9%9B%B6%E5%B9%B4%EF%BC%9A1945%20%E7%8E%B0%E4%BB%A3%E4%B8%96%E7%95%8C%E8%AF%9E%E7%94%9F%E7%9A%84%E6%97%B6%E5%88%BB%E3%80%8B%E8%AF%BB%E5%90%8E%E6%84%9F%20/"/>
      <url>/2016/07/23/%E3%80%8A%E9%9B%B6%E5%B9%B4%EF%BC%9A1945%20%E7%8E%B0%E4%BB%A3%E4%B8%96%E7%95%8C%E8%AF%9E%E7%94%9F%E7%9A%84%E6%97%B6%E5%88%BB%E3%80%8B%E8%AF%BB%E5%90%8E%E6%84%9F%20/</url>
      
        <content type="html"><![CDATA[<p>大部分书籍的视角停留在胜利之前的英雄故事，但作者却着眼点于胜利的那一年，1945年，发生在欧洲、亚洲的光复国、战败国关于欢腾、饥饿、复仇的故事，以及人民创造新世界的努力。1945年那时，各国景象都差不多，不管是光复国还是战败国，一样都是满目疮痍、食不果腹、百废待兴，可能不同的只是人们不同的心理以及对未来的期望。<br />战争胜利之后，除了让之前被囚禁的人们获得人身自由、解决温饱问题之外，作者的笔触也不忘在人性方面进行着笔，例如：</p><blockquote><p>对那些女囚犯来说，帮助最大的是口红，当他们把嘴唇涂得红彤彤的，她们总算变成了人，而不是文在手臂上的号码。他们终于有心思关心起自己的外表来，是口红率先把人性还给了他们。</p></blockquote><span id="more"></span><p>战后的一项工作是让大家回家，可是，经过多年的战乱，家也已经不是那个家了，每个人都需要面对着那个家破人亡的陌生的“家”。<br />而对于犹太人，家更是难以定义。所以，战后一项重要的事情就是犹太人的复国历程，<strong>是整个犹太人在集体寻找回家的路</strong>。<br />在每个民族都应有自己的国家，甚至，每个国家都只应有一个民族的旗帜下，德国人、克罗地亚人、哥萨克人纷纷被遣送回本民族的国家。而犹太人，在复国主义的推动下，在为犹太人身份认同的理念下，他们心目中上帝的应允之地“巴勒斯坦”就成了他们回家的唯一目标。<br />在建设新国家过程，少不了清洗和夺权，有政治上的夺权也有军事上的夺权。<br />清洗涉及到对战前伪政府、通敌分子、军国主义、纳粹党等等的清洗，但是，对这些精英分子的打击，从维持国家的运作、恢复国家经济、重建国家的角度来看，基本是使用了最低限度的打击。很多时候，清洗的行为更多是消除政治上的敌对势力或异己分子，为此来获取权利。</p><blockquote><p>德国人犯有集体罪行，大规模的清洗将导致教育和社会服务走向崩溃。没过多久，盟军便把经济复苏看成比伸张正义更重要的目标。</p></blockquote><p>当然，<strong>正义只有看得见才算得到了伸张</strong>，所以，对一些“禽兽”、“恶魔”的审判是必不可少的，这也是能让社会迅速转向经济建设的必要手段之一。但是，在反思二战的时候，从某种程度上来说，震惊于个别“禽兽”、“恶魔”的恶行，其实是没有看到本质，即<strong>他们背后的罪恶体制让他们的行为显得稀松平常</strong>。<br />1945年，也是零年，是在废墟上建设完美社会的最好时机。<br />在1945年，当时的人们为了建设自由的世界而倾尽毕生精力。他们努力创立了联合国、他们尝试着建立国家资本主义、他们建立了社会主义国家，他们为了全人类的理想社会在探索。<br />当然，像作者说的，这样的世界不会长久，没有什么是永恒的。但这不是我们不向1945年的男男女女，向他们的苦难、憧憬和抱负致敬的理由，<strong>纵然许多期待终将化为灰烬，一如世间万物。</strong></p><h2 id="文章摘录">文章摘录</h2><ul><li>我对人类能以史为鉴的看法一直将信将疑，至少从认识到过去的愚蠢行为可以防止未来再犯同样错误这点来看，我的质疑不是没有道理。<br /></li><li>官方给出的欧战结束的时间，即欧洲胜利日（V-E Day)，是在5月8日。<br /></li><li>有些人在战争中失去了太多，已经不爱说笑了。<br /></li><li>所有少数派都会同强大的外来者结盟，借别人之手摆脱多数派的欺凌，这点是一切殖民社会的共通之处。<br /></li><li>所谓的“最终解决方案”，其实旨在肉体消灭几百万人。<br /></li><li><strong>饿殍(piǎo)遍野：殍：人饿死后的尸体。到处是饿死的人。形容老百姓因饥饿而大量死亡的悲惨景象。先秦·孟轲《孟子·梁惠王上》：“庖有肥肉，厩有肥马，民有饥色，野有饿莩，此率兽而食人也。”</strong><br /></li><li>战争一结束，一切突然显得那么空虚、病态和没有意义！我们一蹶不振，生活穷苦潦倒，灰头土脸，而在众志成城抵抗外敌时，却可以全然忘记这些困难。我们的所有努力最终只收获了破坏，却无力在废墟上建立起我们赖以为生的东西。<br /></li><li>饥荒、荒芜、疾病会酝酿动乱，催生共产主义的幽灵。饥民是反上帝学说的肥沃土壤，他们很容易被那些企图将全能国家等同于上帝的人的蛊惑。<br /></li><li>指责别人正是大多数人的做法。<br /></li><li>人更容易对自己的遭遇顾影自怜。<br /></li><li>食色性也：语出《孟子·告子上》：孟子与告子辩论，告子曰：“食、色，性也。仁，内也，非外也。义，外也，非内也。”是告子的论点之一。<br /></li><li><strong>过去，因为慑于危险，没有挺身而出，如今报复成了掩饰良心不安的一种做法。这种现象具有普遍性，而且穿越时空。</strong><br /></li><li>他过去没做过什么亏心事，所以现在也没有必要靠手指别人来证明自己是个英雄。<br /></li><li>长久以来，波兰人只要“头顶上有只拳头”，就会表现得规规矩矩，但“有朝一日他们有机会爬到别人头上作威作福了”，就变得“野蛮了”。<br /></li><li>华人常被叫做“亚洲的犹太人”。 个人注：华人像早期犹太人一样，勤奋、踏实，不参与政治，因为国内落后而遍布各国，在经济上的地位比较高，很多国家的经济命脉都掌握在华人手里，所以也容易受到像犹太人一样的迫害，特别是在东南亚地区。<br /></li><li>流血只会换来更多的流血。<br /></li><li>他们中间可能有真正的爱国者……但是，就这帮举着旗子、满大街乱窜的乌合之众而言，里面尽是些罪犯和低能儿，只要一看到五六支枪，他们立马就会龟缩到鼠洞里。<br /></li><li>有些地方也暗示着我父亲害怕回家的原因 —— 担心自己变成陌生人。<br /></li><li><strong>虚与委蛇（yí）：虚：假；委蛇：随便应顺。指对人虚情假意，敷衍应酬。《庄子·应帝王》：“乡吾示之以未始出吾宗，吾与之虚而委蛇。”</strong><br /></li><li><strong>不，我们出生在波兰，但我们不是波兰人；我们曾在立陶宛落脚，但我们不是立陶宛人；虽然我们在罗马尼亚见到了生平第一缕曙光，但我们不是罗马尼亚人。我们是犹太人！！！</strong><br /></li><li>犹太复国主义是为了犹太人的身份认同而战。<br /></li><li>流亡的自由政府，维系着形式上的存在。<br /></li><li>但是，总要做点什么，好让人们觉得正义得到了伸张。<br /></li><li>麦克阿瑟将军并未同意裕仁天皇主动承担战争责任的请求，因为他确信，有必要保全天皇，避免发生骚乱。<br /></li><li>德国人犯有集体罪行，大规模的清洗将导致教育和社会服务走向崩溃。没过多久，盟军便把经济复苏看成比伸张正义更重要的目标。<br /></li><li>要想在一个满目疮痍的国家恢复合法性，一种方法是找个具有象征意义的人物，并团结在其周围。 个人注：有点像辛亥革命的孙中山。<br /></li><li>共产党的军民关系搞得更好：他们意识到，要打赢战争，部分靠的是宣传。被老百姓视为一支英雄的人民之师是他们的一笔宝贵财富。<br /></li><li>何应钦曾在东京的陆军士官学校就读，和冈村宁次是师生关系。<br /></li><li>中国内战的关键其实在于东北。日本人在这里建立重工业，开发矿产，谁率先夺取这块心脏地带，谁就可以居于一种几乎坚不可摧的地位。<br /></li><li>戴高乐意识到，一方面，正义只有看得见才算得到了伸张；另一方面，法国已经伤痕累累，禁不起大规模的清洗了，否则社会将面临难以忍受的压力。 个人注：有点像文革时候，邓小平在打倒四人帮之后，也没有进行大规模的清洗。<br /></li><li>只有国家才有权成为惩罚的主体。 要让国家重新垄断对武力的使用权。<br /></li><li>中共为什么要坚持举行审判？干嘛不直接枪毙了流氓恶棍？很明显，他们希望这些处决在人们眼里是合法的。建立某种形式的法制是获取合法性的必要条件。但在摆样子、走过场的公审中，<strong>法律的概念是完全政治化的</strong>。<br /></li><li>然后，被审判的不光是人，还有这些人所代表的体制。<br /></li><li>《复仇女神》神山传说<br />战神山受理的第二桩谋杀案，乃是迈锡尼国王阿迦门农之子俄瑞斯忒斯杀母案。这也是人类历史上的第一个法庭。神话中，阿迦门农之妻克吕泰涅斯忒拉勾结情夫艾奎斯托斯，杀死了自己的丈夫阿迦门农。若干年后，阿迦门农与克吕泰涅斯忒拉的儿子俄瑞斯忒斯为报父仇杀死了自己的生母，并因此受到复仇女神厄里尼厄斯的惩罚与迫害，丧失心智，发疯发狂，痛苦不堪。俄瑞斯忒斯的守护神——太阳神阿波罗提示他去向雅典求助。于是，雅典人在战神山上受理了这起血亲谋杀案。该法庭的法官是智慧与战争女神雅典娜，陪审团由十二位德高望重的雅典市民组成，原告人是克吕泰涅斯忒拉（已死，灵魂未到场），原告律师及代理人是复仇三女神厄里尼厄斯，被告人是俄瑞斯忒斯，被告律师则是太阳神阿波罗。法官与陪审团在听取了原告与被告双方的申诉后，通过投票的方式裁判该案。黑石子代表有罪，白石子代表无罪。十二名陪审团员投票后，发现黑、白石子各六枚，最后雅典娜本人投入一枚关键性的白色石子，从而使白石子的数目超过了黑石子，并因此宣判俄瑞斯忒斯无罪。本次审判的规格与流程，即是后来西方法庭的原始雏形。<br /></li><li>结束血债血偿的恶性循环依旧是举行审判的最大理由。<br /></li><li>麦克阿瑟：当兵打仗这个传统源远流长，充满荣耀。其根植于人类最崇高的品质————也就是牺牲精神。<br /></li><li>你们可以整死我，但你们无权诽谤我！<br /></li><li><strong>主导审判的是公众的情绪；法律知识在回应街谈巷议。</strong><br /></li><li>认识到人类作恶的能力能引导其余人向善；了解人性最坏的一面是一种文明教化的过程。这两点认识是后来进行战争审判的主要动机之一。<br /></li><li><strong>我们永远不能忘记，我们审判这些被告的依据将被明日历史来评价我们。</strong><br /></li><li>这些实干家对自由民主体制的优柔寡断、扯皮推诿和混乱无序早就失去了耐心。 个人注：所以说民主制度不是一个最好的制度，但却是一个最不坏的制度。<br /></li><li>再也不会有明媚而自信的早晨了。<br /></li><li>改造原住民这一策略也许最早可以追溯至古罗马人的教化行为。<strong>一些人认为这一策略源自启蒙主义观念，即人生来理性，而且可以通过正确的教育加以引导。</strong><br /></li><li><strong>先给我们吃的，然后再来谈道德</strong><br /></li><li>在见识过某物的真容后，伴随而来的通常是幻灭。<br /></li><li>“日本鬼子”是野蛮人的形象在战时深入人心。投在广岛和长崎的两枚原子弹导致20万人死亡，事后，杜鲁门总统在给友人的信中写到：“<strong>对付畜生就要用畜生的方法</strong>。”<br /></li><li>德谟克拉西：Democracy 民主的意思<br /></li><li>人类四大自由：言论自由、信仰自由、免于匮乏的自由、免于恐惧的自由。<br /></li><li>1944年，货币体系在“布雷顿森林”度假酒店成立了。 Bretton Woods<br /></li><li>战争是“终极邪恶”。<br /></li><li>1945年也是“零年”，是在废墟上建设新社会的完美时机。<br /></li><li>当然，这样的世界不会长久，没有什么是永恒的。但这不是我们不向1945年的男男女女，向他们的苦难、憧憬和抱负致敬的理由，纵然许多期待终将化为灰烬，一如时间万物。<br /><img src="/img/1945现代世界诞生的时刻.png" /></li></ul>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 历史 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《安全模式最佳实践》读书心得</title>
      <link href="/2016/07/19/%E3%80%8A%E5%AE%89%E5%85%A8%E6%A8%A1%E5%BC%8F%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/"/>
      <url>/2016/07/19/%E3%80%8A%E5%AE%89%E5%85%A8%E6%A8%A1%E5%BC%8F%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>写得比较晦涩的一本书。<br />比较全面的介绍了安全方面的内容，强调安全的整体性。不过，说是最佳实践，更多是从设计的角度的最佳实践，而不是整体案例的最佳实践。里面举的案例零散，不成体系。<br />比较一般的一本书，比较大的收获就是，在看书的过程当中，一些常用的术语、技术上网进行了了解。<br /><span id="more"></span><br /><img src="/img/安全模式1.png" /><br /><img src="/img/安全模式2.png" /><br /><img src="/img/安全模式3.png" /><br /><img src="/img/安全模式4.png" /><br /><img src="/img/安全模式最佳实践.png" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《代码大全2（Code Complete）》读后感</title>
      <link href="/2016/07/15/%E3%80%8A%E4%BB%A3%E7%A0%81%E5%A4%A7%E5%85%A82%EF%BC%88Code%20Complete%EF%BC%89%E3%80%8B%E8%AF%BB%E5%90%8E%E6%84%9F/"/>
      <url>/2016/07/15/%E3%80%8A%E4%BB%A3%E7%A0%81%E5%A4%A7%E5%85%A82%EF%BC%88Code%20Complete%EF%BC%89%E3%80%8B%E8%AF%BB%E5%90%8E%E6%84%9F/</url>
      
        <content type="html"><![CDATA[<p>终于啃完这本近900页的大块头，一本讲述软件构建的最佳实践方面的书。<br />本书除了内容详实、通俗易懂之外，最大的一个优点就是翻译得特别好。<br />像作者说的，人的智力是有限的，而软件需要构建的世界是凌乱的，所以降低复杂度是软件开发的核心。<br />所以，如何降低复杂度是本书的核心，包括分层抽象、分解、复查、规范等等技术。<br />例如：</p><ul><li>将系统 “分解”，是为了使其更易于理解</li><li>进行复查、测试，减少人为的失误</li><li>将子程序编写得短小，以减少大脑的负荷</li><li>基于问题而不是实现细节来编程，从而减少工作量</li></ul><span id="more"></span><h1 id="文章摘录">文章摘录</h1><ul><li>首先为人写程序，其次才是为机器。</li><li>子程序最佳的长度是50-150行。IBM曾经把子程序的长度限制在50行之内。</li><li>犯错不是罪过，从中学不到什么才是罪过。</li><li>在调试过程，设定时间，如果超过这个时间，就暂停或者放弃调错过程。知道何时放弃很难，但这是必须面对的问题。</li><li>与其它行业相比，软件开发行业的经验比书本知识的价值要小。</li><li>编程工作本质上是项无法监督的工作，因为没人清楚你正在干什么。</li><li>承认自己智力有限并通过学习来弥补，你会成为更好的程序员。</li><li>程序员都是大忙人，常常没有时间去考虑怎样改进自己的工作。</li><li>木匠谚语“瞄两次，切一次（Measure twice, cut once)”(三思而后行)</li><li>需求的关键是识别出用户的问题。</li><li>开发过程能够帮助客户更好地理解自己的需求，这是需求变更的主要来源。</li><li>如果你不能向一个六岁小孩解释某件事，那么你自己就没有真正理解它。 Albert Einstein。</li><li>如果你熟悉数据库术语的话，类与对象的关系就如同“模式(Schema)”与“实例（instance）”一样。</li><li>抽象可以让你用一种简化的观点来考虑复杂的概念。</li><li>用错误狐狸代码来处理预期会发生的状况，用断言来处理绝不应该发生的状况。</li><li>伪代码编程过程（Pseudocode Programming Process,PPP)，这种编程过程有助于减少设计和编写文档所需的工作量，同时提高这两项工作的质量。</li><li>不合理地初始化数据是产生编程错误的常见根源之一。</li><li>变量名的平均长度在10-16个字符之间。</li><li>做一项全局决策而不是做许多局部决策。</li><li>一条很好的经验就是，程序主体中仅能出现的文字量就是0和1，任何其他文字量应该换成有描述性的标示。</li><li>平均5小时写出220行的代码。</li><li>测试先行的编程是过去十年中所形成的最有用的软件开发实践之一。</li><li>将单元测试纳入测试框架。</li><li>如果一个错误无法重现，这通常是一个初始化的错误，或者是一个同时间有关的问题，或者是悬空指针的问题。</li><li>重视Pareto法则。</li><li>可以在程序执行开始的时候算出一张查询表，在之后每次需要的时候使用这一表格。</li><li>high-touch环境 ：人性、人情、人味</li><li>有效编程中最重要的工作是思考，而人思考时通常不会看上去很忙。</li><li>以新习惯来代替老习惯，要比干脆戒掉老习惯容易。</li></ul><p><img src="/img/代码大全2.png" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《Google软件测试之道》读后感</title>
      <link href="/2016/06/24/%E3%80%8AGoogle%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95%E4%B9%8B%E9%81%93%E3%80%8B%E8%AF%BB%E5%90%8E%E6%84%9F/"/>
      <url>/2016/06/24/%E3%80%8AGoogle%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95%E4%B9%8B%E9%81%93%E3%80%8B%E8%AF%BB%E5%90%8E%E6%84%9F/</url>
      
        <content type="html"><![CDATA[<p>一本关于测试传道授业解惑方面的书，书中阐述了Google主要采取的测试模型（ACC等）、自动化测试、探索式测试（Exploratory Testing）等方面，同时也讲述了Google在测试组织方面的进化及各岗位的职责分工及协作。经过努力，Google的测试部门已经从原先的测试服务部门进化为工程生产力部门（Engineering Productivity)。<br />从本书能感觉到Google是把自动化测试、开发测试工具提高效率作为测试的首要任务。提高开发和测试的生产力是测试最为关注的问题，所以Google入职的测试员工在技术方面的能力是丝毫不亚于相关的开发人员。测试要在组织中持续灌输质量是全员负责的概念，在开发阶段就必须做好相关的单元测试，通过每日构建，能把问题提前发现、及时处理，不把有问题的代码带到代码库。<br /><span id="more"></span><br />为了支持这些，Google测试工程师在基础设施、测试框架（包括单元测试框架、自动化测试框架等）的设计、开发投入非常大的精力，主要目的就是方便开发同事能在测试框架内方便的进行相关测试、问题定位和修复。所以，在Google，产品进入手工测试的阶段的时候，大部分的单元测试、集成测试，甚至端到端的系统测试都已经处理完毕，在手工测试阶段更多的是进行一些探索性测试，发挥人的主观能动性。按照Google的经验值，测试的成本遵守70-20-10的原则，单元测试占70%，集成测试占20%，系统测试占10%。<br />书中也提到了Google在测试分析、计划、编写测试用例、执行等方面的管理。Google采用的ACC矩阵表模型，能把测试需要关注的特质（A:Attribute）、组件（C:Component）、能力（C:Capability）三方面在一个矩阵中体现，相关的测试用例、风险分析也能整合在一起，方便了测试的过程管理，避免了目前测试计划对实际工作没有任何指导意义的弊病。</p><p><img src="/img/Google+的Capabilities矩阵.png" title="Google+的Capabilities矩阵(ACC)" /><br /><img src="/img/Chrome%20OS的Capabilities矩阵的热点图.png" title="Chrome OS的Capabilities矩阵的热点图（heap map）-风险" /><br />Google在测试工具方面的开发，包括BITE（Browser Integrated Test Environment）、QualityBot，BITE是Chrome的一个扩展，使工程师的注意力集中在实实在在的探索式测试和回归测试上，而不是流程和技术细节。它把测试、用例管理、BUG提交、BUG跟踪、录制/回访等功能整合在一起，避免了测试人员不断在应用和Bug管理平台之间切换的问题。质量机器人（QualityBot）能自动比较网页在不同Chrome版本之间显示和渲染的区别，能区别到像素层面，这样方便测试人员在Chrome版本升级的时候，快速发现对相关网页的影响。<br />本书一个比较特别的地方就是，书中大量采用了对Google各层面员工的采访，在增加趣味性的同时，也使本书的结构显得有点凌乱。</p><h1 id="参考资料">参考资料</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">测试建模 Google ACC http://www.cnblogs.com/liangshi/archive/2012/04/23/2465897.html  </span><br><span class="line">Make 命令教程  http://www.ruanyifeng.com/blog/2015/02/make.html</span><br></pre></td></tr></table></figure><p><img src="/img/软件测试之道.png" /></p><!--more--><h1 id="文章摘录">文章摘录</h1><ul><li>测试需要思考：如何在制约质量和快速发布之间寻找平衡。<br /></li><li>变更Google测试的首要问题是重新定位身为测试人员的意义所在。...一个团队能编写出高质量软件的唯一途径就是全体成员共同对质量负责。<br /></li><li>维持现状的惯性导致任何变革都变得非常困难。<br /></li><li>这里是Google，如果你有想法，尽管去做就是。<br /></li><li>把测试和开发割裂开来，看成两个单独的环节，甚至是两类截然不同的问题，这种做法是错误的，沿着这条路走下去意味着什么问题也解决不了。<br /></li><li>Google是一家以创新和速度为基础的公司。<br /></li><li>测试团队更像是小而精的特种部队，我们依靠的是出色的战术和高级武器。<br /></li><li><strong>质量不是被测试出来的---这句话看似陈词滥调的话却包含一定的道理。</strong><br /></li><li>质量更像是一种预防行为，而不是检测。质量是开发过程的问题，而不是测试问题。我们已经成功的将测试实践融入为开发过程的一部分，并创建了一个增量上线的流程。如果一些项目在线上被证实的确是Bug重重，它将会被回滚到之前的版本。在确保不出现回滚级别Bug发生的前提下，预防了许多客户问题的同时，也很大程度降低了专职测试人员的数量。<strong>在Google，测试的目标就是来判断这种预防工作做的怎么样。</strong><br /></li><li><strong>解铃还须系铃人：You build it,you break it. </strong><br /></li><li>这种测试人员在不同项目之间的借调模式，....另外还能保证一个好的测试想法可以快速在公司内部蔓延。<br /></li><li>Google经常在最初的版本中只包含最基本的可用功能，然后后续快速迭代过程中得到内部和外部用户的反馈，而且每次迭代过程当中都非常注重质量。一个产品在发布给用户使用之前，一定要经理金丝雀版本、开发版本、测试版本、Beta版或正式版本。<br /></li><li>小型测试涵盖单一的代码段，一般运行在完全虚假实现（Fake）的环境里。中型测试涵盖多个模块且重点关注在模块之间的交互上，一般运行在虚假实现环境或真实环境中。大型测试涵盖任意多个模块，一般运行在真实的环境中，并使用真正的用户数据和资源。<br />分别对应单元测试、集成测试、系统测试。<br /></li><li>打包是一个过程，包括将源代码编译成二进制文件，然后把二进制文件统一封装在一个包里面。<br />-为了保证单独的服务可以并行的开发，服务之间的接口需要在项目早期就要定下来。这样，开发者会依赖在协商好的接口上，而不是需要依赖在特定开发的库上。为了不影响服务级别之间的测试，这些接口早期一般做一个虚假的实现。<br /></li><li>审阅设计文档的时候应该有一定的目的性：完整性、正确性、一致性、设计、接口与协议、测试。<br /></li><li><strong>可以做代码编译、测试执行、数据存储、报表展示的通用测试框架。Google工程师专注于测试程序的编写，运行的细节留给通用基础执行框架。</strong><br /></li><li><strong>与其询问他们一个关于某个模糊概念的看法，不如拿一个明确的结论来引起大家的争论。通常来说，都是排除容易下定义难。</strong><br /></li><li>Bug的分级：<br />P0 -Must have： 如果缺失，产品不能发布<br />P1 -Should have： 如果缺失，产品能发布，但不能达到预定目标（功能/性能）<br />P2 -Nice to have： 做了则更好<br />P3 -Neutral： 对产品没有明显的好处，用户不在意<br /></li><li><strong>那些能反驳或者质疑规格说明书的人，往往在工作中有优异的表现。</strong><br /></li><li>Google管理的核心是领导力、洞察力、协商、外部沟通能力、技术水平、战略规划、招聘和面试、完成团队绩效考核。<br /></li><li>在Google，我们关注可衡量指标，达到70%，意味着你制定了比较高的目标，然后努力工作去实现它。达到100%，意味着你可能在设定目标的时候不够有进取心。<br /></li><li>我把自己变成用户，就这么简单。我认为，除非能以某种方式将自己置于用户的视角，否则就不可能真正有效的对一个应用进行测试。<br /></li><li><strong>测试的成本遵守70-20-10的原则，单元测试占70%，集成测试占20%，系统测试占10%</strong><br /></li><li>所以经验就是靠解决一些难题来赢得尊重。<br /></li><li>我还是坚信只应该关注最重要的事情。<br /></li><li><strong>很多年轻的测试工程师，他们编写了很多的测试，但是忘记思考为什么要写这些测试，怎么让这些测试为整理目标，为产品服务。</strong><br /></li><li><strong>团队建设完毕之后，我给他们定下了基表：创造价值，最好还能找到可以复制创造价值的方法。通过价值来驱动团队。</strong><br /></li><li>综合的测试方法：开发自动、脚本化测试、探索性测试、基于风险的测试、自动化功能测试等。<br /></li><li>工程开发工具：集成开发环境、代码审查系统、构建系统、源码控制、静态检查、通用测试框架等等。<br /></li><li><strong>要超出自己习惯的舒适地带。</strong><br /></li><li>随着敏捷开发、持续构建、早期用户介入、总包测试、在线软件交付的不断兴起，软件开发的问题也已经彻底改变。<br /></li><li><strong>测试执行框架、自动构建系统、自动测试、持续集成、版本管理</strong></li></ul>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>搭建Github、Coding博客的技术资料</title>
      <link href="/2016/06/21/%E6%90%AD%E5%BB%BAGithub%E3%80%81Coding%E5%8D%9A%E5%AE%A2%E7%9A%84%E6%8A%80%E6%9C%AF%E8%B5%84%E6%96%99/"/>
      <url>/2016/06/21/%E6%90%AD%E5%BB%BAGithub%E3%80%81Coding%E5%8D%9A%E5%AE%A2%E7%9A%84%E6%8A%80%E6%9C%AF%E8%B5%84%E6%96%99/</url>
      
        <content type="html"><![CDATA[<h1 id="四2024年2月更换新mac-airbook-安装的记录">四、2024年2月更换新mac airbook 安装的记录</h1><h2 id="新版本的一些改变需求修改的部分">新版本的一些改变，需求修改的部分</h2><ol type="1"><li># - 之前必须有空行（不是很确定）</li><li>img 不能有空的 “ ”，需要替换。</li><li>- 换行不起作用 需要在前面加Tab；（不是很 确定）。</li><li>文章中yaml部分的Title 和文件名必须一致。</li><li>&gt; 符号前面和后面要空行。</li><li>代码块的前后标签都需要单独一行。</li><li>行内公式的用法，之前和独行公式混了，需要重新修改。</li><li>` 必须用\ 转义</li><li>多个-之间不要有空行，否则显示出来就是每个-之间都空一行。</li></ol><h2 id="换行的问题">换行的问题</h2><p>参考： <a href="https://www.zhihu.com/question/533970447">hexo 回车换行无效？</a></p><p>markdown的换行规范：</p><blockquote><p>段落与换行</p></blockquote><ol type="1"><li>段落的前后必须是空行：<br />空行指的是行内什么都没有，或者只有空白符（空格或制表符）<br />相邻两行文本，如果中间没有空行 会显示在一行中（换行符被转换为空格）.<br /></li><li>如果需要在段落内加入换行（&lt;br&gt;）：<br />可以在前一行的末尾加入至少两个空格.<br />然后换行写其它的文字.<br /></li><li>Markdown 中的多数区块都需要在两个空行之间。</li></ol><p><strong>markdown的规范是软回车，键盘上的回车键是硬回车。一般回车是代表换段。在markdown中，要创建段落，请使用空白行将一行或多行文本进行分隔。</strong></p><p>问题：</p><blockquote><p>mac上，之前使用hexo（19年配置的）+next皮肤，回车键就是换行符号了，用起来很丝滑。<br />最近hexo频繁报错，我就重新装了个最新版本，必须要加，极为难用，而且之前的也要一个一个改正，把 ，十分痛苦。<br />问问各位大佬，如何改为之前那种使用。</p></blockquote><p>回答：</p><blockquote><p>终于弄好了，尝试更改了marked，marked-i，pand.....等渲染，mathjax数学公式，捣鼓一天，全都不work，有冲突。最后将以前的package.json中的所有依赖重新安装，就和以前一样了。主要是 mathjax 的锅，加了mathjax:true后就没法自动换行了。所以建议大家图省事的话也是找一个没问题的package.json，全部无脑照着安装就行。</p></blockquote><p>个人搞了好久，使用原先的kramed渲染器，回车换行倒是可以了，但数学公式很多地方显示不正常。并且kramed已经停止更新好多年了，所以决定不再用。<br />markdown-it支持KaTex，但对MathJax的支持不好，所以也没用。<br />后来还是保留了pandoc的渲染器。即使需要批量修改原先的md文件，特别是段内换行的问题比较突出。</p><p>发现在pakage.json中，renderer部分就保留了：</p><blockquote><p>"hexo-renderer-ejs": "^2.0.0",<br />"hexo-renderer-pandoc": "^0.4.0",<br />"hexo-renderer-stylus": "^3.0.1",</p></blockquote><p><strong>其实就是使用了next的mathjax引擎，然后安装hexo的pandoc渲染器。</strong></p><p><strong>pandoc语法的几个常见注意事项：</strong></p><ul><li>换行前加两个空格。</li><li>图片使用绝对路径。</li><li>标题前两个回车。</li><li>把本地图片地址变为绝对路径。</li></ul><p>使用 pandoc 渲染器 切换 hexo-renderer-pandoc 后，公式倒是能正常显示，但是图片，文本格式需要按 pandoc 的 markdown 格式修改，因此，做了批量调整：</p><h1 id="pandoc导致历史md文件格式问题在sublime-text中调整方法">pandoc导致历史md文件格式问题在sublime text中调整方法：</h1><blockquote><p>有时，我们可能需要将文件中的换行符替换为其他字符，比如英文逗号、分号等。<br />Sublime Text 使用正则表达式来替换文本内容非常方便。<br />举例，一个回车键替换成 空格+空格+回车，以实现段内换行的效果。<br />你可以通过以下步骤来替换：<br />1. 打开 Sublime Text 编辑器<br />2. 打开要替换的文件<br />3. 按下快捷键「Ctrl + H」（「Cmd + Shift + H」在 Mac 上）<br />4. 在「Find What」输入框中输入需要替换的字符串：\n<br />5. 在「Replace With」输入框中输入替换后的字符串：空格+空格+\n<br />6. 选择「Regular Expression」复选框 <strong>（sublime 里面的 .* 那个按钮）</strong><br />7. 点击「Replace」或「Replace All」按钮进行替换<br />如果你需要使用正则表达式语法，例如匹配数字、单词等，请参考其它资源了解正则表达式语法。<br />注意事项，替换的时候因为yaml部分也增加了两个空格，导致出错，需要把yaml那些多出来的空格删除。</p></blockquote><blockquote><p>软件升级最大的问题，规范变了，导致历史的文章需要根据新的规范做修改。</p></blockquote><p>安装完成后执行 hexo clean. 命令来清空以前生成的 HTML 文件，然后重新 hexo g -d 发布。</p><p>关于markdown换行的一篇好文章。 <a href="https://www.skyue.com/17100814.html">markdown段内换行的说明</a></p><h2 id="渲染器的问题">渲染器的问题</h2><p>Hexo 作为一个优秀的 Markdown 博客框架，自然也诞生了很多适用的 Markdown 渲染器，这里对比分析一下 Hexo 下几种常用的 Markdown 渲染器： hexo-renderer-marked ， hexo-renderer-kramed ， hexo-renderer-pandoc ， hexo-renderer-markdown-it ， hexo-renderer-markdown-it-plus 。</p><p>Next 提供了两个渲染引擎来显示数学方程: MathJax 和 KaTeX。要使用这个特性，您只需要选择一个渲染引擎并打开它的 enable(位于home config file)。 然后你需要安装相应的 Hexo 渲染器来完全支持数学方程式的显示。</p><h2 id="ssh-key的问题">ssh key的问题</h2><p>git config --global user.name “hongyitong”<br />git config --global user.email “hogyuanjiao@hotmail.com”<br />ssh-keygen -t rsa -b 4096 -C “hogyuanjiao@hotmail.com”</p><ol type="1"><li><p>查看本地是否存在SSH密钥<br />命令：ls -al ~/.ssh<br />如果在输出的文件列表中发现id_rsa和id_rsa.pub的存在，证明本地已经存在SSH密钥，请执行第3步</p></li><li><p>查看SSH公钥<br />命令：cat /Users/电脑用户名/.ssh/id_rsa.pub<br />复制打印出来的信息，在GitLab或者GitHub的SSH Keys中进行相应设置即可</p></li><li><p>警告的信息<br />&gt; You've successfully authenticated, but GitHub does not provide shell access.<br />这句话其实一直存在，它只是想告诉你，github 不允许 shell 交互。仅此而已。</p></li></ol><h2 id="mac技巧">mac技巧</h2><p>Mac用户注意了，要使用管理员权限，首先：“sudo -s”，回车，输入密码，再回车，然后：“npm install -g hexo-cli”，回车，等待下载安装即可！</p><h2 id="参考的资料">参考的资料</h2><p><a href="https://blog.l3zc.com/2022/05/mac%E7%8E%AF%E5%A2%83%E4%B8%8B%E4%BD%BF%E7%94%A8hexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/">Mac OS环境下使用Hexo搭建个人博客</a></p><h1 id="三多机搭建更换电脑搭建">三、多机搭建（更换电脑搭建）</h1><p>其实简单说，就是四个部分：<br />1、备份<br />备份source文件夹（所有的MD文件都在这里）；备份站点的_config.yml，主题的的_config.yml;<br />如果担心，可以把整个hexo目录备份下来；<br />2、安装<br />按照指引安装（其实可以按照第一次的安装来处理），只是涉及github 仓库那一部分是不需要了；<br />3、恢复<br />1）把source目录恢复覆盖；<br />2）两个_config.yml的内容，比较，然后把需要的部分copy到新装的文件里面替换对应内容。<strong>注意，这里不要用整个文件覆盖的模式，因为可能对应的软件的版本不一致了。</strong><br />4、安装相关的插件<br />包括支持数学公式、置顶、搜索等等；</p><p>因为之前上传到百度网盘的MD源文件，下载之后的创建日期都变成下载那天的时间，导致之前很多没有定义date时间的都默认成下载的日期了。<br />花了好长时间把MD文件的date属性都加上了发布的时间。</p><p>多机部署参考：<br /><a href="http://lowrank.science/Hexo-Migration/">多机更新 Hexo 博客</a><br />上文中提到的:先在电脑中的某一位置建立好 your_name.github.io 文件夹，比如 D:_name.github.io；<br />因为之前习惯取了hexo的名字，其实 hexo目录就是这里所谓的 youre_name.githubo.io文件夹。</p><h1 id="一第一次搭建过程资料">一、第一次搭建过程资料</h1><p>经过几天的努力，终于把个人的博客搭建得差不多了。个人的博客现在存在Github和Coding两个地方，地址分别是：<br /><a href="http://hongyitong.github.io/">Github版</a> <br> <a href="http://raymanhung.coding.me/">Coding版</a>     <strong>!Coding版本因为有100M的空间限制，无法发布了！</strong></p><p>Coding用不了之后，使用了码云Oschina，具体的设置如下(不过Oschina还有有点不稳定和刷新速度比较慢。)：<br /><a href="http://hongyitong.oschina.io/">Oschina版</a><br /><strong>Oschina免费版限制空间了，因为博客容量太大，无法再从github中pull过来了</strong><br /><a href="https://git.oschina.net/">码云</a><br /><a href="https://javaclear.github.io/2017/01/25/%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%85%8D%E8%B4%B9%E5%8D%9A%E5%AE%A2%EF%BC%9A%E5%88%A9%E7%94%A8Hexo%E5%9C%A8%E7%A0%81%E4%BA%91%E4%B8%8A%E9%83%A8%E7%BD%B2%E5%8D%9A%E5%AE%A2/">搭建个人免费博客：利用Hexo在码云上部署博客</a></p><p>在搭建过程当中，参考了以下的文档，特此记录！<br /><a href="http://sunwhut.com/2015/10/30/buildBlog/">如何利用GitHub Pages和Hexo快速搭建个人博客</a>     <strong>（这个博客不知道什么原因，访问不了:(）</strong><br />参考一下以下几个吧：<br /><a href="http://lowrank.science/Hexo-Github/">知行合一 用 Hexo 搭建博客</a><br /><a href="http://www.wuxubj.cn/2016/08/Hexo-nexT-build-personal-blog/">Hexo+nexT主题搭建个人博客</a><br /><a href="https://linghucong.js.org/2016/04/15/2016-04-15-hexo-github-pages-blog/">手把手教你使用Hexo + Github Pages搭建个人独立博客</a></p><p><a href="https://segmentfault.com/a/1190000004548638">hexo同时托管到coding.net与github</a><br /><a href="http://www.jianshu.com/p/2cbf3e2f6d57">如何将Hexo Blog同时发布到GitHub跟Coding上</a><br /><a href="http://fionat.github.io/2016/04/02/sitemap/">｜Hexo优化｜如何向google提交sitemap（详细）</a></p><h2 id="设置-google站点管理工具-的验证字符串用于提交-sitemap">设置 Google站点管理工具 的验证字符串，用于提交 sitemap</h2><p>获取 google site verification code<br />登录 Google Webmaster Tools，导航到验证方法，并选择 HTML Tag。将会获取到一段代码：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;meta name=&quot;google-site-verification&quot; content=&quot;XXXXXXXXXXXXXXXXXXXXXXX&quot; /&gt;</span><br></pre></td></tr></table></figure><p>将 content 里面的 XXXXXXXXXXXXXXXXXXXXXXX 复制出来。<br />设置主题。编辑 站点配置，新增字段 google_site_verification。<br />google_site_verification: XXXXXXXXXXXXXXXXXXXXXXX</p><p>在google加了github的收录，但是在增加百度收录的时候，一直生成不了baidusitemap.xml文件。暂时放弃。（后来重启电脑就自动有了:-(）<br /><a href="http://www.jianshu.com/p/619dab2d3c08">hexo干货系列：（六）hexo提交搜索引擎（百度+谷歌）</a></p><a href="/2016/11/14/%E6%9C%AC%E5%9C%B0%E9%A2%84%E8%A7%88%E6%B2%A1%E9%97%AE%E9%A2%98deploy%E5%90%8E%E4%B8%BB%E9%A1%B5%E6%98%BE%E7%A4%BA%E5%A4%A7%E9%9D%A2%E7%A7%AF%E7%A9%BA%E7%99%BD/" title="本地预览没问题，deploy后主页显示大面积空白">本地预览没问题，deploy后主页显示大面积空白</a><h1 id="二优化过程及问题">二、优化过程及问题</h1><p><strong>！！！！！如果本地预览没问题，部署到github有问题的话，先 hexo clean掉在重新 hexo d -g</strong></p><h2 id="搜索">1、搜索</h2><p>因为Swiftype收费了，转向了微搜索：<br /><a href="https://github.com/iissnan/hexo-theme-next/issues/784">NexT主题下的两种搜索服务的问题</a></p><p><strong>后来简单点，直接使用了Next主题的本地搜索了，具体见下文！</strong><br /><a href="http://www.ezlippi.com/blog/2017/02/hexo-search.html">Hexo博客添加站内搜索</a><br />补充一点，还需要修改主题下的_config.yml文件，把enable：false改成true</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># Local search</span><br><span class="line">local_search:</span><br><span class="line">  enable: true</span><br></pre></td></tr></table></figure><h2 id="数学公式">2、数学公式</h2><a href="/2016/10/24/%E5%A6%82%E4%BD%95%E5%9C%A8Hexo%E4%B8%AD%E4%BD%BF%E7%94%A8MathJax/" title="如何在Hexo中使用MathJax">如何在Hexo中使用MathJax</a><h2 id="文章发表时间显示了两遍">3、文章发表时间显示了两遍</h2><a href="/2016/11/15/%E6%96%87%E7%AB%A0%E5%8F%91%E8%A1%A8%E6%97%B6%E9%97%B4%E6%98%BE%E7%A4%BA%E4%BA%86%E4%B8%A4%E9%81%8D/" title="文章发表时间显示了两遍">文章发表时间显示了两遍</a><h2 id="hexo文章置顶">4、Hexo文章置顶</h2><a href="/2017/01/06/Hexo%E6%96%87%E7%AB%A0%E7%BD%AE%E9%A1%B6/" title="Hexo文章置顶">Hexo文章置顶</a><h2 id="如何链接站内文章">5、如何链接站内文章</h2><p>A:next或者hexo，链接站内文章的方法？<br />B:关于引用站内文章的链接，可以使用以下语法：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% post_link markdown-learning-by-maxiang 点击这里查看这篇文章 %&#125;</span><br></pre></td></tr></table></figure><p>markdown-learning-by-maxiang是你的文章名称。如果文章不存在，这段代码将会被直接忽略。<br />点击这里查看这篇文章是该链接的标题。如果置空，则自动提取文章的标题。<br />ps：这个功能跟主题无关。</p><h2 id="git远程库与本地联系报错fatal-not-a-git-repository-or-any-of-the-parent-directories-.git">6、git远程库与本地联系报错：fatal: Not a git repository (or any of the parent directories): .git</h2><p>在github上新建了一个仓库，然后相与本地的仓库联系起来<br />$ Git remote add origin https://github.com/liona329/learngit.git fatal: Not a git repository (or any of the parent directories): .git<br />总是报这个错<br /><strong>解决方法：git init</strong><br />成功</p><h2 id="github-build出错信息会发到注册的邮箱">7、github build出错信息会发到注册的邮箱</h2><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[hongyitong/hongyitong.github.io] Page build failure</span><br><span class="line">G</span><br><span class="line">GitHub &lt;support@github.com&gt;</span><br><span class="line">今天, 16:12</span><br><span class="line">你 </span><br><span class="line">The page build failed for the `master` branch with the following error:</span><br><span class="line"></span><br><span class="line">The tag `fancybox` on line 77 in `themes/landscape/README.md` is not a recognized Liquid tag. For more information, see https://help.github.com/articles/page-build-failed-unknown-tag-error/.</span><br><span class="line"></span><br><span class="line">For information on troubleshooting Jekyll see:</span><br><span class="line"></span><br><span class="line">  https://help.github.com/articles/troubleshooting-jekyll-builds</span><br></pre></td></tr></table></figure><h2 id="error-cannot-find-module-hexo-util">8、Error: Cannot find module 'hexo-util'</h2><p>升级NexT主题以后，执行hexo clean，出错：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">✗ hexo clean         </span><br><span class="line">ERROR Script load failed: themes/next/scripts/tags/exturl.js</span><br><span class="line">Error: Cannot find module &#x27;hexo-util&#x27;</span><br><span class="line">    at Function.Module._resolveFilename (module.js:325:15)</span><br><span class="line">    at Function.Module._load (module.js:276:25)</span><br><span class="line">    at Module.require (module.js:353:17)</span><br><span class="line">    at require (/Users/lisanlai/lisanlai.github.io/node_modules/hexo/lib/hexo/index.js:213:21)</span><br><span class="line">    at /Users/lisanlai/lisanlai.github.io/themes/next/scripts/tags/exturl.js:8:12</span><br><span class="line">    at /Users/lisanlai/lisanlai.github.io/node_modules/hexo/lib/hexo/index.js:229:12</span><br><span class="line">    at tryCatcher (/Users/lisanlai/lisanlai.github.io/node_modules/hexo/node_modules/bluebird/js/release/util.js:16:23)</span><br><span class="line">    at Promise._settlePromiseFromHandler (/Users/lisanlai/lisanlai.github.io/node_modules/hexo/node_modules/bluebird/js/release/promise.js:502:31)</span><br><span class="line">    at Promise._settlePromise (/Users/lisanlai/lisanlai.github.io/node_modules/hexo/node_modules/bluebird/js/release/promise.js:559:18)</span><br><span class="line">    at Promise._settlePromise0 (/Users/lisanlai/lisanlai.github.io/node_modules/hexo/node_modules/bluebird/js/release/promise.js:604:10)</span><br><span class="line">    at Promise._settlePromises (/Users/lisanlai/lisanlai.github.io/node_modules/hexo/node_modules/bluebird/js/release/promise.js:683:18)</span><br><span class="line">    at Promise._fulfill (/Users/lisanlai/lisanlai.github.io/node_modules/hexo/node_modules/bluebird/js/release/promise.js:628:18)</span><br><span class="line">    at Promise._resolveCallback (/Users/lisanlai/lisanlai.github.io/node_modules/hexo/node_modules/bluebird/js/release/promise.js:423:57)</span><br><span class="line">    at Promise._settlePromiseFromHandler (/Users/lisanlai/lisanlai.github.io/node_modules/hexo/node_modules/bluebird/js/release/promise.js:514:17)</span><br><span class="line">    at Promise._settlePromise (/Users/lisanlai/lisanlai.github.io/node_modules/hexo/node_modules/bluebird/js/release/promise.js:559:18)</span><br><span class="line">    at Promise._settlePromise0 (/Users/lisanlai/lisanlai.github.io/node_modules/hexo/node_modules/bluebird/js/release/promise.js:604:10)</span><br><span class="line">    at Promise._settlePromises (/Users/lisanlai/lisanlai.github.io/node_modules/hexo/node_modules/bluebird/js/release/promise.js:683:18)</span><br><span class="line">    at Promise._fulfill (/Users/lisanlai/lisanlai.github.io/node_modules/hexo/node_modules/bluebird/js/release/promise.js:628:18)</span><br><span class="line">    at /Users/lisanlai/lisanlai.github.io/node_modules/hexo/node_modules/bluebird/js/release/nodeback.js:42:21</span><br><span class="line">    at /Users/lisanlai/lisanlai.github.io/node_modules/hexo/node_modules/hexo-fs/node_modules/graceful-fs/graceful-fs.js:78:16</span><br><span class="line">    at FSReqWrap.readFileAfterClose [as oncomplete] (fs.js:380:3)</span><br><span class="line">INFO  Deleted database.</span><br></pre></td></tr></table></figure><p><strong>解决办法：重新安装hexo-util模块</strong></p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install -- save-dev hexo-util</span><br></pre></td></tr></table></figure><h2 id="hexo-next的local-search转圈圈的问题">9、HEXO-NexT的Local Search转圈圈的问题</h2><p>可以在浏览器中输入 博客网址：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://localhost:4000/search.xml </span><br></pre></td></tr></table></figure><p>其中 search.xml是搜索的内容文档，在hexo 对应的pubic目录下。</p><p>如果有错，浏览器会报出具体的错误。</p><p>可以找到引起错误的md文件，然后在search.xml文档中找到对应的段落，可以使用https://www.xmlvalidation.com/ 来验证对应段落的具体错误原因。<br />具体的文档可以见：https://guahsu.io/2017/12/Hexo-Next-LocalSearch-cant-work/</p><h2 id="本地预览没问题deploy后主页显示大面积空白">10、本地预览没问题deploy后主页显示大面积空白</h2><a href="/2016/11/14/%E6%9C%AC%E5%9C%B0%E9%A2%84%E8%A7%88%E6%B2%A1%E9%97%AE%E9%A2%98deploy%E5%90%8E%E4%B8%BB%E9%A1%B5%E6%98%BE%E7%A4%BA%E5%A4%A7%E9%9D%A2%E7%A7%AF%E7%A9%BA%E7%99%BD/" title="本地预览没问题，deploy后主页显示大面积空白">本地预览没问题，deploy后主页显示大面积空白</a><h2 id="hexo-博客无法搜索的终极解决方法">11、Hexo 博客无法搜索的终极解决方法</h2><p>http://www.sqlsec.com/2017/12/hexosearch.html</p><h2 id="文章添加阅读次数访问量等">12、文章添加阅读次数，访问量等</h2><p>Hexo+Next主题 文章添加阅读次数，访问量等：<br />https://blog.csdn.net/xr469786706/article/details/78166227<br />hexo博客解决不蒜子统计无法显示问题：<br />https://www.jianshu.com/p/fd3accaa2ae0</p><h2 id="解决macos升级后出现xcrun-error-invalid-active-developer-path-missing-xcrun的问题">13、解决MacOS升级后出现xcrun: error: invalid active developer path, missing xcrun的问题</h2><p>今天升级macOS High Sierra，终端里使用git的时候，弹出一行莫名其妙的错误：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xcrun: error: invalid active developer path (/Library/Developer/CommandLineTools), missing xcrun at: /Library/Developer/CommandLineTools/usr/bin/xcrun </span><br></pre></td></tr></table></figure><p>解决方法，重装xcode command line：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xcode-select --install </span><br></pre></td></tr></table></figure><p>如果没有解决问题，执行以下命令<br /><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo xcode-select -switch / </span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> 技术相关 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 互联网 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《正说清朝十二帝》读后感</title>
      <link href="/2016/06/20/%E3%80%8A%E6%AD%A3%E8%AF%B4%E6%B8%85%E6%9C%9D%E5%8D%81%E4%BA%8C%E5%B8%9D%E3%80%8B%E8%AF%BB%E5%90%8E%E6%84%9F/"/>
      <url>/2016/06/20/%E3%80%8A%E6%AD%A3%E8%AF%B4%E6%B8%85%E6%9C%9D%E5%8D%81%E4%BA%8C%E5%B8%9D%E3%80%8B%E8%AF%BB%E5%90%8E%E6%84%9F/</url>
      
        <content type="html"><![CDATA[<p><strong>黑格尔说过，人类从历史中得到的教训就是：人类从来不记取历史教训。</strong><br />比较了好多关于清朝历史方面的书，《正说清朝十二帝》因为丰富的彩色插图吸引了我。书中每个章节根据内容，配有各类的图片，包括人物画像、物品图、景色图、书法图、名画等等。<br />但可能是正说的缘故，书中文字还是比较晦涩，故事也比较平淡，缺少了《明朝那些事儿》的诙谐、有趣感。<br />清朝处在一个特殊的时代，中国已经从盛唐时期的万国之国，没落成了落后挨打的一方。<br />当时，西方主要的国家，英国、法国、德国、美国，都走上了资本主义发展的道路，这些国家对内努力发展工业，提高科学技术，对外进行殖民扩张。近邻日本也实行了明治维新，实现了国家的现代化。而清朝还沉浸在“天朝统御万国”、“天朝物产丰盛，无所不有，原不藉外夷货物，一通有无”的幻境中。在强敌面前，清朝皇帝采取“率祖旧章”、“持盈保泰”的保守态度，导致国家一步步滑向了灭国的境地。<br /><span id="more"></span><br />掩卷沉思，有两个场面印象特别深刻。<br /><strong>其一，关于道光传位给咸丰的故事</strong><br />本来六阿哥（奕訢）有才有贤，道光也想过立他为太子，但是在一次狩猎后，故事发生了变化。</p><blockquote><p><strong>【藏拙示仁】</strong><br />《清史稿·杜受田传》载:道光晚年诸皇子随其南苑围猎。六子奕訢射猎最多。四子奕詝未获一物，道光究其因，奕詝曰:时方春，鸟兽孳育，不忍伤生…<br />宣宗大喜以为仁孝，于是放弃德才兼备的奕訢，密立有德无才奕詝为储。咸丰后来遇大事缺乏胆识、远略，退缩、逃避，连基本的“德”也是欠缺的。</p></blockquote><p>有句话说得好，德往往是缺才的掩饰。如果评价体系倾向于德，那么就会滋生出一批道义制高点的伪君子。而人才却因此而埋没。管理也一样，在工作上必须有明确的成果作为考核依据，道德的方面可以通过制度、监督进行规避。例如马丁·路德·金在私德上是有问题，但是不能因为这些否认他为黑人争取权利方面的贡献。所以，道光的选择其实也加速了清朝的灭亡。<br /><strong>其二，关于道光皇帝节俭的故事。</strong><br />在道光朝，清政府在鸦片战争中吃了败仗。然而，失败并不可怕，可怕的是不从失败中汲取教训，继续封闭，狂妄自大。本来应该在鸦片战争后，总结教训，卧薪尝胆，弃旧维新，进行改革，消除隐患；道光皇帝却以穿补丁裤子显示节俭，这些纯属是捡了芝麻而丢了西瓜。</p><p>司马光在写下《资治通鉴》的时候，想通过本书“鉴前世之兴衰，考当今之得失”，但是封建皇朝的2000多年，悲剧却还一再上演。<br />杜牧在《阿旁宫赋》中感叹到：<strong>秦人不暇自哀，而后人哀之，后人哀之而不鉴之，亦使后人而复哀后人也。</strong></p><ul><li><strong>【欹（qī）器】</strong><br />孔子观于鲁桓公(公元前711年～前694年在位)之庙，有欹器焉。孔子问于守庙者曰：此为何器?守庙者曰：此盖为宥坐之器。孔子曰：吾闻宥坐之器，虚则欹，中则正，满则覆。孔子顾弟子日：注水焉。弟子挹水而注之，果中而正，满而覆，虚而欹。孔子喟然而叹日：吁!恶有满而不覆者哉!”<br /><img src="/img/欹器.png" title="虚则欹，中则正，满则覆" /><br /><img src="/img/欹器1.png" title="虚则欹，中则正，满则覆" /></li></ul><h1 id="文章摘录">文章摘录</h1><ul><li>泰化否，否生泰<br /></li><li>正如一位哲人说过的，在权利争夺的平行四边形诸力中，两条边的的两个不同方向的分力，斗争的结果，既不是这条边的力，也不是那条边的力，而是对角线的力，就是两个分力所产生的一个合力。<br /></li><li>临大事，有静气。青年天子玄烨在危机时刻，持心坚定，气静不慌。他为了安定惊恐的军心，慌乱的民心，每天游景山，观骑射，以示胸有成竹。有人进行讽谏，康熙置若罔闻。事后他说：“当时我要是表现出一丝惊恐来，就会人心动摇，说不定会出现意外的情况！”他的坚定决心和平静心态，对于稳定大局和安定人心，起了很大的作用。<br /></li><li>看来康熙对待吏治，还缺乏系统的制度：严格制度，使官吏不能贪污；严厉惩处，使官吏不敢贪污；严定薪俸，使官吏不必贪污；严肃教育，使官吏不想贪污。<br /></li><li>教育能影响一个人，而不能决定一个人的人生道路。<br /></li><li>雍正巧妙的将自己隐藏起来。他对父皇表示忠孝，又尽力友善兄弟，并交好朝廷诸臣。他以不争为争，坐收渔人之利。<br /></li><li>雍正为了谋取皇位，韬光养晦，费尽心机。他的心腹戴铎，在康熙五十二年为他谋划到：<br />处英明之父子也，不露其长，恐见其弃；过露其长，恐见其疑，此其所以为难。处众多之手足也，此有好竽，彼有好瑟，此有所争，彼有所胜，此其所以为难。……其诸王阿哥之中，俱当以大度包容，使有才者不为忌，无才者以为靠。<br /></li><li>康熙：“当初拘禁...时，并没有一个人替他说话，只有四阿哥深知大义，多次在我面前为...保奏，像这样的心地和行事，才是能做大事的人。”<br /></li><li>年羹尧与隆科多两人，对雍正来说，是狡兔死，走狗烹；飞鸟尽，良弓藏。对他们自己来说，则是知进不知退，知显不知隐，泰极否来，自酿其祸。<br /></li><li>乾隆自我总结一生有“十全武功”，自诩为“十全老人”。十全武功是：“十功者，平准噶尔二，定回部一，打金川为二，靖台湾为一，降缅甸、安南各一，即今之受廓尔喀降，合为十。”<br /></li><li>在乾隆时期，世界上发生了三件大事：第一件是英国工业革命；第二件是美利坚合众国成立；第三件是法国大革命。这三件大事再加上此前的英国资产阶级革命，具有划时代的意义，影响了世界历史的进程，改变了整个世界的格局。<br /></li><li>据说和珅在狱中，自知生命不久，对着窗外元宵明月，感慨赋诗道：“<strong>对景伤前事，怀才误此身。</strong>”<br /></li><li><strong>嘉庆时期面临一系列的社会危机，但是嘉庆都把上述问题做个个案看待，他没有也不可能从制度上去加以解决。</strong><br /></li><li><strong>道光皇帝年轻时，曾写道：“事愈大，心愈小；情愈急，气愈和。”可见他在年轻时，就十分注意磨炼自己的性格。</strong><br /></li><li>有两个问题值得思考：鸦片战争清朝失败是必然还是偶然？鸦片战争失败的主要历史责任是穆章阿还是道光皇帝？<br /></li><li>【史鱼尸谏】<br />春秋时期，卫国有位贤人蘧伯玉，为人正直且德才兼备，但卫灵公却不肯重用他；另一位叫弥子瑕的，作风不正派，卫灵公反而委以重任。<br />史鱼是卫国一位大臣，看到这种情况，内心很是忧虑，但屡次进谏，卫灵公始终不采纳。<br />后来，史鱼得了重病，奄奄一息，将要去世前，将儿子唤了过来，嘱咐他说：「我在卫朝做官，却不能够进荐贤德的蘧伯玉而劝退弥子瑕，是我身为臣子却没有能够扶正君王的过失啊！生前无法正君，那么死了也无以成礼。我死后，你将我的尸体放在窗下，这样对我就算完成丧礼了。」<br />史鱼的儿子听了，不敢不从父命，于是在史鱼去世后，便将尸体移放在窗下。<br />卫灵公前来吊丧时，见到大臣史鱼的尸体，竟然被放置在窗下，如此轻慢不敬，因而责问史鱼的儿子。史鱼的儿子于是将史鱼生前的遗命告诉了卫灵公。<br />卫灵公听后很惊愕，脸色都变了，说道：「这是我的过失啊！」于是马上让史鱼的儿子，将史鱼的尸体按礼仪安放妥当，回去后，便重用了蘧伯玉，接着又辞退了弥子瑕并疏远他。<br />当孔夫子听到此事后，赞叹地说道：「古来有许多敢于直言相谏的人，但到死了便也结束了，未有像史鱼这样的，死了以后，还用自己的尸体来劝谏君王，以自己一片至诚的忠心使君王受到感化，难道称不上是秉直的人吗？」<br />《论语》有言：「君使臣以礼，臣事君以忠。」身为臣子，为国为民，尽忠职守，劝谏君王，是为臣的本分。<br /></li><li>陆游曾经在《钗头凤》中这样感叹差错凄惨的爱情：<br />红酥手，黄縢酒，满城春色宫墙柳。东风恶，欢情薄。一怀愁绪，几年离索。错、错、错。<br />春如旧，人空瘦，泪痕红浥鲛绡透。桃花落，闲池阁。山盟虽在，锦书难托。莫、莫、莫！<br /></li><li>当年，明成祖朱棣迁都北京，原因之一是“天子守国门”。<br /></li><li>【钩弋（yì）夫人】<br />《汉书·外戚传》：“心欲立焉，以其年稚母少，恐女主颛恣乱国家，犹与久之。”<br />后元二年（公元前87年），武帝弥留之际立刘弗陵为太子，以大司马霍光辅政。刘弗陵即位，为汉昭帝，时年八岁。<br />钩弋夫人被赐死后，有人对欲立其子而杀其母的做法不能理解。《资治通鉴·汉纪十四》中记载了武帝的一段解释：“是非儿曹愚人所知也。往古国家所以乱，由主少母壮也。女主独居骄蹇，淫乱自恣，莫能禁也。汝不闻吕后邪！故不得不先去之也。”对此，早有论者指出“自古帝王遗命多矣，要未有如汉武之奇者。”（明人张燧《千百年眼》）<br /></li><li>中国自公元前221年秦始皇称皇帝以来，直到1912年宣统皇帝退位，历经2132年，有349位皇帝。<br /></li><li><strong>有书真富贵，无事小神仙</strong><br /></li><li>连中三元：乡试：解（jiè）元； 会试：会元； 殿试：状元<br /></li><li>解的几种读音<br />“解甲归田”的“解”“jiě”“解甲”，就是脱去束在身上的铠甲，故“解”当读jiě。<br />读为“jiè”的“解”字，主要含义有：1：押送，如“解差”（旧时押送犯人者）、“解送”（押送财物或者犯人等）；2：“解元”，明清两代称考取乡试考取第一名的人为“解元”（因为唐朝举进士都是由地方官解送入试，故称）<br />“跑马卖解”的“解”“xiè”<br />读为“xiè”的“解”字，旧指杂技表演的各种技艺，如“跑马卖解”（特指骑在在马上表演的技艺）、“解数”（本指战术驾驶，后泛指手段、本领）等。还有，姓也读此音，如：歌手“解晓东”的姓就是这个读音。<br /></li><li>有位哲人说过：“当今列强，有今而无古；希腊、罗马，有古而无今。惟我中国，有古有今。”<br /></li><li>纳兰性德，其父为康熙朝大学士明珠。纳兰性德22岁中进士，著有《纳兰词》。被誉为清代第一词人。其《长相思》云：<br />山一程，水一程，身向榆关那畔行，夜深千帐灯。<br />风一更，雪一更，聒碎乡心梦不成，故国无此声。<br />注：聒（guō）声音吵闹，使人厌烦。 <img src="/img/清朝十二帝.png" title="封面" /></li></ul>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 历史 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《数学之美》读后感</title>
      <link href="/2016/06/15/%E3%80%8A%E6%95%B0%E5%AD%A6%E4%B9%8B%E7%BE%8E%E3%80%8B%E8%AF%BB%E5%90%8E%E6%84%9F/"/>
      <url>/2016/06/15/%E3%80%8A%E6%95%B0%E5%AD%A6%E4%B9%8B%E7%BE%8E%E3%80%8B%E8%AF%BB%E5%90%8E%E6%84%9F/</url>
      
        <content type="html"><![CDATA[<p>一本不错的信息处理数学原理科普书。一些复杂的信息问题、工程问题，经作者之手，把背后的数学原理通过简单的形式展示出来，让普通读者明白到了数学解决问题的能力，体会到了数学的美。<br />这本书，背后也道出了一个道理，那就是基础数学理论研究对计算机科学的发展能起到非常重要的作用。<br />例如，书中提到，布尔代数，在19世纪布尔提出的80多年时间里，一直默默无闻，在实际生活中也没有起到任何作用。直到1938年香农提出在电路开关中应用布尔代数来处理之后，才开始在数字电路方面得到了广泛应用。</p><blockquote><p>布尔（George Boole) 是十九世纪英国一位小学数学老师。他生前没有人认为他是数学家。布尔在工作之余，喜欢阅读数学论著、思考数学问题。1854 年“思维规律” （An Investigation of the Laws of Thought, on which are founded the Mathematical Theories of Logic and Probabilities）一书，第一次向人们展示了如何用数学的方法解决逻辑问题。</p></blockquote><blockquote><p>读者也许会问这么简单的理论能解决什么实际问题。布尔同时代的数学家们也有同样的问题。事实上在布尔代数提出后80 多年里，它确实没有什么像样的应用，直到 1938 年香农在他的硕士论文中指出用布尔代数来实现开关电路，才使得布尔代数成为数字电路的基础。所有的数学和逻辑运算，加、减、乘、除、乘方、开方等等，全部 能转换成二值的布尔运算。</p></blockquote><blockquote><p>不管索引如何复杂，查找的基本操作仍然是布尔运算。布尔运算把逻辑和数学联系起来了。它的最大好处是容易实现，速度快，这对于海量的信息查找是至关重要的。它的不足是只能给出是与否的判断，而不能给出量化的度量。因此，所有搜索引擎在内部检索完毕后，都要对符合要求的网页根据相关性排序，然后才返回给用户。</p></blockquote><p>像作者说的，布尔代数对于数学的意义，等同于量子力学对于物理学的意义，他们将我们对世界的认识从连续状态扩展到了离散的状态。<br />因为作者是搜索方面的专家，所以，作者对搜索涉及的数学建模、有向图、矩阵、统计、概率、迭代等方面进入了深入浅出的描述。<br />印象最深刻的是，作者在讨论到比较两个集合的是否一样的时候，提到了各种算法的优劣性。最基本的算法是采用逐个元素的比较，好一点的是排序后再比较。但是作者提出，其实最优的方案的是，采集每个词的信息指纹----数字，然后对这些数字求和比较，就能马上知道两个集合是否有差异了。因为采用的信息指纹是伪随机数，不同信息指纹加减乘除之后相同的概率非常小。信息指纹在判断网页相似性、论文抄袭等方面发挥了重要的作用。<br />另外说的一点是，因为密码学不是作者的专业，感觉本书在密码学方面讲得不够透。密码方面的知识建议参考<a href="https://www.zhihu.com/question/21518760">(知乎)关于老和尚小和尚讨论密码的问题</a><br /><span id="more"></span></p><h1 id="文章摘录">文章摘录</h1><ul><li>了解了罗塞塔石碑的历史，对于今天很多翻译软件和服务都叫做“Rosetta”就不会觉得奇怪了，这其中包括Google的机器翻译和世界上销量最大的PC机上的翻译软件。<br /></li><li><strong>阿拉伯数字或者说印度数字的革命性不仅在于它的简洁有效，而且标志着数字和文字的分离。</strong><br /></li><li>当司马迁用近53万字记载了中国上千年历史的同时，远在中东的犹太人也用类似的篇幅记载了自创世纪以来，主要是摩西以来他们祖先的历史，这就是《圣经》中的《旧约》部分。<br /></li><li>如果说从字母到此的构词法（Morphology）是词的编码规则，那么语法则是语言的编码和解码规则。不过，相比较而言，词可以被认为是有限而且闭合的集合，而语言则是无限和开放的集合。<br />因此，任何语言都有语法规则覆盖不到的地方，这些例外或者说不准确性，让我们的语言丰富多彩。<br /></li><li>当然，香浓不必要得什么图灵奖，作为信息论的发明人，他在科学史上的地位和图灵是相当的，而且通信领域最高奖就是以他的名字命名的。<br /></li><li><strong>对于人类来讲，一个能把英语翻译成汉语的人，必定能很好理解这两种语言。这就是直觉的作用。在人工智能领域，包括自然语言处理领域，后来把这样的方法论称作“鸟飞派”，也就是看看鸟是怎么飞的，就能模仿鸟造出飞机，而不需要了解空气动力学。事实上我们知道，怀特兄弟发明飞机靠的是空气动力学而不是仿生学。</strong><br /></li><li><strong>自然语言的处理从基于规则方法的传统，现在转入了统计的语言处理方法。</strong><br /></li><li>在数理统计中，我们之所以敢对采样数据进行观察的结果来预测概率，是因为有大数定理（Law of Large Numbers）在背后做支持，他的要求是有足够的观测值。<br /></li><li>这里有一个很好的例子，来自于腾讯搜索部门。最早的语言模型是使用《人民日报》的语料训练的，因为开发者认为这些语料干净、无噪音。但是实际的效果就比较差，经常出现搜索串和网页不匹配的例子。后来改用网页的数据，尽管他们有很多的噪音，但是因为训练数据和应用一致，搜索质量反而好。<br /></li><li>但是对于能找到模式（Pattern）的、量比较大的噪音还是有必要过滤的，而且他们也比较容易处理，比如网页文本中存在的大量的制表符。因此，在成本不高的情况下，有必要对训练数据进行过滤。<br /></li><li>分词的二义性是语言歧义性的一部分，1990年前后，当时清华大学电子工程系工作的郭进博士用统计语言模型成功解决了分词二义性的问题，将汉语分词的错误率降低了一个数量级。<br /></li><li>化学里分子是保持化学性质的最小单位，再往下分到原子，化学性质就变了。<br /></li><li>很多自然语言的处理问题是和通信的解码问题等价的，因此它们完全可以由隐马尔可夫模型来解决。<br /></li><li>隐马尔可夫模型最初应用于通信领域，继而推广到语音和语言处理中，成为连接自然语言处理和通信的桥梁。同时，隐马尔可夫模型也是机器学习的主要工具之一。和几乎所有机器学习的模型工具一样，它需要一个训练方法（鲍姆-韦尔奇算法）和使用的解码算法（维特比算法），掌握了这两类算法，就基本可以使用隐马尔可夫模型这个工具了<br /></li><li>“信息熵（读Shang）”的概念，可以认为，信息量就等于不确定性的多少。<br /></li><li>几乎所有的自然语言处理、信息与信号处理的应用都是一个消除不确定性的过程。<br /></li><li>信息的作用就在于消除不确定性，自然语言处理的大量问题就是寻找相关的信息。<br /></li><li>信息熵的物理含义是对一个信息系统不确定性的度量，在这一点上，它和热力学熵中的概念有相似之处，因为后者就是一个系统无序的度量，从另一个角度讲也是对一种不确定性的度量。<br /></li><li>罗曼•罗兰为那些追求灵魂高尚而非物质富裕的年轻人写下了《巨人三传》，让大家呼吸到巨人的气息。<br /></li><li>贾里尼克教授在学术上给我最大的帮助就是提高了我在学术上的境界。他告诉我最多的是：什么方法不好。在这一点上和股神巴菲特给和他吃饭的投资人的建议有异曲同工之处。巴菲特和那些投资人讲，你们都非常聪明，不需要我告诉你们做什么，我只需要告诉你们不要去做什么（这样可以少犯很多错误），这些不要做的事情，是巴菲特从一生的经验教训中得到的。<br /></li><li>具体的做事方法是术，做事的原理和原则是道。真正做好一件事没有捷径，离不开一万小时的专业训练和努力。<br /></li><li><strong>布尔代数对于数学的意义等同于量子力学对于物理学的意义，他们将我们对世界的认识从连续的状态扩展到离散状态。</strong><br /></li><li>早期的搜索引擎（比如AltaVista以前的所有搜索引擎），由于受计算机速度和容量的限制，只能对重要、关键的主题词建立索引。至今很多学术杂志还要求作者提供3-5个关键词。这样，所有不常见的词和太常见的虚词就找不到了。现在，为了保证对任何搜索都能提供相关的网页，常见的搜索引擎都会对素有的词进行索引。但是，这在工程上却极具挑战性。<br />因此，整个索引就变得非常之大，显然，这不是一台服务器的内存能够存下来的。所以，这些索引需要通过分布式的方式存储到不同的服务器上。普通的做法就是根据网页的序号将索引分成很多份（Shards），分别存储在不同的服务器中。每当接受一个查询时，这个查询就被分发到许许多多的服务器中，这些服务器同事能并行处理用户的请求，并把结果送到主服务器进行合并处理，最后将结果返回给用户。<br />因此，需要根据网页的重要性、质量和访问的频率建立常用和非常用等不同级别的搜索。常用的索引需要访问速度快，附加信息多，更新也要快。<br /></li><li><strong>布尔代数非常简单，但是对数学和计算机发展的意义重大，它不仅把逻辑和数学合二为一，而且给了我们一个看待世界的全新视角，开创了今天的数字化的时代。</strong><br /></li><li>比如Google在2013年时整个索引大约有10,000亿个网页，即使更新最频繁的基础索引也有100亿个网页，加入下载一个网页需要一秒钟，那么下载这100亿个网页则需要317年，如果下载10.000亿个网页则需要32,000年左右，是我们人类有文字记载历史的六倍时间。因此，一个商业的网络爬虫需要有成千上万个服务器，并且通过高速网络连接起来。<br /></li><li>显然各个网站最重要的网页应该是它的首页。在最极端的情况下，如果爬虫非常小，只能下载非常有限的网页，那么应该下载的是所有网站的首页，如果把爬虫再扩大些，应该爬下首页直接链接的网页，因为这些网页是网站设计者自认为相当重要的网页。在这个前提下，显示BFS明显优于DFS。事实上在搜索引擎的爬虫黎，虽然不是简单地采用BFS，但是先爬哪个网页，后爬哪个网页的调度程序，原理上基本上是BFS。<br /></li><li>对于某个网站，一般是由特定的一台或者几台服务器专门下载。这些服务器下载完一个网站，然后再进入下一个网站，而不是每个网站先轮流下载5%，然后回过头来下载第二批。这样可以避免握手的次数太多。要是下载完第一个网站再下载第二个，那么这又有点像DFS，虽然下载同一个网站（或者子网站）时，还是需要BFS的。<br /></li><li><strong>在互联网上，如果一个网页被很多其它的网页所链接，说明它收到普遍的承认和信赖，那么它的排名就高。这就是PageRank的核心思想。</strong>当然，Google的PageRank算法实际上要复杂得多。比如说，对来自不同网页的链接区别对待，因为那些排名高的网页的链接更可靠，于是要给这些链接比较大的权重。<br />破解这个怪圈的应该是布林。他把这个问题变成了一个二维矩阵相乘的问题，并用迭代的方法解决了这个问题。他们先假定所有网页的排名是相同的，并且根据这个初始值，算出各个网页的第一次迭代排名，然后，再根据第一次迭代排名算出第二次排名。他们两人从理论上证明了无论初始化值如何选取，这种算法都能保证网页排名的估计值能收敛到排名的真实值。值得一提的是，这种算法不需要任何人工干预。<br /></li><li>网页排名算法的高明之处在于它把整个互联网当做一个整体来看待。这无意中符合了系统论的观点。相比之下，以前的信息检索大多把每一个网页当做独立的个体对待，大部分人当初只注意了网页内容和查询语句的相关性，忽略了网页之间的关系。<br /></li><li><strong>而且决定搜索质量最有用的信息是用户的点击数据。</strong><br />网页排名的计算主要是矩阵相乘，这种计算很容易分解成许多小任务，在多台计算机上并行处理。矩阵相乘的并行化方法会在第29章介绍Google并行计算工具MapReduce时再做讨论。<br /></li><li>今天，Google搜索引擎比最初复杂、完善了许多。但是PageRank在Google所有算法中依然是至关重要的。在学术界，这个算法被工人为是文献检索中最大的贡献之一，并且被很多大学列为信息检索科学（Information Retrieval）的内容。佩奇也因为这个算法在30岁时当选为美国工程院院士，是继乔布斯和盖茨之后又一位当选院士的辍学生。由于PageRank算法收到专利保护，它带来了两个结果。首先，其他搜索引擎开始时都比较遵守游戏规则，不去侵犯它，这对当时还很弱小的Google是一个很好的保护。第二，它使得斯坦福大学拥有了超过1%的Google股票，收益超过10亿美元。<br /></li><li><strong>在信息检索中，使用最多的权重是“逆文本频率指数”（Inverse Document Frequency，缩写为IDF）。</strong><br /></li><li>有限状态机是一个特殊的有向图，它包括一些状态（节点）和连接这些状态的有向弧。<br /></li><li><strong>全球导航的关键算法是计算机科学图论中的动态规划（Dynamic Programming）的算法。</strong><br />在图论中，一个抽象的图包括一些节点和连接他们的弧。如果再考虑每条弧的长度，或者说权重，那么这个图就是加权图（Weighted Graph）。<br />图中的弧的权重对应地图上的距离或者行车时间、过路费等。<br />正确的数学模型可以将一个计算量看似很大的问题的计算复杂度大大降低。<br /></li><li><strong>辛格这种做事情的哲学，即先帮助用户解决80%的问题，再慢慢解决剩下的20%的问题，是在工业界成功的秘诀之一。</strong><br />在Google，辛格一致坚持寻找简单有效的解决方法，因为他奉行简单的哲学。<br />辛格认为，计算机不必学习人的做法，就如同飞机不必要像鸟一样飞行。<br />其次，辛格坚持每天要分析一些搜索结果不好的例子，以掌握第一手的资料，即使在他成为Google Fellow以后，依然如此。这一点，非常值得从事搜索研究的年轻工程师学习。事实上，我发现中国大部分做搜索的工程师在分析不好的结果上花的时间远比功成名就的辛格要少。<br />辛格非常顾虑年轻人要不怕失败，大胆尝试。有一次，一位刚毕业不久的工程师因为把带有错误的程序推到Google的服务器上而惶惶不可终日。辛格安慰她说，你知道，我在Google犯的一次错误是曾经将所有的网页的相关性得分全部变成了零，于是所有搜索的结果全部都是随机的了。后来，这位出过错的工程师为Google开发出了很多好产品。<br /></li><li>幸福的家庭都是相似的，不幸的家庭各有各的不幸。<br /></li><li>需要特别之处的是，删除虚词，不仅可以提高计算速度，对新闻分类的准确性也大有好处，因为虚词的权重其实是一种噪音，干扰分类的正常进行。这一点与通信中过滤掉低频噪音是同样的原理。通过这件事，我们也可以看出自然语言处理和通信的很多道理是相通的。<br /></li><li>在中学学习语文和大学学习英语文学时，老师都会强调这一点，阅读时要特别关注第一段和最后一段，以及每个段落的第一个句子。<br /></li><li>我们希望有一个方法，异常就能把所有新闻相关性计算出来，这个一步到位的方法就是利用矩阵计算中的奇异值分解（Singular Value Decomposition，简称SVD）<br /></li><li>虽然Google早就有了MapReduce等并行计算的工具，但是由于奇异值分解很难拆成不相关子运算，即使在Google内部以前也无法利用并行计算的优势来分解矩阵。直到2007年，Google中国的张智威博士带领几个中国的工程师及实习生实现了奇异值分解的并行算法，这是Google中国对世界的一个贡献。<br /></li><li>双对角矩阵：除了两行对角线元素非零，剩下的都是零。<br /></li><li>奇异值分解的另一个大问题是存储量较大，因为整个举证都需要存在内存里，而利用余弦原理的聚类则不需要。<br /></li><li>任何一段信息（包括文字、语音、视频、图片等），都可以对应一个不太长的随机数，作为区别这段信息和其他信息的指纹（Fingerprint）。只要算法设计得好，任意两段信息的指纹都很难重复，就如同人类的指纹一样。信息指纹在加密、信息压缩和处理中有广泛的应用。<br /></li><li>从加密的角度来讲，梅森旋转算法还不够好，因为它产生的随机数还有一定的相关性，破解了一个就等于破解了一大批。<br /></li><li>在网页搜索中，有时需要判断两个查询用词完全相关（但是次序可能不同），比如“北京 中关村 星巴克” 和 “星巴克 北京 中关村”用词完全相同，更普遍的讲法是判断两个集合是否相关。<strong>判断两个集合是否相同，而完美的计算方法是计算两个集合的指纹，然后进行比较。计算和比较这些特征值的信息指纹即可。</strong><br />我们知道IDF大的词鉴别能力强，因此只需找出每个网页中IDF最大的几个词，并且算出他们的信息指纹即可。如果两个网页这么计算出来的信息指纹相同，则他们基本上是相同的网页。为了允许有一定的容错能力，Google采用了一种特定的信息指纹——相似哈希（Simhash）。上面的算法稍作改进后还可以判断一篇文章是否抄袭了另一篇文章。具体的做法是，将每一篇文章切成小的片段，然后用上述方法挑选这些片段的特征词集合，并计算它们的指纹。只要比较这些指纹，就能找出打断相同的文字，最后根据时间先后，找到原创的和抄袭的。Google实验室利用这个原理做了一个名为CopyCat的项目，可以准确找出原文和转载（拷贝）的文章。<br /></li><li>Google制定了一个很有意思的广告分成策略：虽然所有的视频都可以插入广告，但是广告的收益全部提供给原创的视频，即使广告是插入到拷贝的视频中。这样一来，所有拷贝和上传别人视频的网站就不能获得收入。没有了经济利益，也就少了很多盗版和拷贝。<br /></li><li>相似哈希的特点是，如果两个网页的相似哈希相差越小，这连个网页的相似性就越高。如果两个网页相同，他们的相似哈希必定相同。如果他们只有少数权重的词不相同，其余的都相同，几乎可以肯定他们的相似哈希也会相同。用64为的相似哈希做对比时，如果只相差一两位，那么对应网页内容重复的可能性大于80%。这样，通过记录每个网页的相似哈希，然后判断一个新网页的相似哈希是否已经出现过，可以找到内容重复的网页，就不必重复索引浪费计算机资源了。<br /></li><li>所谓信息指纹，可以简单理解为将一段信息（文字、图片、视频、音频等）随机地映射到一个多维二进制空间中的一个点（一个二进制数字）。只要这个随机函数做得好，那么不同信息对应的这些点就不会重合，因此，这些二进制的数字就成了原来的信息所具有的独一无二的指纹。<br /></li><li>在中途岛海战前，美军截取的日军密电经常出现AF这样一个地名，应该是太平洋的某个岛屿，但是美军无从知道是哪个。于是，美军就逐个发布与自己控制的岛屿有关的假新闻。当发出“中途岛供水系统坏了”这条假新闻之后，美军从截获的日军情报中又看到了含有AF的电文（日军情报内容是AF供水除了问题），于是断定中途岛就是AF。事实证明判断正确，美军在那里成功的伏击了日本联合舰队。<br /></li><li>日军和重庆间谍约定的密码本就是美国著名作家赛珍珠获得1938年诺贝尔文学奖的《大地》（The Good Earth）一书。这本书很容易找到，解密时接到密码电报的人只要拿着这本书就能解开密码。密码所在的页数就是一个非常简单的公式：发报日期的月数加上天数，再加上10,比如3月11日发报，密码就是3+11+10=24页。这样的密码设计违背了我们前面介绍的“加密函数不应该通过几个自变量和函数值就能推出函数本身”的这个原则，对于这样的密码，破译一篇密文就可可能破译以后的全部密文。<br /></li><li>密码机机密时，每次应该自动转一轮，以防同一密码重复使用，因此即使是同一电文，两次发送的密文也应该是不一样。<br /></li><li>搜索引擎的排名：最早期的常见作弊方法是重复关键词。<br />有了网页排名（PageRank）之后，作弊者发现一个网页被引用的链接越多，排名就可能越靠前，于是就有了专门买卖链接的生意。比如，有人自己创建成千上百个网站，这些网站上没有实质的内容，只有链接到其客户的网站链接。<br /></li><li><strong>而在“道”这个层面解决反作弊的问题，就要透过具体的作弊例子，找到作弊的动机和本质。进而从本质上解决问题。</strong><br /></li><li>学过信息论和有信号处理经验的读者可能知道这么一个事实：如果在发动机很吵的汽车里用手机打电话，对方可能听不清；但是如果知道了汽车发动机的频率，可以加上一个与发动机噪音频率相同、振幅相反的信号，很容易消除发动机的噪音，这样，接听人就可以完全听不到汽车的噪音。<br />事实上，完全随机不相关的高斯白噪音是很难消除的。<br /></li><li>SEO：Search Engine Optimizer 搜索引擎优化者（帮助别人作弊的人）。<br /></li><li>今天的搜索引擎对几乎所有的查询都能给出非常多的信息，但问题是这些信息是否完全可信，尤其是当用户问的是一些需要专业人士认真作答的问题，比如医疗方面的问题。随着互联网的规模越来越大，各种不准确的信息也在不断的增加，那么如何才能从众多的信息源中找到最权威的信息，就成了今年来搜索引擎公司面对的难题。<br /><strong>那么权威性是如何度量的呢？为了说明这一点，我们先要引入一个概念------ 提及（Mention）。如果在各种新闻、学术论文或者其他的网络信息页中，讨论到“吸烟危害”的主题时，某两个组织作为信息源被多次提及，那么我们就有理由相信这两个组织是谈论“吸烟危害”这个主题的权威机构。</strong><br />计算网站或网页权威性的另外一个难点在于，权威度与一般网页质量（比如PageRank）不同，它要和搜索主题相关。<br /></li><li>作为数学家和天文学家的托勒密，他有很多发明和贡献，其中任何一项都足以让他在科学史上占有重要的一席之地。托勒密发明了球坐标（我们今天还在用），定义了包括赤道和零度经线在内的经纬线（今天地图就是这么划的），他提出了黄道，还发明了弧度制。<strong>其实，托勒密在天文学上的地位堪比欧几里得之于几何学，牛顿之于物理学。</strong><br />托勒密集成了毕达哥拉斯的一些思想，他也认为圆是最完美的几何图形，因此他用圆来描述行星运行的规律。<br />根据托勒密的计算，制定了儒略历，即每年365天，每4年增加一个闰年，多一天。1500年来，人们根据他的计算决定农时。但是，经过1500年，托勒密对太阳运动的累计误差，还是多出了10天。1582年，教皇格里高利十三世在儒略历的基础上删除了10天，然后将每一个实际最后一年的闰年改成平年，然后每400年再回插回一个闰年，这就是我们今天用的日历，这个日历几乎没有误差。为了纪念格里高利十三世，<strong>现在的日历也叫格里高利日历。</strong><br /></li><li>波兰天文学家哥白尼发现，如果以太阳为中心来描述行星的运行，只需要8-10个圆，就能计算出一个行星的运行轨迹，他因此提出了日心说。很遗憾的是，哥白尼正确的假设并没有得到比托勒密更好的结果，相比托勒密的模型，他的模型误差要大得多。<br />而哥白尼日心说的不准确性，也是教会和当时的人们认为哥白尼的学说是邪说的一个重要的原因，所以日心说要想让人心服口服地接受，就要更准确的描述行星的运动。<br />开普勒很幸运地发现了行星围绕太阳运转的轨道实际上是椭圆形的，这样不需要用多个小圆套大圆，而只要用一个椭圆就能将星体运动规律描述清楚了。<br /><img src="/img/数学之美1.png" title="图一 开普勒三大定律" /><br /></li><li>一个正确的数学模型应当在形式上是简单的<br />一个正确的模型一开始可能还不如一个精雕细琢的错误模型来的准确，但是，如果我们认定大的方向是对的，就应该坚持下去（日心说一开始并没有地心说准确）<br />正确的模型也可能受噪音的干扰，而显得不准确；这是不应该用一种凑合的修正方法加以弥补，而是要找到噪音的根源，这也许能通往重大的发现。<br /></li><li><strong>在过去20年里，在机器学习和自然语言处理领域，80%的成果来自于数据量的增加。</strong><br /></li><li>马库斯的主张一贯是建立几个世界上最好的专业，而不是专业最齐全的系。我觉得，当今中国的大学，最需要的就是马库斯这样卓有远见的管理者。<br /></li><li>马尔可夫链（Markov chain），他描述了一种状态序列，其每个状态值取决于前面有限个状态。<br /></li><li>所有这些（因果）关系，都可以有一个量化的可信度（Belief），即用一个概率描述。也就是说，贝叶斯网络的弧上可以有附加的权重。马尔可夫假设保证了贝叶斯网络便于计算。我们可以通过这样一张网络估算出一个人患心血管疾病的可能性。<br />可以讲，马尔可夫链是贝叶斯网络的特例，而贝叶斯网络是马尔可夫链的推广。<br />使用贝叶斯网络必须先确定这个网络的拓扑结构，然后还要知道各个状态之间的概率，得到拓扑结构和这些参数的过程分别就叫做结构训练和参数训练，统称训练。<br />一个防止陷入局部最优的方法，就是采用蒙特卡洛（Monte Carlo）的方法，用许多随机数在贝叶斯网络中试一试，看看是否陷入局部最优。这个方法的计算量比较大。最近，新的方法是利用信息论，计算节点之间两两的互信息，只保留互信息较大的节点直接的连接，然后再对简化了的网络进行完备的搜索，找到全局优化的结构。<br /></li><li>它的特殊性在于，变量之间要遵守马尔可夫假设，即每个状态的转移概率只取决于相邻的状态，这一点，它和我们前面介绍的另一种概率图模型 —— 贝叶斯网络相同。而它们的不同之处在于，条件场是无向图，而贝叶斯网络是有向图。<br /></li><li>维特比算法是一个特殊但应用最广的动态规划算法。<br />汉语中每个无声调的拼音对应13个左右的国标汉字。<br /></li><li>移动通信使用过两种技术：频分多址（FDMA）和时分多址（TDMA）<br />CDMA：码分多址。由于这种方法是根据不同的密码区分别发送的，因此称为码分多址。<br /></li><li>这样同一类中各个点到中心的平均距离d较近，而不同类中心之间的平均距离D较远。我们希望的迭代过程是每一次迭代时，d比以前变小，而D变大。<br /></li><li>首先，根据现有的模型，计算各个观测数据输入到模型中的计算结果，这个过程称为期望值计算过程（Expectation），或E过程；接下来，重新计算模型参数，以最大化期望值。在上面的例子中，我们最大化D和-d，这个过程称为最大化的过程（Maximization），或M过程。这一类的算法都称为EM算法。<br />EM算法只需要有一些训练数据，定义一个最大化函数，剩下的事情就交给计算机了。经过若干次的迭代，我们需要的模型就训练好了。这实在是太美妙了，这也许是造物主可以安排的。所以我们把它称为上帝的算法。<br /></li><li>单位搜索量带来的收入一般以千次搜索量带来的收入来衡量，称为RPM。<br /></li><li>广告的点击量与展示的位置有关，放在第一条的广告的点击率理所当然比第二条的点击率要搞很多。因此，在预估点击率时，必须消除这个噪音。<br /></li><li>云计算技术设计的面很广，从存储、计算、资源的调度到权限的管理等。云计算的关键之一是，如何把一个非常大的计算问题，自动分解到许多计算能力不是很强的计算机上，共同完成。<br />这就是MapReduce的根本原理，将一个大任务拆分成小的子任务，并且完成子任务的计算，这个过程叫做Map，将中间结果合并成最终的结果，这个过程叫做Reduce。<br />我们现在发现，Google颇为神秘的云计算中最重要的MapReduce工具，其实原理就是计算机算法中常用的“各个击破”法，<strong>它的原理原来这么简单 —— 将复杂的大问题分解成很多小问题分别求解，然后再把小问题的解合并成原始问题的解。</strong><br /></li><li>而人工神经网络也具有节点，只是它使用了一个新的名词 —— 神经元。而它的有向弧则被看成是连接神经元的神经。<br /></li><li>我们有了训练数据，定义了一个成本函数C，然后按照梯度下降法找到让成本达到最小值的那组参数。这样，人工神经网络的训练就完成了。不过，在实际应用中，我们常常无法获得大量标注好的数据，因此大多数时候，我们不得不通过无监督的训练得到人工神经网络的参数。<br /></li><li>设计这样一个成本函数，本身又是一个难题，使用人工神经网络的研究人员需要根据具体的应用来寻找合适的函数。不过总体来讲，成本函数总要遵循这样一个原则：既然人工神经网络解决的是分类的问题，那么我们希望分完类之后，同一类样本（训练数据）应该相互比较靠近，而不同类的样本应该尽可能的远离。比如前面提到的多维空间里的模式分类问题，就可以把每一个样本点到训练出来的聚类中心（Centroid）的欧几里得距离的均值作为成本函数。对已估计语言模型的条件概率，就可以用熵作为成本函数。定义了成本函数后，就可以用梯度下降法进行无监督的参数训练了。<br /></li><li>贝叶斯网络更容易考虑（上下文）前后的相关性，因此可以解码一个输入的序列，比如将一段语音识别成文字，或者将一个英语句子翻译成中文。而人工神经网络的输出相对孤立，它可以识别一个个字，但是很难处理一个序列，因此它主要的应用常常是估计一个概率模型的参数，比如语音识别中的声学模型参数的训练、机器翻译中语言模型的训练，等等，而不是作为解码器。<br /></li><li>而从20世纪90年代至今，计算能力的提升一半是靠处理器性能的提升，另一半则是靠很多处理器并行工作体现出来的，因此过去训练人工神经网络的方法就必须改变，以适应云计算的要求。Google大脑就是在这样的前提下诞生的，期创新之处也在于利用了云计算的并行处理技术。<br /></li><li>人工神经网络是一个形式上非常简单但分类功能强大的机器学习工具，从中可以再次体会到数学中的简单之美。在现实生活中，真正能够通用的工具在形式上必定是简单的。<br /></li><li>与模型一样，数据也十分重要，但是人们在很长时间里却低估了数据的作用。<br />在中国的远古传说中，有伏羲演八卦的故事，伏羲是中国上古的三皇之一，比炎、黄二帝还要早得多。<br /></li><li>而做实验的目的就是采集数据，因为科学发明需要通过这些数据来推导或者证实。<br />因此，如果一个散户投资人能真正做到“用数据说话”，只需奉行一条投资决策，那就是买指数基金。这当然不是我的发明，而是投资领域注明的经济学家威廉•夏普（William F.Sharpe）和伯顿•麦基尔（Burton G.Malkiel）等人一直倡导的。<br />即统计样本数量不充分，则统计数字毫无意义。至于需要多少数据来统计结果才是准确的，这就需要进行定量分析了。<br /></li><li>让我们先看看有关网页搜索领域的竞争。在大多数人看来，Google的搜索比微软的Bing（在质量上）做得略好一点，是因为Google的算法好。这种看法在2010年以前当然是对的，因为那时Bing搜索在技术和工程方面确实明显落后于Google。但是今天这两家公司在技术上已经相差无几了，Google还能稍稍占优，除了产品设计略微好一些外，很多程度上靠的是数据的力量。<br /></li><li><strong>在搜索用到的诸多种数据中，最重要的数据有两类，即网页本身的数据和用户点击的数据。点击模型贡献了今天搜索排序至少60%--80%的权重。</strong><br />因此，一些公司就想出更激进的办法，如通过搜索条（Toolbar）、浏览器甚至是输入法来手机用户的点击行为。这些做法的好处在于不仅可以手机到用户使用该公司的搜索引擎本身的点击数据，而且还可能手机用户使用其它搜索引擎的数据。比如微软通过IE浏览器，手机用户使用Google搜索是的点击情况。这样一来，如果一家公司能够在浏览器市场占很大的份额，即使它的搜索量很小，也能收集大量的数据。有了这些数据，尤其是有了用户在更好的搜索引擎上的点击数据，一个搜索引擎公司便可以快速改进长尾搜索的质量。当然，有人诟病Bing的这种做法是“抄”Google的搜索结果，其实并没有直接抄，而是借用Google的结果改进自己的点击模型。这在中国市场上也是一样，因此，搜索质量的竞争就转换成了浏览器或者其它客户端软件市场占有率的竞争。<br /></li><li>大数据的好处远不只是成本和准确的问题，它的优势还在于多维度（或叫全方位）。<br />我之所以举医疗行业的例子，是因为除了IT行业，医疗保健是对大数据最热衷的行业。<br /></li><li>高德纳的贡献在于找到了一种方法，使得一个算法好坏的度量和问题的大小不再相关。</li></ul><p><img src="/img/数学之美.png" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《80/20法则》读后感</title>
      <link href="/2016/05/10/%E3%80%8A8020%E6%B3%95%E5%88%99%E3%80%8B%E8%AF%BB%E5%90%8E%E6%84%9F/"/>
      <url>/2016/05/10/%E3%80%8A8020%E6%B3%95%E5%88%99%E3%80%8B%E8%AF%BB%E5%90%8E%E6%84%9F/</url>
      
        <content type="html"><![CDATA[<p>全书看完，受到的启发是，关注20%有价值的事情，只做最重要的事情！<br />根据80/20（帕累托法则）的思维，阐述了一种反常规的思想：我们所说的一分耕耘一分收获并不存在，我们收获的80%往往是耕耘的20%所产生。所以我们的当务之急应该是找出那创造了80%价值的20%投入，这样才能达到事半功倍的效果。<br /><img src="/img/8020-1.png" /><br /><span id="more"></span><br />作者结合80/20的观点，对在企业发展、时间管理、个人生活等方面提出了自己的见解。<br />80/20原则，最早是在质量管理中得到使用。根据工厂的统计发现，80%的次品是因为20%的主要缺陷引起的，从而，优先处理这20%的缺陷，就能大幅度的提高产品的质量，降低次品率。<br />从这引申开，现实生活中，有80%的利润是产生于20%的产品或者客户。这就需要我们思考，我们如何通过优化资源的配置，从而提高公司的竞争力。作者认为，重视和挖掘那些创造了80%利润的客户和产品，忽略和放弃那些低价值的客户和产品。<br />但是，话说回来，那低效的80%产品可能利润不高，但是作为顾客来说，可能在购买20%高利润产品的时候，需要也顺便一起购买80%里面的某些产品。如果没有这些80%，顾客需要分两个地方购买，会不会导致客户的流失？所以，有时候也需要考虑方便客户，站在满足客户的角度上来做出取舍，而不仅仅是仅仅考虑价值的问题。<br />同时，作者也认为，对于忽略和放弃的80%，也需要进行战略分析。我们需要区别对待，通过对市场吸引力、所占位置、收益等进行分析，哪些是新的市场、哪些是朝阳产业、哪些是早期投入等等，来作为取舍的判断。<br />或者，在取舍的时候，也可以考虑通过降低成本的方式，提高低价值用户的利润率。一般情况下，更好的处理方法是将80%的小客户集中起来，为他们提供一套电话销售和订货服务系统。与直接销售相比，这样做既高效，成本又低。<br />80/20的原则，让我们聚焦于那关键的20%，而聚焦到20%，就能简化公司的管理。目前流行观点认为，因为规模化效应，规模化之后，边际成本就会降低、利润增加。但是作者提出，规模增加到某个节点会因为复杂度的增加，导致成本的增加超过了利润的增加，从而降低了公司的利润，规模性效用被复杂性所产生的不可见隐形成本给颠覆了。<br />简单即为美。公司要保持简单，除了控制规模之外，非核心竞争力、非优势的产品和服务进行外包也是一种降低复杂度的一种方法。不过话说回来，作者也同意，大而简单的企业最好，只是需要我们控制复杂度的前提下扩大规模。<br /><img src="/img/8020-2.png" /><br />从时间管理上来说，并不是所有的事情都同等重要，所以我们关键是要学会找出最有价值的20%事情，然后集中资源优先去处理。<br />另外，关于时间管理，我们通常认为，计划的时候一般要充分考虑每个节点的时间，然后排出一个可行的时间计划。但是，你有没有想过，通过设立一个不可能的时间计划表，来促使你只做最有价值的20%，这样，就能通过20%的资源创造了80%的价值。类似，在谈判中，80%的让步也是在最后的20%时间里面做出的，因此，你最重要的诉求点，最好在谈判的最后提出，这样，迫于时间的压力，对方更容易做出让步。<br />引申到个人生活，埋头苦干已经不适合现在的社会了。你努力工作的回报很低，反而独到的思想和做自己想做的事才能为我们带来高额回报。在做事情前，需要利用80/20的思维，考虑哪些事情是值得做的，哪些事情是最有价值的。同时，思考你的方法是否突破了传统，是否提高了效率。这样，才能让你有限的精力投入到最有价值的部分。让你的生活、工作、学习三不误。<br />【一些有哲理的话】<br />1、爱尔兰戏剧家萧伯纳说过：“理性的人改变自己以适应这个世界，非理性的人则坚持改变这个世界来适应个人需求，因此，所有的进步都取决于这些非理性的人。”<br />2、一个数据库，不管它内含数据多么丰富，都称不上信息，它充其量只是信息的原料……一家企业最需要的信息以初始化、有序形式呈现出来时才是可用的。一家企业在做决策特别是战略性决策时，最需要的是企业外部的信息。结果、机遇和威胁只存在于企业外部。<br />3、总部存在的问题并不是开销，而在于它免除了那些一线工作人员的责任，剥夺了他们的主动权。撤销总部后，公司可以专注于客户需求，而不再去理会由管理层引发的问题。<br />在撤销总部前，不同的业务领域受到总部关注和干预的程度有所不同。能自主经营、不受总部“干扰”的产品和服务恰恰盈利最多。这就是为什么在实践中运用80/20利润分析法时，经营者通常会吃惊地发现大多数被遗忘的领域正是最赚钱的领域。这并非偶然现象。（但是，运用80/20分析法有时也会有副作用，那就是盈利最多的部分可能会得到高层管理者更多的关注。结果，这一部分的利润率就会有所下降。）<br />个人注：其实受总部关注最多的恰恰是因为该部分问题较多，因为总部需要处理问题、协调资源。没问题、顺利的部分，总部也不需要花太多精力去关注。<br />4、1909年时他就指出，他的任务是“普及汽车应用”。在那时，这一目标还很可笑——只有富人才买得起车。但是，批量生产、成本仅占早期汽车成本小部分的T型汽车实现了这一目标。不管这是好事还是坏事，总体来看好处要远多于坏处，我们享受到了“福特主义者”提供的“丰饶之角”。<br />5、仅培训那些你确信在未来数年里不打算跳槽的人。 让那些顶级销售人员培训他们，根据受训人员之后的表现来表彰和奖励这些销售明星。 加大对第一轮培训后表现最突出的员工的培训力度。挑出表现最优秀的20%受训员工，并将80%的培训力量用在他们身上。不再对排名后50%的员工进行培训，除非通过培训确实能收到奇效。<br />6、这世上只有四种军官。第一种又懒又笨，不必理会这种人，他们不会成为大害……第二种则是勤奋工作的聪明人，他们有成为优秀军官的潜质，考虑问题非常周全。第三种是勤奋工作的笨人，这些人是威胁，要立即开除，他们会给别人添麻烦。最后一种则既聪明又懒惰，这类人适合做最高职位。 冯·曼施泰因，《论德国军官》<br />7、凡有的，还要加给他，叫他多余；凡没有的，连他所有的，也要夺去。 《马太福音》<br />8、亚里士多德曾经说过幸福是人类一切活动的终极目标。<br />9、如果必须在成功和幸福间做出选择，最好选择幸福。<br />10、医疗保健问题就像大多数问题领域一样，预防总胜过治疗，预防总比治疗更省钱；在初期阶段就控制住疾病的发展总比到了晚期再采取措施管用得多；培养青少年养成坚持一生的良好生活习惯要胜过任何形式的公共卫生投资。 真正花大力气在各级学校开展健康教育无疑是明智之举，而且要一直推动此类健康教育的发展，直到“临界点”真正出现，学生的生活习惯实现彻底转变。<br />11、1798年，托马斯·马尔萨斯，一位古怪的英国传教士在他的《人口论》中不无担忧地指出，“人口的增长比食物供应的增长要快”。马尔萨斯对人口增长的估计很正确，但是他不曾想到农业生产率的增长也会如此之快。 <img src="/img/8020-3.png" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 管理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《软件系统架构使用视点与利益相关者合作》读后感</title>
      <link href="/2016/05/09/%E3%80%8A%E8%BD%AF%E4%BB%B6%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E4%BD%BF%E7%94%A8%E8%A7%86%E7%82%B9%E4%B8%8E%E5%88%A9%E7%9B%8A%E7%9B%B8%E5%85%B3%E8%80%85%E5%90%88%E4%BD%9C%E3%80%8B%E8%AF%BB%E5%90%8E%E6%84%9F/"/>
      <url>/2016/05/09/%E3%80%8A%E8%BD%AF%E4%BB%B6%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E4%BD%BF%E7%94%A8%E8%A7%86%E7%82%B9%E4%B8%8E%E5%88%A9%E7%9B%8A%E7%9B%B8%E5%85%B3%E8%80%85%E5%90%88%E4%BD%9C%E3%80%8B%E8%AF%BB%E5%90%8E%E6%84%9F/</url>
      
        <content type="html"><![CDATA[<p>参考资料：<strong>架构设计中常用的视图模型</strong>----<a href="https://www.ibm.com/developerworks/cn/rational/r-4p1-view/">架构蓝图--软件架构 "4+1" 视图模型</a><br />一本大杂烩，涵盖了软件架构方方面面的知识，包括宏观的、微观的。所以看的过程觉得特别混乱。<br />不过如果在实践软件架构过程中，还是一本好的参考书，在架构处理的过程中的各种情况，作者都有涉猎。<br />另外，就是本书的翻译有点拗口，很多都是按英语的语序直接翻译过来，有些术语的翻译也不是很准确，建议对于一些术语的翻译，最好同时还保留英文，方便读者的理解。<br />本书的读书笔记如下。</p><h1 id="一架构的总体原理">一、架构的总体原理</h1><h2 id="总图">1、总图</h2><p><img src="/img/软件架构1.png" /> <span id="more"></span></p><h2 id="架构的承上启下作用">2、架构的承上启下作用</h2><p><img src="/img/软件架构2.png" /></p><h2 id="架构各元素的关系">3、架构各元素的关系</h2><p><img src="/img/软件架构3.png" /></p><h2 id="视图视角的分组及关系">4、视图、视角的分组及关系</h2><p><img src="/img/软件架构4.png" /></p><h2 id="视角在各视图的应用">5、视角在各视图的应用</h2><p><img src="/img/软件架构5.png" /></p><h1 id="二视点及视图">二、视点及视图</h1><h2 id="情景视图">1、情景视图</h2><p><img src="/img/软件架构6.png" /></p><h2 id="功能视图">2、功能视图</h2><p><img src="/img/软件架构7.png" /></p><h2 id="信息视图">3、信息视图</h2><p><img src="/img/软件架构81.png" /> <img src="/img/软件架构82.png" /> <img src="/img/软件架构83.png" /></p><h2 id="并发视图">4、并发视图</h2><p><img src="/img/软件架构9.png" /></p><h2 id="开发视图">5、开发视图</h2><p><img src="/img/软件架构10.png" /></p><h2 id="部署视图">6、部署视图</h2><p><img src="/img/软件架构11.png" /> <img src="/img/软件架构121.png" /> <img src="/img/软件架构122.png" /></p><h2 id="运维视图">7、运维视图</h2><p>无。</p><h1 id="三视角">三、视角</h1><h2 id="安全性">1、安全性</h2><p><img src="/img/软件架构131.png" /> <img src="/img/软件架构132.png" /> <img src="/img/软件架构133.png" /></p><h2 id="性能和可伸缩性">2、性能和可伸缩性</h2><p><img src="/img/软件架构14.png" /></p><h2 id="可用性和弹性">3、可用性和弹性</h2><p><img src="/img/软件架构151.png" /> <img src="/img/软件架构152.png" /></p><h2 id="演进">4、演进</h2><p>无。</p><h2 id="其它">5、其它</h2><p>位置、开发资源、国际化、法规、易用性等。 <img src="/img/软件架构16.png" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 架构 </tag>
            
            <tag> 视点 </tag>
            
            <tag> 视角 </tag>
            
            <tag> 视图 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《从0到1》读后感</title>
      <link href="/2016/04/20/%E3%80%8A%E4%BB%8E0%E5%88%B01%E3%80%8B%E8%AF%BB%E5%90%8E%E6%84%9F/"/>
      <url>/2016/04/20/%E3%80%8A%E4%BB%8E0%E5%88%B01%E3%80%8B%E8%AF%BB%E5%90%8E%E6%84%9F/</url>
      
        <content type="html"><![CDATA[<p>从0到1，从无到有，是质的变化，是推动人类进步的主要力量，也是一个公司能够拥有垄断利润的关键。每个企业都必须时刻思考如果抓住0到1的机会，而不是挤入从1到n的那种完全竞争状态的市场。<br />从0到1，是一种跨越性的创新，应对了中国那句古话，“道生一，一生二，二生三，三生万物”。道生一，就是从无到有的跳跃。从0到1，所以从奇点形成了我们的宇宙；从0到1，所以有了现代工业革命；从0到1，所以有了现代的信息技术。<br />从0到1是一种思维方式，一种追求。但是如何能够让自己发现0到1的机会，这才是成功者和普通人的区别。作者强调了要敢于挑战、独具一格的性格，要在别人意想不到的地方寻找利基市场，再利用自己技术壁垒，扩大市场，并形成垄断性的利润。那种通过技术的创新改变人类生活的垄断，是有助于社会的进步，因为他们通过解决独一无二的问题取得了创造性垄断的地位。像作者说的，每个企业都在追求垄断性的利润，完全竞争会吞噬企业的利润，使企业只着眼于目前的利益。所以，每个企业要思考未来10、20的世界的样子，然后思考企业能利用到哪个机会，从而抢先一步，占领市场，并且必须使自己的产品或者技术是替代品10倍优势，来保证你的竞争优势。<br />作者在文中也提到了Paypal黑帮的个人背景和性格，这几个人后来都成了硅谷的著名的创业者或投资人（例如火箭发射回收、特斯拉电动汽车的创始人马斯克Musk）。他们那种敢于挑战、独立思考、敢于和别人不一样的风格是他们创业成功的必要素质，而反思中国的教育，却是希望把大家培养成一个模子的，人云亦云的考试机器。<br />作者也思考了未来社会智能机器人的竞争问题，作者认为，人类之间的竞争，是涉及一种资源竞争的零和竞争。而机器人不需要和人类竞争资源，所以机器人应该是人类的好帮手，是人类提升生活水平的工具。当然，当超人工智能的出现，人类的将来是怎么样，其实谁也说不清楚。<br />名句摘录：<br />1、尼采曾在精神错乱前写道：“个人发生精神错乱很少见，但对群体、政党、国家、时代而言，精神错乱却很普遍。”<br /><span id="more"></span> 2、但是音乐在播放，我们没法责备那些随音乐舞动的人。<br />3、托尔斯泰在《安娜·卡列尼娜》中以下面这段文字作为开头：“幸福的家庭总是相似的，不幸的家庭各有各的不幸。”<br />4、国际象棋大师卡帕布兰卡说过：“要想赢，首要工作就是研究残局。”<br />5、唯一的问题是怎样坦然接受我们的悲剧命运。<br />6、今天，社会中流传着以下两点看法：死亡不可避免，而且随机发生。<br />7、风险投资基金通常要10年之后才能退出，因为成功的公司需要时间成长。<br />8、如果你能解释为什么公司使命激动人心，那么你就能吸引你需要的员工。不是解释工作的重要性，而是解释为什么你在做别人从未想过要做的重要事情。这是唯一能让你的理由变得独特的方法。</p><p><img src="/img/从0到1.png" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 管理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《机器学习实战》读后感及笔记</title>
      <link href="/2016/04/16/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%E3%80%8B%E8%AF%BB%E5%90%8E%E6%84%9F%E5%8F%8A%E7%AC%94%E8%AE%B0/"/>
      <url>/2016/04/16/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%E3%80%8B%E8%AF%BB%E5%90%8E%E6%84%9F%E5%8F%8A%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<p>一本关于机器学习的实用教程，涵盖了从原理、案例到程序实现（Python）的全面介绍。<br />机器学习就是从数据中发现数据背后的本质，捕获数据中有价值的信息和模式。<br />机器学习的核心就是算法，而算法的基础就是一序列的数学知识，包括统计、概率、矩阵等方面的知识。机器学习除了能模拟专家做出一些有目标的预测之外，还能根据数据获取背后的分类和关联关系，并做出预测。<br />在这个大数据的时代，数据无处不在，如何利用好这些数据、如何发挥这些数据的价值，是人类的挑战，也是每个人的机会。<br />工业革命使机器成为了人类的另一双手，让人类从繁重的体力劳动转向脑力劳动；而信息技术，特别是人工智能（机器学习）将成为人类的另一个大脑，促进人类从目前的脑力劳动升级到更有创造力的活动中去，造福人类的生活。<br />在以后的年代，因为机器学习的基础就是统计，统计学将成为一门最热门的科学。<br />这本书烧了好多脑，从第一遍的一知半解，再到第二遍的难点突破和脑图笔记分解，到第三遍的框架理解和梳理。<br />也不说那么多了，经过整理的框架如下，具体的内容在脑图文件中可以展开。<br />如果想深入学习机器学习的算法的话，这本书还是一个不错的推荐。</p><p><img src="/img/机器学习.png" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《无声的告白》读后感</title>
      <link href="/2016/04/15/%E3%80%8A%E6%97%A0%E5%A3%B0%E7%9A%84%E5%91%8A%E7%99%BD%E3%80%8B%E8%AF%BB%E5%90%8E%E6%84%9F/"/>
      <url>/2016/04/15/%E3%80%8A%E6%97%A0%E5%A3%B0%E7%9A%84%E5%91%8A%E7%99%BD%E3%80%8B%E8%AF%BB%E5%90%8E%E6%84%9F/</url>
      
        <content type="html"><![CDATA[<p>一个幸福美满的家庭，随着女儿莉迪亚的死亡，轰然倒塌。<br />事情真相，像洋葱一样，一层一层的剥开， 死亡的真相渐渐明朗。<br />一个建立在互不沟通基础上的幸福，像五彩肥皂泡一样，脆弱得承受不了一根稻草的压力。<br />悲剧的根源，有社会的、家庭的、个人的，这一切交错在一起，导演了一出令人惊讶的故事。<br />书中有一句话，“一个来自混血家庭背景的孩子，通常难以找到自己的定位”，一句简单的话语，把美国种族问题现状尖锐的抛了出来。一个混血的孩子，特别是一个华人特征的混血孩子，在美国这种以白人为主的社会里，总是显得格外的与众不同。与众不同的结果就是处处难以融入你身边的社会。不管莉迪亚的爸爸多么鼓励她要学会交朋友、要学会融入集体，但是，这些反而给莉迪亚带来了更大的压力，反而让莉迪亚通过表面的、欺骗性交友的应付她的父亲，为以后悲剧埋下了伏笔。上帝造了人，有不同的色彩，同时也有不同的性格，所以作为父母，需要尊重孩子的特性，接受孩子的不一样，让孩子按照她的想法去生活。<br />莉迪亚母亲玛丽琳的壮志未酬、与众不同的思想，潜移默化的附加到了莉迪亚身上。又因为玛丽琳出走事件的影响，把莉迪亚置于一种一定要完成母亲心愿的高度压力之下。母亲的压力、学习的退步，把自尊心极强的莉迪亚整个信心给摧毁。而玛丽琳就像鸵鸟一样，自欺欺人，看不到或者说不愿意看到真相，生活在一个自己编造的幻觉当中，母女俩维护着一种自欺欺人的平衡当中。<br />而哥哥内斯要到哈佛上学的事件，把莉迪亚唯一能说说心里话，或者说，在这个家庭中唯一能理解自己的人，又要离开了。这把莉迪亚进一步推到了崩溃的边缘。杰克和哥哥内斯的关系成了压垮莉迪亚的最后一根稻草，成了莉迪亚独自走向湖中，独自走完了人生最后一程的推手……<br />这一切，只有那个不起眼的妹妹汉娜，在默默的关注着这一切，也只有她清楚这些故事背后的真相。 <img src="/img/无声的告白.png" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 生活 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《岛上书店》读后感</title>
      <link href="/2016/04/07/%E3%80%8A%E5%B2%9B%E4%B8%8A%E4%B9%A6%E5%BA%97%E3%80%8B%E8%AF%BB%E5%90%8E%E6%84%9F/"/>
      <url>/2016/04/07/%E3%80%8A%E5%B2%9B%E4%B8%8A%E4%B9%A6%E5%BA%97%E3%80%8B%E8%AF%BB%E5%90%8E%E6%84%9F/</url>
      
        <content type="html"><![CDATA[<p>这本书每章都以书店老板A.J.一篇短小书评来开篇。<br />全书用一种质朴、平和的文字，描写与世隔绝的艾丽丝岛上书店老板A.J.平凡又不平常的一生。<br />想要了解一个人，你只需要问一个问题：“你最喜欢哪一本书？”<br />没有谁是一座孤岛。我们不是我们所收集的、得到的、所读的东西，只要我们还活着，我们就是爱，我们所爱的事物，们所爱的人，我认为真的会存活下去。<br />这就是A.J.给我们传递的关于阅读和爱的真谛。<br />因为阅读，他找到了和他有共同阅读兴趣的出版社业务员艾米。<br />因为爱，A.J.收养了孤儿玛雅，像作者所说的：“一旦一个人开始在乎一件事，就不得不在乎一切事。”因为爱，A.J.把中年丧妻之后的消沉抛之脑后，努力使自己的人生变得美好而辽阔。<br />虽然后来因为癌症而离开人世，但他的爱在他深爱的两个女人身上得到了延续，他的阅读兴趣在深爱的两个女人那里得到了传承。<br />本书开篇从艾米代替上任业务员到小岛给书店老板A.J.推销图书开始，到艾米下任到岛上给下任老板兰比亚斯推销图书结束。这些，就像阅读和爱，不断的在你所爱的人中循环着，无始无终。<br />唯爱与阅读，不可辜负。 <img src="/img/岛上书店.png" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 生活 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《乌合之众》读后感</title>
      <link href="/2016/03/29/%E3%80%8A%E4%B9%8C%E5%90%88%E4%B9%8B%E4%BC%97%E3%80%8B%E8%AF%BB%E5%90%8E%E6%84%9F/"/>
      <url>/2016/03/29/%E3%80%8A%E4%B9%8C%E5%90%88%E4%B9%8B%E4%BC%97%E3%80%8B%E8%AF%BB%E5%90%8E%E6%84%9F/</url>
      
        <content type="html"><![CDATA[<p>《乌合之众》，作者勒庞，群体心理学的创始人，这本书也是社会心理学的奠基之作。<br />懂得群体心理学，就像拥有一道强光，照亮了许多历史现象和经济现象，没有它，那些现象就很难看清。<br />作者围绕着群体心理学，思考了群体中人的行为特点进行了讨论。<br />作者认为，人在群体中会失去独立意识，处于无意识的状态，因为感觉到集体力量的强大、有法不责众的心态、在群体中情绪受到暗示和传染，激情燃烧、感情强烈而极端。一个人独处时，他可能是一个有教养的人，但是一旦加入某个群体，就便成了野蛮人，凶残、易怒、充满暴力。<br />回想一下，在中国发生的很多事情，都印证了作者的这个观点，例如：文化大革命就是这么一个典型的例子，反动派、革命派、红卫兵都是一个群体的活生生的体现。<br />群体表现出来的感情，不管好坏，都有简单化和极端化这双重特点。他们可以杀人放火、无恶不作，也可以表现得无比忠诚、勇于牺牲、无私无畏，他们会很容易受到荣誉、名誉、国家或者宗教的名字而赴汤蹈火，牺牲性命。<br />群体的犯罪是在强烈的暗示下进行的，从法律上说是有罪的，但确实不是一般意义上的罪行。<br />很多伟大的领袖都能够利用群体的特点，来达到目的。<br />而信仰的力量永远是最强大的，罗马帝国维持其统治，靠的不是武力，而是靠它激起的虔诚的信仰。通过宗教形式，让信仰在人们心中确立，又通过宗教形式，使得信仰免受讨论。所以伟大领袖的重要作用就是创造信仰，不管是政治信仰、宗教信仰、社会信仰，还是一个主张、一个人物或者一个作品。<br />通过言语的艺术，领导者通过断言、重复、感染，暗示给群体的观念，通过绝对化、简单化的形式，把领导者的思想灌输到了群体当中。<br />人类的进化，几万年来，科学技术得到飞速发展，例如人类已经能够发明计算机来打败人类围棋高手。但是，人类的智慧，现代人和古人，反而没有太多的差异。甚至，古人因为经历的事情更多、受到的苦难更深，在做人境界、智慧上比现代人更胜一筹。所以说群体的心里学、群体的弱点，在现在社会也有同样的价值。<br />其它好观点：<br />1、制度和法律是人们内心精神的体现形式，反映了人们内心精神的需求。既然是人们内心精神诞生了制度和法律，制度和法律就无法改变它。<br />2、从观察中得到的结论往往是不成熟的，我们看得见的现象背后，还有看不清楚的东西。甚至在这些东西背后，还有看不见的东西<br />3、陪审团容易受感情因素影响，但这也侧面缓冲了法律的严厉性。如果是法官必须按照法律行事，而现实上往往是法外有情的。<br />所以最好的律师就首要的是影响陪审员的感情。 <img src="/img/乌合之众.png" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 哲学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《人工智能的未来》读后感</title>
      <link href="/2016/03/25/%E3%80%8A%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%9A%84%E6%9C%AA%E6%9D%A5%E3%80%8B%E8%AF%BB%E5%90%8E%E6%84%9F/"/>
      <url>/2016/03/25/%E3%80%8A%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%9A%84%E6%9C%AA%E6%9D%A5%E3%80%8B%E8%AF%BB%E5%90%8E%E6%84%9F/</url>
      
        <content type="html"><![CDATA[<p>趁着AlphaGO掀起的热潮，这周看完了《人工智能的未来》，一本谈论人工智能关于计算机技术原理、神经学、哲学的书籍。<br />关于人工智能的定义，技术上和哲学上都颇具争议。<br />图灵测试提供了一种技术的、可衡量的手段；但在哲学上，人工智能永远回避不了关于意识或自由意志的问题。<br />关于自由意志，叔本华提出：“你可以做你想做的，但在生活中任何给定的时刻，你只能想做一件确定的事情，除此之外，绝对没有任何其它事情。”这种决定论的思想，和我们认为我们可以选择我所爱、做我所选大相径庭。<br />而作者认为，当机器说出它们的感受和感知经验，而我们相信它们所说的是真的时，它们就真正成了有意识的人。<br />作者通过思维模式识别理论、隐马尔可夫层级模型、遗传算法等人工智能技术，阐述了人工职能领域的进展，同时基于信息科技遵循指数增长的规律，提出了加速回报定律，乐观预计智能机器人在未来几十年内会出现。<br />从最初的人工耳蜗、人工眼球到人工大脑的扩展，非生物系统的引入（特别是人工大脑技术），是否会产生另外的我，而我们大部分思想（甚至全部）存在云端，是否就可以得到“永生”。<br />数学家斯坦·乌拉姆说过：“技术的加速发展和对人类生活模式的改变的进展在朝着人类历史上某种类似奇点的方向发展，在这个奇点之后，我们现在熟知的社会将不复存在”。<br />这一天，对人类来说是喜还是忧？人类是通过自己的智慧毁灭了自己还是得到了永生，谁能说清楚呢？ <img src="/img/人工智能.png" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《佛学常识答问》读后感</title>
      <link href="/2016/03/25/%E3%80%8A%E4%BD%9B%E5%AD%A6%E5%B8%B8%E8%AF%86%E7%AD%94%E9%97%AE%E3%80%8B%E8%AF%BB%E5%90%8E%E6%84%9F/"/>
      <url>/2016/03/25/%E3%80%8A%E4%BD%9B%E5%AD%A6%E5%B8%B8%E8%AF%86%E7%AD%94%E9%97%AE%E3%80%8B%E8%AF%BB%E5%90%8E%E6%84%9F/</url>
      
        <content type="html"><![CDATA[<p>一些我们日常司空见惯的名词，原来别有含义。<br />例如：南无阿弥陀佛的南无（读na mo 一声）是“敬礼”的意思，如来佛不是一个佛而是“佛陀”的统称，例如可以说释伽牟尼如来。<br />佛教教义的基本内容就是关于人世间的苦，然后教导人消灭苦的法。而基本的思想就是空及轮回的思想，依据的原理就是诸法由因缘而起。<br />掩卷沉思，突然想到从小看到大的《西游记》。<br />之前对于唐僧取经，简单的理解就是为了到西天拿到经书，然后就可以成佛了。<br />现在回想，在取经途中，经历各种考验、诱惑、危险才是他们成佛的缘。<br />或者证到无上大觉而成为佛陀，或度己度众生而成了菩萨，或者六根清净证入涅槃而成为罗汉，一切由因缘而起。<br />人生就像一条漫长的取经路，开始的目的就是经书。可是，当我们达到终点的时候，才发现，真正能够成就我们的，恰恰是旅途上你经历的那些事、遇到的那些人。 <img src="/img/佛教常识.png" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 宗教 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《耶路撒冷三千年》读后感</title>
      <link href="/2016/03/25/%E3%80%8A%E8%80%B6%E8%B7%AF%E6%92%92%E5%86%B7%E4%B8%89%E5%8D%83%E5%B9%B4%E3%80%8B%E8%AF%BB%E5%90%8E%E6%84%9F/"/>
      <url>/2016/03/25/%E3%80%8A%E8%80%B6%E8%B7%AF%E6%92%92%E5%86%B7%E4%B8%89%E5%8D%83%E5%B9%B4%E3%80%8B%E8%AF%BB%E5%90%8E%E6%84%9F/</url>
      
        <content type="html"><![CDATA[<p>出于中国政治宣传的需要，作为美国同伙之一的以色列总是以一副欺凌巴勒斯坦的面目出现。<br />但是《耶路撒冷三千年》，却给我们展现了犹太人三千年来备受磨难、颠沛流离、居无定所的历史。<br />因为耶稣受难的缘故，犹太人成了基督教徒、伊斯兰教徒的冲突对象，而耶路撒冷的神圣性又使犹太人的应许之地，成了各大帝国的必争之地，罗马帝国、波斯帝国、拜占庭帝国、奥斯曼帝国的铁骑都曾从耶路撒冷踏过，希特勒的“最终解决方案”，更是使犹太人面临灭族的危险。<br />当你回顾起中国的百年屈辱史，更能感同身受到犹太人对于民族生存、安全的关注，也就自然了解以色列一系列看似强势的行事方式，当然，巴勒斯坦的人民也是如此。<br />但是，作为三大宗教的圣地耶路撒冷，冲突只是唯一的选择吗？ <img src="/img/耶路撒冷.png" /></p>]]></content>
      
      
      <categories>
          
          <category> 读书心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 宗教 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>

<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.1.1">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" integrity="sha256-DfWjNxDkM94fVBWx1H5BMMp0Zq7luBlV8QRcSES7s+0=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"hongyitong.github.io","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.11.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="6、概率与分布 Probability and Distributions 概率论可以看作是布尔逻辑的推广。在机器学习的背景下，它经常以这种方式应用于自动推理系统的形式化设计。 在机器学习和统计学中，有两种主要的概率解释：贝叶斯主义和频率主义(Bishop, 2006;Efron and Hastie, 2016)。贝叶斯主义使用概率来指定用户对事件的不确定性程度。它有时被称为“主观概率”或“置信">
<meta property="og:type" content="article">
<meta property="og:title" content="《机器学习的数学基础》（6&#x2F;7）">
<meta property="og:url" content="http://hongyitong.github.io/2025/10/25/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%85%AD/index.html">
<meta property="og:site_name" content="墨语浮生">
<meta property="og:description" content="6、概率与分布 Probability and Distributions 概率论可以看作是布尔逻辑的推广。在机器学习的背景下，它经常以这种方式应用于自动推理系统的形式化设计。 在机器学习和统计学中，有两种主要的概率解释：贝叶斯主义和频率主义(Bishop, 2006;Efron and Hastie, 2016)。贝叶斯主义使用概率来指定用户对事件的不确定性程度。它有时被称为“主观概率”或“置信">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Part1/%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E7%9A%84%E5%87%A0%E4%BD%95.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Part1/beta%E5%88%86%E5%B8%83.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Part1/%E5%B8%B8%E8%A7%81%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0%E7%9A%84%E5%85%B1%E8%BD%AD%E5%85%88%E9%AA%8C.png">
<meta property="article:published_time" content="2025-10-24T16:00:00.000Z">
<meta property="article:modified_time" content="2025-12-12T03:21:22.920Z">
<meta property="article:author" content="Rayman.hung">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://hongyitong.github.io/img3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Part1/%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E7%9A%84%E5%87%A0%E4%BD%95.png">


<link rel="canonical" href="http://hongyitong.github.io/2025/10/25/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%85%AD/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://hongyitong.github.io/2025/10/25/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%85%AD/","path":"2025/10/25/《机器学习的数学基础》读书笔记之六/","title":"《机器学习的数学基础》（6/7）"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>《机器学习的数学基础》（6/7） | 墨语浮生</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-ZXGEJDXQ33"></script>
  <script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"G-ZXGEJDXQ33","only_pageview":false}</script>
  <script src="/js/third-party/analytics/google-analytics.js"></script>





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">墨语浮生</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Rayman</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section">首页</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section">分类</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section">关于</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section">归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A6%82%E7%8E%87%E4%B8%8E%E5%88%86%E5%B8%83"><span class="nav-text">6、概率与分布</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A6%82%E7%8E%87%E7%A9%BA%E9%97%B4-omega-mathcala-p"><span class="nav-text">6.1 概率空间 \((\Omega, \mathcal{A}, P)\)，</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8A%A0%E6%B3%95%E6%B3%95%E5%88%99%E4%B9%98%E6%B3%95%E6%B3%95%E5%88%99%E5%92%8C%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AE%9A%E7%90%86"><span class="nav-text">6.2
加法法则、乘法法则和贝叶斯定理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%9F%E6%9C%9B%E5%80%BC"><span class="nav-text">6.3 期望值</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%8F%E6%96%B9%E5%B7%AE"><span class="nav-text">6.4 协方差</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9B%B8%E5%85%B3"><span class="nav-text">6.5 相关</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E5%B7%AE%E7%9A%84%E4%B8%89%E4%B8%AA%E8%A1%A8%E8%BE%BE%E5%BC%8F"><span class="nav-text">6.6 方差的三个表达式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E7%9A%84%E5%92%8C%E4%B8%8E%E5%8F%98%E6%8D%A2"><span class="nav-text">6.7 随机变量的和与变换</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BB%9F%E8%AE%A1%E7%8B%AC%E7%AB%8B%E6%80%A7"><span class="nav-text">6.8 (统计)独立性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E7%9A%84%E5%86%85%E7%A7%AF"><span class="nav-text">6.9 随机变量的内积</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83%E7%9A%84%E8%B7%9D%E7%A6%BB%E5%AE%9A%E4%B9%89"><span class="nav-text">6.10 概率分布的距离定义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83"><span class="nav-text">6.11 高斯分布</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B1%E8%BD%AD%E4%B8%8E%E6%8C%87%E6%95%B0%E6%97%8F"><span class="nav-text">6.12 共轭与指数族</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#beta%E5%88%86%E5%B8%83"><span class="nav-text">6.12.1 Beta分布</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%B1%E8%BD%AD"><span class="nav-text">6.12.2 共轭</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8C%87%E6%95%B0%E6%97%8F"><span class="nav-text">6.12.3 指数族</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%98%E9%87%8F%E6%9B%BF%E6%8D%A2%E9%80%86%E5%8F%98%E6%8D%A2"><span class="nav-text">6.13 变量替换&#x2F;逆变换</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%86%E5%B8%83%E5%87%BD%E6%95%B0%E6%8A%80%E6%9C%AF"><span class="nav-text">6.13.1 分布函数技术</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%98%E9%87%8F%E6%9B%BF%E6%8D%A2"><span class="nav-text">6.13.2 变量替换</span></a></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Rayman.hung</p>
  <div class="site-description" itemprop="description">技术分享、读书心得、心情记录</div>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://hongyitong.github.io/2025/10/25/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%85%AD/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Rayman.hung">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="墨语浮生">
      <meta itemprop="description" content="技术分享、读书心得、心情记录">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="《机器学习的数学基础》（6/7） | 墨语浮生">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          《机器学习的数学基础》（6/7）
        </h1>

        </h1>
          
             <p class="post-subtitle">读书笔记之六：概率与分布</p>
          
        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-10-25 00:00:00" itemprop="dateCreated datePublished" datetime="2025-10-25T00:00:00+08:00">2025-10-25</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97%EF%BC%88%E8%87%AA%E7%84%B6%E7%A7%91%E5%AD%A6%EF%BC%89/" itemprop="url" rel="index"><span itemprop="name">读书心得（自然科学）</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h2 id="概率与分布">6、概率与分布</h2>
<p>Probability and Distributions</p>
<p>概率论可以看作是布尔逻辑的推广。在机器学习的背景下，它经常以这种方式应用于自动推理系统的形式化设计。</p>
<p>在机器学习和统计学中，有两种主要的概率解释：贝叶斯主义和频率主义(Bishop,
2006;Efron and Hastie,
2016)。贝叶斯主义使用概率来指定用户对事件的不确定性程度。它有时被称为“主观概率”或“置信程度”。频率主义则考虑感兴趣的事件与所发生事件的总数的相对频率。一个事件的概率定义为当发生事件的总数趋于无限时，该事件的相对频率。详细例子见《概率中贝叶斯派与经典频率主义区别例子.md》</p>
<blockquote>
<p>贝叶斯主义和频率主义两者的核心区别在于：频率主义把参数视为固定值，而贝叶斯主义把参数视为随机变量，因此需要引入先验知识。两者的核心区别确实与“是否考虑先验知识”相关，但更根本的区别在于它们对“参数”的哲学认知不同：固定值
vs. 随机变量。</p>
</blockquote>
<span id="more"></span>
<p>许多机器学习的概率模型描述使用惰性符号和术语，这会令人困惑的。这篇文章也不例外。有多个不同的概念都被称为“概率分布”，读者往往要从上下文中获知其中的含义。一个有助于理解概率分布的技巧是检查我们是在尝试建立一个分类的（离散的随机变量）模型还是一个连续的（连续的随机变量）模型。我们在机器学习中处理的问题的种类与我们要考虑分类模型还是连续模型密切相关。</p>
<p><strong>注：在抽第二枚硬币之前放回了第一枚抽到的硬币，这意味着两次抽硬币是相互独立的。</strong></p>
<h3 id="概率空间-omega-mathcala-p">6.1 概率空间 <span
class="math inline">\((\Omega, \mathcal{A}, P)\)</span>，</h3>
<p>示例：连续两次抛硬币</p>
<ul>
<li><p><strong>样本空间</strong> <span class="math display">\[
\Omega = \{hh, ht, th, tt\}
\]</span> 这里每个元素（样本点）表示两次抛硬币的具体结果。例如 <span
class="math inline">\(hh\)</span> = 第一次正面、第二次正面。</p></li>
<li><p><strong>事件空间</strong> 事件就是样本空间的子集。比如：</p>
<ul>
<li>事件 <span
class="math inline">\(A=\{\text{hh}\}\)</span>：两次都是正面。</li>
<li>事件 <span
class="math inline">\(B=\{\text{ht},\text{th}\}\)</span>：恰好出现一次正面。</li>
<li>事件 <span
class="math inline">\(C=\{\text{hh},\text{ht},\text{th}\}\)</span>：至少出现一次正面。</li>
</ul>
<p>所有可能的事件集合（即事件空间 <span
class="math inline">\(\mathcal{A}\)</span>）在离散情况下就是 <span
class="math inline">\(\Omega\)</span> 的幂集： <span
class="math display">\[
\mathcal{A} = \{ \emptyset, \{hh\}, \{ht\}, \{th\}, \{tt\}, \{hh,ht\},
\dots, \Omega \}.
\]</span></p></li>
<li><p><strong>概率分布</strong>
假设硬币均匀独立，样本空间中每个结果的概率都是 <span
class="math inline">\(1/4\)</span>。那么：</p>
<ul>
<li><span class="math inline">\(P(A)=P(\{hh\})=1/4\)</span>。</li>
<li><span
class="math inline">\(P(B)=P(\{ht,th\})=1/4+1/4=1/2\)</span>。</li>
<li><span class="math inline">\(P(C)=P(\{hh,ht,th\})=3/4\)</span>。</li>
</ul></li>
</ul>
<p><strong>总结</strong></p>
<ul>
<li><strong>样本空间 <span
class="math inline">\(\Omega\)</span></strong>：所有可能的基本结果。</li>
<li><strong>事件空间 <span
class="math inline">\(\mathcal{A}\)</span></strong>：由样本空间的子集组成，每个子集就是一个“事件”。</li>
<li><strong>概率 <span
class="math inline">\(P\)</span></strong>：为每个事件赋予一个数，满足概率公理。</li>
</ul>
<p>给定一个概率空间 <span class="math inline">\((\Omega, \mathcal{A},
P)\)</span>，我们希望使用它来模拟一些现实世界的现象。在机器学习中，我们经常避免直接引用概率空间，而是引用感兴趣的量上的概率，我们用
<span class="math inline">\(\mathcal{T}\)</span>
表示。在这本书中，我们把 <span
class="math inline">\(\mathcal{T}\)</span> 称为目标空间（target
space），把 <span class="math inline">\(\mathcal{T}\)</span>
的元素称为状态。我们引入了一个函数 <span class="math inline">\(X :
\Omega \to \mathcal{T}\)</span>它接受 <span
class="math inline">\(\Omega\)</span>
元素（一个结果，样本点），并返回感兴趣对象 <span
class="math inline">\(x\)</span> 在 <span
class="math inline">\(\mathcal{T}\)</span> 中的特定量（值）。从 <span
class="math inline">\(\Omega\)</span> 到 <span
class="math inline">\(\mathcal{T}\)</span>
的这种关联/映射称为随机变量（random variable）。</p>
<p>例如，考虑投掷两枚硬币并计算正面数。随机变量 <span
class="math inline">\(X\)</span> 映射到三种可能的结果：</p>
<p><span class="math display">\[
\begin{equation}
X(hh) = 2, \quad X(ht) = 1, \quad X(th) = 1, \quad X(tt) = 0
\end{equation}
\]</span> 在这个特殊的情况下，<span class="math inline">\(\mathcal{T} =
\{0,1,2\}\)</span>，我们感兴趣的是 <span
class="math inline">\(\mathcal{T}\)</span>
中元素的概率。<strong>对于有限的样本空间 <span
class="math inline">\(\Omega\)</span> 和有限的 <span
class="math inline">\(\mathcal{T}\)</span>，随机变量对应的函数本质上是一个查找表。</strong>对于任意子集
<span class="math inline">\(S \subseteq
\mathcal{T}\)</span>，我们将<span class="math inline">\(P_X(S) \in
[0,1]\)</span>（概率）与随机变量 <span class="math inline">\(X\)</span>
对应的特定事件联系起来。例 6.1 提供了具体说明。</p>
<p><strong>将 <span
class="math inline">\(X\)</span><u><em>输出的概率</em></u>和<span
class="math inline">\(\Omega\)</span>中样本的概率这两个不同的概念等同起来!</strong>
详见原书Example 6.1。</p>
<p>离散状态数值化特别有用，因为我们经常需要考虑随机变量的期望值</p>
<p>不幸的是，机器学习许多相关文献使用的符号和术语隐藏了样本空间 <span
class="math inline">\(\Omega\)</span>、目标空间 <span
class="math inline">\(\mathcal{T}\)</span> 和随机变量 <span
class="math inline">\(X\)</span> 之间的区别。对于随机变量 <span
class="math inline">\(X\)</span> 的一组可能结果的值 <span
class="math inline">\(x\)</span>，即 <span class="math inline">\(x \in
\mathcal{T}\)</span>，<span class="math inline">\(p(x)\)</span>
表示随机变量 <span class="math inline">\(X\)</span> 取结果 <span
class="math inline">\(x\)</span> 的概率。对于离散随机变量，这表示为
<span class="math inline">\(p(X =
x)\)</span>，这称为概率质量函数（probability mass
function）。概率质量函数通常被称为“分布”。</p>
<p>对于连续变量，<span class="math inline">\(p(x)\)</span>
称为概率密度函数（probability density
function，通常称为密度），而累积分布函数 <span class="math inline">\(P(x
\le X)\)</span> 通常也被称为“分布”。在本章中，我们将使用符号 <span
class="math inline">\(X\)</span> 来表示一元和多元随机变量，并分别用
<span class="math inline">\(x\)</span> 和 <span
class="math inline">\(\boldsymbol{x}\)</span> 表示状态。</p>
<p>我们用“概率分布”表达离散的概率质量函数以及连续的概率密度函数，尽管这在技术上是不正确的。与大多数机器学习文献一样，我们也依赖上下文来区分“概率分布”这个短语的不同用法。</p>
<h3 id="加法法则乘法法则和贝叶斯定理">6.2
加法法则、乘法法则和贝叶斯定理</h3>
<p>1、<strong>求和法则</strong> <span class="math display">\[
p(x) =
\begin{cases}
\sum_{y \in \mathcal{Y}} p(x,y), &amp; \text{如果 $y$ 是离散的}, \\[1em]
\int_{\mathcal{Y}} p(x,y)\,dy, &amp; \text{如果 $y$ 是连续的}.
\end{cases}
\]</span></p>
<p>求和法则将<strong>联合分布与边缘分布</strong>联系了起来。</p>
<p><strong>备注：</strong>
概率建模的许多计算困难都是由于应用了求和法则。当有许多变量或具有许多状态的离散变量时，求和法则执行的是高维求和或积分。执行高维求和或积分通常是难以计算，因为没有已知的多项式时间(polynomial-time)算法来精确计算它们。</p>
<p>2、<strong>乘法法则</strong></p>
<p>乘法法则(<em>product
rule</em>)，它将<strong>联合分布与条件分布</strong>联系起来： <span
class="math display">\[
\begin{equation}
p(\boldsymbol{x}, \boldsymbol{y}) = p(\boldsymbol{y} \mid
\boldsymbol{x}) \, p(\boldsymbol{x}) \qquad (6.22)
\end{equation}
\]</span>
乘积法则可以解释为：两个随机变量的联合分布能被因子分解（乘积形式）为其他两个分布。这两个因子分别是：</p>
<ul>
<li>第一个随机变量的边缘分布<span
class="math inline">\(p(\boldsymbol{x})\)</span></li>
<li>给定第一个随机变量时，第二个随机变量的条件分布<span
class="math inline">\(p(\boldsymbol{y} \mid
\boldsymbol{x})\)</span></li>
</ul>
<p>联合分布中随机变量的顺序是任意的，这意味着： <span
class="math display">\[
p(\boldsymbol{x}, \boldsymbol{y}) = p(\boldsymbol{x} \mid
\boldsymbol{y}) \, p(\boldsymbol{y})
\]</span>
<strong>准确地说，式（6.22）表示的是离散随机变量的概率质量函数。对于连续随机变量，乘积规则用概率密度函数（第
6.2.3 节）表示。</strong></p>
<p>3、<strong>贝叶斯定理</strong></p>
<p>在机器学习和贝叶斯统计中，如果我们观察到部分随机变量，我们通常会对未观察到的（潜在的）随机变量的推断感兴趣。假设我们有一些关于未观测随机变量
<span class="math inline">\(\boldsymbol{x}\)</span> 的先验（prior）知识
<span
class="math inline">\(p(\boldsymbol{x})\)</span>，并且我们可以观察到
<span class="math inline">\(\boldsymbol{x}\)</span> 和第二个随机变量
<span class="math inline">\(\boldsymbol{y}\)</span> 之间的关系 <span
class="math inline">\(p(\boldsymbol{y} \mid
\boldsymbol{x})\)</span>。那么，如果我们观察到 <span
class="math inline">\(\boldsymbol{y}\)</span>，就可以利用贝叶斯定理，在给定
<span class="math inline">\(\boldsymbol{y}\)</span> 的观测值前提下，得出
<span class="math inline">\(\boldsymbol{x}\)</span> 的一些结论。</p>
<p><strong>贝叶斯定理（也叫贝叶斯法则或贝叶斯定律）：</strong> <span
class="math display">\[
\begin{equation}
\underbrace{p(\boldsymbol{x} \mid \boldsymbol{y})}_{\text{后验}}
= \frac{\overbrace{p(\boldsymbol{y} \mid \boldsymbol{x})}^{\text{似然}}
\;
\overbrace{p(\boldsymbol{x})}^{\text{先验}}}{\underbrace{p(\boldsymbol{y})}_{\text{证据}}}
\qquad (6.23)
\end{equation}
\]</span> 似然（likelihood，有时也被称为“测量模型”）<span
class="math inline">\(p(\boldsymbol{y} \mid
\boldsymbol{x})\)</span>描述了<span
class="math inline">\(\boldsymbol{x}\)</span>和<span
class="math inline">\(\boldsymbol{y}\)</span>是如何相关的。对于离散概率分布，它是在已知潜在变量<span
class="math inline">\(\boldsymbol{x}\)</span>前提下，数据<span
class="math inline">\(\boldsymbol{y}\)</span>的概率。注意，似然<span
class="math inline">\(p(\boldsymbol{y} \mid
\boldsymbol{x})\)</span>不是<span
class="math inline">\(\boldsymbol{x}\)</span>的分布，而是<span
class="math inline">\(\boldsymbol{y}\)</span>的分布(<strong>个人注：但是关于<span
class="math inline">\(x\)</span>的函数</strong>)。并且我们称<span
class="math inline">\(p(\boldsymbol{y} \mid
\boldsymbol{x})\)</span>为“<span
class="math inline">\(\boldsymbol{x}\)</span>的似然（给定<span
class="math inline">\(\boldsymbol{y}\)</span>）”或“给定<span
class="math inline">\(\boldsymbol{x}\)</span>，<span
class="math inline">\(\boldsymbol{y}\)</span>的概率”，但不是<span
class="math inline">\(\boldsymbol{y}\)</span>的似然（MacKay,
2003）。</p>
<blockquote>
<p>核心：分清似然函数和条件概率的区别，<strong>同一个表达式 <span
class="math inline">\(p(y \mid
x)\)</span>，既可以是条件概率，也可以是似然函数，取决于我们把哪个当变量、哪个当已知！</strong>详见《似然函数和条件概率的分别.md》</p>
</blockquote>
<blockquote>
<p><strong>贝叶斯定理公式会适合两种情况：</strong><br />
<strong>情况1：X、Y 是两个事件</strong> → 计算条件概率。<br />
<strong>情况2：X 是参数，Y 是数据序列</strong> →
用新观测更新对参数的信念。<br />
<strong>区分标准</strong>：看 X 和 Y
是否是<strong>不同的事件</strong>，还是 <strong>一个事件（参数） +
多次观测</strong>。</p>
</blockquote>
<p><span class="math display">\[
\begin{equation}
p(y) := \int p(y \mid x) p(x) \, dx = \mathbb{E}_{X}[p(y \mid
x)]   \qquad (6.27)
\end{equation}
\]</span></p>
<p>是边缘似然 / 证据（marginal likelihood / evidence）。</p>
<p>(6.27) 的右边使用我们在第 6.4.1 节中定义的期望操作符。
根据定义，边缘似然是 (6.23) 贝叶斯公式的分子对潜在变量 <span
class="math inline">\(\boldsymbol{x}\)</span> 的积分。因此，边缘似然与
<span class="math inline">\(\boldsymbol{x}\)</span> 无关，它保证了后验
<span class="math inline">\(p(\boldsymbol{x} \mid
\boldsymbol{y})\)</span> 被标准化。
边缘似然也可以解释为期望似然，关于先验 <span
class="math inline">\(p(\boldsymbol{x})\)</span> 的期望。
除了用于后验标准化之外，边缘似然在贝叶斯模型选择中也起着重要作用，我们将在第
8.6 节中讨论，证据（evidence）通常很难计算。</p>
<p><strong>贝叶斯定理 (6.23) 允许我们反转由似然给出的 <span
class="math inline">\(\boldsymbol{x}\)</span> 和 <span
class="math inline">\(\boldsymbol{y}\)</span> 之间的关系。
因此，贝叶斯定理有时也被称为概率逆（probabilistic
inverse）。</strong></p>
<p><strong>备注：</strong>
在贝叶斯统计中，后验分布是我们感兴趣的量，因为它包含了所有来自先验和数据的可用信息。<strong>我们把重点放在后验的一些统计量上，例如后验的最大值</strong>，这将在8.3节中讨论。<strong>然而，只关注后验的统计量会导致信息的丢失</strong>。如果我们在更大的背景下思考，那么后验还可以在决策系统中使用。<strong>拥有完整的后验非常有用</strong>，它可以得到对干扰具有鲁棒性的决策。例如，在基于模型的强化学习中，Deisenroth等人(2015)表明，使用似然转移函数的完全后验分布可以非常快速(数据/样本高效)学习，而关注最大的后验则会导致一致性失败。因此，拥有完整的后验对于下游任务非常有用。在第9章中，我们将在线性回归的背景下继续这个讨论。</p>
<h3 id="期望值">6.3 期望值</h3>
<p>期望值的概念是机器学习的中心，概率本身的一些基本概念可以从期望值派生
(Whittle, 2000)。</p>
<p><strong>定义 6.3 期望值</strong></p>
<p>关于单变量连续随机变量 <span class="math inline">\(X \sim
p(X)\)</span> 的函数<br />
<span class="math inline">\(g : \mathbb{R} \to \mathbb{R}\)</span>
的期望值（expected value）为： <span class="math display">\[
\mathbb{E}_{X}[g(x)] = \int_{\mathcal{X}} g(x) \, p(x) \, \mathrm{d}x
\]</span> 相应地，关于离散随机变量 <span class="math inline">\(X \sim
p(X)\)</span> 的函数 <span class="math inline">\(g\)</span> 的期望值为：
<span class="math display">\[
\mathbb{E}_{X}[g(x)] = \sum_{x \in \mathcal{X}} g(x) \, p(x)
\]</span> 其中 <span class="math inline">\(\mathcal{X}\)</span>
是随机变量 <span class="math inline">\(X\)</span>
的可能结果的集合（目标空间）。</p>
<p>在本节中，我们考虑离散随机变量的数值结果。通过<strong>观察函数 <span
class="math inline">\(g\)</span></strong>
以实数作为输入可以看出这一点。</p>
<p><strong>定义 6.4 均值:</strong></p>
<p>状态 <span class="math inline">\(\boldsymbol{x} \in
\mathbb{R}^D\)</span> 的随机变量 <span class="math inline">\(X\)</span>
的均值（mean）为平均值（average），定义为<br />
<span class="math display">\[
\mathbb{E}_{X}[\boldsymbol{x}]
=
\begin{bmatrix}
\mathbb{E}_{X_1}[x_1] \\
\vdots \\
\mathbb{E}_{X_D}[x_D]
\end{bmatrix}
\in \mathbb{R}^{D}
\]</span></p>
<p>对于 <span class="math inline">\(d = 1, \ldots, D\)</span>：<br />
<span class="math display">\[
\mathbb{E}_{X_d}[x_d] :=
\begin{cases}
\int_{\mathcal{X}} x_d \, p(x_d) \, dx_d, &amp; \text{如果 $X$
为连续型随机变量}, \\[1em]
\sum\limits_{x_i \in \mathcal{X}} x_i \, p(x_d = x_i), &amp; \text{如果
$X$ 为离散型随机变量}.
\end{cases}
\qquad (6.32)
\]</span></p>
<p>其中下标 <span class="math inline">\(d\)</span> 表示 <span
class="math inline">\(\boldsymbol{x}\)</span>
对应的维度。上式是对随机变量 <span class="math inline">\(X\)</span>
的目标空间状态 <span class="math inline">\(\mathcal{X}\)</span>
的积分以及求和。</p>
<p>在一个维度中，还有另外两个直观的“平均”概念，即 中位数 (median) 和
众数 (mode)。</p>
<p>如果我们对这些值进行排序，中位数就是“最中间”的值，即 <span
class="math inline">\(50\%\)</span> 的值大于中位数，<span
class="math inline">\(50\%\)</span>
的值小于中位数。这一思想可以推广到连续值，考虑累计分布函数（定义 6.2）为
<span class="math inline">\(0.5\)</span> 的值。
对于不对称或有长尾的分布，中位数提供了一个典型值的估计，该值比平均值更接近人类的直觉。
此外，中位数对异常值的鲁棒性比平均值强。中位数向更高维度的推广是非平凡的，因为目前没有方法可以在不止一个维度中“排序”
(Hallin et al., 2010; Kong and Mizera, 2012)。</p>
<p>众数 (mode)是最常出现的值。对于离散随机变量，众数定义为出现频率最高的
<span class="math inline">\(x\)</span> 的值。
对于连续随机变量，众数定义为密度 <span
class="math inline">\(p(\boldsymbol{x})\)</span> 上的一个峰值。
一个特定的密度 <span class="math inline">\(p(\boldsymbol{x})\)</span>
可能有不止一个众数，而且在高维分布中可能有大量的众数。
因此，找到一个分布的所有众数在计算上是具有挑战性的。</p>
<p>注：在统计学中局部最值也常被称为“众数”（更准确叫 local
modes），严格意义上的众数应该是全局的最值。</p>
<p>定义 6.3 定义了符号 <span class="math inline">\(\mathbb{E}_X\)</span>
的意义，作为一个算子，它表示我们应该取关于概率密度的积分（对于连续分布）或关于所有状态的和（对于离散分布）。<strong>均值的定义（定义
6.4），是期望值的一种特殊情况，通过取 <span
class="math inline">\(g\)</span> 为恒等函数得到。</strong></p>
<h3 id="协方差">6.4 协方差</h3>
<p>对于两个随机变量，我们可以描述它们之间的对应关系。协方差直观地表示随机变量之间的相关性。</p>
<blockquote>
<p>方差的概念是从协方差引出的，而不是相反！</p>
</blockquote>
<p><strong>协方差(一元)</strong></p>
<p>两个单变量随机变量 <span class="math inline">\(X, Y \in
\mathbb{R}\)</span>
之间的协方差（covariance）由其偏离各自均值的期望积给出，即<br />
<span class="math display">\[
\operatorname{Cov}_{X, Y}[x, y] := \mathbb{E}_{X, Y}\left[ \left(x -
\mathbb{E}_{X}[x]\right)\left(y - \mathbb{E}_{Y}[y]\right) \right]
\]</span> <strong>术语：多元随机变量的协方差 <span
class="math inline">\(\operatorname{Cov}[x, y]\)</span>
有时被称为交叉协方差（cross-covariance），其中协方差指的是 <span
class="math inline">\(\operatorname{Cov}[x, x]\)</span>。</strong></p>
<blockquote>
<p><strong>协方差衡量两个随机变量是否一起增减，以及一起变化的强弱与方向。</strong></p>
</blockquote>
<p>备注：
当与期望或协方差相关的随机变量的参数明确时，下标通常被去掉（例如，<span
class="math inline">\(\mathbb{E}_X[x]\)</span> 经常被写成 <span
class="math inline">\(\mathbb{E}[x]\)</span>）。</p>
<p>利用期望的线性性，定义 6.5
中的表达式可以改写为乘积的期望值减去期望值的乘积，即<br />
<span class="math display">\[
\operatorname{Cov}[x, y] = \mathbb{E}[xy] - \mathbb{E}[x]\,\mathbb{E}[y]
\]</span> <strong>一个变量与自身的协方差 <span
class="math inline">\(\operatorname{Cov}[x, x]\)</span>
称为方差（variance），用 <span
class="math inline">\(\mathbb{V}_X[x]\)</span>
表示。方差的平方根称为标准差（standard deviation），通常用 <span
class="math inline">\(\sigma(x)\)</span>
表示。协方差的概念可以推广到多元随机变量。</strong></p>
<p><strong>定义 6.6 协方差（多元）</strong></p>
<p>如果我们考虑两个多元随机变量 <span
class="math inline">\(\mathbf{X}\)</span> 和 <span
class="math inline">\(\mathbf{Y}\)</span>，分别对应状态 <span
class="math inline">\(\mathbf{x} \in \mathbb{R}^D\)</span> 和 <span
class="math inline">\(\mathbf{y} \in \mathbb{R}^E\)</span>，则 <span
class="math inline">\(\mathbf{X}\)</span> 和 <span
class="math inline">\(\mathbf{Y}\)</span> 之间的协方差定义为： <span
class="math display">\[
{Cov}[\mathbf{x}, \mathbf{y}]
= \mathbb{E}\!\left[\mathbf{x} \mathbf{y}^{\top}\right] -
\mathbb{E}[\mathbf{x}]\,\mathbb{E}[\mathbf{y}]^{\top}
= \operatorname{Cov}[\mathbf{y}, \mathbf{x}]^{\top}
\in \mathbb{R}^{D \times E}.
\]</span> <strong>定义 6.6
可以应用于两个相同的多元随机变量，从而产生一个有用的概念，直观地捕捉随机变量的“扩散程度”。对于一个多元随机变量，方差描述了该随机变量的单个维度之间的关系。</strong></p>
<p><strong>方差与协方差矩阵</strong> 状态为 <span
class="math inline">\(\boldsymbol{x} \in \mathbb{R}^{D}\)</span>
且均值向量为 <span class="math inline">\(\boldsymbol{\mu} \in
\mathbb{R}^{D}\)</span> 的随机变量 <span
class="math inline">\(X\)</span> 的方差 (variance) 定义为<br />
<span class="math display">\[
\begin{align}
V_X[\boldsymbol{x}]
&amp;= \mathrm{Cov}_X[\boldsymbol{x}, \boldsymbol{x}] \\
&amp;= \mathbb{E}_X \big[ (\boldsymbol{x} -
\boldsymbol{\mu})(\boldsymbol{x} - \boldsymbol{\mu})^\top \big] \\
&amp;= \mathbb{E}_X[\boldsymbol{x} \boldsymbol{x}^\top] -
\mathbb{E}_X[\boldsymbol{x}] \ \mathbb{E}_X[\boldsymbol{x}]^\top \\
&amp;=
\begin{bmatrix}
\mathrm{Cov}[x_1, x_1] &amp; \mathrm{Cov}[x_1, x_2] &amp; \cdots &amp;
\mathrm{Cov}[x_1, x_D] \\
\mathrm{Cov}[x_2, x_1] &amp; \mathrm{Cov}[x_2, x_2] &amp; \cdots &amp;
\mathrm{Cov}[x_2, x_D] \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\mathrm{Cov}[x_D, x_1] &amp; \mathrm{Cov}[x_D, x_2] &amp; \cdots &amp;
\mathrm{Cov}[x_D, x_D]
\end{bmatrix}.
\end{align}
\]</span> <strong>上式中的 <span class="math inline">\(D \times
D\)</span> 矩阵称为多元随机变量 <span class="math inline">\(X\)</span>
的 协方差矩阵 (covariance matrix)。
协方差矩阵是对称的，且半正定，它描述了数据的扩散程度。</strong>协方差矩阵的对角线元素包含了边缘分布
<span class="math display">\[
\begin{equation}
p(x_i) = \int p(x_1, \ldots, x_D) \ \mathrm{d}x_{\backslash i}
\end{equation}
\]</span> 的方差，其中符号 <span class="math inline">\(\backslash
i\)</span>表示“除了 <span class="math inline">\(i\)</span>
之外的所有变量”。 非对角线元素为交叉协方差项 <span
class="math inline">\(\mathrm{Cov}[x_i, x_j]\)</span>，其中 <span
class="math inline">\(i, j = 1, \ldots, D, \ i \neq j\)</span>。</p>
<h3 id="相关">6.5 <strong>相关</strong></h3>
<p>当我们想比较不同随机变量对之间的协方差时，每个随机变量的方差都会影响协方差的值。协方差的标准化版本称为<strong>相关</strong>（correlation）</p>
<p>两个随机变量 <span class="math inline">\(X, Y\)</span>
的相关（correlation）为： <span class="math display">\[
\operatorname{corr}[x, y] = \frac{\operatorname{Cov}[x,
y]}{\sqrt{\mathbb{V}[x] \, \mathbb{V}[y]}} \in [-1, 1]
\]</span> 相关矩阵是被标准化的随机变量<span
class="math inline">\(x/{\sigma(x)}\)</span>协方差矩阵。换句话说，每个随机变量在相关矩阵中都除以其标准差（方差的平方根）。</p>
<p><strong>经验均值和协方差</strong></p>
<blockquote>
<p><strong>为什么 <span class="math inline">\(N-1\)</span>
是“无偏(unbiased)”？</strong><br />
当我们从总体中抽样时，用样本均值 <span
class="math inline">\(\bar{x}\)</span> 和 <span
class="math inline">\(\bar{y}\)</span>
代替真实均值，会引入<strong>低估总体方差和协方差</strong>的偏差。分母用
<span class="math inline">\(N-1\)</span> 而不是 <span
class="math inline">\(N\)</span>
可以修正这个偏差，使得估计值的期望等于真实值。</p>
</blockquote>
<h3 id="方差的三个表达式">6.6 方差的三个表达式</h3>
<p>1、方差的标准定义</p>
<p>与协方差的定义（定义6.5）相对应，是随机变量 <span
class="math inline">\(X\)</span> 与其期望值 <span
class="math inline">\(\mu\)</span> 的平方偏差的期望，即<br />
<span class="math display">\[
\begin{equation}
\mathbb{V}_{X}[x] := \mathbb{E}_{X} \left[ (x - \mu)^{2} \right]
\tag{6.43}
\end{equation}
\]</span> 式（6.43）中的期望和平均值<span class="math inline">\(\mu =
\mathbb{E}_{X}(x)\)</span>使用（6.32）计算，取决于 <span
class="math inline">\(X\)</span>
是离散的还是连续的随机变量。式（6.43）中表示的方差可以说是一个新的随机变量<span
class="math inline">\(Z := (X - \mu)^{2}\)</span>的均值。
当根据经验估计（6.43）中的方差时，我们需要使用一个两阶段的算法：首先利用数据使用（6.41）计算平均值
<span class="math inline">\(\mu\)</span>，然后使用这个估计值 <span
class="math inline">\(\hat{\mu}\)</span> 计算方差。</p>
<p>2、事实证明，我们可以通过整理表达式来避免两个阶段。方差的标准定义：
<span class="math display">\[
\begin{equation}
\mathbb{V}_{X}[x] := \mathbb{E}_{X}\left[(x-\mu)^2\right]
\end{equation}
\]</span> （6.43）可以转换为所谓的“方差的原始分数公式”（raw-score
formula for variance）： <span class="math display">\[
\begin{equation}
\mathbb{V}_{X}[x] = \mathbb{E}_{X}\left[x^2\right] -
\left(\mathbb{E}_{X}[x]\right)^2 \qquad (6.44)
\end{equation}
\]</span>
(6.44)中的表达式可以这样记住：“平方的均值减去均值的平方”。这种方法只需对数据进行一次遍历计算，因为我们可以同时计算每个观测值
<span class="math inline">\(x_i\)</span> 的平均值 <span
class="math inline">\(\bar{x} = \frac{1}{n}\sum_i x_i\)</span>
和平方平均值 <span class="math inline">\(\frac{1}{n}\sum_i
x_i^2\)</span>。</p>
<p>不幸的是，如果以这种方式计算，它在数值上可能不稳定，尤其是当 <span
class="math inline">\(x_i\)</span>
的数值很大时，减法可能导致有效数字损失。</p>
<p>3、理解方差的第三种方式是，它可以表示为所有观测值对的差的总和。<br />
考虑随机变量 <span class="math inline">\(X\)</span> 的一个样本 <span
class="math inline">\(x_1, \ldots, x_N\)</span>，我们计算 <span
class="math inline">\(x_i\)</span> 和 <span
class="math inline">\(x_j\)</span>
对之间的平方差。通过展开平方差，可以证明 <span
class="math inline">\(N^2\)</span>
个观测值对的差的总和正好与观测值的经验方差相关： <span
class="math display">\[
\begin{equation}
\frac{1}{N^2} \sum_{i,j=1}^{N} (x_i - x_j)^2 = 2 \left[ \frac{1}{N}
\sum_{i=1}^{N} x_i^2 - \left( \frac{1}{N} \sum_{i=1}^{N} x_i \right)^2
\right] \qquad (6.45)
\end{equation}
\]</span> 由此可见，(6.45) 是原始分数公式 (6.44)
的两倍。这意味着，我们可以用观测值两两之间的距离总和（<span
class="math inline">\(N^2\)</span> 个）来表示偏离均值的偏离值总和（<span
class="math inline">\(N\)</span> 个）。
<strong>从几何上来看，这意味着在一个点集中，点两两之间的距离总和与每个点到点集中心的距离总和是等价的。</strong></p>
<h3 id="随机变量的和与变换">6.7 随机变量的和与变换</h3>
<p>均值和协方差在随机变量的仿射变换中表现出一些有用的特性。<br />
假设随机变量 <span class="math inline">\(\boldsymbol{X}\)</span>
的均值向量为 <span
class="math inline">\(\boldsymbol{\mu}\)</span>，协方差矩阵为 <span
class="math inline">\(\boldsymbol{\Sigma}\)</span>，且 <span
class="math inline">\(\boldsymbol{X}\)</span> 的（确定性）仿射变换为：
<span class="math display">\[
\boldsymbol{y} = \boldsymbol{A} \boldsymbol{x} + \boldsymbol{b}
\]</span> 由于 <span class="math inline">\(\boldsymbol{y}\)</span>
本身是一个随机变量，其均值向量和协方差矩阵分别为： <span
class="math display">\[
\begin{equation}
\mathbb{E}_Y[\boldsymbol{y}] = \mathbb{E}_X[\boldsymbol{A}\boldsymbol{x}
+ \boldsymbol{b}]
= \boldsymbol{A}\,\mathbb{E}_X[\boldsymbol{x}] + \boldsymbol{b}
= \boldsymbol{A}\boldsymbol{\mu} + \boldsymbol{b} \qquad (6.50)
\end{equation}
\]</span></p>
<p><span class="math display">\[
\begin{equation}
\mathbb{V}_Y[\boldsymbol{y}] = \mathbb{V}_X[\boldsymbol{A}\boldsymbol{x}
+ \boldsymbol{b}]
= \mathbb{V}_X[\boldsymbol{A}\boldsymbol{x}]
= \boldsymbol{A}\,\mathbb{V}_X[\boldsymbol{x}]\,\boldsymbol{A}^\top
= \boldsymbol{A}\boldsymbol{\Sigma}\boldsymbol{A}^\top \qquad (6.51)
\end{equation}
\]</span></p>
<p>此外，<span class="math inline">\(\boldsymbol{X}\)</span> 与 <span
class="math inline">\(\boldsymbol{Y}\)</span> 的协方差为： <span
class="math display">\[
\begin{align}
\mathrm{Cov}[\boldsymbol{x}, \boldsymbol{y}]
&amp;= \mathbb{E}\big[\boldsymbol{x} (\boldsymbol{A}\boldsymbol{x} +
\boldsymbol{b})^\top\big]
   - \mathbb{E}[\boldsymbol{x}]\,\mathbb{E}[\boldsymbol{A}\boldsymbol{x}
+ \boldsymbol{b}]^\top \\
&amp;= \mathbb{E}[\boldsymbol{x}]\boldsymbol{b}^\top +
\mathbb{E}[\boldsymbol{x}\boldsymbol{x}^\top]\boldsymbol{A}^\top
   - \boldsymbol{\mu}\boldsymbol{b}^\top -
\boldsymbol{\mu}\boldsymbol{\mu}^\top \boldsymbol{A}^\top \\
&amp;= (\mathbb{E}[\boldsymbol{x}\boldsymbol{x}^\top] -
\boldsymbol{\mu}\boldsymbol{\mu}^\top)\,\boldsymbol{A}^\top
= \boldsymbol{\Sigma}\,\boldsymbol{A}^\top
\end{align}
\]</span> 其中，<span class="math inline">\(\boldsymbol{\Sigma} =
\mathbb{E}[\boldsymbol{x}\boldsymbol{x}^\top] -
\boldsymbol{\mu}\boldsymbol{\mu}^\top\)</span> 为 <span
class="math inline">\(\boldsymbol{X}\)</span> 的方差矩阵。</p>
<h3 id="统计独立性">6.8 (统计)独立性</h3>
<p>独立推出不相关（协方差为0）</p>
<p>但这一点是充分不必然的，即，两个随机变量的协方差为零，但在统计上可能不独立。为了理解为什么，回想一下<strong>协方差是只能测量线性相关</strong>。而非线性相关的随机变量可能协方差也为零。</p>
<p>详见《协方差只反映线性相关.md》</p>
<h3 id="随机变量的内积">6.9 随机变量的内积</h3>
<blockquote>
<p><strong>把协方差作为内积的一个例子，协方差符合内积的定义。例如点积符合内积的定义一样。</strong></p>
</blockquote>
<p>两个随机变量的“正交”就意味着它们协方差为零。具体的内积的定义见前文。在用协方差定义内积空间后，可以得出：</p>
<p><span class="math display">\[
\|X\| = \sqrt{\mathbb{V}[X]} = \sigma[X]
\]</span> 这就是标准差。</p>
<blockquote>
<p>在普通向量空间里，向量长短衡量了它在空间中“延伸”的程度。在<em><u>随机变量空间里</u></em>，“长度”衡量的是它在概率空间中“波动”的程度。如果
<span class="math inline">\(\sigma[X] = 0\)</span>，那么 <span
class="math inline">\(\mathbb{V}[X] = 0\)</span>，意味着 <span
class="math inline">\(X\)</span>
<em><u>在概率意义上</u></em>是常数，不会随机变化。</p>
</blockquote>
<p>如果我们考虑两个随机变量 <span
class="math inline">\(X\)</span>、<span class="math inline">\(Y\)</span>
之间的夹角 <span class="math inline">\(\theta\)</span>，我们得到： <span
class="math display">\[
\begin{equation}
\cos\theta = \frac{\langle X, Y \rangle}{\|X\|\,\|Y\|}
= \frac{\operatorname{Cov}[x, y]}{\sqrt{\mathbb{V}[x]\,\mathbb{V}[y]}}
\end{equation}
\]</span> 这是两个随机变量之间的相关性（定义
6.8）。这意味着，当我们从几何角度考虑两个随机变量时，可以把它们的相关性看作是两个随机变量之间夹角的余弦。</p>
<p>根据定义 3.7，我们知道：<span class="math inline">\(X \perp Y \quad
\Longleftrightarrow \quad \langle X, Y\rangle = 0\)</span>
在我们的例子中，这意味着 <span class="math inline">\(X\)</span> 和 <span
class="math inline">\(Y\)</span> 是正交的当且仅当：<span
class="math inline">\(\operatorname{Cov}[X, Y] =
0\)</span>即它们是不相关的。图 6.6 说明了这种关系。</p>
<p><img src="/img3/机器学习的数学基础Part1/随机变量的几何.png" alt="随机变量的几何" style="zoom:50%;" /></p>
<p>随机变量的几何。如果随机变量<span
class="math inline">\(X\)</span>和<span
class="math inline">\(Y\)</span>不相关，则它们是对应向量空间中的正交向量，且勾股定理也适用。</p>
<h3 id="概率分布的距离定义">6.10 概率分布的距离定义</h3>
<p>使用之前内积定义的欧几里得距离来比较概率分布似乎是个不错的选择，但不幸的是，这不是获得分布之间距离的最佳方法。回想一下，概率质量(或密度)是正的，需要加起来等于1。这些限制意味着分布存在于一种叫做统计流形(statistical
manifold)的东西上。对概率分布空间的研究被称为信息几何( information
geometry)。计算分布之间的距离通常使用Kullback-Leibler散度(KL散度)来完成，它是距离的推广，它解释了统计流形的性质。正如欧氏距离是矩阵的一种特殊情况一样(第3.3节)，KL散度是另外两种广义散度的一种特殊情况，它们被称为Bregman散度和f
ff-散度。关于它们区别的研究超出了这本书的范围，读者可以参考信息几何领域的创始人之一Amari(2016)的新书了解更多细节。</p>
<p>具体的例子见《概率分布的距离定义.md》</p>
<h3 id="高斯分布">6.11 高斯分布</h3>
<p>由于高斯分布完全由其均值和协方差来表示，我们通常可以通过对随机变量的均值和协方差进行变换来得到变换后的分布。<strong>高斯分布的边缘分布和条件分布是高斯分布</strong>。高斯随机变量的任何线性/仿射变换后依然服从高斯分布。</p>
<h3 id="共轭与指数族">6.12 共轭与指数族</h3>
<p>在应用概率运算法则时，存在一些“封闭性”，如贝叶斯定理。<strong>封闭是指对一类对象应用特定操作后返回相同类型的对象。</strong></p>
<h4 id="beta分布">6.12.1 <strong>Beta分布</strong></h4>
<p>我们可能想在有限区间上建立一个连续随机变量的模型。Beta分布是一个连续随机变量
<span class="math inline">\(\mu \in [0,1]\)</span>
上的分布，通常用来表示一些二值事件的概率（例如，控制伯努利分布的参数）。Beta分布
<span class="math inline">\(\mathrm{Beta}(\alpha,
\beta)\)</span>（如图6.11所示）本身由两个参数控制 <span
class="math inline">\(\alpha &gt; 0, \ \beta &gt; 0\)</span>，并被定义为
<span class="math display">\[
\begin{equation}
p(\mu \mid \alpha, \beta) = \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)
\, \Gamma(\beta)} \, \mu^{\alpha-1} (1-\mu)^{\beta-1}
\end{equation}
\]</span></p>
<p><span class="math display">\[
\mathbb{E}[\mu] = \frac{\alpha}{\alpha+\beta},
\quad
\mathbb{V}[\mu] = \frac{\alpha \beta}{(\alpha+\beta)^2 (\alpha+\beta+1)}
\]</span></p>
<p>其中 <span class="math inline">\(\Gamma(\cdot)\)</span>
为伽马（Gamma）函数，定义为： <span class="math display">\[
\Gamma(t) := \int_{0}^{\infty} x^{t-1} e^{-x} \, dx, \quad t &gt; 0,
\qquad
\Gamma(t+1) = t \, \Gamma(t)
\]</span> 注意，（6.98）中的Gamma函数的分数标准化了Beta分布。</p>
<p>直观地看，<span class="math inline">\(\alpha\)</span> 将概率质量移向
<span class="math inline">\(1\)</span>，而 <span
class="math inline">\(\beta\)</span> 将概率质量移向 <span
class="math inline">\(0\)</span>。一些特殊的情况：</p>
<ul>
<li>对于 <span class="math inline">\(\alpha = 1 =
\beta\)</span>，我们得到均匀分布 <span
class="math inline">\(\mathcal{U}[0,1]\)</span>。</li>
<li>对于 <span class="math inline">\(\alpha, \beta &lt;
1\)</span>，我们得到双峰分布，峰值在 <span
class="math inline">\(0\)</span> 和 <span
class="math inline">\(1\)</span> 处。</li>
<li>对于 <span class="math inline">\(\alpha, \beta &gt;
1\)</span>，分布是单峰的。</li>
<li>对于 <span class="math inline">\(\alpha, \beta &gt; 1\)</span> 且
<span class="math inline">\(\alpha =
\beta\)</span>，分布是单峰对称的，且集中在区间 <span
class="math inline">\([0,1]\)</span>，即均值在 <span
class="math inline">\(1/2\)</span>。</li>
</ul>
<p><img src="/img3/机器学习的数学基础Part1/beta分布.png" alt="beta分布" style="zoom:50%;" /></p>
<p>有一大堆有名字的分布，它们以不同的方式相互联系(Leemis和McQueston,
2008)。值得记住的是，<strong>每个被命名的分布都是为了特定的原因而创建的，但是可能还有其他的应用。了解创建特定分布背后的原因可以知道如何最好地使用它</strong>。</p>
<h4 id="共轭">6.12.2 共轭</h4>
<p><strong>共轭先验</strong></p>
<p>如果<strong>后验与先验具有相同的形式/类型</strong>，则<strong><em><u>先验</u></em></strong>是<u><strong><em>似然函数</em></strong></u>的共轭(conjugate)。共轭特别方便，因为我们可以通过更新先验分布的参数来用代数方法计算后验分布。</p>
<p><strong>备注：</strong>
<strong>在考虑概率分布的几何时，共轭先验保留了似然的距离结构(Agarwal and
Daum´e III, 2010)</strong></p>
<p>对于二项分布似然函数中的参数<span
class="math inline">\(\mu\)</span>，Beta先验是共轭的。Beta分布是伯努利分布的共轭先验。</p>
<p><img src="/img3/机器学习的数学基础Part1/常见似然函数的共轭先验.png" alt="常见似然函数的共轭先验" style="zoom:50%;" /></p>
<p>表6.2列出了一些<strong>用于概率建模的标准似然的参数的共轭先验</strong>。分布如多项式分布、逆Gamma分布、逆Wishart分布和Dirichlet分布可以在任何统计文本中找到，例如在Bishop(2006)中就进行了描述。</p>
<p>Beta分布是二项分布和伯努利分布似然中<strong>关于参数<span
class="math inline">\(\mu\)</span>的共轭先验</strong>。对于高斯似然函数，我们可以在均值上设置一个共轭高斯先验。高斯似然在表中出现两次的原因是我们需要区分一元和多元情况。在一元（标量）情况下，逆Gamma是<strong>方差</strong>的共轭先验。在多元情形下，我们使用逆Wishart分布作为<strong>协方差矩阵</strong>的共轭先验。Dirichlet分布是多项式似然函数的共轭先验。</p>
<h4 id="指数族">6.12.3 指数族</h4>
<p><strong>分布三个抽象级别</strong>：</p>
<p>在考虑分布（离散或连续的随机变量）时，我们可以有三个可能的抽象级别。</p>
<p>在第一级（这是最具体的级别），我们有一个具有固定参数的特定“命名”分布，例如一个均值为零，方差为单位矩阵的一元高斯分布
<span class="math inline">\(\mathcal{N}(0,1)\)</span>。</p>
<p>而在机器学习中，我们经常使用第二层抽象，即我们采用参数形式固定的分布（如一元高斯分布），并从数据中推断出它的参数。例如，我们假设一个未知均值
<span class="math inline">\(\mu\)</span> 和未知方差 <span
class="math inline">\(\sigma^2\)</span> 的一元高斯分布 <span
class="math inline">\(\mathcal{N}\left(\mu,
\sigma^{2}\right)\)</span>，并使用最大似然拟合来确定最佳参数 <span
class="math inline">\((\mu,
\sigma^2)\)</span>。我们将在第9章讨论线性回归时看到一个例子。</p>
<p>第三个抽象的层次是考虑分布的族，在本书中，我们考虑指数族。一元高斯分布是指数族中的一个例子。许多广泛使用的统计模型，包括表
6.2 中所有的“命名”模型，都属于指数族。它们都可以统一成一个概念。</p>
<p><strong>我们研究指数族的主要动机是它们具有有限维的充分统计信息。此外，共轭分布很容易写出来，而且也来自一个指数族。</strong>从推理的角度来看，指数族的极大似然估计表现得很好，因为它的充分统计量的经验估计是充分统计量总体的最佳估计（回忆一下高斯分布的均值和协方差）。<strong>从优化的角度来看，指数族的的对数似然函数是凹的，允许我们应用有效的优化方法</strong>(第7章)。</p>
<h3 id="变量替换逆变换">6.13 变量替换/逆变换</h3>
<blockquote>
<p>似乎有很多已知的分布，但实际上，我们可以命名的分布是非常有限的。因此，理解随机变量在变换后是如何分布的通常很有用。</p>
</blockquote>
<p>详见《常见分布在变换后的分布.md》</p>
<p>例如，假设 <span class="math inline">\(X\)</span> 是根据一元正态分布
<span class="math inline">\(\mathcal{N}(0,1)\)</span>
得到的一个随机变量。那么 <span class="math inline">\(X^2\)</span>
的分布是什么？ 另一个在机器学习中很常见的例子是，假设 <span
class="math inline">\(X_1\)</span> 和 <span
class="math inline">\(X_2\)</span> 是一元标准正态分布，那么 <span
class="math inline">\(\frac{1}{2} (X_1 + X_2)\)</span>的分布是什么？计算
<span class="math inline">\(\frac{1}{2} (X_1 + X_2)\)</span>
的分布的一个选择是计算 <span class="math inline">\(X_1\)</span> 和 <span
class="math inline">\(X_2\)</span>
的均值和方差，然后组合它们。正如我们在第 6.4.4
节中看到的，当我们考虑随机变量的仿射变换时，我们可以计算变换后得到的随机变量的均值和方差。
然而，我们也可能无法得到变换后分布的函数形式。此外，我们还可能关心随机变量的非线性变换，这时变换后的封闭形式的表达式是不容易得到的。</p>
<p>我们将介绍两种通过随机变量变换获取分布的方法：<strong>一种是使用累积分布函数定义的直接方法，另一种是使用微积分的链式法则(第5.2.2节)的变量替换(change-of-variable)方法</strong>。<strong>变量替换方法被广泛使用，因为它提供了一个用于计算由于转换而产生的分布的“秘诀”。</strong>我们将解释关于一元随机变量的变量替换技术，并将简要地给出多元随机变量的一般情况的结果。</p>
<h4 id="分布函数技术">6.13.1 分布函数技术</h4>
<p>均匀分布在统计中的重要性</p>
<ul>
<li><strong>基准分布</strong>：在随机数生成和蒙特卡罗模拟中，均匀分布是最常用的基础分布（大多数随机数生成器先生成
<span
class="math inline">\(U(0,1)\)</span>，再通过变换得到其他分布）。</li>
<li><strong>概率积分变换</strong>：<strong>定理 6.15</strong>
就是基于这个思想：如果 <span class="math inline">\(X\)</span> 的 CDF
是严格单调的，那么 <span class="math inline">\(Y = F_X(X)\)</span> 服从
<span class="math inline">\(U(0,1)\)</span>。</li>
</ul>
<p>将随机变量<span class="math inline">\(X\)</span>的累积分布函数<span
class="math inline">\(F_X(x)\)</span>作为变换函数<span
class="math inline">\(U(x)\)</span>，可以得到一个有用的结果,这导出了下面的定理。</p>
<blockquote>
<p>任何分布都能化为[0,1]均匀分布。</p>
</blockquote>
<p><strong>定理 6.15</strong> 令 <span class="math inline">\(X\)</span>
为连续随机变量，且具有严格单调的累积分布函数 <span
class="math inline">\(F_X(x)\)</span>。 那么定义为 <span
class="math display">\[
Y := F_X(X)
\]</span> 的随机变量 <span class="math inline">\(Y\)</span>
具有均匀分布。</p>
<p><strong>证明：</strong></p>
<p>对任意 <span class="math inline">\(y\in[0,1]\)</span>，由于 <span
class="math inline">\(F_X\)</span> 严格单调且连续，存在反函数 <span
class="math inline">\(F_X^{-1}:(0,1)\to \mathbb{R}\)</span>。于是 <span
class="math display">\[
\begin{aligned}
F_Y(y)
&amp;:=\mathbb{P}(Y\le y)
=\mathbb{P}\big(F_X(X)\le y\big) \\
&amp;=\mathbb{P}\big(X\le F_X^{-1}(y)\big)
=F_X\!\big(F_X^{-1}(y)\big)
= y,\qquad y\in[0,1].
\end{aligned}
\]</span> 对区间外的 <span class="math inline">\(y\)</span>，有 <span
class="math inline">\(F_Y(y)=0\)</span>（当 <span
class="math inline">\(y&lt;0\)</span>）和 <span
class="math inline">\(F_Y(y)=1\)</span>（当 <span
class="math inline">\(y&gt;1\)</span>）。 因此 <span
class="math inline">\(F_Y(y)=y\)</span>（<span
class="math inline">\(y\in[0,1]\)</span>），即 <span
class="math inline">\(Y\sim \mathrm{Unif}(0,1)\)</span>。</p>
<p>定理6.15被称为<strong>概率积分变换</strong>(probability integral
transform)，它用于推导<strong><em><u>从分布中采样</u></em></strong>的算法，<strong>这个算法通过均匀随机变量的采样结果进行变换（Bishop，2006）。该算法的工作原理是首先从均匀分布生成样本，然后通过逆累计密度函数（假设这是可以得到的）对其进行变换，以从所需分布获得样本。</strong>概率积分变换也用于假设检验样本是否来自特定分布（Lehmann和Romano，2005）。累积分布函数的输出是均匀分布的这一观点也构成了copulas的基础（Nelsen，2006）。</p>
<p>以上这段话采样的例子详见《概率积分变换的采样原理.md》</p>
<p><strong>直观理解</strong>：</p>
<ul>
<li>均匀分布随机数就像一个“百分比刻度”</li>
<li>逆 CDF 就是把这个刻度映射到目标分布的位置</li>
<li>这样就能用均匀分布随机数生成任何我们想要的分布样本</li>
</ul>
<h4 id="变量替换">6.13.2 变量替换</h4>
<p>区间可逆的函数要么严格递增要么严格递减。</p>
<blockquote>
<p>事件的等价变换不会改变概率</p>
</blockquote>
<p>微积分的基本定理中，有一个非常经典的结论：<strong>积分上限函数求导</strong>，<strong>它把导数与积分直接联系了起来</strong>。</p>
<p>详见《积分上限函数求导.md》。</p>
<p><strong>备注：</strong>
“变量替换”这个名字来源于当我们面对一个困难的积分时改变积分变量的想法。
对于一元函数，我们使用换元积分法：<br />
<span class="math display">\[
\begin{equation}
\int f(g(x)) g^{\prime}(x)\, \mathrm{d}x
= \int f(u)\, \mathrm{d}u,
\quad \text{其中} \quad u = g(x)
\qquad (6.133)
\end{equation}
\]</span> 该法则的推导基于微积分的链式法则
(5.32)，以及应用两次微积分基本定理。
微积分基本定理证明了积分和微分在某种程度上是互“逆”的。
通过（松散地）考虑方程<span class="math inline">\(u =
g(x)\)</span>的微小变化（微分），即把<span class="math inline">\(\Delta
u = g^{\prime}(x)\, \Delta x\)</span>看作 <span class="math inline">\(u
= g(x)\)</span> 的微分，可以直观地理解这个规则。 将 <span
class="math inline">\(u = g(x)\)</span> 代入，积分 (6.133)
右边的参数变成了 <span class="math inline">\(f(g(x))\)</span>。 通过假设
<span class="math inline">\(\mathrm{d}u \approx \Delta u =
g^{\prime}(x)\, \Delta x,\mathrm{d}x \approx \Delta
x,\)</span>我们最终得到了 (6.133)。</p>
<p><strong>定理</strong> 6.16:</p>
<p>令 <span class="math inline">\(f(\boldsymbol{x})\)</span>
是多变量连续随机变量 <span class="math inline">\(\boldsymbol{X}\)</span>
的概率密度函数。如果向量值函数 <span
class="math inline">\(\boldsymbol{y} =
U(\boldsymbol{x})\)</span>在定义域内对于所有 <span
class="math inline">\(\boldsymbol{x}\)</span>
可微且可逆，那么对应的随机变量 <span
class="math inline">\(\boldsymbol{Y} =
U(\boldsymbol{X})\)</span>的概率密度函数由下式给出： <span
class="math display">\[
f(\boldsymbol{y}) = f_{\boldsymbol{x}}\bigl(U^{-1}(\boldsymbol{y})\bigr)
\cdot \left|\frac{d}{d \boldsymbol{y}} U^{-1}(\boldsymbol{y})\right|,
\]</span> 其中 <span class="math inline">\(\left|\frac{d}{d
\boldsymbol{y}} U^{-1}(\boldsymbol{y})\right|\)</span>
表示雅可比矩阵的行列式的绝对值。</p>
<p>这个定理的关键是多元随机变量的变量替换遵循单变量变量替换的过程。首先需要求出逆变换
<span class="math inline">\(U^{-1}\)</span>，并将其代入 <span
class="math inline">\(\boldsymbol{x}\)</span> 的密度函数 <span
class="math inline">\(f_{\boldsymbol{x}}(\boldsymbol{x})\)</span>
中，然后计算雅可比矩阵的行列式，并与密度函数相乘得到结果。</p>
<p>全书的读书笔记（共7篇）如下，可点击此链接直接跳转：<br />
<a href="/2025/12/05/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%B8%80/" title="《机器学习的数学基础》（1&#x2F;7）">《机器学习的数学基础》读书笔记之一 ：导言</a><br />
<a href="/2025/10/29/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%BA%8C/" title="《机器学习的数学基础》（2&#x2F;7）">《机器学习的数学基础》读书笔记之二 ：线性代数</a><br />
<a href="/2025/10/28/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%B8%89/" title="《机器学习的数学基础》（3&#x2F;7）">《机器学习的数学基础》读书笔记之三 ：解析几何</a><br />
<a href="/2025/10/27/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%9B%9B/" title="《机器学习的数学基础》（4&#x2F;7）">《机器学习的数学基础》读书笔记之四 ：矩阵分解</a><br />
<a href="/2025/10/26/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%BA%94/" title="《机器学习的数学基础》（5&#x2F;7）">《机器学习的数学基础》读书笔记之五 ：向量微积分</a><br />
<a href="/2025/10/25/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%85%AD/" title="《机器学习的数学基础》（6&#x2F;7）">《机器学习的数学基础》读书笔记之六 ：概率与分布</a><br />
<a href="/2025/10/24/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%B8%83/" title="《机器学习的数学基础》（7&#x2F;7）">《机器学习的数学基础》读书笔记之七 ：连续优化</a></p>

    </div>

    
    
    

    <footer class="post-footer">

        


          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2025/07/27/ESL%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/" rel="prev" title="ESL读书笔记">
                  <i class="fa fa-chevron-left"></i> ESL读书笔记
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2025/10/26/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%BA%94/" rel="next" title="《机器学习的数学基础》（5/7）">
                  《机器学习的数学基础》（5/7） <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Rayman.hung</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
<script src="https://cdn.jsdelivr.net/npm/hexo-generator-searchdb@1.4.0/dist/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js","integrity":"sha256-r+3itOMtGGjap0x+10hu6jW/gZCzxHsoKrOd7gyRSGY="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>

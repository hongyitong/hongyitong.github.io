<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.1.1">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" integrity="sha256-DfWjNxDkM94fVBWx1H5BMMp0Zq7luBlV8QRcSES7s+0=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"hongyitong.github.io","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.11.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="5、向量微积分 Vector Calculus  函数的梯度方向指向最陡峭的上升方向，而不是导数本身。导数是标量，没有方向性；梯度才是决定函数与曲面上升方向的向量。理解这一点有助于区分函数与其图像（曲面）之间的关系。详见《导数和梯度的概念.md》  5.1 泰勒级数 泰勒级数是函数\(f\)的无穷项和的表示。这些项是用\(f\)的导数来确定的。多项式逼近函数的泰勒级数。 泰勒级数 对于一个平滑的函">
<meta property="og:type" content="article">
<meta property="og:title" content="《机器学习的数学基础》（5&#x2F;7）">
<meta property="og:url" content="http://hongyitong.github.io/2025/10/26/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%BA%94/index.html">
<meta property="og:site_name" content="墨语浮生">
<meta property="og:description" content="5、向量微积分 Vector Calculus  函数的梯度方向指向最陡峭的上升方向，而不是导数本身。导数是标量，没有方向性；梯度才是决定函数与曲面上升方向的向量。理解这一点有助于区分函数与其图像（曲面）之间的关系。详见《导数和梯度的概念.md》  5.1 泰勒级数 泰勒级数是函数\(f\)的无穷项和的表示。这些项是用\(f\)的导数来确定的。多项式逼近函数的泰勒级数。 泰勒级数 对于一个平滑的函">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Part1/%E5%8F%98%E6%8D%A2.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Part1/%E5%81%8F%E5%AF%BC%E6%95%B0%E7%9A%84%E7%BB%B4%E5%BA%A6.png">
<meta property="article:published_time" content="2025-10-25T16:00:00.000Z">
<meta property="article:modified_time" content="2025-12-12T03:21:31.958Z">
<meta property="article:author" content="Rayman.hung">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://hongyitong.github.io/img3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Part1/%E5%8F%98%E6%8D%A2.png">


<link rel="canonical" href="http://hongyitong.github.io/2025/10/26/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%BA%94/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://hongyitong.github.io/2025/10/26/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%BA%94/","path":"2025/10/26/《机器学习的数学基础》读书笔记之五/","title":"《机器学习的数学基础》（5/7）"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>《机器学习的数学基础》（5/7） | 墨语浮生</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-ZXGEJDXQ33"></script>
  <script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"G-ZXGEJDXQ33","only_pageview":false}</script>
  <script src="/js/third-party/analytics/google-analytics.js"></script>





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">墨语浮生</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Rayman</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section">首页</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section">分类</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section">关于</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section">归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%90%91%E9%87%8F%E5%BE%AE%E7%A7%AF%E5%88%86"><span class="nav-text">5、向量微积分</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B3%B0%E5%8B%92%E7%BA%A7%E6%95%B0"><span class="nav-text">5.1 泰勒级数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BE%AE%E5%88%86%E6%B3%95%E5%88%99"><span class="nav-text">5.2 微分法则</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%81%8F%E5%BE%AE%E5%88%86%E4%B8%8E%E6%A2%AF%E5%BA%A6"><span class="nav-text">5.3 偏微分与梯度</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99"><span class="nav-text">5.3.1 链式法则</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A4%9A%E5%8F%98%E9%87%8F%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99"><span class="nav-text">5.3.2 多变量链式法则</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%91%E9%87%8F%E5%80%BC%E5%87%BD%E6%95%B0%E7%9A%84%E6%A2%AF%E5%BA%A6"><span class="nav-text">5.4 向量值函数的梯度</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E4%B8%8E%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86%E6%B3%95"><span class="nav-text">5.4.1 反向传播与自动微分法</span></a></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Rayman.hung</p>
  <div class="site-description" itemprop="description">技术分享、读书心得、心情记录</div>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://hongyitong.github.io/2025/10/26/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%BA%94/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Rayman.hung">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="墨语浮生">
      <meta itemprop="description" content="技术分享、读书心得、心情记录">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="《机器学习的数学基础》（5/7） | 墨语浮生">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          《机器学习的数学基础》（5/7）
        </h1>

        </h1>
          
             <p class="post-subtitle">读书笔记之五：向量微积分</p>
          
        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-10-26 00:00:00" itemprop="dateCreated datePublished" datetime="2025-10-26T00:00:00+08:00">2025-10-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97%EF%BC%88%E8%87%AA%E7%84%B6%E7%A7%91%E5%AD%A6%EF%BC%89/" itemprop="url" rel="index"><span itemprop="name">读书心得（自然科学）</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h2 id="向量微积分">5、向量微积分</h2>
<p>Vector Calculus</p>
<blockquote>
<p>函数的梯度方向指向最陡峭的上升方向，而不是导数本身。导数是标量，没有方向性；梯度才是决定函数与曲面上升方向的向量。理解这一点有助于区分函数与其图像（曲面）之间的关系。详见《导数和梯度的概念.md》</p>
</blockquote>
<h3 id="泰勒级数">5.1 泰勒级数</h3>
<p>泰勒级数是函数<span
class="math inline">\(f\)</span>的无穷项和的表示。这些项是用<span
class="math inline">\(f\)</span>的导数来确定的。<strong>多项式逼近函数的泰勒级数</strong>。</p>
<p><strong>泰勒级数</strong></p>
<p>对于一个平滑的函数 $ f ^{},  f: $ （$ f ^{} $ 表示 <span
class="math inline">\(f\)</span> 连续且可微无穷多次）， <span
class="math inline">\(f\)</span> 在 <span
class="math inline">\(x_0\)</span> 的泰勒级数(Taylor
series）定义为：<br />
<span class="math display">\[
T_{\infty}(x) = \sum_{k=0}^{\infty} \frac{f^{(k)}\left(x_{0}\right)}{k
!} \left(x - x_{0}\right)^{k}
\]</span></p>
<p>当 $ x_0 = 0 $ 时，我们得到麦克劳林级数（Maclaurin
series），它是泰勒级数的特殊实例。 如果 $ f(x) = T_{}(x) $，那么 <span
class="math inline">\(f\)</span> 称为解析的（analytic）。</p>
<span id="more"></span>
<p>泰勒级数中的 <span
class="math inline">\(x_0\)</span>（也叫展开点）<strong>原则上可以选任意实数或复数</strong>，不过能不能用它准确表示函数，还得看<strong>收敛性和函数的可导性</strong>条件。</p>
<p><strong>注1</strong>： 一般来说，$ n $
次泰勒多项式是非多项式函数的近似值。 它在 <span
class="math inline">\(x_0\)</span> 附近与 <span
class="math inline">\(f\)</span> 相似。 然而，$ n $ 次泰勒多项式用 $ k n
$ 次多项式表示 <span class="math inline">\(f\)</span> 已经足够精确了，
因为导数 $ f^{(i)},  i &gt; k $ 可能为 <span
class="math inline">\(0\)</span>。</p>
<p><strong>注2</strong>： 泰勒级数是幂级数的特例，幂级数表达式为：<br />
<span class="math display">\[
f(x) = \sum_{k=0}^{\infty} a_{k} (x - c)^{k}
\]</span> 其中 <span class="math inline">\(a_k\)</span>为系数，<span
class="math inline">\(c\)</span>为常数。定义5.4中的式子是它的特殊形式。</p>
<p><strong>注3</strong>：组合符号 <span
class="math inline">\(C_n^m\)</span> 也可写成 <span
class="math inline">\(\binom{n}{m}\)</span>。</p>
<h3 id="微分法则">5.2 微分法则</h3>
<p>下面，我们用 $ f' $ 表示 <span class="math inline">\(f\)</span>
的导数，简要地说明基本的微分规则：</p>
<ol type="1">
<li>乘积法则：</li>
</ol>
<p><span class="math display">\[
(f(x) g(x))&#39; = f&#39;(x) g(x) + f(x) g&#39;(x)
\]</span></p>
<ol start="2" type="1">
<li>除法法则：</li>
</ol>
<p><span class="math display">\[
\left(\frac{f(x)}{g(x)}\right)&#39; = \frac{f&#39;(x) g(x) - f(x)
g&#39;(x)}{\left(g(x)\right)^{2}}
\]</span></p>
<ol start="3" type="1">
<li>加法法则：</li>
</ol>
<p><span class="math display">\[
(f(x) + g(x))&#39; = f&#39;(x) + g&#39;(x)
\]</span></p>
<ol start="4" type="1">
<li>链式法则：</li>
</ol>
<p><span class="math display">\[
(g(f(x)))&#39; = (g \circ f)&#39;(x) = g&#39;(f(x)) \cdot f&#39;(x)
\]</span></p>
<p>​ 这里，$ g f $ 表示函数复合：<br />
<span class="math display">\[
x \mapsto f(x) \mapsto g(f(x))
\]</span></p>
<h3 id="偏微分与梯度">5.3 偏微分与梯度</h3>
<p><strong>导数对多变量函数的推广是梯度(gradient)。</strong></p>
<p>我们通过一次改变一个变量并保持其他变量不变来求函数<span
class="math inline">\(f\)</span>相对于<span
class="math inline">\(x\)</span>的偏导数。<strong>梯度就是这些偏导数</strong>(partial
derivatives)构成的的集合。</p>
<p>将它们组成一个<strong>行向量</strong>： <span class="math display">\[
  \nabla_{\boldsymbol{x}} f = \operatorname{grad} f
  = \frac{\mathrm{d} f}{\mathrm{d} \boldsymbol{x}}
  = \left[
  \frac{\partial f(\boldsymbol{x})}{\partial x_{1}},\
  \frac{\partial f(\boldsymbol{x})}{\partial x_{2}},\
  \cdots,\
  \frac{\partial f(\boldsymbol{x})}{\partial x_{n}}
  \right]
  \in \mathbb{R}^{1 \times n}
  \qquad (5.40)
\]</span></p>
<p>其中 $ n $ 是变量个数，<span class="math inline">\(1\)</span> 是
<span class="math inline">\(f\)</span> 的像/值域/陪域的维数。
这里，我们定义了列向量<span class="math inline">\(\boldsymbol{x} =
\left[ x_{1}, \ldots, x_{n} \right]^{\top} \in \mathbb{R}^n.\)</span> 式
(5.40) 中的行向量称为 <span class="math inline">\(f\)</span>
的梯度(gradient) 或 雅可比矩阵(Jacobian)，是第 5.1 节中导数的推广。</p>
<p><strong>备注（梯度用行向量表示）：</strong></p>
<p>向量通常用列向量表示，将梯度向量定义为列向量在文献中并不少见。
我们将梯度向量定义为行向量的原因有两个：
首先，我们可以一致地将梯度推广到向量值函数<span class="math inline">\(f:
\mathbb{R}^{n} \rightarrow \mathbb{R}^{m}\)</span>（然后梯度变成矩阵）。
其次，我们可以很方便地应用多变量链式法则，而不必注意梯度的维数。</p>
<h4 id="链式法则">5.3.1 链式法则</h4>
<p>考虑两个变量 <span class="math inline">\(x_{1}\)</span>, <span
class="math inline">\(x_{2}\)</span> 的函数<span
class="math inline">\(f: \mathbb{R}^{2} \rightarrow
\mathbb{R}.\)</span>此外，<span class="math inline">\(x_{1}(t)\)</span>
和 <span class="math inline">\(x_{2}(t)\)</span> 本身就是 <span
class="math inline">\(t\)</span> 的函数。 为了计算 <span
class="math inline">\(f\)</span> 相对于 <span
class="math inline">\(t\)</span>
的导数，我们需要对多元函数使用链式法则： <span class="math display">\[
\frac{\mathrm{d} f}{\mathrm{d} t}
=
\begin{bmatrix}
\frac{\partial f}{\partial x_{1}} &amp; \frac{\partial f}{\partial
x_{2}}
\end{bmatrix}
\begin{bmatrix}
\frac{\partial x_{1}(t)}{\partial t} \\
\frac{\partial x_{2}(t)}{\partial t}
\end{bmatrix}
=
\frac{\partial f}{\partial x_{1}} \frac{\partial x_{1}}{\partial t}
+
\frac{\partial f}{\partial x_{2}} \frac{\partial x_{2}}{\partial t}.
\]</span> <strong>其中 <span class="math inline">\(\mathrm{d}\)</span>
表示全导数，<span class="math inline">\(\partial\)</span>
表示偏导数。</strong></p>
<h4 id="多变量链式法则">5.3.2 多变量链式法则</h4>
<p>如果 <span class="math inline">\(f(x_{1}, x_{2})\)</span> 是 <span
class="math inline">\(x_{1}\)</span> 和 <span
class="math inline">\(x_{2}\)</span> 的函数，其中 <span
class="math inline">\(x_{1}(s, t)\)</span> 和 <span
class="math inline">\(x_{2}(s, t)\)</span> 是两个变量 <span
class="math inline">\(s\)</span> 和 <span
class="math inline">\(t\)</span> 的函数，则用链式法则可得偏导数： <span
class="math display">\[
\frac{\partial f}{\partial s}
=
\frac{\partial f}{\partial x_{1}} \frac{\partial x_{1}}{\partial s}
+
\frac{\partial f}{\partial x_{2}} \frac{\partial x_{2}}{\partial s},
\]</span></p>
<p><span class="math display">\[
\frac{\partial f}{\partial t}
=
\frac{\partial f}{\partial x_{1}} \frac{\partial x_{1}}{\partial t}
+
\frac{\partial f}{\partial x_{2}} \frac{\partial x_{2}}{\partial t}.
\]</span></p>
<p>将梯度写成矩阵乘法的形式为： <span class="math display">\[
\frac{\mathrm{d} f}{\mathrm{d}(s, t)}
=
\frac{\partial f}{\partial \boldsymbol{x}}
\frac{\partial \boldsymbol{x}}{\partial (s, t)}
=
\underbrace{
\begin{bmatrix}
\frac{\partial f}{\partial x_{1}} &amp; \frac{\partial f}{\partial
x_{2}}
\end{bmatrix}
}_{\frac{\partial f}{\partial \boldsymbol{x}}}
\underbrace{
\begin{bmatrix}
\frac{\partial x_{1}}{\partial s} &amp; \frac{\partial x_{1}}{\partial
t} \\
\frac{\partial x_{2}}{\partial s} &amp; \frac{\partial x_{2}}{\partial
t}
\end{bmatrix}
}_{\frac{\partial \boldsymbol{x}}{\partial (s, t)}}.
\]</span></p>
<p><strong>这种将链式法则写成矩阵乘法的简洁方法，只有在将梯度定义为行向量时才直接成立。</strong>否则，需要对矩阵进行转置以匹配维数。当对象是向量或矩阵时，转置很简单；但当对象是张量时（将在后面讨论），转置就不再是小事了。</p>
<h3 id="向量值函数的梯度">5.4 向量值函数的梯度</h3>
<p><strong>雅可比矩阵</strong></p>
<p><strong>向量值函数</strong>的所有一阶偏导数的集合称为雅可比矩阵（Jacobian）。雅可比矩阵
<span class="math inline">\(\mathbf{J}\)</span> 是一个 <span
class="math inline">\(m \times n\)</span> 矩阵，我们将其定义如下： <span
class="math display">\[
\mathbf{J}
= \nabla_{\mathbf{x}} \mathbf{f}
= \frac{\mathrm{d} \mathbf{f}(\mathbf{x})}{\mathrm{d} \mathbf{x}}
= \left[
\frac{\partial \mathbf{f}(\mathbf{x})}{\partial x_{1}} \quad
\cdots \quad
\frac{\partial \mathbf{f}(\mathbf{x})}{\partial x_{n}}
\right].
\]</span></p>
<p>具体写成矩阵形式为： <span class="math display">\[
\begin{equation}
\mathbf{J} =
\begin{bmatrix}
\dfrac{\partial f_{1}(\mathbf{x})}{\partial x_{1}} &amp; \cdots &amp;
\dfrac{\partial f_{1}(\mathbf{x})}{\partial x_{n}} \\
\vdots &amp; \ddots &amp; \vdots \\
\dfrac{\partial f_{m}(\mathbf{x})}{\partial x_{1}} &amp; \cdots &amp;
\dfrac{\partial f_{m}(\mathbf{x})}{\partial x_{n}}
\end{bmatrix},
\tag{5.58}
\end{equation}
\]</span> 其中 <span class="math display">\[
\mathbf{x} =
\begin{bmatrix}
x_{1} \\ \vdots \\ x_{n}
\end{bmatrix},
\quad
J(i,j) = \frac{\partial f_{i}}{\partial x_{j}}.
\]</span></p>
<p>雅可比矩阵表示我们想要的坐标变换。如果坐标变换是线性的（如我们的例子），那么它是精确的，（5.66）精确地恢复了（5.62）中的基变化矩阵。如果坐标变换是非线性的，雅可比矩阵则用一个线性变换局部地逼近这个非线性变换。雅可比行列式
<span
class="math inline">\(|\operatorname{det}(\boldsymbol{J})|\)</span>
的绝对值是变换坐标时面积或体积的缩放因子。</p>
<p><img src="/img3/机器学习的数学基础Part1/变换.png" alt="变换" style="zoom:50%;" /></p>
<p><strong>方法</strong>一、</p>
<p>为了开始使用线性代数的方法，我们首先确定 <span
class="math inline">\(\{\boldsymbol{b}_{1},
\boldsymbol{b}_{2}\}\)</span> 和 <span
class="math inline">\(\{\boldsymbol{c}_{1},
\boldsymbol{c}_{2}\}\)</span> 都是 <span
class="math inline">\(\mathbb{R}^2\)</span> 的基。我们要有效地执行的是从
<span class="math inline">\(\{\boldsymbol{b}_{1},
\boldsymbol{b}_{2}\}\)</span> 到 <span
class="math inline">\(\{\boldsymbol{c}_{1},
\boldsymbol{c}_{2}\}\)</span> 的基变换，就得寻找实现基变换的变换矩阵。
利用第2.7.2节的结果，我们确定了所需的<strong>基变换矩阵</strong>为：
<span class="math display">\[
\boldsymbol{J} =
\begin{bmatrix}
-2 &amp; 1 \\
1 &amp; 1
\end{bmatrix} \tag{5.62}
\]</span></p>
<p>它使得 <span class="math inline">\(\boldsymbol{J}\boldsymbol{b}_{1} =
\boldsymbol{c}_{1}, \quad
\boldsymbol{J}\boldsymbol{b}_{2} = \boldsymbol{c}_{2}\)</span>。 矩阵
<span class="math inline">\(\boldsymbol{J}\)</span> 的行列式的绝对值为：
<span class="math display">\[
\left|\det(\boldsymbol{J})\right| = 3,
\]</span> 这正是我们在寻找的缩放因子。也就是说， <span
class="math inline">\((\boldsymbol{c}_{1}, \boldsymbol{c}_{2})\)</span>
所张成的平行四边形的面积是 <span
class="math inline">\((\boldsymbol{b}_{1}, \boldsymbol{b}_{2})\)</span>
所张成面积的三倍。</p>
<p>把 <span class="math inline">\(\{\boldsymbol{b}_{1},
\boldsymbol{b}_{2}\}\)</span> 坐标变换成 <span
class="math inline">\(\{\boldsymbol{c}_{1},
\boldsymbol{c}_{2}\}\)</span> 的坐标，是左乘<span
class="math inline">\(\boldsymbol{J}^{-1}\)</span>；反向是左乘<span
class="math inline">\(\boldsymbol{J}\)</span>。</p>
<p><strong>方法二</strong>、</p>
<p>线性代数方法适用于线性变换；对于非线性变换（与第6.7节有关），我们有基于偏微分的更一般的方法。</p>
<p>在这种方法中，我们考虑执行变量变换的函数<span
class="math inline">\(\boldsymbol{f}: \mathbb{R}^{2} \rightarrow
\mathbb{R}^{2}.\)</span>在我们的例子中，<span
class="math inline">\(\boldsymbol{f}\)</span> 将关于 <span
class="math inline">\((\boldsymbol{b}_{1}, \boldsymbol{b}_{2})\)</span>
的任意向量 <span class="math inline">\(\boldsymbol{x} \in
\mathbb{R}^{2}\)</span> 的坐标映射到关于 <span
class="math inline">\((\boldsymbol{c}_{1}, \boldsymbol{c}_{2})\)</span>
的坐标 <span class="math inline">\(\boldsymbol{y} \in
\mathbb{R}^{2}\)</span>。
我们想确定这个映射，这样就可以计算出一个面积（或体积）在 <span
class="math inline">\(\boldsymbol{f}\)</span> 变换下是如何变化的。
为此，我们需要找出 <span
class="math inline">\(\boldsymbol{f}(\boldsymbol{x})\)</span> 在 <span
class="math inline">\(\boldsymbol{x}\)</span> 微小变化时的变化方式。
雅可比矩阵 <span class="math display">\[
\frac{\mathrm{d} \boldsymbol{f}}{\mathrm{d} \boldsymbol{x}} \in
\mathbb{R}^{2 \times 2}
\]</span> 正是这个问题的答案。 由下式定义的映射： <span
class="math display">\[
y_{1} = -2x_{1} + x_{2}, \qquad
y_{2} = x_{1} + x_{2},
\]</span> 我们得到了 <span class="math inline">\(\boldsymbol{x}\)</span>
和 <span class="math inline">\(\boldsymbol{y}\)</span> 之间的函数关系。
这允许我们计算偏导数： <span class="math display">\[
\frac{\partial y_{1}}{\partial x_{1}}=-2, \quad
\frac{\partial y_{1}}{\partial x_{2}}=1, \quad
\frac{\partial y_{2}}{\partial x_{1}}=1, \quad
\frac{\partial y_{2}}{\partial x_{2}}=1.
\]</span></p>
<p>将它们组合成雅可比矩阵： <span class="math display">\[
\boldsymbol{J} =
\begin{bmatrix}
\frac{\partial y_{1}}{\partial x_{1}} &amp; \frac{\partial
y_{1}}{\partial x_{2}} \\
\frac{\partial y_{2}}{\partial x_{1}} &amp; \frac{\partial
y_{2}}{\partial x_{2}}
\end{bmatrix}
=
\begin{bmatrix}
-2 &amp; 1 \\
1 &amp; 1
\end{bmatrix} \tag{5.66}
\]</span></p>
<p>雅可比矩阵表示我们想要的坐标变换。
如果坐标变换是线性的（如我们的例子），那么它是精确的，（5.66）正好恢复了（5.62）中的基变化矩阵。
<strong>如果坐标变换是非线性的，雅可比矩阵则用一个线性变换局部地逼近这个非线性变换</strong>。
雅可比行列式的绝对值 <span class="math display">\[
\left|\det(\boldsymbol{J})\right| = 3
\]</span> 就是变换坐标时面积或体积的缩放因子。在我们的例子中，结果正好是
3。</p>
<p><img src="/img3/机器学习的数学基础Part1/偏导数的维度.png" alt="变换" style="zoom:50%;" /></p>
<p><strong>向量值函数的梯度</strong></p>
<p>给定： <span class="math display">\[
\mathbf{f}(\mathbf{x}) = \mathbf{A} \mathbf{x}
\]</span> 其中：</p>
<ul>
<li><span class="math inline">\(\mathbf{f}(\mathbf{x}) \in
\mathbb{R}^M\)</span>（输出是 <span class="math inline">\(M\)</span>
维向量）</li>
<li><span class="math inline">\(\mathbf{A} \in \mathbb{R}^{M \times
N}\)</span></li>
<li><span class="math inline">\(\mathbf{x} \in
\mathbb{R}^N\)</span></li>
</ul>
<p>这是一个<strong>线性映射</strong>：<span
class="math inline">\(\mathbf{x} \mapsto \mathbf{A}
\mathbf{x}\)</span>。</p>
<p>向量值函数的梯度是 <span class="math inline">\(A\)</span>。</p>
<h4 id="反向传播与自动微分法">5.4.1 反向传播与自动微分法</h4>
<p>在许多机器学习应用中，我们通过执行梯度下降（第7.1节）来找到好的模型参数，这基于我们可以计算目标函数相对于模型参数的梯度。对于给定的目标函数，我们可以通过微积分和应用链式法则来获得关于模型参数的梯度；见第5.2.2节。在第5.3节中，我们已经研究了线性回归模型的平方损失函数的梯度。</p>
<p>反向传播是数值分析中称为自动微分(automatic
differentiation)技术的一个特例。我们可以把自动微分看作是一套技术，它通过处理中间变量和应用链式法则，在数值上（而不是符号化地）计算函数的精确（达到机器精度）梯度。自动微分应用了一系列基本算术运算，例如加法和乘法，以及基本函数，例如：<span
class="math inline">\(⁡ \sin , \cos , \exp ,
\log\)</span>将链式法则应用到这些运算中，可以自动计算相当复杂函数的梯度。自动微分适用于一般的计算机程序，有正向和反向两种模式。</p>
<p>直观地说，正向和反向模式在乘法的顺序上是不同的。由于矩阵乘法的结合性，我们有以下两种选择：
<span class="math display">\[
\begin{equation}
\frac{\mathrm{d} y}{\mathrm{d} x} =
\left(
\frac{\mathrm{d} y}{\mathrm{d} b}
\frac{\mathrm{d} b}{\mathrm{d} a}
\right)
\frac{\mathrm{d} a}{\mathrm{d} x}
\qquad (5.120)
\end{equation}
\]</span></p>
<p><span class="math display">\[
\begin{equation}
\frac{\mathrm{d} y}{\mathrm{d} x} =
\frac{\mathrm{d} y}{\mathrm{d} b}
\left(
\frac{\mathrm{d} b}{\mathrm{d} a}
\frac{\mathrm{d} a}{\mathrm{d} x}
\right)
\qquad (5.121)
\end{equation}
\]</span></p>
<p>方程（5.120）是反向模式（reverse mode），因为梯度通过数据流向后传播。
方程（5.121）是正向模式（forward
mode），其中梯度随数据从左到右流过整个图。</p>
<p>下面，我们将重点介绍反向模式的自动微分，即反向传播。</p>
<blockquote>
<p>在神经网络中，输入的维数通常比标签的维数高得多，所以反向模式比正向模式计算量要少得多。</p>
</blockquote>
<p>具体的例子详见《损失函数的梯度计算例子.md》</p>
<p>全书的读书笔记（共7篇）如下，可点击此链接直接跳转：<br />
<a href="/2025/12/05/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%B8%80/" title="《机器学习的数学基础》（1&#x2F;7）">《机器学习的数学基础》读书笔记之一 ：导言</a><br />
<a href="/2025/10/29/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%BA%8C/" title="《机器学习的数学基础》（2&#x2F;7）">《机器学习的数学基础》读书笔记之二 ：线性代数</a><br />
<a href="/2025/10/28/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%B8%89/" title="《机器学习的数学基础》（3&#x2F;7）">《机器学习的数学基础》读书笔记之三 ：解析几何</a><br />
<a href="/2025/10/27/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%9B%9B/" title="《机器学习的数学基础》（4&#x2F;7）">《机器学习的数学基础》读书笔记之四 ：矩阵分解</a><br />
<a href="/2025/10/26/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%BA%94/" title="《机器学习的数学基础》（5&#x2F;7）">《机器学习的数学基础》读书笔记之五 ：向量微积分</a><br />
<a href="/2025/10/25/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%85%AD/" title="《机器学习的数学基础》（6&#x2F;7）">《机器学习的数学基础》读书笔记之六 ：概率与分布</a><br />
<a href="/2025/10/24/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%B8%83/" title="《机器学习的数学基础》（7&#x2F;7）">《机器学习的数学基础》读书笔记之七 ：连续优化</a></p>

    </div>

    
    
    

    <footer class="post-footer">

        


          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2025/10/25/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%85%AD/" rel="prev" title="《机器学习的数学基础》（6/7）">
                  <i class="fa fa-chevron-left"></i> 《机器学习的数学基础》（6/7）
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2025/10/27/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%9B%9B/" rel="next" title="《机器学习的数学基础》（4/7）">
                  《机器学习的数学基础》（4/7） <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Rayman.hung</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
<script src="https://cdn.jsdelivr.net/npm/hexo-generator-searchdb@1.4.0/dist/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js","integrity":"sha256-r+3itOMtGGjap0x+10hu6jW/gZCzxHsoKrOd7gyRSGY="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>

<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.1.1">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" integrity="sha256-DfWjNxDkM94fVBWx1H5BMMp0Zq7luBlV8QRcSES7s+0=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"hongyitong.github.io","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.11.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="3、解析几何 Analytic Geometry 内积及其相应的范数和度量可以得到相似性和距离的直观概念  范数表示向量的长度，范数可以由内积引出（但不必要，例如曼哈顿范数就不是） 内积可以是不同的定义（所以不同的内积定义的长度是不一样的），内积由唯一的对称正定矩阵确定（正定矩阵的定义符合要求）。 内积来计算向量的长度和向量之间的夹角.">
<meta property="og:type" content="article">
<meta property="og:title" content="《机器学习的数学基础》（3&#x2F;7）">
<meta property="og:url" content="http://hongyitong.github.io/2025/10/28/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%B8%89/index.html">
<meta property="og:site_name" content="墨语浮生">
<meta property="og:description" content="3、解析几何 Analytic Geometry 内积及其相应的范数和度量可以得到相似性和距离的直观概念  范数表示向量的长度，范数可以由内积引出（但不必要，例如曼哈顿范数就不是） 内积可以是不同的定义（所以不同的内积定义的长度是不一样的），内积由唯一的对称正定矩阵确定（正定矩阵的定义符合要求）。 内积来计算向量的长度和向量之间的夹角.">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-10-27T16:00:00.000Z">
<meta property="article:modified_time" content="2025-12-12T06:40:24.529Z">
<meta property="article:author" content="Rayman.hung">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://hongyitong.github.io/2025/10/28/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%B8%89/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://hongyitong.github.io/2025/10/28/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%B8%89/","path":"2025/10/28/《机器学习的数学基础》读书笔记之三/","title":"《机器学习的数学基础》（3/7）"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>《机器学习的数学基础》（3/7） | 墨语浮生</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-ZXGEJDXQ33"></script>
  <script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"G-ZXGEJDXQ33","only_pageview":false}</script>
  <script src="/js/third-party/analytics/google-analytics.js"></script>





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">墨语浮生</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Rayman</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section">首页</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section">分类</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section">关于</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section">归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A7%A3%E6%9E%90%E5%87%A0%E4%BD%95"><span class="nav-text">3、解析几何</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%8C%83%E6%95%B0"><span class="nav-text">3.1 范数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%86%85%E7%A7%AF"><span class="nav-text">3.2 内积</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%AD%A3%E5%AE%9A%E7%9F%A9%E9%98%B5"><span class="nav-text">3.3 正定矩阵</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%95%BF%E5%BA%A6%E5%92%8C%E8%B7%9D%E7%A6%BB"><span class="nav-text">3.4 长度和距离</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B7%9D%E7%A6%BB%E5%92%8C%E5%BA%A6%E9%87%8F"><span class="nav-text">3.5 距离和度量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A7%92%E5%BA%A6%E5%92%8C%E6%AD%A3%E4%BA%A4"><span class="nav-text">3.6 角度和正交</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%AD%A3%E4%BA%A4%E7%9F%A9%E9%98%B5"><span class="nav-text">3.7 正交矩阵</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%87%BD%E6%95%B0%E7%9A%84%E5%86%85%E7%A7%AF"><span class="nav-text">3.8 函数的内积</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%AD%A3%E4%BA%A4%E6%8A%95%E5%BD%B1"><span class="nav-text">3.9 正交投影</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%97%8B%E8%BD%AC"><span class="nav-text">3.10 旋转</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Rayman.hung</p>
  <div class="site-description" itemprop="description">技术分享、读书心得、心情记录</div>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://hongyitong.github.io/2025/10/28/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%B8%89/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Rayman.hung">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="墨语浮生">
      <meta itemprop="description" content="技术分享、读书心得、心情记录">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="《机器学习的数学基础》（3/7） | 墨语浮生">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          《机器学习的数学基础》（3/7）
        </h1>

        </h1>
          
             <p class="post-subtitle">读书笔记之三：解析几何</p>
          
        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-10-28 00:00:00" itemprop="dateCreated datePublished" datetime="2025-10-28T00:00:00+08:00">2025-10-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97%EF%BC%88%E8%87%AA%E7%84%B6%E7%A7%91%E5%AD%A6%EF%BC%89/" itemprop="url" rel="index"><span itemprop="name">读书心得（自然科学）</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h2 id="解析几何">3、解析几何</h2>
<p>Analytic Geometry</p>
<p><strong>内积及其相应的范数和度量可以得到相似性和距离的直观概念</strong></p>
<blockquote>
<p>范数表示向量的长度，范数可以由内积引出（但不必要，例如曼哈顿范数就不是）<br />
内积可以是不同的定义（所以不同的内积定义的长度是不一样的），<strong>内积由唯一的对称正定矩阵确定</strong>（正定矩阵的定义符合要求）。<br />
内积来计算向量的长度和向量之间的夹角.</p>
</blockquote>
<span id="more"></span>
<h3 id="范数">3.1 范数</h3>
<p>向量空间 <span class="math inline">\(V\)</span>
的范数是一个指定每个向量 <span class="math inline">\(x\)</span>
的长度的函数:</p>
<p><span class="math display">\[
\|\cdot\|: V \to \mathbb{R}
\]</span></p>
<p><span class="math display">\[
x \mapsto \|x\|
\]</span></p>
<p>并且对于任何 <span class="math inline">\(\lambda \in
\mathbb{R}\)</span> 以及 <span class="math inline">\(x, y \in
V\)</span>，以下成立：</p>
<ul>
<li><strong>绝对一次齐次性</strong> (Absolutely homogeneous)：<span
class="math inline">\(\|\lambda x\| = |\lambda| \|x\|\)</span></li>
<li><strong>三角不等式</strong> (Triangle inequality)：<span
class="math inline">\(\|x + y\| \leq \|x\| + \|y\|\)</span></li>
<li><strong>正定性</strong> (Positive definite)：<span
class="math inline">\(\|x\| \geq 0\)</span> 且 <span
class="math inline">\(\|x\| = 0 \iff x = 0\)</span></li>
</ul>
<p>注意，范数可以是多种，例如<span class="math inline">\(\mathcal
l_1\)</span>范数（<em>曼哈顿范数</em>）、<span
class="math inline">\(\mathcal
l_2\)</span>范数（<em>欧氏距离</em>）。</p>
<h3 id="内积">3.2 内积</h3>
<p>内积可以引入一些直观的几何概念，例如<strong>向量的长度和两个向量之间的角度或距离</strong>。内积的一个主要目的是确定向量之间是否正交。</p>
<p><strong>3.2.1 点积</strong></p>
<p>我们可能已经熟悉了一种特殊类型的内积，<span
class="math inline">\(\mathbb{R}^n\)</span>中的标量积/点积（scalar
product/dot product）：</p>
<p><span class="math display">\[
\mathbf{x}^\top \mathbf{y} = \sum_{i=1}^n x_i y_i \tag{3.5}
\]</span></p>
<p>在这本书中，我们将<strong>把这种特殊的内积称为点积</strong>。但是，<strong>内积是具有特定性质的更一般的概念</strong>，我们现在将介绍这些概念。</p>
<p><strong>定义 3.3：内积与内积空间</strong></p>
<p>设 <span class="math inline">\(V\)</span> 为向量空间，<span
class="math inline">\(\langle \cdot,\cdot \rangle : V \times V \to
\mathbb{R}\)</span> 为一个双线性映射，即它将 <span
class="math inline">\(V\)</span> 中两个向量映射为一个实数，则有：</p>
<ul>
<li>若 <span class="math inline">\(\langle \cdot,\cdot \rangle\)</span>
是<strong>正定且对称</strong>的，则称其为 <span
class="math inline">\(V\)</span> 上的内积，常记作 <span
class="math inline">\(\langle x, y \rangle\)</span>；</li>
<li>则 <span class="math inline">\((V, \langle \cdot,\cdot
\rangle)\)</span> 被称为一个内积空间（或带内积的实向量空间）；</li>
<li><strong>若该内积为点积，则称 <span class="math inline">\((V, \langle
\cdot,\cdot \rangle)\)</span> 为一个欧氏向量空间</strong>。</li>
</ul>
<p><strong>注1</strong>：本书将上述所有空间统称为“内积空间”。<br />
<strong>注2</strong>：点积只是内积的一种特殊的定义，而非唯一。<br />
<strong>注3</strong>，正定性是指<span class="math inline">\(\forall
\boldsymbol{x} \in V \setminus \{\mathbf{0}\}:\quad
\boldsymbol{x}^{\top} \boldsymbol{A} \boldsymbol{x} &gt;
0.\)</span>,而不是：<span
class="math inline">\(\boldsymbol{x}^T\boldsymbol{A} \boldsymbol{y} &gt;
0.\)</span></p>
<h3 id="正定矩阵">3.3 正定矩阵</h3>
<blockquote>
<p>正定矩阵一定是对称的。《正定矩阵和半正定矩阵的区别.md》<br />
<span class="math inline">\(A\)</span> 正定 <span
class="math inline">\(\iff\)</span> 所有特征值 <span
class="math inline">\(\lambda_i &gt; 0\)</span>。（故行列式也大于0）</p>
</blockquote>
<p><strong>对称正定矩阵</strong></p>
<p><strong><u><em>对称正定矩阵在机器学习中起着重要的作用，它们是通过内积定义的</em></u></strong>。在
4.3
节矩阵分解中将涉及到对称正定矩阵。对称半正定矩阵的思想也是机器学习中核技巧的关键（12.4
节）。</p>
<p>考虑一个 <span class="math inline">\(n\)</span> 维向量空间 <span
class="math inline">\(V\)</span> 和内积 <span class="math display">\[
\langle \cdot, \cdot \rangle : V \times V \to \mathbb{R}
\]</span> （见定义 3.3），以及 <span class="math inline">\(V\)</span>
的有序基 <span class="math display">\[
B = (\boldsymbol{b}_{1}, \ldots, \boldsymbol{b}_{n}).
\]</span> 对于合适的 <span class="math inline">\(\psi_i, \lambda_j \in
\mathbb{R}\)</span>，任何向量 <span
class="math inline">\(\boldsymbol{x}, \boldsymbol{y} \in V\)</span>
都可以写成基向量的线性组合： <span class="math display">\[
\boldsymbol{x} = \sum_{i=1}^{n} \psi_i \boldsymbol{b}_i \in V,
\qquad
\boldsymbol{y} = \sum_{j=1}^{n} \lambda_j \boldsymbol{b}_j \in V.
\]</span></p>
<p>由于内积的双线性，对于所有 <span
class="math inline">\(\boldsymbol{x}, \boldsymbol{y} \in
V\)</span>，有： <span class="math display">\[
\langle \boldsymbol{x}, \boldsymbol{y} \rangle
= \left\langle \sum_{i=1}^{n} \psi_i \boldsymbol{b}_i,\ \sum_{j=1}^{n}
\lambda_j \boldsymbol{b}_j \right\rangle
= \sum_{i=1}^{n}\sum_{j=1}^{n} \psi_i \langle \boldsymbol{b}_i,
\boldsymbol{b}_j \rangle \lambda_j
= \hat{\boldsymbol{x}}^{\top} \boldsymbol{A} \hat{\boldsymbol{y}}.
\]</span></p>
<p>以 <span class="math inline">\(n = 2\)</span> 为例，考虑内积： <span
class="math display">\[
\begin{aligned}
\left\langle \sum_{i=1}^{2} \psi_i \boldsymbol{b}_i, \sum_{j=1}^{2}
\lambda_j \boldsymbol{b}_j \right\rangle  &amp; = \left\langle \psi_1
\boldsymbol{b}_1 + \psi_2 \boldsymbol{b}_2,\ \lambda_1 \boldsymbol{b}_1
+ \lambda_2 \boldsymbol{b}_2 \right\rangle \\
&amp; = \psi_1 \left\langle \boldsymbol{b}_1, \lambda_1 \boldsymbol{b}_1
+ \lambda_2 \boldsymbol{b}_2 \right\rangle
  + \psi_2 \left\langle \boldsymbol{b}_2, \lambda_1 \boldsymbol{b}_1 +
\lambda_2 \boldsymbol{b}_2 \right\rangle.
  \end{aligned}
\]</span></p>
<p>展开后得到： <span class="math display">\[
\begin{aligned}
&amp; \psi_1 [ \lambda_1 \langle \boldsymbol{b}_1, \boldsymbol{b}_1
\rangle + \lambda_2 \langle \boldsymbol{b}_1, \boldsymbol{b}_2 \rangle ]
+ \psi_2 [ \lambda_1 \langle \boldsymbol{b}_2, \boldsymbol{b}_1 \rangle
+ \lambda_2 \langle \boldsymbol{b}_2, \boldsymbol{b}_2 \rangle ] \\
&amp;= [\psi_1, \psi_2]
\begin{bmatrix}
\langle \boldsymbol{b}_1, \boldsymbol{b}_1 \rangle &amp; \langle
\boldsymbol{b}_1, \boldsymbol{b}_2 \rangle \\
\langle \boldsymbol{b}_2, \boldsymbol{b}_1 \rangle &amp; \langle
\boldsymbol{b}_2, \boldsymbol{b}_2 \rangle
\end{bmatrix}
\begin{bmatrix}
\lambda_1 \\ \lambda_2
\end{bmatrix} \\
&amp;= [\psi_1, \psi_2]\, \boldsymbol{A} \begin{bmatrix} \lambda_1 \\
\lambda_2 \end{bmatrix},
\end{aligned}
\]</span> 其中 <span class="math inline">\(A_{ij} := \langle
\boldsymbol{b}_i, \boldsymbol{b}_j \rangle\)</span>，<span
class="math inline">\(\hat{\boldsymbol{x}},
\hat{\boldsymbol{y}}\)</span> 为 <span
class="math inline">\(\boldsymbol{x}, \boldsymbol{y}\)</span> 相对于基
<span class="math inline">\(B\)</span> 的坐标。这意味着内积 <span
class="math inline">\(\langle \cdot, \cdot \rangle\)</span> 由 <span
class="math inline">\(\boldsymbol{A}\)</span>
唯一确定。由于内积是对称的，矩阵 <span
class="math inline">\(\boldsymbol{A}\)</span>
也是对称的。此外，内积的正定性意味着： <span class="math display">\[
\forall \boldsymbol{x} \in V \setminus \{\mathbf{0}\}:\quad
\boldsymbol{x}^{\top} \boldsymbol{A} \boldsymbol{x} &gt; 0.
\]</span></p>
<p><strong>定义 3.4 对称正定矩阵</strong></p>
<p>一个对称矩阵 <span class="math inline">\(\boldsymbol{A} \in
\mathbb{R}^{n \times n}\)</span> 称为 对称正定矩阵（symmetric, positive
definite），如果它满足： <span class="math display">\[
\forall \boldsymbol{x} \in V \setminus \{\mathbf{0}\} : \quad
\boldsymbol{x}^{\top} \boldsymbol{A} \boldsymbol{x} &gt; 0.
\]</span></p>
<p>如果只满足 <span class="math display">\[
\forall \boldsymbol{x} \in V \setminus \{\mathbf{0}\} : \quad
\boldsymbol{x}^{\top} \boldsymbol{A} \boldsymbol{x} \ge 0,
\]</span> 则称 <span class="math inline">\(\boldsymbol{A}\)</span>
为对称半正定矩阵（symmetric, positive semidefinite）。</p>
<p><strong>例 3.4 对称正定矩阵</strong></p>
<p>考虑矩阵 <span class="math display">\[
\boldsymbol{A}_1 =
\begin{bmatrix} 9 &amp; 6 \\ 6 &amp; 5 \end{bmatrix}, \quad
\boldsymbol{A}_2 =
\begin{bmatrix} 9 &amp; 6 \\ 6 &amp; 3 \end{bmatrix}.
\]</span></p>
<p>对于任意 <span class="math inline">\(\boldsymbol{x} = [x_1, x_2]^\top
\neq \mathbf{0}\)</span>，有 <span class="math display">\[
\begin{aligned}
\boldsymbol{x}^{\top} \boldsymbol{A}_1 \boldsymbol{x}
&amp; = [x_1, x_2]
\begin{bmatrix} 9 &amp; 6 \\ 6 &amp; 5 \end{bmatrix}
\begin{bmatrix} x_1 \\ x_2 \end{bmatrix}  \\
&amp; = 9 x_1^2 + 12 x_1 x_2 + 5 x_2^2  \\
&amp;= (3 x_1 + 2 x_2)^2 + x_2^2 &gt; 0,
\end{aligned}
\]</span> 因此 <span class="math inline">\(\boldsymbol{A}_1\)</span>
是对称且正定的。而</p>
<p><span class="math display">\[
\boldsymbol{x}^{\top} \boldsymbol{A}_2 \boldsymbol{x}
= 9 x_1^2 + 12 x_1 x_2 + 3 x_2^2
= (3 x_1 + 2 x_2)^2 - x_2^2,
\]</span> 当 <span class="math inline">\(\boldsymbol{x} = [2,
-3]^\top\)</span> 时，<span class="math inline">\(\boldsymbol{x}^{\top}
\boldsymbol{A}_2 \boldsymbol{x} &lt; 0\)</span>，所以 <span
class="math inline">\(\boldsymbol{A}_2\)</span>
仅对称，但不是正定矩阵。</p>
<p><strong>如果 <span class="math inline">\(\boldsymbol{A} \in
\mathbb{R}^{n \times n}\)</span> 是对称正定的，则可以定义内积：</strong>
<span class="math display">\[
\langle \boldsymbol{x}, \boldsymbol{y} \rangle =
\hat{\boldsymbol{x}}^\top \boldsymbol{A} \hat{\boldsymbol{y}},
\]</span> 其中 <span class="math inline">\(\hat{\boldsymbol{x}},
\hat{\boldsymbol{y}}\)</span> 为 <span
class="math inline">\(\boldsymbol{x}, \boldsymbol{y}\)</span>
相对于有序基 <span class="math inline">\(B\)</span> 的坐标。</p>
<p><strong>定理 3.5 对称正定矩阵与内积</strong></p>
<p>设 <span class="math inline">\(V\)</span>
是一个实值、有限维向量空间，<span class="math inline">\(B\)</span> 是
<span class="math inline">\(V\)</span> 的一个有序基： <span
class="math display">\[
B = (\boldsymbol{b}_1, \ldots, \boldsymbol{b}_n),
\]</span> 并且 <span class="math inline">\(\langle \cdot , \cdot \rangle
: V \times V \to \mathbb{R}\)</span> 是 <span
class="math inline">\(V\)</span> 上的一个内积。</p>
<p><strong>定理：</strong>内积 <span class="math inline">\(\langle \cdot
, \cdot \rangle\)</span> 当且仅当存在一个对称正定矩阵 <span
class="math inline">\(\boldsymbol{A} \in \mathbb{R}^{n \times
n}\)</span>，使得对于任意 <span class="math inline">\(\boldsymbol{x},
\boldsymbol{y} \in V\)</span>，有： <span class="math display">\[
\langle \boldsymbol{x}, \boldsymbol{y} \rangle =
\hat{\boldsymbol{x}}^\top \boldsymbol{A} \hat{\boldsymbol{y}},
\]</span> 其中 <span class="math inline">\(\hat{\boldsymbol{x}},
\hat{\boldsymbol{y}}\)</span> 为 <span
class="math inline">\(\boldsymbol{x}, \boldsymbol{y}\)</span> 相对于基
<span class="math inline">\(B\)</span> 的坐标向量。</p>
<p><strong>如果 <span class="math inline">\(\boldsymbol{A}\)</span>
是对称正定矩阵，则它具有以下性质：</strong></p>
<ul>
<li><p>零空间（核）只包含零向量： 对于所有 <span
class="math inline">\(\boldsymbol{x} \neq \mathbf{0}\)</span>，有 <span
class="math display">\[
    \boldsymbol{x}^\top \boldsymbol{A} \boldsymbol{x} &gt; 0,
\]</span> ​ 这意味着如果 <span class="math inline">\(\boldsymbol{x} \neq
\mathbf{0}\)</span>，则 <span class="math inline">\(\boldsymbol{A}
\boldsymbol{x} \neq \mathbf{0}\)</span>。</p></li>
<li><p><strong>对角元素为正</strong>： 矩阵 <span
class="math inline">\(\boldsymbol{A}\)</span> 的对角元素 <span
class="math inline">\(a_{ii}\)</span> 满足 <span class="math display">\[
a_{ii} = \boldsymbol{e}_i^\top \boldsymbol{A} \boldsymbol{e}_i &gt; 0,
\]</span> 其中 <span class="math inline">\(\boldsymbol{e}_i\)</span> 是
<span class="math inline">\(\mathbb{R}^n\)</span> 的标准基向量</p></li>
</ul>
<h3 id="长度和距离">3.4 长度和距离</h3>
<p>在第3.1节中，我们已经讨论了可以用来计算向量长度的范数。<strong><u><em>内积与范数密切相关，因为任何内积都自然地引出范数</em></u></strong>：
<span class="math display">\[
\|\boldsymbol{x}\| := \sqrt{\langle \boldsymbol{x}, \boldsymbol{x}
\rangle}
\]</span>
这使得我们可以用内积来计算向量的长度。<strong><em><u>然而，并不是每一个范数都是由内积引起的。曼哈顿范数就是一种没有对应内积的范数</u></em></strong>。在下面，<strong>我们将集中讨论由内积导出的范数，并介绍一些几何概念，如长度、距离和角度。</strong></p>
<p><strong>柯西-施瓦兹不等式</strong></p>
<p>对于一个内积向量空间 <span class="math inline">\((V, \langle \cdot,
\cdot \rangle)\)</span>，其引出的范数 <span
class="math inline">\(\|\cdot\|\)</span>
满足<strong>柯西-施瓦兹不等式（Cauchy-Schwarz Inequality）</strong>：
<span class="math display">\[
|\langle \boldsymbol{x}, \boldsymbol{y} \rangle| \leqslant
\|\boldsymbol{x}\| \, \|\boldsymbol{y}\|
\]</span></p>
<h3 id="距离和度量">3.5 距离和度量</h3>
<p>考虑一个内积空间 <span class="math inline">\((V, \langle \cdot, \cdot
\rangle)\)</span>，对于任意 <span class="math inline">\(\boldsymbol{x},
\boldsymbol{y} \in V\)</span>，定义： <span class="math display">\[
d(\boldsymbol{x}, \boldsymbol{y}) := \|\boldsymbol{x} - \boldsymbol{y}\|
= \sqrt{\langle \boldsymbol{x} - \boldsymbol{y}, \boldsymbol{x} -
\boldsymbol{y} \rangle}
\]</span> 该式称为 <span class="math inline">\(\boldsymbol{x}\)</span>
和 <span class="math inline">\(\boldsymbol{y}\)</span>
之间的<strong>距离（distance）</strong>。如果我们采用点积（dot
product）作为内积，则该距离被称为<strong>欧几里得距离（Euclidean
distance）</strong>。定义映射： <span class="math display">\[
d : V \times V \to \mathbb{R}, \quad (\boldsymbol{x}, \boldsymbol{y})
\mapsto d(\boldsymbol{x}, \boldsymbol{y})
\]</span> 称为该空间上的一个<strong>度量（metric）</strong>。
与向量的长度类似，向量之间的距离不一定需要依赖于内积：一般的范数已经足够用于定义距离。<strong>如果一个范数由内积引出，那么该范数导出的距离也依赖于所选用的内积，不同内积可能导致不同的距离定义。</strong></p>
<p>乍一看，内积和度量的一系列属性看起来非常相似。然而，通过比较定义3.3和定义3.6，我们发现
<span class="math inline">\(\langle \boldsymbol{x}, \boldsymbol{y}
\rangle\)</span> 和 <span class="math inline">\(d(\boldsymbol{x},
\boldsymbol{y})\)</span> 的表现是<strong>相反的</strong>：</p>
<p>当 <span class="math inline">\(\boldsymbol{x}\)</span> 和 <span
class="math inline">\(\boldsymbol{y}\)</span>
非常相似时，其内积值较大，而度量值却很小（因为度量涉及两个向量的差，即
<span class="math inline">\(\boldsymbol{x} -
\boldsymbol{y}\)</span>）。</p>
<h3 id="角度和正交">3.6 角度和正交</h3>
<p>直观地说，<strong>两个向量之间的夹角告诉我们它们的<em><u>方向</u></em>有多相似。</strong>(个人注：和长度无关！)</p>
<p>两个向量 <span class="math inline">\(\boldsymbol{x}\)</span> 和 <span
class="math inline">\(\boldsymbol{y}\)</span>
是<strong>正交（orthogonal）</strong>的，当且仅当 <span
class="math display">\[
\langle \boldsymbol{x}, \boldsymbol{y} \rangle = 0,
\]</span> 写作 <span class="math inline">\(\boldsymbol{x} \perp
\boldsymbol{y}\)</span>。若 <span
class="math inline">\(\|\boldsymbol{x}\| = 1 =
\|\boldsymbol{y}\|\)</span>，即两个向量都是单位向量，则称 <span
class="math inline">\(\boldsymbol{x}\)</span> 和 <span
class="math inline">\(\boldsymbol{y}\)</span>
为<strong>标准正交（orthonormal）</strong>的。</p>
<p><strong>两个向量 <span class="math inline">\(\boldsymbol{x}\)</span>
和 <span class="math inline">\(\boldsymbol{y}\)</span> 之间的角度 <span
class="math inline">\(\omega\)</span>
取决于所采用的内积。</strong>关于一种内积正交的向量不一定关于其他内积正交。</p>
<p><strong>夹角的定义：</strong>两个非零向量 <span
class="math inline">\(\boldsymbol{x}\)</span> 和 <span
class="math inline">\(\boldsymbol{y}\)</span> 之间的夹角 <span
class="math inline">\(\omega\)</span> 满足： <span
class="math display">\[
\cos \omega = \frac{\langle \boldsymbol{x}, \boldsymbol{y}
\rangle}{\|\boldsymbol{x}\| \, \|\boldsymbol{y}\|}
\]</span></p>
<h3 id="正交矩阵">3.7 正交矩阵</h3>
<p>设<strong><u><em>方阵</em></u></strong> <span
class="math inline">\(\boldsymbol{A} \in \mathbb{R}^{n \times
n}\)</span>。当且仅当 <span
class="math inline">\(\boldsymbol{A}\)</span>
的<strong>列向量构成一组<em><u>标准正交</u></em></strong>（orthonormal）的向量组时，<span
class="math inline">\(\boldsymbol{A}\)</span> 被称为正交矩阵（orthogonal
matrix）。</p>
<p>换句话说，<span class="math inline">\(\boldsymbol{A}\)</span>
是正交矩阵当且仅当满足： <span class="math display">\[
\boldsymbol{A} \boldsymbol{A}^{\top} = \boldsymbol{I} =
\boldsymbol{A}^{\top} \boldsymbol{A}
\]</span>
<strong>正交矩阵保持长度不变</strong>；正交矩阵变换是特殊的，因为当一个向量
<span class="math inline">\(\boldsymbol{x}\)</span> 被<strong>正交矩阵
<span class="math inline">\(\boldsymbol{A}\)</span>
变换后，其长度保持不变</strong>。以点积为内积时，可以如下推导： <span
class="math display">\[
\|\boldsymbol{A} \boldsymbol{x}\|^{2}
= (\boldsymbol{A} \boldsymbol{x})^{\top} (\boldsymbol{A} \boldsymbol{x})
= \boldsymbol{x}^{\top} \boldsymbol{A}^{\top} \boldsymbol{A}
\boldsymbol{x}
= \boldsymbol{x}^{\top} \boldsymbol{I} \boldsymbol{x}
= \boldsymbol{x}^{\top} \boldsymbol{x}
= \|\boldsymbol{x}\|^{2}
\]</span>
因此，<strong><em><u>正交变换</u></em>不会改变向量的长度或两向量之间的角度（即它<em><u>保持内积结构</u></em>）</strong>，这使得正交矩阵广泛应用于几何变换、图像旋转、特征值分解等领域。<strong>这表明正交矩阵定义的变换是旋转（也可能是翻转）</strong>.</p>
<p><strong>正交补</strong>：<strong>三维向量空间中的平面<span
class="math inline">\(U\)</span>可以由它的法向量来描述</strong>。法向量张成某子空间
<span class="math inline">\(U\)</span> 的正交补 <span
class="math inline">\(U^\perp\)</span>;在 <span
class="math inline">\(n\)</span>
维向量空间和仿射空间中，通常可以用正交补来描述超平面。</p>
<h3 id="函数的内积">3.8 函数的内积</h3>
<p>设两个函数 <span class="math inline">\(\boldsymbol{u}: \mathbb{R}
\rightarrow \mathbb{R}\)</span> 与 <span
class="math inline">\(\boldsymbol{v}: \mathbb{R} \rightarrow
\mathbb{R}\)</span>，则它们在区间 <span class="math inline">\([a,
b]\)</span> 上的内积可以定义为： <span class="math display">\[
\langle u, v \rangle := \int_{a}^{b} u(x)\, v(x)\, dx, \quad \text{其中
} a, b &lt; \infty
\]</span>
与一般的内积一样，我们可以使用这个定义来引出函数的范数和正交性。特别地，若<span
class="math inline">\(\langle u, v \rangle = 0,\)</span>则称函数 <span
class="math inline">\(u\)</span> 和 <span
class="math inline">\(v\)</span>
在该内积下是<strong>正交的</strong>。</p>
<p><strong>一个例子：</strong> <span
class="math inline">\(\sin(x)\)</span> 与 <span
class="math inline">\(\cos(x)\)</span> 的正交性</p>
<p>若取函数 <span class="math inline">\(u(x) = \sin(x)\)</span>，<span
class="math inline">\(v(x) = \cos(x)\)</span>，则 <span
class="math display">\[
f(x) = u(x) v(x) = \sin(x) \cos(x)
\]</span> 如图 3.8 所示，该函数是一个奇函数，即满足：<span
class="math inline">\(f(-x) = -f(x)\)</span> 因此，在对称区间 <span
class="math inline">\([a, b] = [-\pi, \pi]\)</span> 上，其积分为 0，即：
<span class="math display">\[
\int_{-\pi}^{\pi} \sin(x) \cos(x)\, dx = 0
\]</span> 由此可知，<span class="math inline">\(\sin(x)\)</span> 与
<span class="math inline">\(\cos(x)\)</span> 是正交函数。</p>
<p>如果积分区间为 <span class="math inline">\([-\pi,
\pi]\)</span>，则下列函数集： <span class="math display">\[
\{1, \cos(x), \cos(2x), \cos(3x), \ldots\}
\]</span>
构成一个正交函数集。也就是说，集合中任意两个不同的函数在该区间上的内积为
0。</p>
<p>该集合张成了一个函数的巨大子空间。这个子空间中的函数在 <span
class="math inline">\([-\pi, \pi)\)</span>
上是偶函数且具有周期性。<strong>将任意函数投影到这个子空间，是傅里叶级数（Fourier
series）展开的核心思想。</strong>(<strong>备注：傅里叶级数中的正交函数集</strong>)</p>
<h3 id="正交投影">3.9 正交投影</h3>
<p>投影是一类重要的线性变换（还有旋转和反射）。</p>
<p><strong>投影</strong>（Projection）</p>
<p>设 <span class="math inline">\(V\)</span> 为一个向量空间，<span
class="math inline">\(U\subseteq V\)</span>为 <span
class="math inline">\(V\)</span> 的一个子空间。如果一个线性映射 <span
class="math inline">\(\pi: V \rightarrow U\)</span> 满足： <span
class="math display">\[
\pi^2 = \pi \circ \pi = \pi,
\]</span> 则称 <span class="math inline">\(\pi\)</span> 是一个从 <span
class="math inline">\(V\)</span> 到 <span
class="math inline">\(U\)</span>
的<strong>投影（projection）</strong>。</p>
<p>由于<strong>线性映射可以由矩阵表示</strong>，因此上述定义也适用于一类特殊的矩阵，这类矩阵被称为<strong>投影矩阵（projection
matrix）</strong>，记作 <span
class="math inline">\(\boldsymbol{P}_\pi\)</span>，它满足： <span
class="math display">\[
\boldsymbol{P}_\pi^2 = \boldsymbol{P}_\pi
\]</span>
也就是说，<strong>投影矩阵是满足幂等性质（idempotent）的线性变换矩阵</strong>。<strong>投影
<span class="math inline">\(\pi_{U}(\boldsymbol{x}) \in
\mathbb{R}^{n}\)</span> 仍然是 <span class="math inline">\(n\)</span>
维向量而不是标量。</strong>我们可以用张成子空间 <span
class="math inline">\(U\)</span> 的基向量 <span
class="math inline">\(\boldsymbol{b}\)</span>
来表示投影，这样我们就只需要一个坐标 <span
class="math inline">\(\lambda\)</span>
来表示投影(针对一维子空间（线）上的投影)，而不再需要 <span
class="math inline">\(n\)</span> 个坐标。在第四章矩阵分解中，我们将展示
<span class="math inline">\(\pi_{U}(\boldsymbol{x})\)</span> 是 <span
class="math inline">\(\boldsymbol{P}_{\pi}\)</span>
的特征向量，对应的特征值为 <span class="math inline">\(1\)</span>。</p>
<p>通过投影，我们可以近似求解无解的线性方程组 <span
class="math inline">\(\boldsymbol{A} \boldsymbol{x} =
\boldsymbol{b}\)</span>。线性方程组无解，意味着 <span
class="math inline">\(\boldsymbol{b}\)</span> 不在 <span
class="math inline">\(\boldsymbol{A}\)</span>
的张成空间中，也就是说，向量 <span
class="math inline">\(\boldsymbol{b}\)</span> 不在 <span
class="math inline">\(\boldsymbol{A}\)</span>
的列所张成的子空间内。<strong>具体的原理见资料《线性方程解的本质.md》文件。</strong>如果线性方程不能精确求解，那么我们可以尝试找到一个近似解（approximate
solution）。其思想是在 <span
class="math inline">\(\boldsymbol{A}\)</span>
的列所张成的子空间中找到最接近 <span
class="math inline">\(\boldsymbol{b}\)</span> 的向量，即计算 <span
class="math inline">\(\boldsymbol{b}\)</span> 在 <span
class="math inline">\(\boldsymbol{A}\)</span>
的列所张成的子空间上的正交投影。这类问题在实践中经常出现，这个解叫做超定系统的最小二乘解（least-squares
solution）（假设点积为内积）。</p>
<h3 id="旋转">3.10 旋转</h3>
<p>旋转(rotation)是一种线性映射（更具体地说，是欧氏向量空间的自同构），它将平面绕原点旋转<span
class="math inline">\(θ\)</span>角，即原点是一个不动点。</p>
<p><span class="math inline">\(\mathbb{R}^{2}\)</span>
中，旋转使物体绕平面内的一个原点旋转。如果旋转角度是正的，我们就称为逆时针旋转。</p>
<p>在 <span class="math inline">\(\mathbb{R}^{3}\)</span> 中的旋转与
<span class="math inline">\(\mathbb{R}^{2}\)</span> 不同的是，在 <span
class="math inline">\(\mathbb{R}^{3}\)</span>
中，我们可以<strong>围绕其中一维的轴旋转</strong>任何二维平面。确定通用旋转矩阵的最简单方法是确定标准基
<span class="math inline">\(e_1, e_2, e_3\)</span>
旋转得到的像，并确保这些像 <span class="math inline">\(\boldsymbol{R
e}_1, \boldsymbol{R e}_2, \boldsymbol{R e}_3\)</span>
彼此正交。然后，我们可以通过组合标准基的像得到一个通用的旋转矩阵 <span
class="math inline">\(\boldsymbol{R}\)</span></p>
<p><span class="math inline">\(\mathbb{R}^{n}\)</span> 中的旋转：</p>
<p>从二维和三维推广到 <span class="math inline">\(n\)</span>
维的欧氏向量空间的旋转可以直观地描述为：固定其 <span
class="math inline">\(n-2\)</span> 维，旋转 <span
class="math inline">\(\mathbb{R}^{n}\)</span>
空间中的二维平面。就像在三维情况下，我们可以旋转任意平面（<span
class="math inline">\(\mathbb{R}^{n}\)</span> 的二维子空间）。</p>
<p><strong>旋转的性质</strong>:</p>
<p><strong>旋转表现出许多有用的性质，这些性质可以通过将它们视为正交矩阵来说明</strong>（定义
3.8）：</p>
<ul>
<li>旋转保持距离，即</li>
</ul>
<p><span class="math display">\[
    \|\boldsymbol{x} - \boldsymbol{y}\| = \left\|
\boldsymbol{R}_{\theta}(\boldsymbol{x}) -
\boldsymbol{R}_{\theta}(\boldsymbol{y}) \right\|
\]</span></p>
<p>​ 换句话说，任意两点经过旋转变换后，它们之间的距离保持不变。</p>
<ul>
<li><p>旋转保持角度，即 <span
class="math inline">\(\boldsymbol{R}_{\theta} \boldsymbol{(x)}\)</span>
和 <span class="math inline">\(\boldsymbol{R}_{\theta}
\boldsymbol{(y)}\)</span> 之间的夹角与原始向量 <span
class="math inline">\(\boldsymbol{x}\)</span> 和 <span
class="math inline">\(\boldsymbol{y}\)</span> 之间的夹角相同。</p></li>
<li><p>三维（或更高维）旋转通常是不可交换的。因此，应用旋转的顺序是重要的，即使它们是围绕同一点旋转。</p>
<p>在二维空间中，旋转是可交换的。也就是说，对于所有 <span
class="math inline">\(\phi, \theta \in [0, 2\pi)\)</span>，都有： <span
class="math display">\[
\boldsymbol{R}(\phi)\boldsymbol{R}(\theta) =
\boldsymbol{R}(\theta)\boldsymbol{R}(\phi)
\]</span>
当且仅当它们围绕同一个点（例如原点）旋转时，二维旋转矩阵形成一个关于乘法的阿贝尔群。</p></li>
</ul>
<p>全书的读书笔记（共7篇）如下：<br />
<a href="/2025/12/05/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%B8%80/" title="《机器学习的数学基础》（1&#x2F;7）">《机器学习的数学基础》读书笔记之一 ：导言</a><br />
<a href="/2025/10/29/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%BA%8C/" title="《机器学习的数学基础》（2&#x2F;7）">《机器学习的数学基础》读书笔记之二 ：线性代数</a><br />
<a href="/2025/10/28/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%B8%89/" title="《机器学习的数学基础》（3&#x2F;7）">《机器学习的数学基础》读书笔记之三 ：解析几何</a><br />
<a href="/2025/10/27/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%9B%9B/" title="《机器学习的数学基础》（4&#x2F;7）">《机器学习的数学基础》读书笔记之四 ：矩阵分解</a><br />
<a href="/2025/10/26/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%BA%94/" title="《机器学习的数学基础》（5&#x2F;7）">《机器学习的数学基础》读书笔记之五 ：向量微积分</a><br />
<a href="/2025/10/25/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%85%AD/" title="《机器学习的数学基础》（6&#x2F;7）">《机器学习的数学基础》读书笔记之六 ：概率与分布</a><br />
<a href="/2025/10/24/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%B8%83/" title="《机器学习的数学基础》（7&#x2F;7）">《机器学习的数学基础》读书笔记之七 ：连续优化</a></p>

    </div>

    
    
    

    <footer class="post-footer">

        


          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2025/10/27/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%9B%9B/" rel="prev" title="《机器学习的数学基础》（4/7）">
                  <i class="fa fa-chevron-left"></i> 《机器学习的数学基础》（4/7）
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2025/10/29/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%BA%8C/" rel="next" title="《机器学习的数学基础》（2/7）">
                  《机器学习的数学基础》（2/7） <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Rayman.hung</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
<script src="https://cdn.jsdelivr.net/npm/hexo-generator-searchdb@1.4.0/dist/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js","integrity":"sha256-r+3itOMtGGjap0x+10hu6jW/gZCzxHsoKrOd7gyRSGY="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>

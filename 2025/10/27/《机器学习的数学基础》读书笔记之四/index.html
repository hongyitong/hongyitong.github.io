<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.1.1">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" integrity="sha256-DfWjNxDkM94fVBWx1H5BMMp0Zq7luBlV8QRcSES7s+0=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"hongyitong.github.io","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.11.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="4、矩阵分解 Matrix Decomposition 在本章中，我们将介绍矩阵的三个方面：如何概括矩阵，如何分解矩阵，以及如何利用矩阵分解进行矩阵近似，矩阵分解的一个类比是因数分解(factoring of numbers)。">
<meta property="og:type" content="article">
<meta property="og:title" content="《机器学习的数学基础》（4&#x2F;7）">
<meta property="og:url" content="http://hongyitong.github.io/2025/10/27/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%9B%9B/index.html">
<meta property="og:site_name" content="墨语浮生">
<meta property="og:description" content="4、矩阵分解 Matrix Decomposition 在本章中，我们将介绍矩阵的三个方面：如何概括矩阵，如何分解矩阵，以及如何利用矩阵分解进行矩阵近似，矩阵分解的一个类比是因数分解(factoring of numbers)。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Part1/%E5%9B%BE%E8%A7%A3%E7%89%B9%E5%BE%81%E5%80%BC%E5%88%86%E8%A7%A3.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Part1/%E5%9B%BE%E8%A7%A3SVD_1.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Part1/%E5%9B%BE%E8%A7%A3SVD_2.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Part1/%E7%9F%A9%E9%98%B5%E5%88%86%E7%B1%BB%E5%85%B3%E7%B3%BB%E5%9B%BE.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Part1/%E5%B8%B8%E8%A7%81%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%E6%80%BB%E7%BB%93%E8%A1%A8.png">
<meta property="article:published_time" content="2025-10-26T16:00:00.000Z">
<meta property="article:modified_time" content="2025-12-10T03:10:02.950Z">
<meta property="article:author" content="Rayman.hung">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="数学">
<meta property="article:tag" content="算法">
<meta property="article:tag" content="统计">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://hongyitong.github.io/img3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Part1/%E5%9B%BE%E8%A7%A3%E7%89%B9%E5%BE%81%E5%80%BC%E5%88%86%E8%A7%A3.png">


<link rel="canonical" href="http://hongyitong.github.io/2025/10/27/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%9B%9B/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://hongyitong.github.io/2025/10/27/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%9B%9B/","path":"2025/10/27/《机器学习的数学基础》读书笔记之四/","title":"《机器学习的数学基础》（4/7）"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>《机器学习的数学基础》（4/7） | 墨语浮生</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-ZXGEJDXQ33"></script>
  <script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"G-ZXGEJDXQ33","only_pageview":false}</script>
  <script src="/js/third-party/analytics/google-analytics.js"></script>





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">墨语浮生</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Rayman</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section">首页</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section">分类</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section">关于</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section">归档</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section">标签</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3"><span class="nav-text">4、矩阵分解</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A1%8C%E5%88%97%E5%BC%8F%E4%B8%8E%E8%BF%B9"><span class="nav-text">4.1 行列式与迹</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E5%80%BC%E5%92%8C%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F"><span class="nav-text">4.2 特征值和特征向量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B0%B1%E5%AE%9A%E7%90%86"><span class="nav-text">4.3 谱定理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E5%80%BC%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F%E8%A1%8C%E5%88%97%E5%BC%8F%E8%BF%B9%E7%9A%84%E8%81%94%E7%B3%BB"><span class="nav-text">4.4
特征值、特征向量、行列式、迹的联系</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3-1"><span class="nav-text">4.5 矩阵分解</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#cholesky%E5%88%86%E8%A7%A3"><span class="nav-text">4.5.1 Cholesky分解</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E5%80%BC%E5%88%86%E8%A7%A3%E4%B8%8E%E5%AF%B9%E8%A7%92%E5%8C%96"><span class="nav-text">4.5.2 特征值分解与对角化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3"><span class="nav-text">4.5.3 奇异值分解</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E5%80%BC%E5%88%86%E8%A7%A3-vs.-%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3"><span class="nav-text">4.5.4 特征值分解 vs. 奇异值分解</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%9F%A9%E9%98%B5%E9%80%BC%E8%BF%91"><span class="nav-text">4.5.5 矩阵逼近</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9F%A9%E9%98%B5%E5%88%86%E7%B1%BB%E5%85%B3%E7%B3%BB%E5%9B%BE"><span class="nav-text">4.6 矩阵分类关系图</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Rayman.hung</p>
  <div class="site-description" itemprop="description">技术分享、读书心得、心情记录</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives">
          <span class="site-state-item-count">159</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags">
        <span class="site-state-item-count">314</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/hongyitong" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;hongyitong" rel="noopener" target="_blank">GitHub</a>
      </span>
  </div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://hongyitong.github.io/2025/10/27/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%9B%9B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Rayman.hung">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="墨语浮生">
      <meta itemprop="description" content="技术分享、读书心得、心情记录">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="《机器学习的数学基础》（4/7） | 墨语浮生">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          《机器学习的数学基础》（4/7）
        </h1>

        </h1>
          
             <p class="post-subtitle">读书笔记之四：矩阵分解</p>
          
        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-10-27 00:00:00" itemprop="dateCreated datePublished" datetime="2025-10-27T00:00:00+08:00">2025-10-27</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97%EF%BC%88%E8%87%AA%E7%84%B6%E7%A7%91%E5%AD%A6%EF%BC%89/" itemprop="url" rel="index"><span itemprop="name">读书心得（自然科学）</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h2 id="矩阵分解">4、矩阵分解</h2>
<p>Matrix Decomposition</p>
<p>在本章中，我们将介绍矩阵的三个方面：如何概括矩阵，如何分解矩阵，以及<strong>如何利用矩阵分解进行矩阵近似</strong>，<strong>矩阵分解的一个类比是因数分解</strong>(factoring
of numbers)。</p>
<span id="more"></span>
<h3 id="行列式与迹">4.1 行列式与迹</h3>
<ul>
<li><p><strong>行列式只适用于方阵</strong>。</p></li>
<li><p>可以看作是将矩阵映射到一个实数的函数。</p></li>
<li><p>当且仅当行列式不等于0时，矩阵可逆。</p></li>
<li><p><strong>上三角（下三角）矩阵的行列式是其对角元素的积。</strong></p></li>
<li><p>行列式度量体积；列向量代表各条边。</p></li>
<li><p>一个方阵 <span class="math inline">\(\boldsymbol{A} \in
\mathbb{R}^{n \times n}\)</span> 的行列式 <span
class="math inline">\(\operatorname{det}(\boldsymbol{A}) \neq
0\)</span>，当且仅当 <span
class="math inline">\(\operatorname{rk}(\boldsymbol{A}) =
n.\)</span>换句话说，<span class="math inline">\(\boldsymbol{A}\)</span>
可逆当且仅当它是满秩的</p></li>
</ul>
<blockquote>
<p>为什么：行列式只适用于方阵。<br />
行列式衡量方阵线性变换的体积(面积、体积、n维体积)对应的变化，而非方阵既不构成同维线性变换，也不满足代数定义，并且可逆只针对方阵有意义。</p>
</blockquote>
<p>对于 <span class="math inline">\(n \times n\)</span>
矩阵的行列式，需要一个通用的算法来解决 <span class="math inline">\(n
&gt; 3\)</span> 的情况，我们将在下面探讨这个问题。定理 4.2 将计算 <span
class="math inline">\(n \times n\)</span> 矩阵的行列式的问题简化为计算
<span class="math inline">\((n - 1) \times (n - 1)\)</span>
矩阵的行列式。通过递归地应用<strong>拉普拉斯展开（Laplace
Expansion）</strong>，我们最终可以通过计算 <span class="math inline">\(2
\times 2\)</span> 矩阵的行列式来计算 <span class="math inline">\(n
\times n\)</span> 矩阵的行列式。</p>
<p><strong>行列式具有以下特性</strong>：</p>
<ul>
<li>矩阵相乘的行列式是相应行列式的乘积，即</li>
</ul>
<p><span class="math display">\[
\operatorname{det}(\boldsymbol{AB}) = \operatorname{det}(\boldsymbol{A})
\operatorname{det}(\boldsymbol{B})
\]</span></p>
<ul>
<li>矩阵转置不改变行列式的值，即</li>
</ul>
<p><span class="math display">\[
\operatorname{det}(\boldsymbol{A}) =
\operatorname{det}(\boldsymbol{A}^{\top})
\]</span></p>
<ul>
<li>如果 <span class="math inline">\(\boldsymbol{A}\)</span>
是正则的（可逆的），</li>
</ul>
<p><span class="math display">\[
\operatorname{det}(\boldsymbol{A}^{-1}) =
\frac{1}{\operatorname{det}(\boldsymbol{A})}
\]</span></p>
<ul>
<li><p><strong>相似矩阵（定义 2.22）具有相同的行列式。因此，对于线性映射
<span class="math inline">\(\Phi: V \rightarrow V\)</span>，<span
class="math inline">\(\Phi\)</span> 的所有变换矩阵 <span
class="math inline">\(\boldsymbol{A}_\Phi\)</span>
都具有相同的行列式。因此，线性映射基的选择不影响行列式的值。</strong>（注意前提是<span
class="math inline">\(V \rightarrow V\)</span>）</p></li>
<li><p>将一列/行的倍数加到另一列/行不会更改 <span
class="math inline">\(\operatorname{det}(\boldsymbol{A})\)</span>。</p></li>
<li><p>用标量 <span class="math inline">\(\lambda \in
\mathbb{R}\)</span> 乘矩阵 <span
class="math inline">\(\boldsymbol{A}\)</span> 的行/列，将 <span
class="math inline">\(\lambda\)</span> 倍缩放 <span
class="math inline">\(\operatorname{det}(\boldsymbol{A})\)</span>。特别地，</p></li>
</ul>
<p><span class="math display">\[
\operatorname{det}(\lambda \boldsymbol{A}) = \lambda^n
\operatorname{det}(\boldsymbol{A})
\]</span></p>
<ul>
<li>交换两行/列将更改 <span
class="math inline">\(\operatorname{det}(\boldsymbol{A})\)</span>
的符号。</li>
</ul>
<p><strong>迹</strong></p>
<p>方阵 <span class="math inline">\(\boldsymbol{A} \in \mathbb{R}^{n
\times n}\)</span> 的<strong>迹</strong>（trace）定义为：</p>
<p><span class="math display">\[
\operatorname{tr}(\boldsymbol{A}) := \sum_{i=1}^{n} a_{ii}
\]</span></p>
<p>即，迹是 <span class="math inline">\(\boldsymbol{A}\)</span>
的对角元素之和。</p>
<p>迹满足以下属性：</p>
<ol type="1">
<li>对于 <span class="math inline">\(\boldsymbol{A}, \boldsymbol{B} \in
\mathbb{R}^{n \times n}\)</span>，有：</li>
</ol>
<p><span class="math display">\[
tr⁡(A+B)== \operatorname{tr}(\boldsymbol{A}) +
\operatorname{tr}(\boldsymbol{B})
\]</span></p>
<ol start="2" type="1">
<li>对于 <span class="math inline">\(\boldsymbol{A} \in \mathbb{R}^{n
\times n}\)</span>，<span class="math inline">\(\alpha \in
\mathbb{R}\)</span>，有：</li>
</ol>
<p><span class="math display">\[
tr⁡(αA) = \alpha \operatorname{tr}({A})
\]</span></p>
<ol start="3" type="1">
<li>单位矩阵的迹为：</li>
</ol>
<p><span class="math display">\[
tr⁡(In) = n
\]</span></p>
<ol start="4" type="1">
<li>对于 <span class="math inline">\(\boldsymbol{A} \in \mathbb{R}^{n
\times k}\)</span>，<span class="math inline">\(\boldsymbol{B} \in
\mathbb{R}^{k \times n}\)</span>，有：</li>
</ol>
<p><span class="math display">\[
tr⁡(AB)= \operatorname{tr}({B}{A})
\]</span></p>
<p>可以证明，<strong>只有一个函数同时满足这四个性质——即迹</strong>（Gohberg
et al., 2012）。</p>
<blockquote>
<p><strong>线性映射的矩阵表示依赖于基，而线性映射<span
class="math inline">\(\phi\)</span>的迹独立于基。 </strong>
具体原理见资料《迹的相似不变性.md》</p>
</blockquote>
<p><strong>特征多项式</strong>:</p>
<p>将行列式和迹作为描述方阵的函数来讨论。</p>
<p>对于 <span class="math inline">\(\lambda \in \mathbb{R}\)</span>
和一个方阵 <span class="math inline">\(\boldsymbol{A} \in \mathbb{R}^{n
\times n}\)</span>，其特征多项式（Characteristic
Polynomial）定义为：</p>
<p><span class="math display">\[
p_A(\lambda) := \det(\boldsymbol{A} - \lambda \boldsymbol{I}) = c_0 +
c_1 \lambda + c_2 \lambda^2 + \cdots + c_{n-1} \lambda^{n-1} + (-1)^n
\lambda^n
\]</span></p>
<p>其中 <span class="math inline">\(c_0, \ldots, c_{n-1} \in
\mathbb{R}\)</span>。特别地，</p>
<p><span class="math display">\[
c_0 = \det(\boldsymbol{A}), \quad c_{n-1} = (-1)^{n-1}
\operatorname{tr}(\boldsymbol{A})
\]</span></p>
<p>特征多项式将允许我们计算特征值和特征向量。</p>
<h3 id="特征值和特征向量">4.2 特征值和特征向量</h3>
<p>指向同一方向的两个向量称为共向的(codirected)。如果两个向量指向相同或相反的方向，则它们是共线的（collinear）。</p>
<p>令 <span class="math inline">\(A \in \mathbb{R}^{n \times n}\)</span>
为一个<u><strong>方阵</strong></u>。</p>
<p><span class="math display">\[
\boldsymbol{A}\boldsymbol{x} = \lambda \boldsymbol{x}
\]</span></p>
<p>我们称这个方程为特征方程（eigenvalue equation）。其中 <span
class="math inline">\(\lambda \in \mathbb{R}\)</span> 为 <span
class="math inline">\(\boldsymbol{A}\)</span>
的特征值（eigenvalue），<span class="math inline">\(\boldsymbol{x} \in
\mathbb{R}^{n} \setminus \{\boldsymbol{0}\}\)</span>
为相应的特征向量（eigenvector）。</p>
<p>以下说法是等效的（个人注：详见《特征值和齐次方程解的关系.md》）：</p>
<ol type="1">
<li><span class="math inline">\(\lambda\)</span> 是 <span
class="math inline">\(\boldsymbol{A} \in \mathbb{R}^{n \times
n}\)</span> 的特征值。<br />
</li>
<li>存在 <span class="math inline">\(\boldsymbol{x} \in \mathbb{R}^{n}
\backslash \{\mathbf{0}\}\)</span>，使得 <span
class="math inline">\(\boldsymbol{A}\boldsymbol{x} = \lambda
\boldsymbol{x},\)</span> 或等价地 <span
class="math inline">\((\boldsymbol{A} - \lambda \boldsymbol{I}_{n})
\boldsymbol{x} = \mathbf{0}\)</span> 有非平凡解，即 <span
class="math inline">\(\boldsymbol{x} \neq \mathbf{0}\)</span>。<br />
</li>
<li><span class="math inline">\(\operatorname{rk}\!\left(\boldsymbol{A}
- \lambda \boldsymbol{I}_{n}\right) &lt; n\)</span><br />
</li>
<li><span class="math inline">\(\det\!\left(\boldsymbol{A} - \lambda
\boldsymbol{I}_{n}\right) = 0\)</span></li>
</ol>
<p><strong>特征空间和特征谱</strong>:</p>
<p>对于 <span class="math inline">\(A \in \mathbb{R}^{n \times
n}\)</span>，<span class="math inline">\(\boldsymbol{A}\)</span>
对应于特征值 <span class="math inline">\(\lambda\)</span>
的特征向量集合张成 <span class="math inline">\(\mathbb{R}^{n}\)</span>
的子空间，这个子空间称为 <span
class="math inline">\(\boldsymbol{A}\)</span> 关于 <span
class="math inline">\(\lambda\)</span>
的特征空间（eigenspace），记为：<span
class="math inline">\(E_{\lambda}\)</span> ; <strong><span
class="math inline">\(\boldsymbol{A}\)</span>
的特征值构成的集合称为特征谱（eigenspectrum），或 <span
class="math inline">\(\boldsymbol{A}\)</span> 的谱。</strong></p>
<blockquote>
<p><strong>特征值为 0
的特征向量是矩阵零空间中的非零向量；它揭示了矩阵在某些方向上把向量“压扁”为零；</strong></p>
</blockquote>
<p>单位矩阵 <span class="math inline">\(\boldsymbol{I} \in \mathbb{R}^{n
\times n}\)</span> 的特征方程为：</p>
<p><span class="math display">\[
P_{I}(\lambda) = \operatorname{det}(\boldsymbol{I} - \lambda
\boldsymbol{I}) = (1 - \lambda)^n = 0
\]</span></p>
<p>它只拥有 <span class="math inline">\(\lambda = 1\)</span>
这个特征值，并出现 $ n $ 次。而且，对于任意 <span
class="math inline">\(\boldsymbol{x} \in \mathbb{R}^{n} \setminus
\{\boldsymbol{0}\}\)</span>，有：</p>
<p><span class="math display">\[
\boldsymbol{I} \boldsymbol{x} = \lambda \boldsymbol{x} = 1 \cdot
\boldsymbol{x}
\]</span></p>
<p>因此，单位矩阵的唯一特征空间 <span class="math inline">\(E_1\)</span>
张成 <span class="math inline">\(n\)</span> 维，<span
class="math inline">\(\mathbb{R}^{n}\)</span> 的 <span
class="math inline">\(n\)</span> 个标准基向量都是 <span
class="math inline">\(\boldsymbol{I}\)</span>
的特征向量。(另外：单位矩阵保持所有方向不变，因此所有方向都是特征向量)。</p>
<p><strong>关于特征值和特征向量的常用特性包括</strong>：</p>
<ul>
<li><p>矩阵 <span class="math inline">\(\boldsymbol{A}\)</span> 及其转置
<span class="math inline">\(\boldsymbol{A}^{\top}\)</span>
具有相同的特征值，但不一定具有相同的特征向量。</p></li>
<li><p>特征空间 <span class="math inline">\(E_{\lambda}\)</span> 是
<span class="math inline">\(\boldsymbol{A} - \lambda
\boldsymbol{I}\)</span> 的零空间，因为：</p></li>
</ul>
<p><span class="math display">\[
  \boldsymbol{A} \boldsymbol{x} = \lambda \boldsymbol{x}
  \Longleftrightarrow
  \boldsymbol{A} \boldsymbol{x} - \lambda \boldsymbol{x} =
\boldsymbol{0}
  \Longleftrightarrow
  (\boldsymbol{A} - \lambda \boldsymbol{I}) \boldsymbol{x} =
\boldsymbol{0}
  \Longleftrightarrow
  \boldsymbol{x} \in \ker(\boldsymbol{A} - \lambda \boldsymbol{I})
\]</span></p>
<ul>
<li><p>相似矩阵（定义
2.22）具有相同的特征值。基变换得到的是相似矩阵。因此，<strong>线性映射
<span class="math inline">\(\Phi\)</span>
的特征值与它的变换矩阵的基的选择无关。这使得特征值、行列式和迹成为线性映射的重要不变量。</strong></p></li>
<li><p>对称正定矩阵总是有正的实特征值。</p></li>
</ul>
<blockquote>
<p>假设：</p>
<ul>
<li><span class="math inline">\(A\)</span> 是一个线性变换在基底<span
class="math inline">\(\mathcal{B}\)</span> 下的矩阵<br />
</li>
<li>换到另一个基底<span class="math inline">\(\mathcal{C}\)</span>
，基变换矩阵为 <span class="math inline">\(P\)</span></li>
</ul>
<p>那么在新基底下的矩阵是：<br />
<span class="math display">\[
A&#39; = P^{-1} A P
\]</span></p>
<p>这就叫 <strong>相似矩阵（similar matrix）</strong>。<br />
<strong>上式右边过程：P作用在新基上坐标转换成旧基坐标，用A做变换，然后转换成新基坐标。详见“线性代数“的“2.8
变量变换和基变换区别“。</strong></p>
</blockquote>
<p><strong>剪切映射</strong>：</p>
<p><strong>剪切映射（Shearing Mapping）</strong>
是一种保持某些几何特征（如面积、体积）不变，但改变形状的线性变换。在二维或三维欧几里得空间中，它通过“滑动”坐标轴方向的某些分量，使图形在不旋转也不缩放的前提下发生<strong>倾斜变形</strong>。<br />
<strong>几何直观</strong>：</p>
<ul>
<li>剪切会把一个<strong>矩形变成平行四边形</strong>。<br />
</li>
<li>面积保持不变。<br />
</li>
<li>角度和形状会改变，但线之间的平行关系保持。<br />
</li>
<li>单位正方形在剪切后变成平行四边形，但底边长度不变。</li>
</ul>
<p>详见《剪切映射的变换矩阵.md》</p>
<p>一个矩阵 <span class="math inline">\(\boldsymbol{A} \in \mathbb{R}^{n
\times n}\)</span> 的 <span class="math inline">\(n\)</span> 个特征向量
<span class="math inline">\(\boldsymbol{x}_{1}, \ldots,
\boldsymbol{x}_{n}\)</span> 对应 <span class="math inline">\(n\)</span>
个不同的特征值 <span class="math inline">\(\lambda_{1}, \ldots,
\lambda_{n}\)</span>，则它们是线性独立的。这个定理指出，具有 <span
class="math inline">\(n\)</span> 个不同特征值的矩阵的特征向量构成 <span
class="math inline">\(\mathbb{R}^{n}\)</span> 的一组基。<br />
给定一个矩阵 <span class="math inline">\(\boldsymbol{A} \in
\mathbb{R}^{m \times n}\)</span>，我们总能通过<span
class="math inline">\(\boldsymbol{S} := \boldsymbol{A}^{\top}
\boldsymbol{A}\)</span> 得到一个<strong>对称的半正定矩阵</strong><span
class="math inline">\(\boldsymbol{S} \in \mathbb{R}^{n \times
n}\)</span>。<br />
<strong>注：很多地方只需要半正定矩阵即可。</strong></p>
<h3 id="谱定理">4.3 谱定理</h3>
<p>Spectral Theorem</p>
<p>如果 <span class="math inline">\(\boldsymbol{A} \in \mathbb{R}^{n
\times n}\)</span> 是<strong>对称的</strong>，则存在 <span
class="math inline">\(\boldsymbol{A}\)</span> 的特征向量组成向量空间
<span class="math inline">\(V\)</span>
的一个正交基，且每个特征值都是实数。</p>
<p>谱定理的直接含义是：对称矩阵 <span
class="math inline">\(\boldsymbol{A}\)</span>
存在特征值分解（具有实特征值），并且我们可以由特征向量构造一个标准正交基，使得
<span class="math display">\[
\boldsymbol{A} = \boldsymbol{P} \boldsymbol{D} \boldsymbol{P}^{\top},
\]</span> 其中 <span class="math inline">\(\boldsymbol{D}\)</span>
是对角矩阵，<span class="math inline">\(\boldsymbol{P}\)</span> 的列是
<span class="math inline">\(\boldsymbol{A}\)</span> 的单位特征向量，满足
<span class="math inline">\(\boldsymbol{P}^{\top} \boldsymbol{P} =
\boldsymbol{I}\)</span>。</p>
<h3 id="特征值特征向量行列式迹的联系">4.4
特征值、特征向量、行列式、迹的联系</h3>
<p><strong>定理</strong> 4.16：</p>
<p>设矩阵 <span class="math inline">\(\boldsymbol{A} \in \mathbb{R}^{n
\times n}\)</span>，则其行列式等于其所有特征值的乘积，即 <span
class="math display">\[
\det(\boldsymbol{A}) = \prod_{i=1}^{n} \lambda_i
\]</span> 其中 <span class="math inline">\(\lambda_i \in
\mathbb{C}\)</span>（可重复）为矩阵 <span
class="math inline">\(\boldsymbol{A}\)</span> 的特征值。</p>
<p><strong>定理</strong> 4.17：</p>
<p>设矩阵 <span class="math inline">\(\boldsymbol{A} \in \mathbb{R}^{n
\times n}\)</span>，则其迹等于其所有特征值的和，即 <span
class="math display">\[
\operatorname{tr}(\boldsymbol{A}) = \sum_{i=1}^{n} \lambda_i
\]</span> 其中 <span class="math inline">\(\lambda_i \in
\mathbb{C}\)</span>（可重复）为矩阵 <span
class="math inline">\(\boldsymbol{A}\)</span> 的特征值。</p>
<h3 id="矩阵分解-1">4.5 矩阵分解</h3>
<p>Matrix Decomposition</p>
<h4 id="cholesky分解">4.5.1 Cholesky分解</h4>
<p><strong>对称正定矩阵</strong> <span
class="math inline">\(\boldsymbol{A}\)</span> 可以分解为： <span
class="math display">\[
\boldsymbol{A} = \boldsymbol{L} \boldsymbol{L}^{\top},
\]</span> 其中 <span class="math inline">\(\boldsymbol{L}\)</span>
是具有正对角元素的下三角矩阵。 <span
class="math inline">\(\boldsymbol{L}\)</span> 被称为 <span
class="math inline">\(\boldsymbol{A}\)</span> 的Cholesky 因子（Cholesky
factor），且是唯一的。<strong>对称正定矩阵的类平方根运算，即Cholesky分解。</strong></p>
<h4 id="特征值分解与对角化">4.5.2 特征值分解与对角化</h4>
<p><strong>矩阵 <span class="math inline">\(\boldsymbol{A} \in
\mathbb{R}^{n \times n}\)</span>
的对角化，实际是在另一个基中表示相同线性映射的一种方法，这个基由 <span
class="math inline">\(\boldsymbol{A}\)</span>
的特征向量组成。</strong></p>
<p>定理 4.20<strong>特征值分解</strong>，Eigendecomposition</p>
<p>设 <span class="math inline">\(\boldsymbol{A} \in \mathbb{R}^{n
\times n}\)</span>，则当且仅当 <span
class="math inline">\(\boldsymbol{A}\)</span> 有 <span
class="math inline">\(n\)</span> 个线性无关的特征向量时，<span
class="math inline">\(\boldsymbol{A}\)</span>
可以被对角化，即存在可逆矩阵 <span
class="math inline">\(\boldsymbol{P}\)</span> 和对角矩阵 <span
class="math inline">\(\boldsymbol{D}\)</span>，使得： <span
class="math display">\[
\boldsymbol{A} = \boldsymbol{P} \boldsymbol{D} \boldsymbol{P}^{-1}
\]</span></p>
<p>其中：<span class="math inline">\(\boldsymbol{P} \in \mathbb{R}^{n
\times n}\)</span> 的列是 <span
class="math inline">\(\boldsymbol{A}\)</span> 的 <span
class="math inline">\(n\)</span> 个线性无关的特征向量；<strong><span
class="math inline">\(\boldsymbol{D}\)</span> 是对角矩阵，其对角元素为
<span class="math inline">\(\boldsymbol{A}\)</span>
的特征值（可重复）</strong>；因此，只有当 <span
class="math inline">\(\boldsymbol{A}\)</span> 的特征向量张成 <span
class="math inline">\(\mathbb{R}^n\)</span>，即 <span
class="math inline">\(\boldsymbol{A}\)</span> 是非亏损的，<span
class="math inline">\(\boldsymbol{A}\)</span> 才可以对角化。</p>
<p>定理 4.21<strong><em><u>对称矩阵</u></em>的特征值分解</strong></p>
<p>设 <span class="math inline">\(\boldsymbol{S} \in \mathbb{R}^{n
\times n}\)</span> 是对称矩阵（即 <span
class="math inline">\(\boldsymbol{S}^{\top} =
\boldsymbol{S}\)</span>），则 <span
class="math inline">\(\boldsymbol{S}\)</span> 一定可以被对角化。</p>
<p>根据谱定理，对于任意对称矩阵 <span
class="math inline">\(\boldsymbol{S}\)</span>，存在一个标准正交基，由
<span class="math inline">\(\boldsymbol{S}\)</span>
的特征向量构成。即存在正交矩阵 <span
class="math inline">\(\boldsymbol{P}\)</span>，使得：</p>
<p><span class="math display">\[
\boldsymbol{S} = \boldsymbol{P} \boldsymbol{D} \boldsymbol{P}^\top
\]</span></p>
<p>其中：</p>
<ul>
<li><span class="math inline">\(\boldsymbol{P} \in \mathbb{R}^{n \times
n}\)</span> 是正交矩阵，列向量为单位正交的特征向量；</li>
<li><span class="math inline">\(\boldsymbol{D}\)</span>
是对角矩阵，对角元素为 <span
class="math inline">\(\boldsymbol{S}\)</span> 的特征值；</li>
<li>此分解方式也可表示为 <span class="math inline">\(\boldsymbol{D} =
\boldsymbol{P}^\top \boldsymbol{S} \boldsymbol{P}\)</span>。</li>
</ul>
<p><img src="/img3/机器学习的数学基础Part1/图解特征值分解.png" alt="图解特征值分解" style="zoom:50%;" /></p>
<blockquote>
<p><strong>在相同的向量空间中应用相同的基变化，然后撤消</strong>。</p>
</blockquote>
<h4 id="奇异值分解">4.5.3 奇异值分解</h4>
<p>singular value decomposition, SVD</p>
<p>表示线性映射 <span class="math inline">\(\Phi: V \rightarrow
W\)</span> 的矩阵 <span class="math inline">\(\boldsymbol{A}\)</span>
的奇异值分解量化了这两个向量空间的潜在几何变化。</p>
<p><strong>奇异值分解（SVD）公式</strong></p>
<p><span class="math display">\[
\boldsymbol{A} = \boldsymbol{U} \boldsymbol{\Sigma} \boldsymbol{V}^\top
\]</span></p>
<p>以及它在几何上的意义（旋转+缩放+旋转）.</p>
<p>其中 <span class="math inline">\(\boldsymbol{U} \in \mathbb{R}^{m
\times m}\)</span> 为正交矩阵，其列向量为 <span
class="math inline">\(\boldsymbol{u}_i, \ i=1, \ldots, m\)</span>；<span
class="math inline">\(\boldsymbol{V} \in \mathbb{R}^{n \times
n}\)</span> 也为正交矩阵，其列向量为<span
class="math inline">\(\boldsymbol{v}_i, \ i=1, \ldots,
n\)</span>。另外，<span
class="math inline">\(\boldsymbol{\Sigma}\)</span> 为 <span
class="math inline">\(m \times n\)</span> 矩阵，且<span
class="math inline">\(\Sigma_{ii} = \sigma_i \geqslant 0, \quad
\Sigma_{ij} = 0, \ i \neq j .\)</span> <span
class="math inline">\(\boldsymbol{\Sigma}\)</span> 的对角元素<span
class="math inline">\(\sigma_i, \quad i = 1, \ldots,
r\)</span>称为奇异值（singular values），<span
class="math inline">\(\boldsymbol{u}_i\)</span>
称为左奇异向量（left-singular vectors），<span
class="math inline">\(\boldsymbol{v}_i\)</span>
称为右奇异向量（right-singular
vectors）。按照惯例，奇异值是有序的：<span
class="math inline">\(\sigma_1 \geqslant \sigma_2 \geqslant \cdots
\geqslant \sigma_r \geqslant 0 .\)</span></p>
<p>奇异值矩阵 <span class="math inline">\(\boldsymbol{\Sigma}\)</span>
是唯一的。需要注意 <span class="math inline">\(\boldsymbol{\Sigma} \in
\mathbb{R}^{m \times n}\)</span> 是矩形的，且与 <span
class="math inline">\(\boldsymbol{A}\)</span> 尺寸相同。这意味着 <span
class="math inline">\(\boldsymbol{\Sigma}\)</span>
有一个包含奇异值的对角子矩阵，其他元素为零。</p>
<p><strong>注1：</strong>SVD 对任意 <span
class="math inline">\(\boldsymbol{A} \in \mathbb{R}^{m \times
n}\)</span> 都成立。<br />
<strong>注2：旋转矩阵</strong>就是一个行列式为 <span
class="math inline">\(+1\)</span> 的正交矩阵，也就是： <span
class="math inline">\(R^\top R = I, \quad \det(R) = 1\)</span><br />
<strong>注3：</strong>标准正交基(ONB)<br />
<strong>注4：</strong>左奇异向量和右奇异向量的直观理解，详见《SVD
中的左奇异向量和右奇异向量.md》</p>
<h4 id="特征值分解-vs.-奇异值分解">4.5.4 特征值分解 vs. 奇异值分解</h4>
<p>考虑特征值分解 <span class="math display">\[
\boldsymbol{A} = \boldsymbol{P} \boldsymbol{D} \boldsymbol{P}^{-1}
\]</span> 和奇异值分解 <span class="math display">\[
\boldsymbol{A} = \boldsymbol{U} \boldsymbol{\Sigma} \boldsymbol{V}^{-1},
\]</span> 下面回顾一下它们的核心内容：</p>
<ul>
<li><p>对于任意矩阵 <span class="math inline">\(\mathbb{R}^{m \times
n}\)</span>，奇异值分解总是存在的。而特征值分解仅对方阵 <span
class="math inline">\(\mathbb{R}^{n \times n}\)</span>
有效，并且仅当我们能找到 <span
class="math inline">\(\mathbb{R}^n\)</span>
的特征向量的基时才存在。</p></li>
<li><p>特征值分解矩阵 <span
class="math inline">\(\boldsymbol{P}\)</span>
中的向量不一定是正交的，即基的变化不是简单的旋转和缩放。而另一方面，奇异值分解中矩阵
<span class="math inline">\(\boldsymbol{U}\)</span> 和 <span
class="math inline">\(\boldsymbol{V}\)</span>
中的向量是正交的，因此它们确实表示旋转。</p></li>
<li><p>特征值分解和奇异值分解都是三个线性映射的组合：</p>
<ul>
<li>定义域的<strong>基变换</strong>；</li>
<li>每个新基向量的缩放都是独立的，并从定义域映射到陪域；</li>
<li>陪域的<strong>基变换</strong>。
特征值分解和奇异值分解的一个关键区别是，<strong>在奇异值分解中，定义域和陪域可以是不同维数的向量空间。</strong></li>
</ul></li>
<li><p>在奇异值分解中，左奇异向量矩阵 <span
class="math inline">\(\boldsymbol{U}\)</span> 和右奇异向量矩阵 <span
class="math inline">\(\boldsymbol{V}\)</span>
通常不是互逆的（它们在不同的向量空间中执行基变换）。在特征值分解中，基变化矩阵
<span class="math inline">\(\boldsymbol{U}\)</span> 和 <span
class="math inline">\(\boldsymbol{U}^{-1}\)</span> 是互逆的。</p></li>
<li><p>在奇异值分解中，对角矩阵 <span
class="math inline">\(\boldsymbol{\Sigma}\)</span>
中的项都是实的、非负的，这对于特征值分解中的对角矩阵则不一定成立。</p></li>
<li><p>奇异值分解和特征值分解通过它们的投影关系密切相关：</p>
<ul>
<li><span class="math inline">\(\boldsymbol{A}\)</span> 的左奇异向量是
<span class="math inline">\(\boldsymbol{A} \boldsymbol{A}^\top\)</span>
的特征向量；</li>
<li><span class="math inline">\(\boldsymbol{A}\)</span> 的右奇异向量是
<span class="math inline">\(\boldsymbol{A}^\top \boldsymbol{A}\)</span>
的特征向量；</li>
<li><span class="math inline">\(\boldsymbol{A}\)</span> 的非零奇异值是
<span class="math inline">\(\boldsymbol{A} \boldsymbol{A}^\top\)</span>
和 <span class="math inline">\(\boldsymbol{A}^\top
\boldsymbol{A}\)</span> 的非零特征值的平方根。</li>
</ul></li>
<li><p>对于对称矩阵 <span class="math inline">\(\boldsymbol{A} \in
\mathbb{R}^{n \times
n}\)</span>，特征值分解和奇异值分解是相同的，这符合谱定理（定理
4.15）。</p></li>
</ul>
<p><img src="/img3/机器学习的数学基础Part1/图解SVD_1.png" alt="图解SVD_1" style="zoom:50%;" /></p>
<p><img src="/img3/机器学习的数学基础Part1/图解SVD_2.png" alt="图解SVD_2" style="zoom:50%;" /></p>
<h4 id="矩阵逼近">4.5.5 矩阵逼近</h4>
<p>我们可以使用奇异值分解（SVD）将秩<span
class="math inline">\(r\)</span> 矩阵<span
class="math inline">\(\boldsymbol{A}\)</span>降为秩 <span
class="math inline">\(k\)</span> 矩阵 <span
class="math inline">\(\widehat{\boldsymbol{A}}\)</span>
，这是一种取主要成分并达到最优的（在谱范数意义上）方式。我们可以将秩<span
class="math inline">\(k\)</span>矩阵对<span
class="math inline">\(\boldsymbol{A}\)</span>的逼近解释为有损压缩的一种方法。因此，矩阵的低秩逼近出现在许多机器学习应用中，例如图像处理、噪声滤波和不适定问题的正则化。此外，它在降维和主成分分析中起着关键作用</p>
<p>秩为 <span class="math inline">\(r\)</span> 的矩阵 <span
class="math inline">\(\boldsymbol{A} \in \mathbb{R}^{m \times
n}\)</span> 能被写成秩 <span class="math inline">\(1\)</span> 矩阵 <span
class="math inline">\(\boldsymbol{A}_i\)</span> 的和： <span
class="math display">\[
\boldsymbol{A} = \sum_{i=1}^{r} \sigma_{i} \boldsymbol{u}_{i}
\boldsymbol{v}_{i}^{\top}
= \sum_{i=1}^{r} \sigma_{i} \boldsymbol{A}_{i},
\]</span> 其中，外积矩阵 <span
class="math inline">\(\boldsymbol{A}_{i}\)</span> 的权重为第 <span
class="math inline">\(i\)</span> 个奇异值 <span
class="math inline">\(\sigma_{i}\)</span>。</p>
<h3 id="矩阵分类关系图">4.6 矩阵分类关系图</h3>
<p>Matrix Phylogeny</p>
<blockquote>
<p>可对角化其实就是可特征值分解。</p>
</blockquote>
<p><img src="/img3/机器学习的数学基础Part1/矩阵分类关系图.png" alt="矩阵分类关系图" style="zoom:50%;" /></p>
<p>注：英文对应的中文翻译见文后。</p>
<p>在第2章和第3章中，我们介绍了线性代数和解析几何的基础知识。在这一章中，我们研究了矩阵和线性映射的基本特征。图4.13描述了不同类型矩阵之间关系的
Phylogeny（黑色箭头表示子集）以及我们可以对其执行的操作（蓝色）。</p>
<p>我们考虑所有实矩阵（real matrices）<span
class="math inline">\(\boldsymbol{A} \in \mathbb{R}^{n \times
m}\)</span>。对于非方阵（其中 <span class="math inline">\(n \neq
m\)</span>），奇异值分解总是存在的，正如我们在本章中看到的。以方阵（square
matrices）<span class="math inline">\(\boldsymbol{A} \in \mathbb{R}^{n
\times n}\)</span> 为中心，行列式告诉我们方阵是否具有逆矩阵（inverse
matrix），即它是否属于正则可逆矩阵类。如果 <span class="math inline">\(n
\times n\)</span> 矩阵具有 <span class="math inline">\(n\)</span>
个线性无关的特征向量，则矩阵是非退化的（non-defective），并且存在特征值分解（定理4.12）。我们知道，重复的特征值可能导致矩阵退化，这种矩阵是不能对角化的。</p>
<p>非奇异矩阵和非退化矩阵是不同的。例如，旋转矩阵是可逆的（行列式是非零的），但不一定可对角化（特征值不能保证是实数）。例子详见《非奇异矩阵和非退化矩阵.md》</p>
<p>我们进一步研究了非退化 <span class="math inline">\(n \times
n\)</span> 方阵的分支。如果条件<span
class="math inline">\(\boldsymbol{A}^{\top} \boldsymbol{A} =
\boldsymbol{A} \boldsymbol{A}^{\top}\)</span>成立，则 <span
class="math inline">\(\boldsymbol{A}\)</span>
是正规的（normal）。此外，如果更严格的条件<span
class="math inline">\(\boldsymbol{A}^{\top} \boldsymbol{A} =
\boldsymbol{A} \boldsymbol{A}^{\top} = \boldsymbol{I}\)</span>成立，则
<span class="math inline">\(\boldsymbol{A}\)</span>
称为正交（orthogonal，见定义3.8）。正交矩阵集是正则（可逆）矩阵的子集，满足<span
class="math inline">\(\boldsymbol{A}^{\top} =
\boldsymbol{A}^{-1}.\)</span></p>
<p>正规矩阵有一个常见的子集，即对称矩阵 <span
class="math inline">\(\boldsymbol{S} \in \mathbb{R}^{n \times
n}\)</span>，它满足<span class="math inline">\(\boldsymbol{S} =
\boldsymbol{S}^{\top}.\)</span>对称矩阵只有实特征值。对称矩阵的子集由正定矩阵
<span class="math inline">\(\boldsymbol{P}\)</span> 组成，正定矩阵 <span
class="math inline">\(\boldsymbol{P}\)</span> 对所有 <span
class="math inline">\(\boldsymbol{x} \in \mathbb{R}^{n} \backslash
\{\mathbf{0}\}\)</span> 满足条件<span
class="math inline">\(\boldsymbol{x}^{\top} \boldsymbol{P}
\boldsymbol{x} &gt; 0.\)</span>在这种情况下，存在唯一的 Cholesky
分解（Cholesky
decomposition，定理4.18）。<strong>正定矩阵只有正特征值且总是可逆的（即，具有非零行列式）。</strong></p>
<p>对称矩阵的另一个子集由对角矩阵（diagonal matrices）<span
class="math inline">\(\boldsymbol{D}\)</span>
组成。对角矩阵在乘法和加法下是闭合的，但不一定形成一个群（只有当所有的对角项都不为零时才是这种情况，这样矩阵才是可逆的）。一种特殊的对角矩阵是单位矩阵
<span class="math inline">\(\boldsymbol{I}\)</span>。</p>
<figure>
<img src="/img3/机器学习的数学基础Part1/常见矩阵分解总结表.png"
alt="常见矩阵分解总结表" />
<figcaption aria-hidden="true">常见矩阵分解总结表</figcaption>
</figure>
<p>上表来源《鸢尾花套书》Book4_ch24 “矩阵力量”</p>
<p><strong>矩阵分类关系图的英文翻译：</strong></p>
<table>
<thead>
<tr class="header">
<th>English</th>
<th>中文</th>
<th>说明（可选）</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>No basis of eigenvectors</td>
<td>特征向量不是基</td>
<td>不可对角化</td>
</tr>
<tr class="even">
<td>Basis of eigenvectors</td>
<td>特征向量是基</td>
<td>可对角化</td>
</tr>
<tr class="odd">
<td>Singular</td>
<td>奇异的</td>
<td><span class="math inline">\(\det=0\)</span> ，不可逆</td>
</tr>
<tr class="even">
<td>Regular</td>
<td>正则的</td>
<td>可逆矩阵</td>
</tr>
<tr class="odd">
<td>Defective</td>
<td>退化的</td>
<td>不具备完备特征向量组</td>
</tr>
<tr class="even">
<td>Square</td>
<td>方阵</td>
<td>行数 = 列数</td>
</tr>
<tr class="odd">
<td>Normal</td>
<td>正规的</td>
<td><span class="math inline">\(A^TA = AA^T\)</span></td>
</tr>
<tr class="even">
<td>Symmetric</td>
<td>对称的</td>
<td><span class="math inline">\(A = A^T\)</span></td>
</tr>
<tr class="odd">
<td>Diagonal</td>
<td>对角的</td>
<td>主对角线以外全是0</td>
</tr>
<tr class="even">
<td>Identity matrix</td>
<td>单位矩阵</td>
<td><span class="math inline">\(I\)</span></td>
</tr>
<tr class="odd">
<td>Orthogonal</td>
<td>正交的</td>
<td><span class="math inline">\(A^TA = I\)</span></td>
</tr>
<tr class="even">
<td>Rotation</td>
<td>旋转矩阵</td>
<td>正交且 $ =+1$</td>
</tr>
<tr class="odd">
<td>Positive definite</td>
<td>正定的</td>
<td>特征值 &gt; 0</td>
</tr>
<tr class="even">
<td>Eigenvalues</td>
<td>特征值</td>
<td>—</td>
</tr>
</tbody>
</table>
<p>全书的读书笔记（共7篇）如下，可点击此链接直接跳转：<br />
<a href="/2025/12/05/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%B8%80/" title="《机器学习的数学基础》（1&#x2F;7）">《机器学习的数学基础》读书笔记之一 ：导言</a><br />
<a href="/2025/10/29/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%BA%8C/" title="《机器学习的数学基础》（2&#x2F;7）">《机器学习的数学基础》读书笔记之二 ：线性代数</a><br />
<a href="/2025/10/28/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%B8%89/" title="《机器学习的数学基础》（3&#x2F;7）">《机器学习的数学基础》读书笔记之三 ：解析几何</a><br />
<a href="/2025/10/27/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%9B%9B/" title="《机器学习的数学基础》（4&#x2F;7）">《机器学习的数学基础》读书笔记之四 ：矩阵分解</a><br />
<a href="/2025/10/26/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%BA%94/" title="《机器学习的数学基础》（5&#x2F;7）">《机器学习的数学基础》读书笔记之五 ：向量微积分</a><br />
<a href="/2025/10/25/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%85%AD/" title="《机器学习的数学基础》（6&#x2F;7）">《机器学习的数学基础》读书笔记之六 ：概率与分布</a><br />
<a href="/2025/10/24/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%B8%83/" title="《机器学习的数学基础》（7&#x2F;7）">《机器学习的数学基础》读书笔记之七 ：连续优化</a></p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a>
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
              <a href="/tags/%E6%95%B0%E5%AD%A6/" rel="tag"># 数学</a>
              <a href="/tags/%E7%AE%97%E6%B3%95/" rel="tag"># 算法</a>
              <a href="/tags/%E7%BB%9F%E8%AE%A1/" rel="tag"># 统计</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2025/10/26/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%BA%94/" rel="prev" title="《机器学习的数学基础》（5/7）">
                  <i class="fa fa-chevron-left"></i> 《机器学习的数学基础》（5/7）
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2025/10/28/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%B8%89/" rel="next" title="《机器学习的数学基础》（3/7）">
                  《机器学习的数学基础》（3/7） <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Rayman.hung</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
<script src="https://cdn.jsdelivr.net/npm/hexo-generator-searchdb@1.4.0/dist/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js","integrity":"sha256-r+3itOMtGGjap0x+10hu6jW/gZCzxHsoKrOd7gyRSGY="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>

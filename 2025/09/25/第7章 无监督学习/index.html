<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.1.1">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" integrity="sha256-DfWjNxDkM94fVBWx1H5BMMp0Zq7luBlV8QRcSES7s+0=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"hongyitong.github.io","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.11.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="《Practical Statistics for Data Scientists》书籍英文版 《面向数据科学家的实用统计学》中文版书籍 第7章 无监督学习 Unsupervised Learning 无监督学习这个术语指的是在没有标注数据（即已知感兴趣结果的数据）的情况下，从数据中提取意义的统计方法。在第四章到第六章中，目标是构建一个模型（一套规则），用一组预测变量来预测一个响应变量。这就是监督">
<meta property="og:type" content="article">
<meta property="og:title" content="第7章 无监督学习">
<meta property="og:url" content="http://hongyitong.github.io/2025/09/25/%E7%AC%AC7%E7%AB%A0%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="忆桐之家的博客">
<meta property="og:description" content="《Practical Statistics for Data Scientists》书籍英文版 《面向数据科学家的实用统计学》中文版书籍 第7章 无监督学习 Unsupervised Learning 无监督学习这个术语指的是在没有标注数据（即已知感兴趣结果的数据）的情况下，从数据中提取意义的统计方法。在第四章到第六章中，目标是构建一个模型（一套规则），用一组预测变量来预测一个响应变量。这就是监督">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E9%9D%A2%E5%90%91%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%AE%B6%E7%9A%84%E5%AE%9E%E7%94%A8%E7%BB%9F%E8%AE%A1%E5%AD%A6/F7.1.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E9%9D%A2%E5%90%91%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%AE%B6%E7%9A%84%E5%AE%9E%E7%94%A8%E7%BB%9F%E8%AE%A1%E5%AD%A6/F7.2.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E9%9D%A2%E5%90%91%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%AE%B6%E7%9A%84%E5%AE%9E%E7%94%A8%E7%BB%9F%E8%AE%A1%E5%AD%A6/F7.3.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E9%9D%A2%E5%90%91%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%AE%B6%E7%9A%84%E5%AE%9E%E7%94%A8%E7%BB%9F%E8%AE%A1%E5%AD%A6/F7.4.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E9%9D%A2%E5%90%91%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%AE%B6%E7%9A%84%E5%AE%9E%E7%94%A8%E7%BB%9F%E8%AE%A1%E5%AD%A6/F7.5.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E9%9D%A2%E5%90%91%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%AE%B6%E7%9A%84%E5%AE%9E%E7%94%A8%E7%BB%9F%E8%AE%A1%E5%AD%A6/F7.6.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E9%9D%A2%E5%90%91%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%AE%B6%E7%9A%84%E5%AE%9E%E7%94%A8%E7%BB%9F%E8%AE%A1%E5%AD%A6/F7.7.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E9%9D%A2%E5%90%91%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%AE%B6%E7%9A%84%E5%AE%9E%E7%94%A8%E7%BB%9F%E8%AE%A1%E5%AD%A6/F7.8.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E9%9D%A2%E5%90%91%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%AE%B6%E7%9A%84%E5%AE%9E%E7%94%A8%E7%BB%9F%E8%AE%A1%E5%AD%A6/F7.9.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E9%9D%A2%E5%90%91%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%AE%B6%E7%9A%84%E5%AE%9E%E7%94%A8%E7%BB%9F%E8%AE%A1%E5%AD%A6/F7.10.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E9%9D%A2%E5%90%91%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%AE%B6%E7%9A%84%E5%AE%9E%E7%94%A8%E7%BB%9F%E8%AE%A1%E5%AD%A6/F7.11.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E9%9D%A2%E5%90%91%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%AE%B6%E7%9A%84%E5%AE%9E%E7%94%A8%E7%BB%9F%E8%AE%A1%E5%AD%A6/F7.12.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E9%9D%A2%E5%90%91%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%AE%B6%E7%9A%84%E5%AE%9E%E7%94%A8%E7%BB%9F%E8%AE%A1%E5%AD%A6/F7.13.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E9%9D%A2%E5%90%91%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%AE%B6%E7%9A%84%E5%AE%9E%E7%94%A8%E7%BB%9F%E8%AE%A1%E5%AD%A6/F7.14.png">
<meta property="article:published_time" content="2025-09-24T16:00:00.000Z">
<meta property="article:modified_time" content="2025-09-25T03:04:25.146Z">
<meta property="article:author" content="Rayman.hung">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="Python">
<meta property="article:tag" content="统计">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="数据科学">
<meta property="article:tag" content="R">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://hongyitong.github.io/img3/%E9%9D%A2%E5%90%91%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%AE%B6%E7%9A%84%E5%AE%9E%E7%94%A8%E7%BB%9F%E8%AE%A1%E5%AD%A6/F7.1.png">


<link rel="canonical" href="http://hongyitong.github.io/2025/09/25/%E7%AC%AC7%E7%AB%A0%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://hongyitong.github.io/2025/09/25/%E7%AC%AC7%E7%AB%A0%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/","path":"2025/09/25/第7章 无监督学习/","title":"第7章 无监督学习"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>第7章 无监督学习 | 忆桐之家的博客</title>
  





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">忆桐之家的博客</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Rayman&Tony</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section">首页</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section">分类</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section">关于</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section">归档</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section">标签</a></li><li class="menu-item menu-item-commonweal"><a href="/404.html" rel="section">公益 404</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC7%E7%AB%A0-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="nav-number">1.</span> <span class="nav-text">第7章 无监督学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90"><span class="nav-number">1.1.</span> <span class="nav-text">主成分分析</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E4%BE%8B%E5%AD%90"><span class="nav-number">1.1.1.</span> <span class="nav-text">一个简单的例子</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E4%B8%BB%E6%88%90%E5%88%86"><span class="nav-number">1.1.2.</span> <span class="nav-text">计算主成分</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%A7%A3%E9%87%8A%E4%B8%BB%E6%88%90%E5%88%86"><span class="nav-number">1.1.3.</span> <span class="nav-text">解释主成分</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AF%B9%E5%BA%94%E5%88%86%E6%9E%90"><span class="nav-number">1.1.4.</span> <span class="nav-text">对应分析</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#k-%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB"><span class="nav-number">1.2.</span> <span class="nav-text">K-均值聚类</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E4%BE%8B%E5%AD%90-1"><span class="nav-number">1.2.1.</span> <span class="nav-text">一个简单的例子</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#k-%E5%9D%87%E5%80%BC%E7%AE%97%E6%B3%95"><span class="nav-number">1.2.2.</span> <span class="nav-text">K-均值算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%A7%A3%E9%87%8A%E7%B0%87"><span class="nav-number">1.2.3.</span> <span class="nav-text">解释簇</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%80%89%E6%8B%A9%E7%B0%87%E7%9A%84%E6%95%B0%E9%87%8F"><span class="nav-number">1.2.4.</span> <span class="nav-text">选择簇的数量</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB"><span class="nav-number">1.3.</span> <span class="nav-text">层次聚类</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E4%BE%8B%E5%AD%90-2"><span class="nav-number">1.3.1.</span> <span class="nav-text">一个简单的例子</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A0%91%E7%8A%B6%E5%9B%BE"><span class="nav-number">1.3.2.</span> <span class="nav-text">树状图</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95"><span class="nav-number">1.3.3.</span> <span class="nav-text">聚类算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%9B%B8%E5%BC%82%E5%BA%A6%E5%BA%A6%E9%87%8F"><span class="nav-number">1.3.4.</span> <span class="nav-text">相异度度量</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%81%9A%E7%B1%BB"><span class="nav-number">1.4.</span> <span class="nav-text">基于模型的聚类</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A4%9A%E5%85%83%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83"><span class="nav-number">1.4.1.</span> <span class="nav-text">多元正态分布</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83%E6%B7%B7%E5%90%88"><span class="nav-number">1.4.2.</span> <span class="nav-text">正态分布混合</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%80%89%E6%8B%A9%E7%B0%87%E7%9A%84%E6%95%B0%E9%87%8F-1"><span class="nav-number">1.4.3.</span> <span class="nav-text">选择簇的数量</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BC%A9%E6%94%BE%E5%92%8C%E5%88%86%E7%B1%BB%E5%8F%98%E9%87%8F"><span class="nav-number">1.5.</span> <span class="nav-text">缩放和分类变量</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%98%E9%87%8F%E7%BC%A9%E6%94%BE"><span class="nav-number">1.5.1.</span> <span class="nav-text">变量缩放</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%BB%E5%AF%BC%E5%8F%98%E9%87%8F"><span class="nav-number">1.5.2.</span> <span class="nav-text">主导变量</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%86%E7%B1%BB%E6%95%B0%E6%8D%AE%E5%92%8C-gowers-%E8%B7%9D%E7%A6%BB"><span class="nav-number">1.5.3.</span> <span class="nav-text">分类数据和 Gower&#39;s
距离</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B7%B7%E5%90%88%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E9%97%AE%E9%A2%98"><span class="nav-number">1.5.4.</span> <span class="nav-text">混合数据的聚类问题</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">1.6.</span> <span class="nav-text">总结</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Rayman.hung</p>
  <div class="site-description" itemprop="description">技术分享、读书心得、亲子时刻</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives">
          <span class="site-state-item-count">903</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags">
        <span class="site-state-item-count">1151</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/hongyitong" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;hongyitong" rel="noopener" target="_blank">GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="http://blog.sina.com.cn/yitonghong" title="忆桐之家 → http:&#x2F;&#x2F;blog.sina.com.cn&#x2F;yitonghong" rel="noopener" target="_blank">忆桐之家</a>
      </span>
      <span class="links-of-author-item">
        <a href="http://douban.com/people/2780741" title="豆瓣 → http:&#x2F;&#x2F;douban.com&#x2F;people&#x2F;2780741" rel="noopener" target="_blank">豆瓣</a>
      </span>
      <span class="links-of-author-item">
        <a href="http://www.zhihu.com/people/rayman-36" title="知乎 → http:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;rayman-36" rel="noopener" target="_blank">知乎</a>
      </span>
      <span class="links-of-author-item">
        <a href="http://www.liaoxuefeng.com/" title="廖雪峰的官方网站 → http:&#x2F;&#x2F;www.liaoxuefeng.com&#x2F;" rel="noopener" target="_blank">廖雪峰的官方网站</a>
      </span>
      <span class="links-of-author-item">
        <a href="http://www.vaikan.com/" title="外刊IT评论 → http:&#x2F;&#x2F;www.vaikan.com&#x2F;" rel="noopener" target="_blank">外刊IT评论</a>
      </span>
  </div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://hongyitong.github.io/2025/09/25/%E7%AC%AC7%E7%AB%A0%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Rayman.hung">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="忆桐之家的博客">
      <meta itemprop="description" content="技术分享、读书心得、亲子时刻">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="第7章 无监督学习 | 忆桐之家的博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          第7章 无监督学习
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2025-09-25 00:00:00 / 修改时间：11:04:25" itemprop="dateCreated datePublished" datetime="2025-09-25T00:00:00+08:00">2025-09-25</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/" itemprop="url" rel="index"><span itemprop="name">读书心得</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p><a
href="/img3/面向数据科学家的实用统计学/Practical-Statistics-for-Data-Scientists.pdf">《Practical
Statistics for Data Scientists》书籍英文版</a><br />
<a
href="/img3/面向数据科学家的实用统计学/面向数据科学家的实用统计学.pdf">《面向数据科学家的实用统计学》中文版书籍</a></p>
<h2 id="第7章-无监督学习"><strong>第7章 无监督学习</strong></h2>
<p>Unsupervised Learning</p>
<p><strong>无监督学习</strong>这个术语指的是在<strong>没有标注数据</strong>（即已知感兴趣结果的数据）的情况下，从数据中提取意义的统计方法。在第四章到第六章中，目标是构建一个模型（一套规则），用一组预测变量来预测一个响应变量。这就是<strong>监督学习</strong>。与此相反，<strong>无监督学习</strong>也构建数据的模型，但它<strong>不区分响应变量和预测变量</strong>。</p>
<p>无监督学习可以用于实现不同的目标。在某些情况下，当没有带标签的响应变量时，它可用于创建预测规则。例如，<strong>聚类方法</strong>可以用于识别有意义的数据组。我们可以使用用户在网站上的点击和人口统计数据，将不同类型的用户分组。然后，网站可以根据这些不同类型进行<strong>个性化</strong>。</p>
<p>在另一些情况下，目标可能是将数据的<strong>维度降至</strong>一个更易于管理的变量集。然后，这个缩减后的集合可以作为输入用于预测模型，比如回归或分类。例如，我们可能有成千上万个传感器来监测一个工业过程。通过将数据简化为一个更小的特征集，我们也许能够构建一个比包含数千个传感器数据流更强大、更可解释的<strong>过程故障预测模型</strong>。</p>
<p>最后，无监督学习可以被视为<strong>探索性数据分析</strong>（参见第一章）的延伸，适用于您面对大量变量和记录的情况。其目的是深入了解一组数据以及不同变量之间的相互关系。无监督技术让您能够筛选和分析这些变量，并发现其中的关系。
<span id="more"></span></p>
<blockquote>
<p>通用注解：</p>
<p><strong>无监督学习与预测</strong>（Unsupervised Learning and
Prediction）</p>
<p>无监督学习在<strong>预测</strong>中可以扮演重要角色，无论是对于回归还是分类问题。在某些情况下，我们想在没有任何标注数据的情况下预测一个类别。例如，我们可能想根据一组卫星传感器数据来预测某个区域的植被类型。由于我们没有响应变量来训练模型，聚类为我们提供了一种识别<strong>常见模式</strong>并对区域进行<strong>分类</strong>的方法。</p>
<p>聚类是解决<strong>“冷启动问题”</strong>（cold-start
problem）的一个特别重要的工具。在这种类型的问题中，例如发起一项新的营销活动或识别潜在的新型欺诈或垃圾邮件，我们最初可能没有任何响应变量来训练模型。随着时间的推移，当数据被收集起来后，我们可以更多地了解该系统并构建一个传统的预测模型。但聚类通过识别<strong>人口细分</strong>来帮助我们更快地启动学习过程。</p>
<p>无监督学习作为回归和分类技术的<strong>构建模块</strong>也很重要。对于<strong>大数据</strong>，如果一个小的子群体在整体群体中没有得到很好的代表，那么训练出的模型在该子群体上的表现可能不佳。通过<strong>聚类</strong>，可以识别并标记这些子群体。然后，可以为不同的子群体分别拟合单独的模型。或者，子群体可以用其自身的<strong>特征</strong>来表示，迫使整体模型明确地将子群体身份作为预测变量来考虑。</p>
</blockquote>
<h3 id="主成分分析"><strong>主成分分析</strong></h3>
<p>Principal Components Analysis</p>
<p>通常，变量会<strong>共同变化（协变）</strong>，其中一个变量的某些变异实际上被另一个变量的变异所<strong>重复</strong>（例如，餐馆账单和给的小费）。<strong>主成分分析（PCA）</strong>是一种用于发现<strong>数值变量如何协变</strong>的技术。</p>
<p><strong>主成分分析的关键术语</strong></p>
<ul>
<li><p><strong>主成分（Principal component）</strong>
预测变量的<strong>线性组合</strong>。</p></li>
<li><p><strong>载荷（Loadings）</strong>
将预测变量转换为成分的<strong>权重</strong>。
同义词：<strong>权重（Weights）</strong></p></li>
<li><p><strong>碎石图（Screeplot）</strong>
<strong>成分方差图</strong>，显示了成分的相对重要性，可以是<strong>解释方差</strong>或<strong>解释方差的比例</strong>。</p></li>
</ul>
<p>PCA
的思想是将多个<strong>数值预测变量</strong>组合成一个<strong>更小的变量集</strong>，这些变量是原始变量集的<strong>加权线性组合</strong>。这个更小的变量集（即<strong>主成分</strong>）“解释”了<strong>大部分原始变量集的变异性</strong>，从而<strong>降低了数据的维度</strong>。用于形成主成分的权重揭示了原始变量对新主成分的相对贡献。</p>
<p>PCA
最初由<strong>卡尔·皮尔逊</strong>提出。在一篇可能是关于<strong>无监督学习</strong>的开创性论文中，皮尔逊认识到在许多问题中，预测变量存在变异性，因此他开发了
PCA 作为一种对这种变异性进行建模的技术。PCA
可以被看作是<strong>线性判别分析</strong>的无监督版本；参见第201页的“判别分析”。</p>
<h4 id="一个简单的例子"><strong>一个简单的例子</strong></h4>
<p>对于两个变量 <span class="math inline">\(X_1\)</span> 和 <span
class="math inline">\(X_2\)</span>，有两个主成分 <span
class="math inline">\(Z_i\)</span>（<span
class="math inline">\(i=1\)</span> 或 <span
class="math inline">\(2\)</span>）：</p>
<p><span class="math display">\[
Z_i = w_{i,1}X_1 + w_{i,2}X_2
\]</span> 权重 <span class="math inline">\(w_{i,1}, w_{i,2}\)</span>
被称为<strong>成分载荷</strong>。这些载荷将原始变量转换为主成分。第一个主成分
<span class="math inline">\(Z_1\)</span>
是<strong>最能解释总变异性</strong>（variation）的线性组合。第二个主成分
<span class="math inline">\(Z_2\)</span>
与第一个<strong>正交</strong>，并尽可能多地解释<strong>剩余的变异性</strong>。（如果还有额外的成分，每个新增的成分都会与其他的正交。）</p>
<blockquote>
<p>通用注解：</p>
<p>通常，也对<strong>偏离预测变量均值</strong>的偏差而不是对变量本身的值来计算主成分。</p>
</blockquote>
<p>您可以使用 <code>princomp</code> 函数在 R
中计算主成分。以下代码对雪佛龙（CVX）和埃克森美孚（XOM）的股票价格回报进行了主成分分析（PCA）：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">oil_px <span class="operator">&lt;-</span> sp500_px<span class="punctuation">[</span><span class="punctuation">,</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;CVX&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;XOM&#x27;</span><span class="punctuation">)</span><span class="punctuation">]</span></span><br><span class="line">pca <span class="operator">&lt;-</span> princomp<span class="punctuation">(</span>oil_px<span class="punctuation">)</span></span><br><span class="line">pca<span class="operator">$</span>loadings</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Loadings:</span><br><span class="line">        Comp.1 Comp.2</span><br><span class="line">CVX -0.747  0.665</span><br><span class="line">XOM -0.665 -0.747</span><br><span class="line"></span><br><span class="line">               Comp.1 Comp.2</span><br><span class="line">SS loadings     1.0    1.0</span><br><span class="line">Proportion Var  0.5    0.5</span><br><span class="line">Cumulative Var  0.5    1.0</span><br></pre></td></tr></table></figure>
<p>在 Python 中，我们可以使用 <code>scikit-learn</code> 实现
<code>sklearn.decomposition.PCA</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pcs = PCA(n_components=<span class="number">2</span>)</span><br><span class="line">pcs.fit(oil_px)</span><br><span class="line">loadings = pd.DataFrame(pcs.components_, columns=oil_px.columns)</span><br><span class="line">loadings</span><br></pre></td></tr></table></figure>
<p>第一个主成分的 CVX 和 XOM 权重分别为 -0.747 和
-0.665，第二个主成分的权重分别为 0.665 和
-0.747。这该如何解释呢？第一个主成分<strong>本质上是 CVX 和 XOM
的平均值</strong>，反映了这两家能源公司之间的相关性。第二个主成分则衡量了<strong>CVX
和 XOM 的股票价格何时出现分歧</strong>。</p>
<p>将主成分与数据一起绘制出来是很有启发性的。这里我们用 R
创建一个可视化图：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">loadings <span class="operator">&lt;-</span> pca<span class="operator">$</span>loadings</span><br><span class="line">ggplot<span class="punctuation">(</span>data<span class="operator">=</span>oil_px<span class="punctuation">,</span> aes<span class="punctuation">(</span>x<span class="operator">=</span>CVX<span class="punctuation">,</span> y<span class="operator">=</span>XOM<span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">geom_point<span class="punctuation">(</span>alpha<span class="operator">=</span><span class="number">.3</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">stat_ellipse<span class="punctuation">(</span>type<span class="operator">=</span><span class="string">&#x27;norm&#x27;</span><span class="punctuation">,</span> level<span class="operator">=</span><span class="number">.99</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">geom_abline<span class="punctuation">(</span>intercept <span class="operator">=</span> <span class="number">0</span><span class="punctuation">,</span> slope <span class="operator">=</span> loadings<span class="punctuation">[</span><span class="number">2</span><span class="punctuation">,</span><span class="number">1</span><span class="punctuation">]</span><span class="operator">/</span>loadings<span class="punctuation">[</span><span class="number">1</span><span class="punctuation">,</span><span class="number">1</span><span class="punctuation">]</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">geom_abline<span class="punctuation">(</span>intercept <span class="operator">=</span> <span class="number">0</span><span class="punctuation">,</span> slope <span class="operator">=</span> loadings<span class="punctuation">[</span><span class="number">2</span><span class="punctuation">,</span><span class="number">2</span><span class="punctuation">]</span><span class="operator">/</span>loadings<span class="punctuation">[</span><span class="number">1</span><span class="punctuation">,</span><span class="number">2</span><span class="punctuation">]</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<p>以下代码在 Python 中创建了一个类似的可视化图：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">abline</span>(<span class="params">slope, intercept, ax</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Calculate coordinates of a line based on slope and intercept&quot;&quot;&quot;</span></span><br><span class="line">    x_vals = np.array(ax.get_xlim())</span><br><span class="line">    <span class="keyword">return</span> (x_vals, intercept + slope * x_vals)</span><br><span class="line">ax = oil_px.plot.scatter(x=<span class="string">&#x27;XOM&#x27;</span>, y=<span class="string">&#x27;CVX&#x27;</span>, alpha=<span class="number">0.3</span>, figsize=(<span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">ax.set_xlim(-<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">ax.set_ylim(-<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">ax.plot(*abline(loadings.loc[<span class="number">0</span>, <span class="string">&#x27;CVX&#x27;</span>] / loadings.loc[<span class="number">0</span>, <span class="string">&#x27;XOM&#x27;</span>], <span class="number">0</span>, ax),</span><br><span class="line">        <span class="string">&#x27;--&#x27;</span></span><br><span class="line">        , color=<span class="string">&#x27;C1&#x27;</span>)</span><br><span class="line">ax.plot(*abline(loadings.loc[<span class="number">1</span>, <span class="string">&#x27;CVX&#x27;</span>] / loadings.loc[<span class="number">1</span>, <span class="string">&#x27;XOM&#x27;</span>], <span class="number">0</span>, ax),</span><br><span class="line">        <span class="string">&#x27;--&#x27;</span></span><br><span class="line">        , color=<span class="string">&#x27;C1&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>结果如图7-1所示。
虚线显示了两个主成分的方向：第一个沿着椭圆的长轴，第二个沿着短轴。您可以看到，这两个股票回报的<strong>大部分变异性都由第一个主成分解释</strong>。这是有道理的，因为能源股价格倾向于作为一个群体移动。</p>
<p><img src="/img3/面向数据科学家的实用统计学/F7.1.png" alt="F7.1" style="zoom:40%;" /></p>
<blockquote>
<p>通用注解：</p>
<p>第一个主成分的权重都是负的，但将所有权重的符号反转并<strong>不会改变主成分</strong>。例如，对于第一个主成分，使用0.747和0.665的权重与使用负权重是等效的，就像由原点和(1,1)定义的无限线与由原点和(-1,-1)定义的线是同一条一样。</p>
</blockquote>
<h4 id="计算主成分"><strong>计算主成分</strong></h4>
<p>Computing the Principal Components</p>
<p>从两个变量扩展到更多变量是直接的。对于第一个成分，只需将额外的预测变量包含在线性组合中，并分配权重，以<strong>优化从所有预测变量中收集的协变</strong>（
the covariation from all the predictor
variables）到这个第一个主成分中（协方差（covariance）是统计术语；参见第202页的“协方差矩阵”）。主成分的计算是一种经典的统计方法，它依赖于<strong>数据的相关矩阵或协方差矩阵</strong>，并且执行迅速，<strong>不依赖于迭代</strong>。如前所述，主成分分析<strong>仅适用于数值变量</strong>，不适用于分类变量。</p>
<p>完整的流程可以描述如下：</p>
<ol type="1">
<li>在创建第一个主成分时，PCA
得到的预测变量线性组合能<strong>最大化解释的总方差百分比</strong>。</li>
<li>这个线性组合随后成为第一个“新”预测变量 <span
class="math inline">\(Z_1\)</span>。</li>
<li>PCA
重复此过程，使用相同的变量但采用不同的权重，来创建第二个新预测变量 <span
class="math inline">\(Z_2\)</span>。权重的分配使得 <span
class="math inline">\(Z_1\)</span> 和 <span
class="math inline">\(Z_2\)</span> <strong>不相关</strong>。</li>
<li>该过程持续进行，直到您拥有与原始变量 <span
class="math inline">\(X_i\)</span> 一样多的新变量（或成分）<span
class="math inline">\(Z_i\)</span>。</li>
<li>选择保留<strong>足以解释大部分方差</strong>的成分数量。</li>
<li>到目前为止，结果是每个成分的一组权重。最后一步是通过将权重应用于原始值，将原始数据转换为新的<strong>主成分得分</strong>。这些新得分随后可以用作<strong>简化后的预测变量集</strong>。</li>
</ol>
<h4 id="解释主成分"><strong>解释主成分</strong></h4>
<p>Interpreting Principal Components</p>
<p>主成分的性质通常能揭示<strong>数据的结构</strong>信息。有几种标准的<strong>可视化</strong>方式可以帮助您深入了解主成分。其中一种方法是<strong>碎石图</strong>，用于可视化主成分的相对重要性（其名称源于该图与<strong>碎石坡</strong>的相似之处；这里的y轴是<strong>特征值</strong>）。以下
R 代码显示了标普500指数中几家顶尖公司的例子：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">syms <span class="operator">&lt;-</span> <span class="built_in">c</span><span class="punctuation">(</span> <span class="string">&#x27;AAPL&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;MSFT&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;CSCO&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;INTC&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;CVX&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;XOM&#x27;</span><span class="punctuation">,</span></span><br><span class="line"><span class="string">&#x27;SLB&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;COP&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;JPM&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;WFC&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;USB&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;AXP&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;WMT&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;TGT&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;HD&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;COST&#x27;</span><span class="punctuation">)</span></span><br><span class="line">top_sp <span class="operator">&lt;-</span> sp500_px<span class="punctuation">[</span>row.names<span class="punctuation">(</span>sp500_px<span class="punctuation">)</span><span class="operator">&gt;=</span><span class="string">&#x27;2005-01-01&#x27;</span><span class="punctuation">,</span> syms<span class="punctuation">]</span></span><br><span class="line">sp_pca <span class="operator">&lt;-</span> princomp<span class="punctuation">(</span>top_sp<span class="punctuation">)</span></span><br><span class="line">screeplot<span class="punctuation">(</span>sp_pca<span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<p>从 scikit-learn 的结果中创建加载图的信息可在
<code>explained_variance_</code> 中获得。这里，我们将其转换为一个 pandas
数据框，并用它来制作一个<strong>条形图</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">syms = <span class="built_in">sorted</span>([<span class="string">&#x27;AAPL&#x27;</span>, <span class="string">&#x27;MSFT&#x27;</span>, <span class="string">&#x27;CSCO&#x27;</span>, <span class="string">&#x27;INTC&#x27;</span>, <span class="string">&#x27;CVX&#x27;</span>, <span class="string">&#x27;XOM&#x27;</span>, <span class="string">&#x27;SLB&#x27;</span>, <span class="string">&#x27;COP&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;JPM&#x27;</span>, <span class="string">&#x27;WFC&#x27;</span>, <span class="string">&#x27;USB&#x27;</span>, <span class="string">&#x27;AXP&#x27;</span>, <span class="string">&#x27;WMT&#x27;</span>, <span class="string">&#x27;TGT&#x27;</span>, <span class="string">&#x27;HD&#x27;</span>, <span class="string">&#x27;COST&#x27;</span>])</span><br><span class="line">top_sp = sp500_px.loc[sp500_px.index &gt;= <span class="string">&#x27;2011-01-01&#x27;</span>, syms]</span><br><span class="line">sp_pca = PCA()</span><br><span class="line">sp_pca.fit(top_sp)</span><br><span class="line">explained_variance = pd.DataFrame(sp_pca.explained_variance_)</span><br><span class="line">ax = explained_variance.head(<span class="number">10</span>).plot.bar(legend=<span class="literal">False</span>, figsize=(<span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">ax.set_xlabel(<span class="string">&#x27;Component&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>如图7-2所示，第一个主成分的方差相当大（通常如此），但其他靠前的主成分也具有显著性。</p>
<p><img src="/img3/面向数据科学家的实用统计学/F7.2.png" alt="F7.2" style="zoom:40%;" /></p>
<p>通常，绘制<strong>顶层主成分的权重</strong>会非常有启发性。在 R
中，一种方法是结合使用 <code>tidyr</code> 包的 <code>gather</code>
函数和 <code>ggplot</code>：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">library<span class="punctuation">(</span>tidyr<span class="punctuation">)</span></span><br><span class="line">loadings <span class="operator">&lt;-</span> sp_pca<span class="operator">$</span>loadings<span class="punctuation">[</span><span class="punctuation">,</span><span class="number">1</span><span class="operator">:</span><span class="number">5</span><span class="punctuation">]</span></span><br><span class="line">loadings<span class="operator">$</span>Symbol <span class="operator">&lt;-</span> row.names<span class="punctuation">(</span>loadings<span class="punctuation">)</span></span><br><span class="line">loadings <span class="operator">&lt;-</span> gather<span class="punctuation">(</span>loadings<span class="punctuation">,</span> <span class="string">&#x27;Component&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;Weight&#x27;</span><span class="punctuation">,</span></span><br><span class="line">ggplot<span class="punctuation">(</span>loadings<span class="punctuation">,</span> aes<span class="punctuation">(</span>x<span class="operator">=</span>Symbol<span class="punctuation">,</span> y<span class="operator">=</span>Weight<span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">geom_bar<span class="punctuation">(</span>stat<span class="operator">=</span><span class="string">&#x27;identity&#x27;</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">facet_grid<span class="punctuation">(</span>Component <span class="operator">~</span></span><br><span class="line">.<span class="punctuation">,</span> scales<span class="operator">=</span><span class="string">&#x27;free_y&#x27;</span><span class="punctuation">)</span></span><br><span class="line"><span class="operator">-</span>Symbol<span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<p>以下是用于在 Python 中创建相同可视化图的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">loadings = pd.DataFrame(sp_pca.components_[<span class="number">0</span>:<span class="number">5</span>, :], columns=top_sp.columns)</span><br><span class="line">maxPC = <span class="number">1.01</span> * np.<span class="built_in">max</span>(np.<span class="built_in">max</span>(np.<span class="built_in">abs</span>(loadings.loc[<span class="number">0</span>:<span class="number">5</span>, :])))</span><br><span class="line">f, axes = plt.subplots(<span class="number">5</span>, <span class="number">1</span>, figsize=(<span class="number">5</span>, <span class="number">5</span>), sharex=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">for</span> i, ax <span class="keyword">in</span> <span class="built_in">enumerate</span>(axes):</span><br><span class="line">pc_loadings = loadings.loc[i, :]</span><br><span class="line">colors = [<span class="string">&#x27;C0&#x27;</span> <span class="keyword">if</span> l &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="string">&#x27;C1&#x27;</span> <span class="keyword">for</span> l <span class="keyword">in</span> pc_loadings]</span><br><span class="line">ax.axhline(color=<span class="string">&#x27;#888888&#x27;</span>)</span><br><span class="line"></span><br><span class="line">pc_loadings.plot.bar(ax=ax, color=colors)</span><br><span class="line">ax.set_ylabel(<span class="string">f&#x27;PC<span class="subst">&#123;i+<span class="number">1</span>&#125;</span>&#x27;</span>)</span><br><span class="line">ax.set_ylim(-maxPC, maxPC)</span><br></pre></td></tr></table></figure>
<p><img src="/img3/面向数据科学家的实用统计学/F7.3.png" alt="F7.3" style="zoom:50%;" /></p>
<p>前五个成分的载荷如图7-3所示。</p>
<ul>
<li><strong>第一个主成分</strong>的载荷符号<strong>都相同</strong>：这对于所有列都共享一个共同因素（在本例中为<strong>整体股市趋势</strong>）的数据来说是很典型的。</li>
<li><strong>第二个成分</strong>捕捉了<strong>能源股</strong>与其他股票相比的价格变化。</li>
<li><strong>第三个成分</strong>主要是<strong>苹果和CostCo</strong>的走势对比。</li>
<li><strong>第四个成分</strong>对比了斯伦贝谢（SLB）与其他能源股的走势。</li>
<li>最后，<strong>第五个成分</strong>主要由<strong>金融公司</strong>主导。</li>
</ul>
<blockquote>
<p>通用注解：如何选择成分数量？How Many Components to Choose?
如果您的目标是<strong>降低数据的维度</strong>，您必须决定选择多少个主成分。最常见的方法是使用<strong>临时规则</strong>来选择那些能够解释“大部分”方差的成分。您可以通过<strong>碎石图</strong>进行直观操作，例如在图7-2中所示。或者，您可以选择<strong>累积方差超过某个阈值</strong>（如80%）的顶层成分。您还可以检查载荷，以确定该成分是否具有<strong>直观的解释</strong>。<strong>交叉验证</strong>为选择显著成分的数量提供了一种更正式的方法（更多内容请参见第155页的“交叉验证”）。</p>
</blockquote>
<h4 id="对应分析"><strong>对应分析</strong></h4>
<p>Correspondence Analysis</p>
<p><strong>PCA不能用于分类数据</strong>；然而，有一种与其<strong>有所关联的技术</strong>是<strong>对应分析（Correspondence
analysis）</strong>。其目标是<strong>识别类别之间或分类特征之间的关联</strong>。对应分析和主成分分析之间的相似之处主要在于<strong>底层</strong>——用于<strong>维度缩放的矩阵代数</strong>。对应分析主要用于<strong>低维分类数据的图形分析</strong>，不像
PCA 那样被用作<strong>大数据背景下的降维准备步骤</strong>。</p>
<p>输入可以看作一个表格，其中行代表一个变量，列代表另一个变量，而单元格代表记录计数。输出（经过一些矩阵代数处理后）是一个<strong>双标图（biplot）</strong>——一个<strong>散点图</strong>，其坐标轴经过缩放（百分比表示该维度解释了多少方差）。坐标轴上单位的含义与原始数据没有直观的联系，散点图的主要价值在于<strong>以图形方式展示相互关联的变量</strong>（通过在图上的邻近性）。例如，参见图7-4，其中家务任务根据是共同完成还是单独完成（垂直轴）以及主要由妻子还是丈夫负责（水平轴）进行排列。
从任务的分配来看，对应分析和这个示例的精神都已有数十年历史。</p>
<p><img src="/img3/面向数据科学家的实用统计学/F7.4.png" alt="F7.4" style="zoom:40%;" /></p>
<p>R 中有多种用于对应分析的软件包。这里，我们使用 <code>ca</code>
包：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ca_analysis <span class="operator">&lt;-</span> ca<span class="punctuation">(</span>housetasks<span class="punctuation">)</span></span><br><span class="line">plot<span class="punctuation">(</span>ca_analysis<span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<p>在 Python 中，我们可以使用 <code>prince</code> 包，它使用
scikit-learn API 实现了对应分析：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ca = prince.CA(n_components=<span class="number">2</span>)</span><br><span class="line">ca = ca.fit(housetasks)</span><br><span class="line">ca.plot_coordinates(housetasks, figsize=(<span class="number">6</span>, <span class="number">6</span>))</span><br></pre></td></tr></table></figure>
<p><strong>关键思想</strong></p>
<ul>
<li>主成分是预测变量的线性组合（<strong>仅限数值数据</strong>）。</li>
<li><strong>计算主成分是为了最小化成分之间的相关性，从而减少冗余。</strong></li>
<li>数量有限的成分通常可以解释因变量的大部分方差。</li>
<li>这个有限的主成分集可以用来代替（数量更多的）原始预测变量，从而达到降维的目的。</li>
<li>一种表面上相似的分类数据技术是对应分析，但它在大数据环境中并不实用。</li>
</ul>
<h3 id="k-均值聚类"><strong>K-均值聚类</strong></h3>
<p>K-Means Clustering</p>
<p><strong>聚类</strong>是一种将数据划分为不同组的技术，其中<strong>每组中的记录彼此相似</strong>。聚类的一个目标是识别<strong>重要且有意义的数据组</strong>。这些组可以直接使用，可以进行更深入的分析，或者作为<strong>特征或结果</strong>传递给预测性回归或分类模型。<strong>K-均值（K-means）</strong>是第一个被开发的聚类方法；由于其算法相对简单且能够扩展到大型数据集，它仍然被广泛使用。</p>
<blockquote>
<p>个人注：<strong>K最近邻 (K-Nearest Neighbors, KNN)</strong> 和
<strong>K均值 (K-Means)</strong>
算法虽然名字里都带“K”，但它们在机器学习领域扮演着截然不同的角色。</p>
<ul>
<li><strong>KNN</strong>
是一个<strong>监督学习</strong>算法，主要用于<strong>分类</strong>或<strong>回归</strong>。它需要<strong>有标签</strong>的训练数据。</li>
<li><strong>K均值</strong>
是一个<strong>无监督学习</strong>算法，主要用于<strong>聚类</strong>。它处理<strong>没有标签</strong>的数据。</li>
</ul>
<p>简单来说，KNN 是用来<strong>预测</strong>新数据点的类别或值，而
K均值是用来<strong>发现</strong>数据中固有的分组。</p>
</blockquote>
<p><strong>K-均值聚类的关键术语</strong></p>
<ul>
<li><strong>簇（Cluster）</strong> 一组彼此相似的记录。</li>
<li><strong>簇均值（Cluster mean）</strong>
簇中记录的变量均值向量。</li>
<li><strong>K</strong> 簇的数量。</li>
</ul>
<p>K-均值通过最小化<strong>每条记录到其所属簇均值的平方距离总和</strong>来将数据划分为
<span class="math inline">\(K\)</span>
个簇。这被称为<strong>簇内平方和（within-cluster sum of
squares）</strong>或<strong>簇内SS（within-cluster
SS）</strong>。K-均值不保证簇的大小相同，但它会找到<strong>分隔度最佳</strong>的簇。</p>
<blockquote>
<p>通用注解：标准化 Normalization</p>
<p>通常通过减去均值并除以标准差来对连续变量进行<strong>标准化</strong>（归一化）。否则，<strong>尺度较大的变量</strong>会主导聚类过程（参见第243页的“标准化（归一化、z-分数）”）。</p>
</blockquote>
<h4 id="一个简单的例子-1"><strong>一个简单的例子</strong></h4>
<p>从一个包含 <span class="math inline">\(n\)</span>
条记录和仅有两个变量 <span class="math inline">\(x\)</span> 和 <span
class="math inline">\(y\)</span> 的数据集开始考虑。假设我们想将数据分为
<span class="math inline">\(K = 4\)</span> 个簇。这意味着将每条记录
<span class="math inline">\((x_i, y_i)\)</span> 分配给一个簇 <span
class="math inline">\(k\)</span>。给定将 <span
class="math inline">\(n_k\)</span> 条记录分配到簇 <span
class="math inline">\(k\)</span> 的任务，该簇的中心 <span
class="math inline">\((\bar{x}_k, \bar{y}_k)\)</span>
是簇中所有点的均值：</p>
<p>簇均值： <span class="math display">\[
\bar{x}_k = \frac{1}{n_k} \sum_{i \in \text{Cluster } k} x_i
\]</span> <span class="math display">\[
\bar{y}_k = \frac{1}{n_k} \sum_{i \in \text{Cluster } k} y_i
\]</span></p>
<blockquote>
<p>警告：Cluster Mean
在对具有多个变量的记录进行聚类时（这是典型情况），<strong>簇均值</strong>这个术语指的不是一个单一的数字，而是<strong>变量均值的向量</strong>。</p>
</blockquote>
<p>一个簇内的平方和由下式给出：</p>
<p><span class="math display">\[
SS_k = \sum_{i \in \text{Cluster } k} (x_i - \bar{x}_k)^2 + (y_i -
\bar{y}_k)^2
\]</span></p>
<p>K-均值通过最小化所有四个簇的簇内平方和总和 <span
class="math inline">\(SS_1 + SS_2 + SS_3 + SS_4\)</span>
来寻找记录的最佳分配：</p>
<p><span class="math display">\[
\sum_{k=1}^{4} SS_k
\]</span></p>
<p>聚类的一个典型用途是<strong>定位数据中自然、分离的簇</strong>。另一个应用是将数据<strong>划分为预定数量的独立组</strong>，其中聚类用于确保这些组彼此尽可能地不同。</p>
<p>例如，假设我们想将每日股票回报分为四个组。K-均值聚类可用于将数据分离为最佳分组。请注意，每日股票回报的报告方式实际上是<strong>标准化</strong>的，因此我们不需要对数据进行归一化。在
R 中，K-均值聚类可以使用 <code>kmeans</code>
函数执行。例如，以下代码基于两个变量——埃克森美孚（XOM）和雪佛龙（CVX）的每日股票回报——寻找四个簇：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df <span class="operator">&lt;-</span> sp500_px<span class="punctuation">[</span>row.names<span class="punctuation">(</span>sp500_px<span class="punctuation">)</span><span class="operator">&gt;=</span><span class="string">&#x27;2011-01-01&#x27;</span><span class="punctuation">,</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;XOM&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;CVX&#x27;</span><span class="punctuation">)</span><span class="punctuation">]</span></span><br><span class="line">km <span class="operator">&lt;-</span> kmeans<span class="punctuation">(</span>df<span class="punctuation">,</span> centers<span class="operator">=</span><span class="number">4</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<p>在 Python 中，我们使用 scikit-learn 的
<code>sklearn.cluster.KMeans</code> 方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df = sp500_px.loc[sp500_px.index &gt;= <span class="string">&#x27;2011-01-01&#x27;</span>, [<span class="string">&#x27;XOM&#x27;</span>, <span class="string">&#x27;CVX&#x27;</span>]]</span><br><span class="line">kmeans = KMeans(n_clusters=<span class="number">4</span>).fit(df)</span><br></pre></td></tr></table></figure>
<p>每条记录的簇分配以 <code>cluster</code> 组件的形式返回（R）：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt; df$cluster &lt;- factor(km$cluster)</span><br><span class="line">&gt; head(df)</span><br><span class="line">                  XOM        CVX cluster</span><br><span class="line">2011-01-03 0.73680496 0.2406809 2</span><br><span class="line">2011-01-04 0.16866845 -0.5845157 1</span><br><span class="line">2011-01-05 0.02663055 0.4469854 2</span><br><span class="line">2011-01-06 0.24855834 -0.9197513 1</span><br><span class="line">2011-01-07 0.33732892 0.1805111 2</span><br><span class="line">2011-01-10 0.00000000 -0.4641675 1</span><br></pre></td></tr></table></figure>
<p>在 scikit-learn 中，簇标签在 <code>labels_</code> 字段中可用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">&#x27;cluster&#x27;</span>] = kmeans.labels_</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
<p>前六条记录被分配到簇1或簇2。簇的均值也一并返回（R）：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt; centers &lt;- data.frame(cluster=factor(1:4), km$centers)</span><br><span class="line">&gt; centers</span><br><span class="line">  cluster         XOM         CVX</span><br><span class="line">1       1 -0.3284864 -0.5669135</span><br><span class="line">2       2  0.2410159  0.3342130</span><br><span class="line">3       3 -1.1439800 -1.7502975</span><br><span class="line">4       4  0.9568628  1.3708892</span><br></pre></td></tr></table></figure>
<p>在 scikit-learn 中，簇中心在 <code>cluster_centers_</code>
字段中可用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">centers = pd.DataFrame(kmeans.cluster_centers_, columns=[<span class="string">&#x27;XOM&#x27;</span>, <span class="string">&#x27;CVX&#x27;</span>])</span><br><span class="line">centers</span><br></pre></td></tr></table></figure>
<p>簇1和3代表<strong>“下行”市场</strong>，而簇2和4代表<strong>“上行”市场</strong>。</p>
<p>由于
K-均值算法使用随机起始点，因此后续的运行和不同的方法实现可能会产生不同的结果。一般来说，您应该检查<strong>波动是否不太大</strong>。</p>
<p>在这个只有两个变量的例子中，<strong>可视化簇及其均值</strong>是直截了当的：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ggplot<span class="punctuation">(</span>data<span class="operator">=</span>df<span class="punctuation">,</span> aes<span class="punctuation">(</span>x<span class="operator">=</span>XOM<span class="punctuation">,</span> y<span class="operator">=</span>CVX<span class="punctuation">,</span> color<span class="operator">=</span>cluster<span class="punctuation">,</span> shape<span class="operator">=</span>cluster<span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">geom_point<span class="punctuation">(</span>alpha<span class="operator">=</span><span class="number">.3</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">geom_point<span class="punctuation">(</span>data<span class="operator">=</span>centers<span class="punctuation">,</span> aes<span class="punctuation">(</span>x<span class="operator">=</span>XOM<span class="punctuation">,</span> y<span class="operator">=</span>CVX<span class="punctuation">)</span><span class="punctuation">,</span> size<span class="operator">=</span><span class="number">3</span><span class="punctuation">,</span> stroke<span class="operator">=</span><span class="number">2</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<p><code>seaborn</code> 的 <code>scatterplot</code>
函数可以很容易地按属性对点进行颜色 (<code>hue</code>) 和样式
(<code>style</code>) 设置：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">ax = sns.scatterplot(x=<span class="string">&#x27;XOM&#x27;</span>, y=<span class="string">&#x27;CVX&#x27;</span>, hue=<span class="string">&#x27;cluster&#x27;</span>, style=<span class="string">&#x27;cluster&#x27;</span>,</span><br><span class="line">ax=ax, data=df)</span><br><span class="line">ax.set_xlim(-<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">ax.set_ylim(-<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">centers.plot.scatter(x=<span class="string">&#x27;XOM&#x27;</span>, y=<span class="string">&#x27;CVX&#x27;</span>, ax=ax, s=<span class="number">50</span>, color=<span class="string">&#x27;black&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>最终的图（如图7-5所示）显示了簇分配和簇均值。
请注意，K-均值会将记录分配给簇，即使这些簇<strong>没有很好地分离</strong>（如果您需要将记录最佳地划分为多个组，这可能是有用的）。</p>
<p><img src="/img3/面向数据科学家的实用统计学/F7.5.png" alt="F7.5" style="zoom:50%;" /></p>
<h4 id="k-均值算法"><strong>K-均值算法</strong></h4>
<p>K-Means Algorithm</p>
<p>通常，K-均值可以应用于具有 <span class="math inline">\(p\)</span>
个变量 <span class="math inline">\(X_1, \dots, X_p\)</span>
的数据集。虽然
K-均值的精确解在计算上非常困难，但<strong>启发式算法</strong>提供了一种有效的方式来计算<strong>局部最优解</strong>。</p>
<p>该算法从用户指定的 <span class="math inline">\(K\)</span>
值和一组初始簇均值开始，然后<strong>迭代</strong>以下步骤：</p>
<ol type="1">
<li>根据<strong>平方距离</strong>将每条记录分配给最近的簇均值。</li>
<li>根据记录的分配计算新的簇均值。</li>
</ol>
<p>当记录到簇的分配<strong>不再变化</strong>时，算法收敛。对于第一次迭代，您需要指定一组<strong>初始簇均值</strong>。通常，您可以通过将每条记录随机分配给
<span class="math inline">\(K\)</span>
个簇中的一个，然后找到这些簇的均值来实现。</p>
<p><strong>由于该算法不保证能找到最佳解决方案，因此建议使用不同的随机样本来初始化算法，并多次运行。当使用多组迭代时，K-均值的结果由簇内平方和最低的那次迭代给出。</strong></p>
<p>R 函数 <code>kmeans</code> 的 <code>nstart</code>
参数允许您指定要尝试的<strong>随机起始点数量</strong>。例如，以下代码运行
K-均值以寻找5个簇，使用了10个不同的起始簇均值：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">syms <span class="operator">&lt;-</span> <span class="built_in">c</span><span class="punctuation">(</span> <span class="string">&#x27;AAPL&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;MSFT&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;CSCO&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;INTC&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;CVX&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;XOM&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;SLB&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;COP&#x27;</span><span class="punctuation">,</span></span><br><span class="line"><span class="string">&#x27;JPM&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;WFC&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;USB&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;AXP&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;WMT&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;TGT&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;HD&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;COST&#x27;</span><span class="punctuation">)</span></span><br><span class="line">df <span class="operator">&lt;-</span> sp500_px<span class="punctuation">[</span>row.names<span class="punctuation">(</span>sp500_px<span class="punctuation">)</span> <span class="operator">&gt;=</span> <span class="string">&#x27;2011-01-01&#x27;</span><span class="punctuation">,</span> syms<span class="punctuation">]</span></span><br><span class="line">km <span class="operator">&lt;-</span> kmeans<span class="punctuation">(</span>df<span class="punctuation">,</span> centers<span class="operator">=</span><span class="number">5</span><span class="punctuation">,</span> nstart<span class="operator">=</span><span class="number">10</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<p>该函数会自动返回10个不同起始点中的<strong>最佳解决方案</strong>。您可以使用
<code>iter.max</code>
参数来设置算法每次随机开始所允许的最大迭代次数。</p>
<p>scikit-learn 算法默认重复10次（<code>n_init</code>）。参数
<code>max_iter</code>（默认300）可用于控制迭代次数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">syms = <span class="built_in">sorted</span>([<span class="string">&#x27;AAPL&#x27;</span>, <span class="string">&#x27;MSFT&#x27;</span>, <span class="string">&#x27;CSCO&#x27;</span>, <span class="string">&#x27;INTC&#x27;</span>, <span class="string">&#x27;CVX&#x27;</span>, <span class="string">&#x27;XOM&#x27;</span>, <span class="string">&#x27;SLB&#x27;</span>, <span class="string">&#x27;COP&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;JPM&#x27;</span>, <span class="string">&#x27;WFC&#x27;</span>, <span class="string">&#x27;USB&#x27;</span>, <span class="string">&#x27;AXP&#x27;</span>, <span class="string">&#x27;WMT&#x27;</span>, <span class="string">&#x27;TGT&#x27;</span>, <span class="string">&#x27;HD&#x27;</span>, <span class="string">&#x27;COST&#x27;</span>])</span><br><span class="line">top_sp = sp500_px.loc[sp500_px.index &gt;= <span class="string">&#x27;2011-01-01&#x27;</span>, syms]</span><br><span class="line">kmeans = KMeans(n_clusters=<span class="number">5</span>).fit(top_sp)</span><br></pre></td></tr></table></figure>
<h4 id="解释簇"><strong>解释簇</strong></h4>
<p>Interpreting the Clusters</p>
<p><strong>簇分析</strong>的一个重要部分可能涉及<strong>簇的解释</strong>。来自
<code>kmeans</code>
的两个最重要的输出是<strong>簇的大小</strong>和<strong>簇均值</strong>。对于前一小节中的例子，所得簇的大小由这个
R 命令给出：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">km<span class="operator">$</span>size</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[1] 106 186 285 288 266</span><br></pre></td></tr></table></figure>
<p>在 Python 中，我们可以使用标准库中的 <code>collections.Counter</code>
类来获取此信息。由于实现上的差异和算法固有的随机性，结果会有所不同：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line">Counter(kmeans.labels_)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Counter(&#123;4: 302, 2: 272, 0: 288, 3: 158, 1: 111&#125;)</span><br></pre></td></tr></table></figure>
<p>簇的大小相对均衡。<strong>不平衡的簇</strong>可能是由<strong>遥远的异常值</strong>或与数据其余部分<strong>截然不同的记录组</strong>造成的——这两种情况都可能需要进一步检查。</p>
<p>您可以使用 <code>gather</code> 函数结合 <code>ggplot</code>
绘制簇的中心：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">centers <span class="operator">&lt;-</span> as.data.frame<span class="punctuation">(</span>t<span class="punctuation">(</span>centers<span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line"><span class="built_in">names</span><span class="punctuation">(</span>centers<span class="punctuation">)</span> <span class="operator">&lt;-</span> paste<span class="punctuation">(</span><span class="string">&quot;Cluster&quot;</span><span class="punctuation">,</span> <span class="number">1</span><span class="operator">:</span><span class="number">5</span><span class="punctuation">)</span></span><br><span class="line">centers<span class="operator">$</span>Symbol <span class="operator">&lt;-</span> row.names<span class="punctuation">(</span>centers<span class="punctuation">)</span></span><br><span class="line">centers <span class="operator">&lt;-</span> gather<span class="punctuation">(</span>centers<span class="punctuation">,</span> <span class="string">&#x27;Cluster&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;Mean&#x27;</span><span class="punctuation">,</span></span><br><span class="line"><span class="operator">-</span>Symbol<span class="punctuation">)</span></span><br><span class="line">centers<span class="operator">$</span>Color <span class="operator">=</span> centers<span class="operator">$</span>Mean <span class="operator">&gt;</span> <span class="number">0</span></span><br><span class="line">ggplot<span class="punctuation">(</span>centers<span class="punctuation">,</span> aes<span class="punctuation">(</span>x<span class="operator">=</span>Symbol<span class="punctuation">,</span> y<span class="operator">=</span>Mean<span class="punctuation">,</span> fill<span class="operator">=</span>Color<span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">geom_bar<span class="punctuation">(</span>stat<span class="operator">=</span><span class="string">&#x27;identity&#x27;</span><span class="punctuation">,</span> position<span class="operator">=</span><span class="string">&#x27;identity&#x27;</span><span class="punctuation">,</span> width<span class="operator">=</span><span class="number">.75</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">facet_grid<span class="punctuation">(</span>Cluster <span class="operator">~</span></span><br><span class="line">.<span class="punctuation">,</span> scales<span class="operator">=</span><span class="string">&#x27;free_y&#x27;</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<p>在 Python 中创建这个可视化图的代码与我们用于 PCA 的代码类似：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">centers = pd.DataFrame(kmeans.cluster_centers_, columns=syms)</span><br><span class="line">f, axes = plt.subplots(<span class="number">5</span>, <span class="number">1</span>, figsize=(<span class="number">5</span>, <span class="number">5</span>), sharex=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">for</span> i, ax <span class="keyword">in</span> <span class="built_in">enumerate</span>(axes):</span><br><span class="line">center = centers.loc[i, :]</span><br><span class="line">maxPC = <span class="number">1.01</span> * np.<span class="built_in">max</span>(np.<span class="built_in">max</span>(np.<span class="built_in">abs</span>(center)))</span><br><span class="line">colors = [<span class="string">&#x27;C0&#x27;</span> <span class="keyword">if</span> l &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="string">&#x27;C1&#x27;</span> <span class="keyword">for</span> l <span class="keyword">in</span> center]</span><br><span class="line">ax.axhline(color=<span class="string">&#x27;#888888&#x27;</span>)</span><br><span class="line">center.plot.bar(ax=ax, color=colors)</span><br><span class="line">ax.set_ylabel(<span class="string">f&#x27;Cluster <span class="subst">&#123;i + <span class="number">1</span>&#125;</span>&#x27;</span>)</span><br><span class="line">ax.set_ylim(-maxPC, maxPC)</span><br></pre></td></tr></table></figure>
<p>生成的图（如图7-6所示）揭示了每个簇的性质。
例如，<strong>簇4和簇5</strong>分别对应于市场<strong>下行</strong>和<strong>上行</strong>的日子。<strong>簇2和簇3</strong>的特点分别是<strong>消费股</strong>市场<strong>上行</strong>和<strong>能源股</strong>市场<strong>下行</strong>的日子。最后，<strong>簇1</strong>捕捉了<strong>能源股上涨而消费股下跌</strong>的日子。</p>
<p><img src="/img3/面向数据科学家的实用统计学/F7.6.png" alt="F7.6" style="zoom:50%;" /></p>
<blockquote>
<p>通用注解：<strong>簇分析对比 PCA</strong> Cluster Analysis Versus
PCA</p>
<p>簇均值的图在精神上类似于观察<strong>主成分分析 (PCA)
的载荷</strong>；参见第289页的“解释主成分”。一个主要区别是，与 PCA
不同，<strong>簇均值的符号是有意义的</strong>。PCA
识别<strong>变异的主要方向</strong>，而簇分析则寻找<strong>彼此邻近的记录组</strong>。</p>
</blockquote>
<h4 id="选择簇的数量"><strong>选择簇的数量</strong></h4>
<p>Selecting the Number of Clusters</p>
<p>K-均值算法要求您指定<strong>簇的数量 <span
class="math inline">\(K\)</span></strong>。有时，簇的数量是由<strong>应用本身</strong>决定的。例如，一家管理销售团队的公司可能希望将客户聚类为不同的“人物画像”，以集中和指导销售拜访。在这种情况下，<strong>管理层面的考虑</strong>将决定所需客户细分的数量——例如，两个可能无法提供有用的客户区分，而八个可能又太多而难以管理。</p>
<p>在没有实际或管理考虑来决定簇数量的情况下，可以使用<strong>统计方法</strong>。目前没有一个单一的标准方法来找到“最佳”的簇数量。</p>
<p>一种常见的方法，称为<strong>“手肘法”（elbow
method）</strong>，是识别一组簇何时能解释数据中的“大部分”方差。在此集合之外添加新簇对解释方差的贡献相对较小。手肘是解释方差的<strong>累积曲线在急剧上升后趋于平缓的点</strong>，因此得名。</p>
<p><img src="/img3/面向数据科学家的实用统计学/F7.7.png" alt="F7.7" style="zoom:50%;" /></p>
<p>图7-7显示了贷款数据的<strong>累计方差解释百分比</strong>，簇的数量从2到15不等。
在这个例子中，手肘在哪里？没有一个明显的候选点，因为解释方差的<strong>增量增加是逐渐下降的</strong>。这在<strong>没有明确界定簇</strong>的数据中是相当典型的。这也许是手肘法的一个缺点，但它确实揭示了数据的性质。</p>
<p>在 R 中，<code>kmeans</code>
函数没有提供应用手肘法的单一命令，但可以很容易地从 <code>kmeans</code>
的输出中应用，如下所示：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">pct_var <span class="operator">&lt;-</span> data.frame<span class="punctuation">(</span>pct_var <span class="operator">=</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">num_clusters <span class="operator">=</span> <span class="number">2</span><span class="operator">:</span><span class="number">14</span><span class="punctuation">)</span></span><br><span class="line">totalss <span class="operator">&lt;-</span> kmeans<span class="punctuation">(</span>df<span class="punctuation">,</span> centers<span class="operator">=</span><span class="number">14</span><span class="punctuation">,</span> nstart<span class="operator">=</span><span class="number">50</span><span class="punctuation">,</span> iter.max<span class="operator">=</span><span class="number">100</span><span class="punctuation">)</span><span class="operator">$</span>totss</span><br><span class="line"><span class="keyword">for</span> <span class="punctuation">(</span>i <span class="keyword">in</span> <span class="number">2</span><span class="operator">:</span><span class="number">14</span><span class="punctuation">)</span> <span class="punctuation">&#123;</span></span><br><span class="line">kmCluster <span class="operator">&lt;-</span> kmeans<span class="punctuation">(</span>df<span class="punctuation">,</span> centers<span class="operator">=</span>i<span class="punctuation">,</span> nstart<span class="operator">=</span><span class="number">50</span><span class="punctuation">,</span> iter.max<span class="operator">=</span><span class="number">100</span><span class="punctuation">)</span></span><br><span class="line">pct_var<span class="punctuation">[</span>i<span class="operator">-</span><span class="number">1</span><span class="punctuation">,</span> <span class="string">&#x27;pct_var&#x27;</span><span class="punctuation">]</span> <span class="operator">&lt;-</span> kmCluster<span class="operator">$</span>betweenss <span class="operator">/</span> totalss</span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>对于 KMeans 的结果，我们从 <code>inertia_</code>
属性中获取此信息。转换为 pandas 数据框后，我们可以使用其
<code>plot</code> 方法来创建图表：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">inertia = []</span><br><span class="line"><span class="keyword">for</span> n_clusters <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, <span class="number">14</span>):</span><br><span class="line">kmeans = KMeans(n_clusters=n_clusters, random_state=<span class="number">0</span>).fit(top_sp)</span><br><span class="line">inertia.append(kmeans.inertia_ / n_clusters)</span><br><span class="line">inertias = pd.DataFrame(&#123;<span class="string">&#x27;n_clusters&#x27;</span>: <span class="built_in">range</span>(<span class="number">2</span>, <span class="number">14</span>), <span class="string">&#x27;inertia&#x27;</span>: inertia&#125;)</span><br><span class="line">ax = inertias.plot(x=<span class="string">&#x27;n_clusters&#x27;</span>, y=<span class="string">&#x27;inertia&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Number of clusters(k)&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Average Within-Cluster Squared Distances&#x27;</span>)</span><br><span class="line">plt.ylim((<span class="number">0</span>, <span class="number">1.1</span> * inertias.inertia.<span class="built_in">max</span>()))</span><br><span class="line">ax.legend().set_visible(<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>在评估要保留多少个簇时，也许最重要的检验是：<strong>这些簇在新数据上复制的可能性有多大？这些簇是否可解释，并且与数据的总体特征相关，还是仅仅反映了一个特定的实例？您可以通过使用交叉验证</strong>来部分评估这一点；参见第155页的“交叉验证”。</p>
<p>总的来说，<strong>没有一个单一的规则</strong>可以可靠地指导应该产生多少个簇。</p>
<blockquote>
<p>通用注解：</p>
<p>有几种更正式的方法可以根据统计学或信息论来确定簇的数量。例如，Robert
Tibshirani、Guenther Walther 和 Trevor Hastie
提出了一种基于统计理论的<strong>“间隙（gap）”统计量</strong>来识别手肘。对于大多数应用来说，理论方法可能不是必需的，甚至不一定是合适的。</p>
</blockquote>
<p><strong>关键思想</strong></p>
<ul>
<li>所需的簇数量 <strong><span class="math inline">\(K\)</span>
由用户选择</strong>。</li>
<li>算法通过<strong>迭代地将记录分配给最近的簇均值</strong>来发展簇，直到簇分配不再改变为止。</li>
<li>通常，实际考虑主导 <span class="math inline">\(K\)</span>
的选择；不存在一个由统计学决定的最佳簇数量。</li>
</ul>
<h3 id="层次聚类"><strong>层次聚类</strong></h3>
<p>Hierarchical Clustering</p>
<p><strong>层次聚类（Hierarchical clustering）是
K-均值的一种替代方案，它可以产生截然不同</strong>的簇。层次聚类允许用户<strong>可视化</strong>指定不同数量簇的效果。它在发现<strong>异常或离群的群体或记录</strong>方面更为敏感。层次聚类还适合于<strong>直观的图形显示</strong>，从而更容易解释簇。</p>
<p><strong>层次聚类的关键术语</strong></p>
<ul>
<li><strong>树状图（Dendrogram）</strong>
记录及其所属簇层次结构的<strong>视觉表示</strong>。</li>
<li><strong>距离（Distance）</strong>
衡量一条记录与另一条记录<strong>有多近</strong>的度量。</li>
<li><strong>相异度（Dissimilarity）</strong>
衡量一个簇与另一个簇<strong>有多近</strong>的度量。</li>
</ul>
<p>层次聚类的灵活性是有代价的，它不能很好地扩展到拥有数百万条记录的大型数据集。即使对于只有数万条记录的适度大小的数据，层次聚类也<strong>可能需要密集的计算资源</strong>。事实上，<strong>层次聚类的大多数应用都集中在相对较小的数据集上</strong>。</p>
<h4 id="一个简单的例子-2"><strong>一个简单的例子</strong></h4>
<p>层次聚类处理包含 <span class="math inline">\(n\)</span> 条记录和
<span class="math inline">\(p\)</span>
个变量的数据集，并基于两个基本构建块：</p>
<ul>
<li>一个<strong>距离度量 <span
class="math inline">\(d_{i,j}\)</span></strong>，用于衡量两条记录 <span
class="math inline">\(i\)</span> 和 <span
class="math inline">\(j\)</span> 之间的距离。</li>
<li>一个<strong>相异度度量 <span
class="math inline">\(D_{A,B}\)</span></strong>，用于根据每个簇成员之间的距离
<span class="math inline">\(d_{i,j}\)</span> 来衡量两个簇 <span
class="math inline">\(A\)</span> 和 <span
class="math inline">\(B\)</span> 之间的差异。</li>
</ul>
<p>对于涉及<strong>数值数据</strong>的应用，最重要的选择是<strong>相异度度量</strong>。层次聚类首先将<strong>每条记录</strong>设置为一个<strong>单独的簇</strong>，然后<strong>迭代地将最不相似的簇合并</strong>。</p>
<p>在 R 中，<code>hclust</code>
函数可用于执行层次聚类。<code>hclust</code> 与 <code>kmeans</code>
的一个巨大区别是，它作用于<strong>成对距离 <span
class="math inline">\(d_{i,j}\)</span></strong>，而不是数据本身。您可以使用
<code>dist</code>
函数来计算这些距离。例如，以下代码对一组公司的股票回报应用层次聚类：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">syms1 <span class="operator">&lt;-</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;GOOGL&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;AMZN&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;AAPL&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;MSFT&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;CSCO&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;INTC&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;CVX&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;XOM&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;SLB&#x27;</span><span class="punctuation">,</span></span><br><span class="line"><span class="string">&#x27;COP&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;JPM&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;WFC&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;USB&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;AXP&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;WMT&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;TGT&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;HD&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;COST&#x27;</span><span class="punctuation">)</span></span><br><span class="line"><span class="comment"># take transpose: to cluster companies, we need the stocks along the rows</span></span><br><span class="line">df <span class="operator">&lt;-</span> t<span class="punctuation">(</span>sp500_px<span class="punctuation">[</span>row.names<span class="punctuation">(</span>sp500_px<span class="punctuation">)</span> <span class="operator">&gt;=</span> <span class="string">&#x27;2011-01-01&#x27;</span><span class="punctuation">,</span> syms1<span class="punctuation">]</span><span class="punctuation">)</span></span><br><span class="line">d <span class="operator">&lt;-</span> dist<span class="punctuation">(</span>df<span class="punctuation">)</span></span><br><span class="line">hcl <span class="operator">&lt;-</span> hclust<span class="punctuation">(</span>d<span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<p>聚类算法将对数据框的记录（行）进行聚类。因为我们想对<strong>公司</strong>进行聚类，所以我们需要<strong>转置</strong>（<code>t</code>）数据框，将股票放在行中，日期放在列中。</p>
<p><code>scipy</code> 包在 <code>scipy.cluster.hierarchy</code>
模块中提供了许多不同的层次聚类方法。这里我们使用 <code>linkage</code>
函数和“complete”（完全）方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">syms1 = [<span class="string">&#x27;AAPL&#x27;</span>, <span class="string">&#x27;AMZN&#x27;</span>, <span class="string">&#x27;AXP&#x27;</span>, <span class="string">&#x27;COP&#x27;</span>, <span class="string">&#x27;COST&#x27;</span>, <span class="string">&#x27;CSCO&#x27;</span>, <span class="string">&#x27;CVX&#x27;</span>, <span class="string">&#x27;GOOGL&#x27;</span>, <span class="string">&#x27;HD&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;INTC&#x27;</span>, <span class="string">&#x27;JPM&#x27;</span>, <span class="string">&#x27;MSFT&#x27;</span>, <span class="string">&#x27;SLB&#x27;</span>, <span class="string">&#x27;TGT&#x27;</span>, <span class="string">&#x27;USB&#x27;</span>, <span class="string">&#x27;WFC&#x27;</span>, <span class="string">&#x27;WMT&#x27;</span>, <span class="string">&#x27;XOM&#x27;</span>]</span><br><span class="line">df = sp500_px.loc[sp500_px.index &gt;= <span class="string">&#x27;2011-01-01&#x27;</span>, syms1].transpose()</span><br><span class="line">Z = linkage(df, method=<span class="string">&#x27;complete&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h4 id="树状图"><strong>树状图</strong></h4>
<p>The Dendrogram</p>
<p>层次聚类可以自然地以树的形式进行图形化展示，这被称为<strong>树状图（dendrogram）</strong>。这个名字来源于希腊词根
<em>dendro</em>（树）和 <em>gramma</em>（图画）。在 R 中，您可以使用
<code>plot</code> 命令轻松生成它：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot<span class="punctuation">(</span>hcl<span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<p>在 Python 中，我们可以使用 <code>dendrogram</code> 方法来绘制
<code>linkage</code> 函数的结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line">dendrogram(Z, labels=df.index, ax=ax, color_threshold=<span class="number">0</span>)</span><br><span class="line">plt.xticks(rotation=<span class="number">90</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&#x27;distance&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>结果如图7-8所示（请注意，我们现在绘制的是相互相似的公司，而不是日期）。
树的叶子对应于记录。树枝的长度表示相应簇之间的<strong>相异程度</strong>。Google
和 Amazon
的回报彼此之间以及与其他股票的回报都<strong>非常不相似</strong>。<strong>石油股</strong>（SLB,
CVX, XOM,
COP）在自己的簇中，<strong>苹果</strong>（AAPL）单独成簇，其余的彼此相似。</p>
<p><img src="/img3/面向数据科学家的实用统计学/F7.8.png" alt="F7.8" style="zoom:50%;" /></p>
<p>与
K-均值不同，<strong>不必预先指定簇的数量</strong>。在图形上，您可以通过一条上下滑动的<strong>水平线</strong>来识别不同数量的簇；簇被定义为<strong>水平线与垂直线相交的地方</strong>。要提取特定数量的簇，您可以使用
<code>cutree</code> 函数：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cutree<span class="punctuation">(</span>hcl<span class="punctuation">,</span> k<span class="operator">=</span><span class="number">4</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">GOOGL AMZN AAPL MSFT CSCO INTC CVX XOM SLB COP JPM WFC </span><br><span class="line">   1    2    3    3    3    3    4    4    4    4    3    3 </span><br><span class="line"> USB  AXP  WMT  TGT   HD COST </span><br><span class="line">   3    3    3    3    3    3 </span><br></pre></td></tr></table></figure>
<p>在 Python 中，您可以使用 <code>fcluster</code>
方法实现相同的功能：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">memb = fcluster(Z, <span class="number">4</span>, criterion=<span class="string">&#x27;maxclust&#x27;</span>)</span><br><span class="line">memb = pd.Series(memb, index=df.index)</span><br><span class="line"><span class="keyword">for</span> key, item <span class="keyword">in</span> memb.groupby(memb):</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;key&#125;</span> : <span class="subst">&#123;<span class="string">&#x27;, &#x27;</span>.join(item.index)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>提取的簇数量被设置为4，您可以看到 Google 和 Amazon
各自属于自己的簇。<strong>石油股</strong>都属于另一个簇。其余的股票都在<strong>第四个簇</strong>中。</p>
<h4 id="聚类算法"><strong>聚类算法</strong></h4>
<p>The Agglomerative Algorithm</p>
<p>层次聚类的主要算法是聚类算法，它迭代地合并相似的簇。聚类算法从每条记录构成其自身的单个记录簇开始，然后逐步建立越来越大的簇。第一步是计算所有记录对之间的距离。</p>
<p>对于每对记录 <span class="math inline">\(x_1, x_2, \dots,
x_p\)</span> 和 <span class="math inline">\(y_1, y_2, \dots,
y_p\)</span>，我们使用<strong>距离度量</strong>（参见第241页的“距离度量”）来衡量两条记录之间的距离
<span
class="math inline">\(d_{x,y}\)</span>。例如，我们可以使用<strong>欧几里得距离</strong>：</p>
<p><span class="math display">\[
d_{x,y} = \sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2 + \dots + (x_p - y_p)^2}
\]</span> 现在我们转向<strong>簇间距离</strong>。考虑两个簇 <span
class="math inline">\(A\)</span> 和 <span
class="math inline">\(B\)</span>，每个簇都有一组独特的记录，<span
class="math inline">\(A=\{a_1, a_2, \dots, a_m\}\)</span> 和 <span
class="math inline">\(B=\{b_1, b_2, \dots,
b_q\}\)</span>。我们可以通过使用 <span class="math inline">\(A\)</span>
的成员和 <span class="math inline">\(B\)</span>
的成员之间的距离来衡量簇之间的<strong>相异度</strong> <span
class="math inline">\(D_{A,B}\)</span>。</p>
<p>一种相异度度量是<strong>完全链接法（complete-linkage
method）</strong>，它是 <span class="math inline">\(A\)</span> 和 <span
class="math inline">\(B\)</span>
之间所有记录对的<strong>最大距离</strong>：</p>
<p><span class="math display">\[
D_{A,B} = \max d_{a_i, b_j} \text{ for all pairs } i, j
\]</span>
这一定义将相异度定义为所有成对差异中<strong>最大的那个</strong>。</p>
<p>聚类算法的主要步骤是：</p>
<ol type="1">
<li>创建一个初始簇集，其中<strong>每个簇</strong>由数据中的<strong>一条单一记录</strong>构成。</li>
<li>计算所有<strong>簇对</strong> <span class="math inline">\(C_k,
C_l\)</span> 之间的<strong>相异度</strong> <span
class="math inline">\(D_{C_k, C_l}\)</span>。</li>
<li><strong>合并</strong>根据 <span class="math inline">\(D_{C_k,
C_l}\)</span> 测得的<strong>最不相似</strong>的两个簇 <span
class="math inline">\(C_k\)</span> 和 <span
class="math inline">\(C_l\)</span>。</li>
<li>如果剩余的簇多于一个，则返回步骤2。否则，完成。</li>
</ol>
<h4 id="相异度度量"><strong>相异度度量</strong></h4>
<p>Measures of Dissimilarity</p>
<p>有四种常见的相异度度量：<strong>完全链接（complete
linkage）</strong>、<strong>单链接（single
linkage）</strong>、<strong>平均链接（average
linkage）</strong>和<strong>最小方差（minimum
variance）</strong>。这些（以及其他度量）都受到大多数层次聚类软件的支持，包括
<code>hclust</code> 和 <code>linkage</code>。</p>
<ul>
<li>前面定义的<strong>完全链接法</strong>倾向于产生成员彼此相似的簇。</li>
<li><strong>单链接法</strong>是两个簇中记录之间的<strong>最小距离</strong>：
<span class="math display">\[D_{A,B} = \min d_{a_i, b_j} \text{ for all
pairs } i, j\]</span>
这是一种<strong>“贪婪”方法</strong>，产生的簇可能包含<strong>相当不一致的元素</strong>。</li>
<li><strong>平均链接法</strong>是所有距离对的<strong>平均值</strong>，代表了单链接和完全链接方法之间的一种折中。</li>
<li>最后，<strong>最小方差法</strong>，也称为<strong>沃德法（Ward's
method）</strong>，类似于
<strong>K-均值</strong>，因为它<strong>最小化簇内平方和</strong>（参见第294页的“K-均值聚类”）。</li>
</ul>
<p>图7-9对埃克森美孚和雪佛龙的股票回报应用了使用这四种度量的层次聚类。对于每种度量，都保留了四个簇。</p>
<p><img src="/img3/面向数据科学家的实用统计学/F7.9.png" alt="F7.9" style="zoom:80%;" /></p>
<p>结果<strong>截然不同</strong>：单链接度量将几乎所有的点都分配给一个单独的簇。除了<strong>最小方差法</strong>（R：<code>Ward.D</code>；Python：<code>ward</code>）之外，所有度量最终都至少有一个簇只包含几个离群点。最小方差法与
K-均值聚类最相似；请与图7-5进行比较。</p>
<p><strong>关键思想</strong></p>
<ul>
<li>层次聚类从<strong>每条记录</strong>都在其自己的簇中开始。</li>
<li>簇<strong>逐步</strong>与邻近的簇合并，直到所有记录都属于一个<strong>单一的簇</strong>（聚类算法）。</li>
<li><strong>聚类历史被保留</strong>并绘制出来，用户可以在<strong>不预先指定簇数量</strong>的情况下，在不同阶段<strong>可视化</strong>簇的数量和结构。</li>
<li>簇间距离以不同的方式计算，所有这些方式都依赖于所有<strong>记录间距离</strong>的集合。</li>
</ul>
<h3 id="基于模型的聚类"><strong>基于模型的聚类</strong></h3>
<p>Model-Based Clustering</p>
<p>像层次聚类和
K-均值这样的聚类方法是基于<strong>启发式</strong>的，主要依赖于寻找其成员彼此接近的簇，这是通过直接用数据来衡量的（不涉及概率模型）。在过去的20年里，人们投入了大量精力来开发<strong>基于模型的聚类方法</strong>。华盛顿大学的
Adrian Raftery
和其他研究人员对基于模型的聚类做出了重要贡献，包括理论和软件。这些技术以<strong>统计理论</strong>为基础，并提供了更严谨的方法来确定簇的性质和数量。例如，它们可以用于以下情况：可能有一组记录彼此相似但不一定彼此接近（例如，回报方差高的科技股），而另一组记录既相似又接近（例如，方差低的公用事业股）。</p>
<h4 id="多元正态分布"><strong>多元正态分布</strong></h4>
<p>Multivariate Normal Distribution</p>
<p>最广泛使用的基于模型的聚类方法都依赖于<strong>多元正态分布</strong>。多元正态分布是<strong>正态分布</strong>向一组
<span class="math inline">\(p\)</span> 个变量 <span
class="math inline">\(X_1, X_2, \dots, X_p\)</span>
的推广。该分布由一组均值 <span class="math inline">\(\mu = (\mu_1,
\mu_2, \dots, \mu_p)\)</span> 和一个<strong>协方差矩阵 <span
class="math inline">\(\Sigma\)</span></strong>
定义。协方差矩阵是衡量变量之间如何关联的度量（有关协方差的详细信息，请参见第202页的“协方差矩阵”）。协方差矩阵
<span class="math inline">\(\Sigma\)</span> 由 <span
class="math inline">\(p\)</span> 个方差 <span
class="math inline">\(\sigma_1^2, \sigma_2^2, \dots, \sigma_p^2\)</span>
和所有变量对 <span class="math inline">\(i \neq j\)</span> 的协方差
<span class="math inline">\(\sigma_{i,j}\)</span>
组成。将变量放在行和列中，矩阵如下所示：</p>
<p><span class="math display">\[
\Sigma = \begin{pmatrix} \sigma_1^2 &amp; \sigma_{1,2} &amp; \cdots
&amp; \sigma_{1,p} \\ \sigma_{2,1} &amp; \sigma_2^2 &amp; \cdots &amp;
\sigma_{2,p} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\sigma_{p,1} &amp; \sigma_{p,2} &amp; \cdots &amp; \sigma_p^2
\end{pmatrix}
\]</span></p>
<p>请注意，协方差矩阵关于从左上到右下的对角线是<strong>对称</strong>的。由于
<span class="math inline">\(\sigma_{i,j} =
\sigma_{j,i}\)</span>，所以只有 <span
class="math inline">\(p(p-1)/2\)</span>
个协方差项。总的来说，协方差矩阵有 <span class="math inline">\(p(p-1)/2
+ p\)</span> 个参数。该分布表示为：</p>
<p><span class="math display">\[
(X_1, X_2, \dots, X_p) \sim N_p(\mu, \Sigma)
\]</span></p>
<p>这是一种符号表示，意思是所有变量都呈<strong>正态分布</strong>，并且整体分布由变量均值向量和协方差矩阵完全描述。</p>
<p><img src="/img3/面向数据科学家的实用统计学/F7.10.png" alt="F7.10" style="zoom:60%;" /></p>
<p>图7-10显示了两个变量 <span class="math inline">\(X\)</span> 和 <span
class="math inline">\(Y\)</span>
的多元正态分布的<strong>概率等高线</strong>（例如，0.5概率等高线包含50%的分布）。</p>
<blockquote>
<p>个人注：<strong>概率等高线</strong>（probability contours)
这些等高线连接着具有<strong>相同概率密度</strong>的点，就像地形图上的等高线连接着具有相同海拔的点一样。</p>
</blockquote>
<p>均值为 <span class="math inline">\(\mu_x = 0.5\)</span> 和 <span
class="math inline">\(\mu_y = -0.5\)</span>，协方差矩阵为：</p>
<p><span class="math display">\[
\Sigma = \begin{pmatrix} 1 &amp; 1 \\ 1 &amp; 2 \end{pmatrix}
\]</span></p>
<p>由于协方差 <span class="math inline">\(\sigma_{xy}\)</span>
是正的，因此 <span class="math inline">\(X\)</span> 和 <span
class="math inline">\(Y\)</span> <strong>正相关</strong>。</p>
<h4 id="正态分布混合"><strong>正态分布混合</strong></h4>
<p>Mixtures of Normals</p>
<p>基于模型的聚类背后的关键思想是，<strong>每条记录</strong>被假定为服从
<span class="math inline">\(K\)</span>
个<strong>多元正态分布</strong>中的一个，其中 <span
class="math inline">\(K\)</span>
是簇的数量。每个分布都有<strong>不同的均值 <span
class="math inline">\(\mu\)</span> 和协方差矩阵 <span
class="math inline">\(\Sigma\)</span></strong>。例如，如果您有两个变量
<span class="math inline">\(X\)</span> 和 <span
class="math inline">\(Y\)</span>，那么每行 <span
class="math inline">\((X_i, Y_i)\)</span> 被建模为从 <span
class="math inline">\(K\)</span> 个多元正态分布中的一个进行采样：<span
class="math inline">\(N(\mu_1, \Sigma_1), N(\mu_2, \Sigma_2), \dots,
N(\mu_K, \Sigma_K)\)</span>。</p>
<p>R 有一个非常丰富的基于模型聚类的包，名为 <code>mclust</code>，最初由
Chris Fraley 和 Adrian Raftery 开发。使用这个包，我们可以对之前用
K-均值和层次聚类分析过的股票回报数据应用基于模型的聚类：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">&gt;</span> library<span class="punctuation">(</span>mclust<span class="punctuation">)</span></span><br><span class="line"><span class="operator">&gt;</span> df <span class="operator">&lt;-</span> sp500_px<span class="punctuation">[</span>row.names<span class="punctuation">(</span>sp500_px<span class="punctuation">)</span> <span class="operator">&gt;=</span> <span class="string">&#x27;2011-01-01&#x27;</span><span class="punctuation">,</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;XOM&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;CVX&#x27;</span><span class="punctuation">)</span><span class="punctuation">]</span></span><br><span class="line"><span class="operator">&gt;</span> mcl <span class="operator">&lt;-</span> Mclust<span class="punctuation">(</span>df<span class="punctuation">)</span></span><br><span class="line"><span class="operator">&gt;</span> summary<span class="punctuation">(</span>mcl<span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Mclust VEE (ellipsoidal, equal shape and orientation) model with 2 components:</span><br><span class="line">log.likelihood n df BIC ICL</span><br><span class="line">-2255.134 1131 9 -4573.546 -5076.856</span><br><span class="line">Clustering table:</span><br><span class="line">1 2</span><br><span class="line">963 168</span><br></pre></td></tr></table></figure>
<p>scikit-learn 有 <code>sklearn.mixture.GaussianMixture</code>
类用于基于模型的聚类：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df = sp500_px.loc[sp500_px.index &gt;= <span class="string">&#x27;2011-01-01&#x27;</span>, [<span class="string">&#x27;XOM&#x27;</span>, <span class="string">&#x27;CVX&#x27;</span>]]</span><br><span class="line">mclust = GaussianMixture(n_components=<span class="number">2</span>).fit(df)</span><br><span class="line">mclust.bic(df)</span><br></pre></td></tr></table></figure>
<p>如果您执行此代码，会注意到计算时间比其他过程长得多。使用
<code>predict</code> 函数提取簇分配，我们可以对簇进行可视化：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cluster <span class="operator">&lt;-</span> factor<span class="punctuation">(</span>predict<span class="punctuation">(</span>mcl<span class="punctuation">)</span><span class="operator">$</span>classification<span class="punctuation">)</span></span><br><span class="line">ggplot<span class="punctuation">(</span>data<span class="operator">=</span>df<span class="punctuation">,</span> aes<span class="punctuation">(</span>x<span class="operator">=</span>XOM<span class="punctuation">,</span> y<span class="operator">=</span>CVX<span class="punctuation">,</span> color<span class="operator">=</span>cluster<span class="punctuation">,</span> shape<span class="operator">=</span>cluster<span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">geom_point<span class="punctuation">(</span>alpha<span class="operator">=</span><span class="number">.8</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<p>以下是创建类似图形的 Python 代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">colors = [<span class="string">f&#x27;C<span class="subst">&#123;c&#125;</span>&#x27;</span> <span class="keyword">for</span> c <span class="keyword">in</span> mclust.predict(df)]</span><br><span class="line">df.plot.scatter(x=<span class="string">&#x27;XOM&#x27;</span>, y=<span class="string">&#x27;CVX&#x27;</span>, c=colors, alpha=<span class="number">0.5</span>, ax=ax)</span><br><span class="line">ax.set_xlim(-<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">ax.set_ylim(-<span class="number">3</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p>结果如图7-11所示。
存在两个簇：一个簇在数据的<strong>中间</strong>，另一个簇在数据的<strong>外缘</strong>。这与使用
K-均值（图7-5）和层次聚类（图7-9）获得的簇<strong>非常不同</strong>，后者找到的是紧凑的簇。</p>
<p><img src="/img3/面向数据科学家的实用统计学/F7.11.png" alt="F7.11" style="zoom:50%;" /></p>
<p>您可以使用 <code>summary</code> 函数提取正态分布的参数：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">&gt;</span> summary<span class="punctuation">(</span>mcl<span class="punctuation">,</span> parameters<span class="operator">=</span><span class="literal">TRUE</span><span class="punctuation">)</span><span class="operator">$</span>mean</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">          [,1]       [,2]</span><br><span class="line">XOM 0.05783847 -0.04374944</span><br><span class="line">CVX 0.07363239 -0.21175715</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; summary(mcl, parameters=TRUE)$variance</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">, , 1</span><br><span class="line"></span><br><span class="line">          XOM       CVX</span><br><span class="line">XOM 0.3002049 0.3060989</span><br><span class="line">CVX 0.3060989 0.5496727</span><br><span class="line"></span><br><span class="line">, , 2</span><br><span class="line"></span><br><span class="line">         XOM      CVX</span><br><span class="line">XOM 1.046318 1.066860</span><br><span class="line">CVX 1.066860 1.915799</span><br></pre></td></tr></table></figure>
<p>在 Python 中，您可以从结果的 <code>means_</code> 和
<code>covariances_</code> 属性中获取此信息：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Mean&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(mclust.means_)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Covariances&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(mclust.covariances_)</span><br></pre></td></tr></table></figure>
<p>这两个分布有相似的均值和相关性，但第二个分布有大得多的方差和协方差。由于算法的随机性，不同运行的结果可能会略有差异。</p>
<p><code>mclust</code>
产生的簇可能看起来令人惊讶，但事实上，它们说明了该方法的统计性质。基于模型的聚类的目标是找到最能拟合的多元正态分布集。股票数据似乎具有类似正态的形状：参见图7-10的等高线。然而，<strong>实际上，股票回报的分布比正态分布具有更长的尾部</strong>。为了解决这个问题，<code>mclust</code>
将一个分布拟合到大部分数据上，然后拟合第二个具有更大方差的分布。</p>
<h4 id="选择簇的数量-1"><strong>选择簇的数量</strong></h4>
<p>Selecting the Number of Clusters</p>
<p>与 K-均值和层次聚类不同，<code>mclust</code> 在 R
中自动选择簇的数量（在本例中为两个）。它通过选择贝叶斯信息准则（BIC）具有最大值的簇数量来实现（BIC类似于AIC；参见第156页的“模型选择和逐步回归”）。BIC
通过选择拟合最佳的模型来工作，同时对模型中的参数数量进行惩罚。在基于模型的聚类中，添加更多的簇总是会改善拟合，但代价是引入额外的模型参数。</p>
<blockquote>
<p>警告：</p>
<p>请注意，在大多数情况下，BIC通常是最小化的。<code>mclust</code>
包的作者决定定义 BIC 为相反的符号，以便更容易解释绘图。</p>
</blockquote>
<p><code>mclust</code>
拟合14个不同模型，并随着组件数量的增加而自动选择最佳模型。您可以使用
<code>mclust</code> 中的函数绘制这些模型的 BIC 值：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot<span class="punctuation">(</span>mcl<span class="punctuation">,</span> what<span class="operator">=</span><span class="string">&#x27;BIC&#x27;</span><span class="punctuation">,</span> ask<span class="operator">=</span><span class="literal">FALSE</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<p>簇的数量（或者说不同多元正态模型（组件）的数量）显示在 x
轴上（参见图7-12）。</p>
<p><img src="/img3/面向数据科学家的实用统计学/F7.12.png" alt="F7.12" style="zoom:60%;" /></p>
<p>另一方面，<code>GaussianMixture</code>
的实现不会尝试各种组合。如下所示，使用 Python
运行多种组合是直截了当的。这个实现按惯例定义 BIC，因此计算出的 BIC
值为正，我们需要将其最小化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">results = []</span><br><span class="line">covariance_types = [<span class="string">&#x27;full&#x27;</span>, <span class="string">&#x27;tied&#x27;</span>, <span class="string">&#x27;diag&#x27;</span>, <span class="string">&#x27;spherical&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> n_components <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">9</span>):</span><br><span class="line">    <span class="keyword">for</span> covariance_type <span class="keyword">in</span> covariance_types:</span><br><span class="line">        mclust = GaussianMixture(n_components=n_components, warm_start=<span class="literal">True</span>,</span><br><span class="line">                                 covariance_type=covariance_type)</span><br><span class="line">        mclust.fit(df)</span><br><span class="line">        results.append(&#123;</span><br><span class="line">            <span class="string">&#x27;bic&#x27;</span>: mclust.bic(df),</span><br><span class="line">            <span class="string">&#x27;n_components&#x27;</span>: n_components,</span><br><span class="line">            <span class="string">&#x27;covariance_type&#x27;</span>: covariance_type,</span><br><span class="line">        &#125;)</span><br><span class="line"></span><br><span class="line">results = pd.DataFrame(results)</span><br><span class="line">colors = [<span class="string">&#x27;C0&#x27;</span>, <span class="string">&#x27;C1&#x27;</span>, <span class="string">&#x27;C2&#x27;</span>, <span class="string">&#x27;C3&#x27;</span>]</span><br><span class="line">styles = [<span class="string">&#x27;C0-&#x27;</span>,<span class="string">&#x27;C1:&#x27;</span>,<span class="string">&#x27;C0-.&#x27;</span>, <span class="string">&#x27;C1--&#x27;</span>]</span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line"><span class="keyword">for</span> i, covariance_type <span class="keyword">in</span> <span class="built_in">enumerate</span>(covariance_types):</span><br><span class="line">    subset = results.loc[results.covariance_type == covariance_type, :]</span><br><span class="line">    subset.plot(x=<span class="string">&#x27;n_components&#x27;</span>, y=<span class="string">&#x27;bic&#x27;</span>, ax=ax, label=covariance_type,</span><br><span class="line">                kind=<span class="string">&#x27;line&#x27;</span>, style=styles[i])</span><br></pre></td></tr></table></figure>
<p>使用 <code>warm_start</code>
参数，计算将重用上一次拟合的信息。这将加快后续计算的收敛速度。</p>
<p>这个图与用于确定
K-均值簇数量的<strong>手肘图</strong>相似，不同之处在于绘制的值是<strong>BIC</strong>而不是<strong>解释方差的百分比</strong>（参见图7-7）。一个很大的区别是，<code>mclust</code>
没有显示一条线，而是显示<strong>14条不同的线</strong>！这是因为
<code>mclust</code>
实际上为每种簇大小拟合了14个不同的模型，并最终选择了<strong>最拟合的模型</strong>。<code>GaussianMixture</code>
实现的方法较少，因此线条数量只有四条。</p>
<p>为什么 <code>mclust</code>
要拟合这么多模型来确定最佳多元正态集呢？这是因为有<strong>不同的方法来参数化协方差矩阵
<span class="math inline">\(\Sigma\)</span></strong>
以进行模型拟合。在大多数情况下，您不需要担心模型的细节，只需使用
<code>mclust</code> 选择的模型即可。在本例中，根据
BIC，三个不同的模型（称为 VEE、VEV 和
VVE）使用两个组件提供了最佳拟合。</p>
<blockquote>
<p>通用注解：</p>
<p>基于模型的聚类是一个丰富且<strong>快速发展的研究领域</strong>，本书的涵盖范围只占该领域的一小部分。事实上，<code>mclust</code>
的帮助文件目前长达154页。对于数据科学家遇到的<strong>大多数问题</strong>来说，深入研究基于模型的聚类的细微差别<strong>可能超出了需要</strong>。</p>
</blockquote>
<p>基于模型的聚类技术确实存在一些局限性。这些方法需要一个<strong>潜在的数据模型假设</strong>，而聚类结果<strong>非常依赖于这个假设</strong>。其计算要求<strong>甚至高于层次聚类</strong>，这使得它难以扩展到大数据。最后，该算法<strong>比其他方法更复杂，更难理解</strong>。</p>
<p><strong>关键思想</strong></p>
<ul>
<li>假设簇源自<strong>不同的数据生成过程</strong>，具有<strong>不同的概率分布</strong>。</li>
<li>拟合不同的模型，假设有不同数量的（通常为<strong>正态</strong>）分布。</li>
<li>该方法选择<strong>拟合数据良好</strong>且<strong>不使用过多参数</strong>（即，不过拟合）的模型（以及相关的簇数量）。</li>
</ul>
<h3 id="缩放和分类变量"><strong>缩放和分类变量</strong></h3>
<p>Scaling and Categorical Variables</p>
<p>无监督学习技术通常要求<strong>数据经过适当的缩放</strong>。这与许多回归和分类技术不同，在这些技术中缩放并不重要（一个例外是
K-近邻；参见第238页的“K-近邻”）。</p>
<p><strong>数据缩放的关键术语</strong></p>
<ul>
<li><strong>缩放（Scaling）</strong>
压缩或扩展数据，通常是为了将多个变量带到<strong>相同的尺度</strong>。</li>
<li><strong>归一化（Normalization）</strong>
一种缩放方法——<strong>减去均值并除以标准差</strong>。
同义词：<strong>标准化（Standardization）</strong></li>
<li><strong>Gower's distance</strong>
一种应用于<strong>混合数值和分类数据</strong>的缩放算法，将所有变量带到<strong>0-1的范围</strong>。</li>
</ul>
<p>例如，对于个人贷款数据，变量具有<strong>截然不同的单位和量级</strong>。有些变量的值相对较小（例如，工作年限），而另一些变量的值则非常大（例如，以美元计的贷款金额）。如果数据没有经过缩放，那么
PCA、K-均值和其他聚类方法将<strong>被大值的变量主导</strong>，而<strong>忽略小值的变量</strong>。</p>
<p>对于某些聚类过程，<strong>分类数据</strong>可能会带来特殊问题。与
K-近邻一样，<strong>无序因子变量</strong>通常使用<strong>独热编码</strong>（one
hot
encoding）转换为一组<strong>二元（0/1）变量</strong>（参见第242页的“独热编码”）。二元变量不仅可能与其他数据处于不同的尺度，而且二元变量只有两个值这一事实，对于
PCA 和 K-均值等技术来说也可能存在问题。</p>
<h4 id="变量缩放"><strong>变量缩放</strong></h4>
<p>Scaling the Variables</p>
<p>在应用聚类过程之前，需要对具有<strong>截然不同尺度和单位</strong>的变量进行<strong>适当的归一化</strong>。例如，让我们在不进行归一化的情况下，将
<code>kmeans</code> 应用于一组贷款违约数据：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">defaults <span class="operator">&lt;-</span> loan_data<span class="punctuation">[</span>loan_data<span class="operator">$</span>outcome<span class="operator">==</span><span class="string">&#x27;default&#x27;</span><span class="punctuation">,</span><span class="punctuation">]</span></span><br><span class="line">df <span class="operator">&lt;-</span> defaults<span class="punctuation">[</span><span class="punctuation">,</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;loan_amnt&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;annual_inc&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;revol_bal&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;open_acc&#x27;</span><span class="punctuation">,</span></span><br><span class="line"><span class="string">&#x27;dti&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;revol_util&#x27;</span><span class="punctuation">)</span><span class="punctuation">]</span></span><br><span class="line">km <span class="operator">&lt;-</span> kmeans<span class="punctuation">(</span>df<span class="punctuation">,</span> centers<span class="operator">=</span><span class="number">4</span><span class="punctuation">,</span> nstart<span class="operator">=</span><span class="number">10</span><span class="punctuation">)</span></span><br><span class="line">centers <span class="operator">&lt;-</span> data.frame<span class="punctuation">(</span>size<span class="operator">=</span>km<span class="operator">$</span>size<span class="punctuation">,</span> km<span class="operator">$</span>centers<span class="punctuation">)</span></span><br><span class="line"><span class="built_in">round</span><span class="punctuation">(</span>centers<span class="punctuation">,</span> digits<span class="operator">=</span><span class="number">2</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">   size loan_amnt annual_inc revol_bal open_acc  dti revol_util</span><br><span class="line">1    52  22570.19  489783.40  85161.35    13.33 6.91      59.65</span><br><span class="line">2  1192  21856.38  165473.54  38935.88    12.61 13.48      63.67</span><br><span class="line">3 13902  10606.48   42500.30  10280.52     9.59 17.71      58.11</span><br><span class="line">4  7525  18282.25   83458.11  19653.82    11.66 16.77      62.27</span><br></pre></td></tr></table></figure>
<p>以下是相应的 Python 代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">defaults = loan_data.loc[loan_data[<span class="string">&#x27;outcome&#x27;</span>] == <span class="string">&#x27;default&#x27;</span>,]</span><br><span class="line">columns = [<span class="string">&#x27;loan_amnt&#x27;</span>, <span class="string">&#x27;annual_inc&#x27;</span>, <span class="string">&#x27;revol_bal&#x27;</span>, <span class="string">&#x27;open_acc&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;dti&#x27;</span>, <span class="string">&#x27;revol_util&#x27;</span>]</span><br><span class="line">df = defaults[columns]</span><br><span class="line">kmeans = KMeans(n_clusters=<span class="number">4</span>, random_state=<span class="number">1</span>).fit(df)</span><br><span class="line">counts = Counter(kmeans.labels_)</span><br><span class="line">centers = pd.DataFrame(kmeans.cluster_centers_, columns=columns)</span><br><span class="line">centers[<span class="string">&#x27;size&#x27;</span>] = [counts[i] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>)]</span><br><span class="line">centers</span><br></pre></td></tr></table></figure>
<p>变量 <code>annual_inc</code> 和 <code>revol_bal</code>
<strong>主导了簇</strong>，并且簇的大小<strong>差异很大</strong>。簇1只有52个成员，其收入和循环信贷余额相对较高。</p>
<p>一种常见的变量缩放方法是将它们<strong>转换为z-分数</strong>，即<strong>减去均值并除以标准差</strong>。这被称为<strong>标准化</strong>或<strong>归一化</strong>（关于使用
z-分数的更多讨论，请参见第243页的“标准化（归一化，z-分数）”）：</p>
<p><span class="math display">\[
z = \frac{x-\bar{x}}{s}
\]</span></p>
<p>让我们看看将 <code>kmeans</code>
应用于<strong>归一化数据</strong>后簇发生了什么：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">df0 <span class="operator">&lt;-</span> scale<span class="punctuation">(</span>df<span class="punctuation">)</span></span><br><span class="line">km0 <span class="operator">&lt;-</span> kmeans<span class="punctuation">(</span>df0<span class="punctuation">,</span> centers<span class="operator">=</span><span class="number">4</span><span class="punctuation">,</span> nstart<span class="operator">=</span><span class="number">10</span><span class="punctuation">)</span></span><br><span class="line">centers0 <span class="operator">&lt;-</span> scale<span class="punctuation">(</span>km0<span class="operator">$</span>centers<span class="punctuation">,</span> center<span class="operator">=</span><span class="literal">FALSE</span><span class="punctuation">,</span></span><br><span class="line">scale<span class="operator">=</span><span class="number">1</span> <span class="operator">/</span> <span class="built_in">attr</span><span class="punctuation">(</span>df0<span class="punctuation">,</span> <span class="string">&#x27;scaled:scale&#x27;</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">centers0 <span class="operator">&lt;-</span> scale<span class="punctuation">(</span>centers0<span class="punctuation">,</span> center<span class="operator">=</span><span class="operator">-</span><span class="built_in">attr</span><span class="punctuation">(</span>df0<span class="punctuation">,</span> <span class="string">&#x27;scaled:center&#x27;</span><span class="punctuation">)</span><span class="punctuation">,</span> scale<span class="operator">=</span><span class="literal">FALSE</span><span class="punctuation">)</span></span><br><span class="line">centers0 <span class="operator">&lt;-</span> data.frame<span class="punctuation">(</span>size<span class="operator">=</span>km0<span class="operator">$</span>size<span class="punctuation">,</span> centers0<span class="punctuation">)</span></span><br><span class="line"><span class="built_in">round</span><span class="punctuation">(</span>centers0<span class="punctuation">,</span> digits<span class="operator">=</span><span class="number">2</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">   size loan_amnt annual_inc revol_bal open_acc  dti revol_util</span><br><span class="line">1  7355  10467.65   51134.87  11523.31     7.48 15.78      77.73</span><br><span class="line">2  5309  10363.43   53523.09   6038.26     8.68 11.32      30.70</span><br><span class="line">3  3713  25894.07  116185.91  32797.67    12.41 16.22      66.14</span><br><span class="line">4  6294  13361.61   55596.65  16375.27    14.25 24.23      59.61</span><br></pre></td></tr></table></figure>
<p>在 Python 中，我们可以使用 scikit-learn 的
<code>StandardScaler</code>。<code>inverse_transform</code>
方法允许将簇中心<strong>转换回原始尺度</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scaler = preprocessing.StandardScaler()</span><br><span class="line">df0 = scaler.fit_transform(df * <span class="number">1.0</span>)</span><br><span class="line">kmeans = KMeans(n_clusters=<span class="number">4</span>, random_state=<span class="number">1</span>).fit(df0)</span><br><span class="line">counts = Counter(kmeans.labels_)</span><br><span class="line">centers = pd.DataFrame(scaler.inverse_transform(kmeans.cluster_centers_),</span><br><span class="line">columns=columns)</span><br><span class="line">centers[<span class="string">&#x27;size&#x27;</span>] = [counts[i] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>)]</span><br><span class="line">centers</span><br></pre></td></tr></table></figure>
<p>簇的大小<strong>更加均衡</strong>，并且簇不再被
<code>annual_inc</code> 和 <code>revol_bal</code>
主导，从而揭示了数据中<strong>更有趣的结构</strong>。请注意，在上述代码中，中心被<strong>重新缩放回原始单位</strong>。如果我们没有对它们进行缩放，得到的值将是
z-分数，因此可解释性会降低。</p>
<blockquote>
<p>通用注解：</p>
<p>缩放对于 <strong>PCA</strong> 也非常重要。使用
z-分数等同于在计算主成分时使用<strong>相关矩阵</strong>（参见第30页的“相关性”），而不是协方差矩阵。计算
PCA 的软件通常都有一个使用相关矩阵的选项（在 R 中，<code>princomp</code>
函数有一个 <code>cor</code> 参数）。</p>
</blockquote>
<h4 id="主导变量"><strong>主导变量</strong></h4>
<p>Dominant Variables</p>
<p>即使变量以相同的尺度测量并准确反映相对重要性（例如，对股票价格的走势），有时<strong>重新缩放变量</strong>仍然是有用的。</p>
<p>假设我们在第289页的“解释主成分”中添加了 Google (GOOGL) 和 Amazon
(AMZN) 到分析中。下面我们来看看如何在 R 中完成：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">syms <span class="operator">&lt;-</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;GOOGL&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;AMZN&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;AAPL&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;MSFT&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;CSCO&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;INTC&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;CVX&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;XOM&#x27;</span><span class="punctuation">,</span></span><br><span class="line"><span class="string">&#x27;SLB&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;COP&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;JPM&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;WFC&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;USB&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;AXP&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;WMT&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;TGT&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;HD&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;COST&#x27;</span><span class="punctuation">)</span></span><br><span class="line">top_sp1 <span class="operator">&lt;-</span> sp500_px<span class="punctuation">[</span>row.names<span class="punctuation">(</span>sp500_px<span class="punctuation">)</span> <span class="operator">&gt;=</span> <span class="string">&#x27;2005-01-01&#x27;</span><span class="punctuation">,</span> syms<span class="punctuation">]</span></span><br><span class="line">sp_pca1 <span class="operator">&lt;-</span> princomp<span class="punctuation">(</span>top_sp1<span class="punctuation">)</span></span><br><span class="line">screeplot<span class="punctuation">(</span>sp_pca1<span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<p>在 Python 中，我们按如下方式获取碎石图：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">syms = [<span class="string">&#x27;GOOGL&#x27;</span>, <span class="string">&#x27;AMZN&#x27;</span>, <span class="string">&#x27;AAPL&#x27;</span>, <span class="string">&#x27;MSFT&#x27;</span>, <span class="string">&#x27;CSCO&#x27;</span>, <span class="string">&#x27;INTC&#x27;</span>, <span class="string">&#x27;CVX&#x27;</span>, <span class="string">&#x27;XOM&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;SLB&#x27;</span>, <span class="string">&#x27;COP&#x27;</span>, <span class="string">&#x27;JPM&#x27;</span>, <span class="string">&#x27;WFC&#x27;</span>, <span class="string">&#x27;USB&#x27;</span>, <span class="string">&#x27;AXP&#x27;</span>, <span class="string">&#x27;WMT&#x27;</span>, <span class="string">&#x27;TGT&#x27;</span>, <span class="string">&#x27;HD&#x27;</span>, <span class="string">&#x27;COST&#x27;</span>]</span><br><span class="line">top_sp1 = sp500_px.loc[sp500_px.index &gt;= <span class="string">&#x27;2005-01-01&#x27;</span>, syms]</span><br><span class="line">sp_pca1 = PCA()</span><br><span class="line">sp_pca1.fit(top_sp1)</span><br><span class="line">explained_variance = pd.DataFrame(sp_pca1.explained_variance_)</span><br><span class="line">ax = explained_variance.head(<span class="number">10</span>).plot.bar(legend=<span class="literal">False</span>, figsize=(<span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">ax.set_xlabel(<span class="string">&#x27;Component&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/img3/面向数据科学家的实用统计学/F7.13.png" alt="F7.13" style="zoom:50%;" /></p>
<p>碎石图显示了前几个主成分的方差。在本例中，图7-13中的碎石图显示<strong>第一和第二成分的方差比其他成分大得多</strong>。
这通常表明<strong>一个或两个变量主导了载荷</strong>。在这个例子中，情况确实如此：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">round(sp_pca1$loadings[,1:2], 3)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">       Comp.1 Comp.2</span><br><span class="line">GOOGL   0.781  0.609</span><br><span class="line">AMZN    0.593 -0.792</span><br><span class="line">AAPL    0.078  0.004</span><br><span class="line">MSFT    0.029  0.002</span><br><span class="line">CSCO    0.017 -0.001</span><br><span class="line">INTC    0.020 -0.001</span><br><span class="line">CVX     0.068 -0.021</span><br><span class="line">XOM     0.053 -0.005</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>在 Python 中，我们使用以下代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loadings = pd.DataFrame(sp_pca1.components_[<span class="number">0</span>:<span class="number">2</span>, :], columns=top_sp1.columns)</span><br><span class="line">loadings.transpose()</span><br></pre></td></tr></table></figure>
<p>前两个主成分几乎完全由 <strong>GOOGL</strong> 和
<strong>AMZN</strong> 主导。这是因为 GOOGL 和 AMZN
的<strong>股价波动主导了变异性</strong>。</p>
<p>为了处理这种情况，您可以<strong>保持现状</strong>、<strong>重新缩放变量</strong>（参见第319页的“变量缩放”），或者<strong>将主导变量从分析中排除并单独处理</strong>。没有“正确”的方法，处理方式取决于具体的应用。</p>
<h4 id="分类数据和-gowers-距离"><strong>分类数据和 Gower's
距离</strong></h4>
<p>Categorical Data and Gower’s Distance</p>
<p>对于<strong>分类数据</strong>，您必须将其转换为数值数据，可以通过<strong>排序</strong>（对于有序因子）或<strong>编码为一组二元（虚拟）变量</strong>来完成。如果数据由<strong>混合的连续和二元变量</strong>组成，您通常会希望<strong>缩放变量</strong>，使其范围相似；参见第319页的“变量缩放”。一种流行的方法是使用
<strong>Gower's 距离</strong>。</p>
<p>Gower's
距离的基本思想是根据<strong>数据的类型</strong>对<strong>每个变量</strong>应用<strong>不同的距离度量</strong>：</p>
<ul>
<li>对于<strong>数值变量</strong>和<strong>有序因子</strong>，距离被计算为两条记录之间<strong>差值的绝对值</strong>（曼哈顿距离）。</li>
<li>对于<strong>分类变量</strong>，如果两条记录的类别<strong>不同</strong>，距离为<strong>1</strong>；如果类别<strong>相同</strong>，距离为<strong>0</strong>。</li>
</ul>
<p>Gower's 距离的计算过程如下：</p>
<ol type="1">
<li>计算<strong>每条记录</strong>所有<strong>变量对 <span
class="math inline">\(i\)</span> 和 <span
class="math inline">\(j\)</span></strong> 的距离 <span
class="math inline">\(d_{i,j}\)</span>。</li>
<li>将每对距离 <span class="math inline">\(d_{i,j}\)</span>
进行缩放，使其最小值<strong>为0</strong>，最大值<strong>为1</strong>。</li>
<li>使用简单或加权平均值，将变量之间的成对缩放距离相加，以创建<strong>距离矩阵</strong>。</li>
</ol>
<p>为了说明 Gower's 距离，我们从 R 中的贷款数据中提取几行：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">&gt;</span> x <span class="operator">&lt;-</span> loan_data<span class="punctuation">[</span><span class="number">1</span><span class="operator">:</span><span class="number">5</span><span class="punctuation">,</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;dti&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;payment_inc_ratio&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;home_&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;purpose_&#x27;</span><span class="punctuation">)</span><span class="punctuation">]</span></span><br><span class="line"><span class="operator">&gt;</span> x</span><br><span class="line"><span class="comment"># A tibble: 5 × 4</span></span><br><span class="line">    dti payment_inc_ratio home_     purpose_</span><br><span class="line">  <span class="operator">&lt;</span>dbl<span class="operator">&gt;</span>             <span class="operator">&lt;</span>dbl<span class="operator">&gt;</span> <span class="operator">&lt;</span>fct<span class="operator">&gt;</span>     <span class="operator">&lt;</span>fct<span class="operator">&gt;</span></span><br><span class="line"><span class="number">1</span>  <span class="number">1.00</span>             <span class="number">2.97</span>  OWN       credit_card</span><br><span class="line"><span class="number">2</span>  <span class="number">5.55</span>             <span class="number">3.83</span>  RENT      debt_consolidation</span><br><span class="line"><span class="number">3</span> <span class="number">18.08</span>             <span class="number">7.69</span>  MORTGAGE  debt_consolidation</span><br><span class="line"><span class="number">4</span> <span class="number">10.08</span>             <span class="number">5.58</span>  RENT      home_improvement</span><br><span class="line"><span class="number">5</span>  <span class="number">6.56</span>             <span class="number">2.92</span>  MORTGAGE  major_purchase</span><br></pre></td></tr></table></figure>
<p>R 的 <code>cluster</code> 包中的 <code>daisy</code> 函数可用于计算
Gower's 距离：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">library<span class="punctuation">(</span>cluster<span class="punctuation">)</span></span><br><span class="line">daisy<span class="punctuation">(</span>x<span class="punctuation">,</span> metric<span class="operator">=</span><span class="string">&#x27;gower&#x27;</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Dissimilarities :</span><br><span class="line">          1         2         3         4</span><br><span class="line">2 0.6220479                              </span><br><span class="line">3 0.6863877 0.8143398                    </span><br><span class="line">4 0.6329040 0.7608561 0.4307083          </span><br><span class="line">5 0.3772789 0.5389727 0.3091088 0.5056250</span><br><span class="line"></span><br><span class="line">Metric :  mixed ; Types = I, I, N, N </span><br><span class="line">Number of objects : 5</span><br></pre></td></tr></table></figure>
<p>截至撰写本文时，Gower's 距离在任何流行的 Python
包中都不可用。然而，正在进行将其包含在 <code>scikit-learn</code>
中的活动。一旦实现发布，我们将更新随附的源代码。</p>
<p>所有距离都在<strong>0到1之间</strong>。距离最大的记录对是<strong>2和3</strong>：它们的
<code>home</code> 和 <code>purpose</code> 值都不同，并且
<code>dti</code>（债务收入比）和 <code>payment_inc_ratio</code>
的水平也大相径庭。记录<strong>3和5</strong>的距离最小，因为它们的
<code>home</code> 和 <code>purpose</code> 值相同。</p>
<p>您可以将从 <code>daisy</code> 计算出的 Gower's 距离矩阵传递给
<code>hclust</code>
进行<strong>层次聚类</strong>（参见第304页的“层次聚类”）：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">df <span class="operator">&lt;-</span> defaults<span class="punctuation">[</span>sample<span class="punctuation">(</span>nrow<span class="punctuation">(</span>defaults<span class="punctuation">)</span><span class="punctuation">,</span> <span class="number">250</span><span class="punctuation">)</span><span class="punctuation">,</span></span><br><span class="line"><span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;dti&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;payment_inc_ratio&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;home&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;purpose&#x27;</span><span class="punctuation">)</span><span class="punctuation">]</span></span><br><span class="line">d <span class="operator">=</span> daisy<span class="punctuation">(</span>df<span class="punctuation">,</span> metric<span class="operator">=</span><span class="string">&#x27;gower&#x27;</span><span class="punctuation">)</span></span><br><span class="line">hcl <span class="operator">&lt;-</span> hclust<span class="punctuation">(</span>d<span class="punctuation">)</span></span><br><span class="line">dnd <span class="operator">&lt;-</span> as.dendrogram<span class="punctuation">(</span>hcl<span class="punctuation">)</span></span><br><span class="line">plot<span class="punctuation">(</span>dnd<span class="punctuation">,</span> leaflab<span class="operator">=</span><span class="string">&#x27;none&#x27;</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<p><img src="/img3/面向数据科学家的实用统计学/F7.14.png" alt="F7.14" style="zoom:67%;" /></p>
<p>生成的<strong>树状图</strong>如图7-14所示。 单个记录在 x
轴上无法区分，但我们可以将树状图在0.5处水平切割，并使用此代码检查其中一个子树中的记录：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dnd_cut <span class="operator">&lt;-</span> cut<span class="punctuation">(</span>dnd<span class="punctuation">,</span> h<span class="operator">=</span><span class="number">0.5</span><span class="punctuation">)</span></span><br><span class="line">df<span class="punctuation">[</span>labels<span class="punctuation">(</span>dnd_cut<span class="operator">$</span>lower<span class="punctuation">[[</span><span class="number">1</span><span class="punctuation">]</span><span class="punctuation">]</span><span class="punctuation">)</span><span class="punctuation">,</span><span class="punctuation">]</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">      dti payment_inc_ratio home_          purpose_</span><br><span class="line">44532 21.22          8.37694   OWN debt_consolidation</span><br><span class="line">39826 22.59          6.22827   OWN debt_consolidation</span><br><span class="line">13282 31.00          9.64200   OWN debt_consolidation</span><br><span class="line">31510 26.21         11.94380   OWN debt_consolidation</span><br><span class="line">6693  26.96          9.45600   OWN debt_consolidation</span><br><span class="line">7356  25.81          9.39257   OWN debt_consolidation</span><br><span class="line">9278  21.00         14.71850   OWN debt_consolidation</span><br><span class="line">13520 29.00         18.86670   OWN debt_consolidation</span><br><span class="line">14668 25.75         17.53440   OWN debt_consolidation</span><br><span class="line">19975 22.70         17.12170   OWN debt_consolidation</span><br><span class="line">23492 22.68         18.50250   OWN debt_consolidation</span><br></pre></td></tr></table></figure>
<p>这个子树完全由<strong>拥有房产</strong>且贷款目的标记为<strong>“债务合并”</strong>的记录组成。虽然并非所有子树都存在严格的分离，但这表明分类变量倾向于在簇中被分组在一起。</p>
<h4 id="混合数据的聚类问题"><strong>混合数据的聚类问题</strong></h4>
<p>Problems with Clustering Mixed Data</p>
<p>K-均值和 PCA
<strong>最适合连续变量</strong>。对于较小的数据集，最好使用带有 Gower's
距离的<strong>层次聚类</strong>。原则上，K-均值也可以应用于<strong>二元或分类数据</strong>。通常会使用“独热编码”表示法（参见第242页的“独热编码”）将分类数据转换为数值。然而，在实践中，将
K-均值和 PCA 与二元数据一起使用可能会很困难。</p>
<p>如果使用标准的
z-分数，<strong>二元变量将主导簇的定义</strong>。这是因为 0/1
变量只取两个值，而
K-均值可以通过将所有值为0或1的记录分配给单个簇来获得较小的簇内平方和。例如，将
<code>kmeans</code> 应用于包含因子变量 <code>home</code> 和
<code>pub_rec_zero</code> 的贷款违约数据，如R中所示：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">df <span class="operator">&lt;-</span> model.matrix<span class="punctuation">(</span><span class="operator">~</span> <span class="operator">-</span><span class="number">1</span> <span class="operator">+</span> dti <span class="operator">+</span> payment_inc_ratio <span class="operator">+</span> home_ <span class="operator">+</span> pub_rec_zero<span class="punctuation">,</span></span><br><span class="line">data<span class="operator">=</span>defaults<span class="punctuation">)</span></span><br><span class="line">df0 <span class="operator">&lt;-</span> scale<span class="punctuation">(</span>df<span class="punctuation">)</span></span><br><span class="line">km0 <span class="operator">&lt;-</span> kmeans<span class="punctuation">(</span>df0<span class="punctuation">,</span> centers<span class="operator">=</span><span class="number">4</span><span class="punctuation">,</span> nstart<span class="operator">=</span><span class="number">10</span><span class="punctuation">)</span></span><br><span class="line">centers0 <span class="operator">&lt;-</span> scale<span class="punctuation">(</span>km0<span class="operator">$</span>centers<span class="punctuation">,</span> center<span class="operator">=</span><span class="literal">FALSE</span><span class="punctuation">,</span></span><br><span class="line">scale<span class="operator">=</span><span class="number">1</span><span class="operator">/</span><span class="built_in">attr</span><span class="punctuation">(</span>df0<span class="punctuation">,</span> <span class="string">&#x27;scaled:scale&#x27;</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line"><span class="built_in">round</span><span class="punctuation">(</span>scale<span class="punctuation">(</span>centers0<span class="punctuation">,</span> center<span class="operator">=</span><span class="operator">-</span><span class="built_in">attr</span><span class="punctuation">(</span>df0<span class="punctuation">,</span> <span class="string">&#x27;scaled:center&#x27;</span><span class="punctuation">)</span><span class="punctuation">,</span> scale<span class="operator">=</span><span class="literal">FALSE</span><span class="punctuation">)</span><span class="punctuation">,</span> <span class="number">2</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">   dti payment_inc_ratio home_MORTGAGE home_OWN home_RENT pub_rec_zero</span><br><span class="line">1 17.20              9.27          0.00     1.00      0.00         0.92</span><br><span class="line">2 16.99              9.11          0.00     0.00      1.00         1.00</span><br><span class="line">3 16.50              8.06          0.52     0.00      0.48         1.00</span><br><span class="line">4 17.46              8.42          1.00     0.00      0.00         0.00</span><br></pre></td></tr></table></figure>
<p>在 Python 中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">columns = [<span class="string">&#x27;dti&#x27;</span>, <span class="string">&#x27;payment_inc_ratio&#x27;</span>, <span class="string">&#x27;home_&#x27;</span>, <span class="string">&#x27;pub_rec_zero&#x27;</span>]</span><br><span class="line">df = pd.get_dummies(defaults[columns])</span><br><span class="line">scaler = preprocessing.StandardScaler()</span><br><span class="line">df0 = scaler.fit_transform(df * <span class="number">1.0</span>)</span><br><span class="line">kmeans = KMeans(n_clusters=<span class="number">4</span>, random_state=<span class="number">1</span>).fit(df0)</span><br><span class="line">centers = pd.DataFrame(scaler.inverse_transform(kmeans.cluster_centers_),</span><br><span class="line">columns=df.columns)</span><br><span class="line">centers</span><br></pre></td></tr></table></figure>
<p>前四个簇本质上是<strong>因子变量不同水平的代理</strong>。为了避免这种行为，您可以将二元变量<strong>缩放</strong>，使其方差小于其他变量。另外，对于非常大的数据集，您可以将聚类应用于<strong>具有特定分类值的不同数据子集</strong>。例如，您可以将聚类分别应用于那些有房贷的人、拥有房产的人或租房的人所获得的贷款。</p>
<p><strong>关键思想</strong></p>
<ul>
<li>以不同尺度测量的变量需要<strong>转换为相似的尺度</strong>，以确保它们对算法的影响不主要由其尺度决定。</li>
<li>一种常见的缩放方法是<strong>归一化（标准化）</strong>——减去均值并除以标准差。</li>
<li>另一种方法是 <strong>Gower's
距离</strong>，它将所有变量缩放到0-1的范围（常用于混合数值和分类数据）。</li>
</ul>
<h3 id="总结"><strong>总结</strong></h3>
<p>对于数值数据的<strong>降维</strong>，主要工具是<strong>主成分分析</strong>或
<strong>K-均值聚类</strong>。两者都需要注意<strong>数据的正确缩放</strong>，以确保有意义的数据缩减。</p>
<p>对于<strong>高度结构化</strong>且簇<strong>分隔良好</strong>的数据进行聚类时，所有方法可能都会产生相似的结果。每种方法都有其自身的优势。<strong>K-均值</strong>可以扩展到非常大的数据，且易于理解。<strong>层次聚类</strong>可以应用于混合数据类型——数值和分类——并易于进行直观的显示（树状图）。<strong>基于模型的聚类</strong>建立在统计理论基础上，提供了一种比启发式方法更严谨的方法。然而，对于<strong>非常大的数据</strong>，K-均值是主要使用的方法。</p>
<p>对于像贷款和股票数据这样的<strong>嘈杂数据</strong>（以及数据科学家将面临的大部分数据），选择就更加<strong>严峻</strong>了。K-均值、层次聚类，尤其是基于模型的聚类，都会产生<strong>截然不同</strong>的解决方案。数据科学家应该如何进行？不幸的是，<strong>没有简单的经验法则来指导选择</strong>。最终，使用的方法将取决于<strong>数据大小</strong>和<strong>应用的目标</strong>。</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/AI/" rel="tag"># AI</a>
              <a href="/tags/Python/" rel="tag"># Python</a>
              <a href="/tags/%E7%BB%9F%E8%AE%A1/" rel="tag"># 统计</a>
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a>
              <a href="/tags/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6/" rel="tag"># 数据科学</a>
              <a href="/tags/R/" rel="tag"># R</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2025/09/25/t%E6%A3%80%E9%AA%8C%E5%92%8C%E8%87%AA%E5%8A%A9%E6%B3%95bootstrap%E7%9A%84%E5%B7%AE%E5%88%AB/" rel="prev" title="t检验和自助法bootstrap的差别">
                  <i class="fa fa-chevron-left"></i> t检验和自助法bootstrap的差别
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2025/09/25/%E7%AC%AC6%E7%AB%A0%20%E7%BB%9F%E8%AE%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="next" title="第6章 统计机器学习">
                  第6章 统计机器学习 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Rayman.hung</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
<script src="https://cdn.jsdelivr.net/npm/hexo-generator-searchdb@1.4.0/dist/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js","integrity":"sha256-r+3itOMtGGjap0x+10hu6jW/gZCzxHsoKrOd7gyRSGY="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>

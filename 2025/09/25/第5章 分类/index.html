<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.1.1">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" integrity="sha256-DfWjNxDkM94fVBWx1H5BMMp0Zq7luBlV8QRcSES7s+0=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"hongyitong.github.io","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.11.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="个人注：以下使用gemini翻译 20250916  《Practical Statistics for Data Scientists》书籍英文版 《面向数据科学家的实用统计学》中文版书籍 第5章 分类 数据科学家经常需要为商业问题提供自动化决策。一封电子邮件是钓鱼邮件吗？一个客户是否可能流失？一个网络用户是否可能点击广告？这些都是分类问题，一种监督学习形式。我们首先在已知结果的数据上训练一个">
<meta property="og:type" content="article">
<meta property="og:title" content="第5章 分类">
<meta property="og:url" content="http://hongyitong.github.io/2025/09/25/%E7%AC%AC5%E7%AB%A0%20%E5%88%86%E7%B1%BB/index.html">
<meta property="og:site_name" content="忆桐之家的博客">
<meta property="og:description" content="个人注：以下使用gemini翻译 20250916  《Practical Statistics for Data Scientists》书籍英文版 《面向数据科学家的实用统计学》中文版书籍 第5章 分类 数据科学家经常需要为商业问题提供自动化决策。一封电子邮件是钓鱼邮件吗？一个客户是否可能流失？一个网络用户是否可能点击广告？这些都是分类问题，一种监督学习形式。我们首先在已知结果的数据上训练一个">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E9%9D%A2%E5%90%91%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%AE%B6%E7%9A%84%E5%AE%9E%E7%94%A8%E7%BB%9F%E8%AE%A1%E5%AD%A6/F5.1.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E9%9D%A2%E5%90%91%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%AE%B6%E7%9A%84%E5%AE%9E%E7%94%A8%E7%BB%9F%E8%AE%A1%E5%AD%A6/F5.2.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E9%9D%A2%E5%90%91%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%AE%B6%E7%9A%84%E5%AE%9E%E7%94%A8%E7%BB%9F%E8%AE%A1%E5%AD%A6/F5.3.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E9%9D%A2%E5%90%91%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%AE%B6%E7%9A%84%E5%AE%9E%E7%94%A8%E7%BB%9F%E8%AE%A1%E5%AD%A6/F5.4.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E9%9D%A2%E5%90%91%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%AE%B6%E7%9A%84%E5%AE%9E%E7%94%A8%E7%BB%9F%E8%AE%A1%E5%AD%A6/F5.5.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E9%9D%A2%E5%90%91%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%AE%B6%E7%9A%84%E5%AE%9E%E7%94%A8%E7%BB%9F%E8%AE%A1%E5%AD%A6/F5.6.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E9%9D%A2%E5%90%91%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%AE%B6%E7%9A%84%E5%AE%9E%E7%94%A8%E7%BB%9F%E8%AE%A1%E5%AD%A6/F5.7.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E9%9D%A2%E5%90%91%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%AE%B6%E7%9A%84%E5%AE%9E%E7%94%A8%E7%BB%9F%E8%AE%A1%E5%AD%A6/F5.8.png">
<meta property="article:published_time" content="2025-09-24T16:00:00.000Z">
<meta property="article:modified_time" content="2025-09-25T03:06:19.319Z">
<meta property="article:author" content="Rayman.hung">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="Python">
<meta property="article:tag" content="统计">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="数据科学">
<meta property="article:tag" content="R">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://hongyitong.github.io/img3/%E9%9D%A2%E5%90%91%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%AE%B6%E7%9A%84%E5%AE%9E%E7%94%A8%E7%BB%9F%E8%AE%A1%E5%AD%A6/F5.1.png">


<link rel="canonical" href="http://hongyitong.github.io/2025/09/25/%E7%AC%AC5%E7%AB%A0%20%E5%88%86%E7%B1%BB/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://hongyitong.github.io/2025/09/25/%E7%AC%AC5%E7%AB%A0%20%E5%88%86%E7%B1%BB/","path":"2025/09/25/第5章 分类/","title":"第5章 分类"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>第5章 分类 | 忆桐之家的博客</title>
  





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">忆桐之家的博客</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Rayman&Tony</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section">首页</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section">分类</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section">关于</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section">归档</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section">标签</a></li><li class="menu-item menu-item-commonweal"><a href="/404.html" rel="section">公益 404</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC5%E7%AB%A0-%E5%88%86%E7%B1%BB"><span class="nav-number">1.</span> <span class="nav-text">第5章 分类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF"><span class="nav-number">1.1.</span> <span class="nav-text">朴素贝叶斯</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E7%B2%BE%E7%A1%AE%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E4%B8%8D%E5%88%87%E5%AE%9E%E9%99%85"><span class="nav-number">1.1.1.</span> <span class="nav-text">为什么精确贝叶斯分类不切实际</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9C%B4%E7%B4%A0%E8%A7%A3%E6%B3%95"><span class="nav-number">1.1.2.</span> <span class="nav-text">朴素解法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E5%80%BC%E9%A2%84%E6%B5%8B%E5%8F%98%E9%87%8F"><span class="nav-number">1.1.3.</span> <span class="nav-text">数值预测变量</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90"><span class="nav-number">1.2.</span> <span class="nav-text">判别分析</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5"><span class="nav-number">1.2.1.</span> <span class="nav-text">协方差矩阵</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%B4%B9%E8%88%8D%E5%B0%94%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB"><span class="nav-number">1.2.2.</span> <span class="nav-text">费舍尔线性判别</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E4%BE%8B%E5%AD%90"><span class="nav-number">1.2.3.</span> <span class="nav-text">一个简单的例子</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92"><span class="nav-number">1.3.</span> <span class="nav-text">逻辑回归</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%80%BB%E8%BE%91%E5%93%8D%E5%BA%94%E5%87%BD%E6%95%B0%E4%B8%8E-logit"><span class="nav-number">1.3.1.</span> <span class="nav-text">逻辑响应函数与 Logit</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B-glm"><span class="nav-number">1.3.2.</span> <span class="nav-text">逻辑回归与广义线性模型
(GLM)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.3.3.</span> <span class="nav-text">广义线性模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84%E9%A2%84%E6%B5%8B%E5%80%BC"><span class="nav-number">1.3.4.</span> <span class="nav-text">逻辑回归的预测值</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%B3%BB%E6%95%B0%E4%B8%8E%E5%87%A0%E7%8E%87%E6%AF%94%E7%9A%84%E8%A7%A3%E9%87%8A"><span class="nav-number">1.3.5.</span> <span class="nav-text">系数与几率比的解释</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%B8%8E%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9B%B8%E4%BC%BC%E7%82%B9%E4%B8%8E%E5%B7%AE%E5%BC%82"><span class="nav-number">1.3.6.</span> <span class="nav-text">线性回归与逻辑回归：相似点与差异</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0"><span class="nav-number">1.3.7.</span> <span class="nav-text">模型评估</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AF%84%E4%BC%B0%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.4.</span> <span class="nav-text">评估分类模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B7%B7%E6%B7%86%E7%9F%A9%E9%98%B5"><span class="nav-number">1.4.1.</span> <span class="nav-text">混淆矩阵</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BD%95%E8%A7%81%E7%B1%BB%E5%88%AB%E9%97%AE%E9%A2%98"><span class="nav-number">1.4.2.</span> <span class="nav-text">罕见类别问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%B2%BE%E7%A1%AE%E7%8E%87%E5%8F%AC%E5%9B%9E%E7%8E%87%E5%92%8C%E7%89%B9%E5%BC%82%E5%BA%A6"><span class="nav-number">1.4.3.</span> <span class="nav-text">精确率、召回率和特异度</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#roc%E6%9B%B2%E7%BA%BF"><span class="nav-number">1.4.4.</span> <span class="nav-text">ROC曲线</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#auc-%E6%9B%B2%E7%BA%BF%E4%B8%8B%E9%9D%A2%E7%A7%AF"><span class="nav-number">1.4.5.</span> <span class="nav-text">AUC (曲线下面积)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8F%90%E5%8D%87%E5%BA%A6"><span class="nav-number">1.4.6.</span> <span class="nav-text">提升度</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8D%E5%B9%B3%E8%A1%A1%E6%95%B0%E6%8D%AE%E7%9A%84%E7%AD%96%E7%95%A5"><span class="nav-number">1.5.</span> <span class="nav-text">不平衡数据的策略</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%AC%A0%E9%87%87%E6%A0%B7"><span class="nav-number">1.5.1.</span> <span class="nav-text">欠采样</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BF%87%E9%87%87%E6%A0%B7%E5%92%8C%E4%B8%8A%E4%B8%8B%E5%8A%A0%E6%9D%83"><span class="nav-number">1.5.2.</span> <span class="nav-text">过采样和上&#x2F;下加权</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E7%94%9F%E6%88%90"><span class="nav-number">1.5.3.</span> <span class="nav-text">数据生成</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E6%88%90%E6%9C%AC%E7%9A%84%E5%88%86%E7%B1%BB"><span class="nav-number">1.5.4.</span> <span class="nav-text">基于成本的分类</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8E%A2%E7%B4%A2%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C"><span class="nav-number">1.5.5.</span> <span class="nav-text">探索预测结果</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">1.6.</span> <span class="nav-text">总结</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Rayman.hung</p>
  <div class="site-description" itemprop="description">技术分享、读书心得、亲子时刻</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives">
          <span class="site-state-item-count">903</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags">
        <span class="site-state-item-count">1151</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/hongyitong" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;hongyitong" rel="noopener" target="_blank">GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="http://blog.sina.com.cn/yitonghong" title="忆桐之家 → http:&#x2F;&#x2F;blog.sina.com.cn&#x2F;yitonghong" rel="noopener" target="_blank">忆桐之家</a>
      </span>
      <span class="links-of-author-item">
        <a href="http://douban.com/people/2780741" title="豆瓣 → http:&#x2F;&#x2F;douban.com&#x2F;people&#x2F;2780741" rel="noopener" target="_blank">豆瓣</a>
      </span>
      <span class="links-of-author-item">
        <a href="http://www.zhihu.com/people/rayman-36" title="知乎 → http:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;rayman-36" rel="noopener" target="_blank">知乎</a>
      </span>
      <span class="links-of-author-item">
        <a href="http://www.liaoxuefeng.com/" title="廖雪峰的官方网站 → http:&#x2F;&#x2F;www.liaoxuefeng.com&#x2F;" rel="noopener" target="_blank">廖雪峰的官方网站</a>
      </span>
      <span class="links-of-author-item">
        <a href="http://www.vaikan.com/" title="外刊IT评论 → http:&#x2F;&#x2F;www.vaikan.com&#x2F;" rel="noopener" target="_blank">外刊IT评论</a>
      </span>
  </div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://hongyitong.github.io/2025/09/25/%E7%AC%AC5%E7%AB%A0%20%E5%88%86%E7%B1%BB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Rayman.hung">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="忆桐之家的博客">
      <meta itemprop="description" content="技术分享、读书心得、亲子时刻">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="第5章 分类 | 忆桐之家的博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          第5章 分类
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2025-09-25 00:00:00 / 修改时间：11:06:19" itemprop="dateCreated datePublished" datetime="2025-09-25T00:00:00+08:00">2025-09-25</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/" itemprop="url" rel="index"><span itemprop="name">读书心得</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <blockquote>
<p>个人注：以下使用gemini翻译 20250916</p>
</blockquote>
<p><a
href="/img3/面向数据科学家的实用统计学/Practical-Statistics-for-Data-Scientists.pdf">《Practical
Statistics for Data Scientists》书籍英文版</a><br />
<a
href="/img3/面向数据科学家的实用统计学/面向数据科学家的实用统计学.pdf">《面向数据科学家的实用统计学》中文版书籍</a></p>
<h2 id="第5章-分类"><strong>第5章 分类</strong></h2>
<p>数据科学家经常需要为商业问题提供自动化决策。一封电子邮件是钓鱼邮件吗？一个客户是否可能流失？一个网络用户是否可能点击广告？这些都是<strong>分类问题</strong>，一种<strong>监督学习</strong>形式。我们首先在已知结果的数据上训练一个模型，然后将该模型应用于结果未知的数据。分类也许是<strong>预测</strong>最重要的形式：其目标是预测一条记录是1还是0（例如，钓鱼/非钓鱼、点击/不点击、流失/不流失），或者在某些情况下，预测它属于几个类别中的一个（例如，Gmail
将你的收件箱过滤为“主要”、“社交”、“推广”或“论坛”）。</p>
<p>很多时候，我们需要的不仅仅是一个简单的二元分类，我们还想知道一个案例属于某个类别的<strong>预测概率</strong>。大多数算法都可以返回一个属于目标类别的<strong>概率分数（probability
score）（倾向性）</strong>（propensity），而不仅仅是简单地分配一个二元分类。事实上，对于<strong>逻辑回归</strong>，R
的默认输出是<strong>对数几率</strong>（log-odds）尺度，这必须被转换为倾向性。在
Python 的 <code>scikit-learn</code>
中，逻辑回归与大多数分类方法一样，提供了两种预测方法：<code>predict</code>（返回类别）和
<code>predict_proba</code>（返回每个类别的概率）。然后，可以使用一个<strong>滑动截止点</strong>（
sliding cutoff）将倾向性分数转换为决策。一般方法如下：</p>
<ol type="1">
<li><strong>设定一个截止概率</strong>：为目标类别设定一个截止概率，如果记录的概率高于这个截止点，我们就认为它属于该类别。</li>
<li><strong>估算概率</strong>：使用任何模型估算一条记录属于目标类别的概率。</li>
<li><strong>做出决策</strong>：如果这个概率高于截止概率，则将新记录分配给目标类别。</li>
</ol>
<p>截止点越高，被预测为1的记录就越少；截止点越低，被预测为1的记录就越多。</p>
<p>本章将介绍几种用于分类和估算倾向性的关键技术；下一章将描述既可用于分类也可用于数值预测的其他方法。</p>
<span id="more"></span>
<p><strong>多于两个类别的情况</strong>：</p>
<p>绝大多数问题涉及二元响应。然而，一些分类问题涉及的响应变量可能有多个可能的结果。例如，在客户订阅合同的周年纪念日，可能会有三种结果：客户流失（Y=2）、转为按月合同（Y=1），或签订新的长期合同（Y=0）。目标是预测
<span class="math inline">\(j=0, 1, 2\)</span> 中的 <span
class="math inline">\(Y=j\)</span>。本章中的大多数分类方法都可以直接或经过简单调整后应用于具有多于两个结果的响应变量。</p>
<p>即使在结果多于两个的情况下，问题通常也可以通过使用<strong>条件概率</strong>重新定义为一系列二元问题。例如，为了预测合同结果，你可以解决两个二元预测问题：</p>
<ul>
<li>预测 <span class="math inline">\(Y=0\)</span> 还是 <span
class="math inline">\(Y&gt;0\)</span>。</li>
<li>在给定 <span class="math inline">\(Y&gt;0\)</span> 的条件下，预测
<span class="math inline">\(Y=1\)</span> 还是 <span
class="math inline">\(Y=2\)</span>。</li>
</ul>
<p>在这种情况下，将问题分解为两个案例是有意义的：（1）客户是否流失；（2）如果他们不流失，他们会选择哪种类型的合同。从模型拟合的角度来看，将多类别问题转换为一系列二元问题通常是有利的，当某个类别比其他类别常见得多时，这种方法尤其有效。</p>
<h3 id="朴素贝叶斯"><strong>朴素贝叶斯</strong></h3>
<p>Naive Bayes</p>
<p>朴素贝叶斯算法利用<strong>在给定结果下观察到预测变量的概率</strong>，来估计我们真正感兴趣的：<strong>在给定一组预测变量值下，观察到结果
<span class="math inline">\(Y=i\)</span> 的概率</strong>。</p>
<p><strong>朴素贝叶斯的关键术语</strong></p>
<ul>
<li><p><strong>条件概率（Conditional probability）</strong>
在给定某个其他事件（例如，<span
class="math inline">\(Y=i\)</span>）的情况下，观察到某个事件（例如，<span
class="math inline">\(X=i\)</span>）的概率，写作 <span
class="math inline">\(P(X_i | Y_i)\)</span>。</p></li>
<li><p><strong>后验概率（Posterior probability）</strong>
在结合了预测变量信息之后，某个结果出现的概率（与不考虑预测变量信息的<strong>先验概率</strong>（prior
probability）相反）。</p></li>
</ul>
<p>为了理解朴素贝叶斯分类，我们可以从想象<strong>完整或精确的贝叶斯分类</strong>开始。对于每一条要分类的记录：</p>
<ol type="1">
<li>找到所有具有相同<strong>预测变量配置文件</strong>（即，预测变量值完全相同）的其他记录。</li>
<li>确定这些记录属于哪些类别，并找出<strong>最普遍（即概率最高）的类别</strong>。</li>
<li>将该类别分配给新记录。</li>
</ol>
<p>上述方法相当于在样本中找到所有与待分类的新记录完全相似的记录，其所有预测变量值都相同。</p>
<blockquote>
<p><strong>通用注解：</strong></p>
<p>在标准的朴素贝叶斯算法中，预测变量必须是<strong>分类（因子）变量</strong>。对于如何使用连续变量，请参阅第200页的“数值预测变量”中的两种变通方法。</p>
</blockquote>
<h4
id="为什么精确贝叶斯分类不切实际"><strong>为什么精确贝叶斯分类不切实际</strong></h4>
<p>Why Exact Bayesian Classification Is Impractical</p>
<p>当预测变量的数量超过少数几个时，许多待分类的记录将找不到精确匹配。考虑一个基于人口统计变量来预测投票的模型。即使是一个相当大的样本，也可能找不到一个完全匹配的新记录，例如：一个来自美国中西部的、高收入的、男性、西班牙裔，在上次选举中投了票，但在上上次选举中没有投票，有三个女儿和一个儿子，并且离了婚。这还只有八个变量，对于大多数分类问题来说，这数量非常小。如果只增加一个有五个同样频繁的类别的新变量，匹配的概率就会<strong>降低五倍</strong>。</p>
<h4 id="朴素解法">朴素解法</h4>
<p>The Naive Solution</p>
<p>在<strong>朴素贝叶斯</strong>解法中，我们不再将概率计算局限于与待分类记录完全匹配的记录。相反，我们使用<strong>整个数据集</strong>。朴素贝叶斯算法的步骤如下：</p>
<ol type="1">
<li><p>对于一个二元响应 <span class="math inline">\(Y=i\)</span>（<span
class="math inline">\(i=0\)</span> 或 <span
class="math inline">\(1\)</span>），估计每个预测变量的<strong>个体条件概率</strong>
<span class="math inline">\(P(X_j | Y=i)\)</span>；这些是在观察到 <span
class="math inline">\(Y=i\)</span>
时，预测变量值出现在记录中的概率。该概率通过训练集中属于 <span
class="math inline">\(Y=i\)</span> 记录的 <span
class="math inline">\(X_j\)</span> 值的比例来估计。</p></li>
<li><p>将这些概率相乘，然后乘以属于 <span
class="math inline">\(Y=i\)</span> 的记录比例。</p></li>
<li><p>为所有类别重复步骤1和2。</p></li>
<li><p>通过将步骤2为类别 <span class="math inline">\(i\)</span>
计算出的值除以所有类别此类值的总和，来估计类别 <span
class="math inline">\(i\)</span> 的概率。</p></li>
<li><p>将该记录分配给在此组预测变量值下具有最高概率的类别。</p></li>
</ol>
<p>这个朴素贝叶斯算法也可以用方程来表示，用于计算在给定一组预测变量
<span class="math inline">\(X_1, \dots, X_p\)</span> 时，观察到结果
<span class="math inline">\(Y=i\)</span> 的概率：</p>
<p><span class="math display">\[
P(Y=i | X_1, X_2, \dots, X_p)
\]</span> 以下是使用精确贝叶斯分类计算类别概率的完整公式：</p>
<p><span class="math display">\[
P(Y=i | X_1, X_2, \dots, X_p) = \frac{P(Y=i)P(X_1, \dots, X_p |
Y=i)}{P(Y=0)P(X_1, \dots, X_p | Y=0) + P(Y=1)P(X_1, \dots, X_p | Y=1)}
\]</span></p>
<p>在<strong>朴素贝叶斯条件独立性</strong>的假设下，这个方程变为：</p>
<p><span class="math display">\[
P(Y=i | X_1, X_2, \dots, X_p) = \frac{P(Y=i)P(X_1 | Y=i) \dots P(X_p |
Y=i)}{P(Y=0)P(X_1 | Y=0) \dots P(X_p | Y=0) + P(Y=1)P(X_1 | Y=1) \dots
P(X_p | Y=1)}
\]</span></p>
<p><strong>为什么这个公式被称为“朴素”？</strong>我们做了一个简化的假设：<strong>在观察到某个结果时，预测变量向量的精确条件概率，可以由个体条件概率的乘积来很好地估计</strong>。换句话说，在估计
<span class="math inline">\(P(X_j | Y=i)\)</span> 而不是 <span
class="math inline">\(P(X_1, X_2, \dots, X_p | Y=i)\)</span>
时，我们假设 <span class="math inline">\(X_j\)</span>
<strong>独立于</strong>所有其他预测变量 <span
class="math inline">\(X_k\)</span>（<span class="math inline">\(k \neq
j\)</span>）。</p>
<p>在 R 语言中，可以使用几个包来估计朴素贝叶斯模型。以下代码使用
<code>klaR</code> 包对贷款支付数据进行模型拟合：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">library<span class="punctuation">(</span>klaR<span class="punctuation">)</span></span><br><span class="line">naive_model <span class="operator">&lt;-</span> NaiveBayes<span class="punctuation">(</span>outcome <span class="operator">~</span> purpose_ <span class="operator">+</span> home_ <span class="operator">+</span> emp_len_<span class="punctuation">,</span></span><br><span class="line">data <span class="operator">=</span> na.omit<span class="punctuation">(</span>loan_data<span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">naive_model<span class="operator">$</span>table</span><br></pre></td></tr></table></figure>
<p>模型的输出是条件概率 <span class="math inline">\(P(X_j |
Y=i)\)</span>。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">$purpose_</span><br><span class="line">var</span><br><span class="line">grouping credit_card debt_consolidation home_improvement major_purchase</span><br><span class="line">paid off 0.18759649 0.55215915 0.07150104 0.05359270</span><br><span class="line">default 0.15151515 0.57571347 0.05981209 0.03727229</span><br><span class="line">var</span><br><span class="line">grouping medical other small_business</span><br><span class="line">paid off 0.01424728 0.09990737 0.02099599</span><br><span class="line">default 0.01433549 0.11561025 0.04574126</span><br><span class="line">$home_</span><br><span class="line">var</span><br><span class="line">grouping MORTGAGE OWN RENT</span><br><span class="line">paid off 0.4894800 0.0808963 0.4296237</span><br><span class="line">default 0.4313440 0.0832782 0.4853778</span><br><span class="line">$emp_len_</span><br><span class="line">var</span><br><span class="line">grouping &lt; 1 Year &gt; 1 Year</span><br><span class="line">paid off 0.03105289 0.96894711</span><br><span class="line">default 0.04728508 0.95271492</span><br></pre></td></tr></table></figure>
<p>在 Python 中，我们可以使用 <code>scikit-learn</code> 中的
<code>sklearn.naive_bayes.MultinomialNB</code>。在拟合模型之前，我们需要将分类特征转换为虚拟变量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">predictors = [<span class="string">&#x27;purpose_&#x27;</span>, <span class="string">&#x27;home_&#x27;</span>, <span class="string">&#x27;emp_len_&#x27;</span>]</span><br><span class="line">outcome = <span class="string">&#x27;outcome&#x27;</span></span><br><span class="line">X = pd.get_dummies(loan_data[predictors], prefix=</span><br><span class="line">y = loan_data[outcome]</span><br><span class="line"><span class="string">&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#x27;&#x27;</span>)</span><br><span class="line">naive_model = MultinomialNB(alpha=<span class="number">0.01</span>, fit_prior=<span class="literal">True</span>)</span><br><span class="line">naive_model.fit(X, y)</span><br></pre></td></tr></table></figure>
<p>可以通过 <code>feature_log_prob_</code>
属性从拟合模型中获得条件概率。</p>
<p>该模型可用于预测一笔新贷款的结果。我们使用数据集的最后一条记录进行测试：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">new_loan <span class="operator">&lt;-</span> loan_data<span class="punctuation">[</span><span class="number">147</span><span class="punctuation">,</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;purpose_&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;home_&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;emp_len_&#x27;</span><span class="punctuation">)</span><span class="punctuation">]</span></span><br><span class="line">row.names<span class="punctuation">(</span>new_loan<span class="punctuation">)</span> <span class="operator">&lt;-</span> <span class="literal">NULL</span></span><br><span class="line">new_loan</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">purpose_ home_ emp_len_</span><br><span class="line">1 small_business MORTGAGE &gt; 1 Year</span><br></pre></td></tr></table></figure>
<p>在 Python 中，我们按如下方式获取此值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">new_loan = X.loc[<span class="number">146</span>:<span class="number">146</span>, :]</span><br></pre></td></tr></table></figure>
<p>在这种情况下，模型预测为<strong>违约</strong>（R）：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predict<span class="punctuation">(</span>naive_model<span class="punctuation">,</span> new_loan<span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$class</span><br><span class="line">[1] default</span><br><span class="line">Levels: paid off default</span><br><span class="line">$posterior</span><br><span class="line">paid off default</span><br><span class="line">[1,] 0.3463013 0.6536987</span><br></pre></td></tr></table></figure>
<p>正如我们所讨论的，<code>scikit-learn</code>
的分类模型有两个方法：<code>predict</code>，返回预测的类别；以及
<code>predict_proba</code>，返回类别概率：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;predicted class: &#x27;</span>, naive_model.predict(new_loan)[<span class="number">0</span>])</span><br><span class="line">probabilities = pd.DataFrame(naive_model.predict_proba(new_loan),</span><br><span class="line">columns=loan_data[outcome].cat.categories)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;predicted probabilities&#x27;</span>, probabilities)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">predicted class: default</span><br><span class="line">predicted probabilities</span><br><span class="line">   default  paid off</span><br><span class="line">0  0.653696  0.346304</span><br></pre></td></tr></table></figure>
<p>预测还返回了违约概率的后验估计。众所周知，朴素贝叶斯分类器会产生<strong>有偏的估计</strong>（biased
estimates.）。然而，当目标是根据 <span
class="math inline">\(Y=1\)</span>
的概率对记录进行<strong>排序</strong>时，不需要无偏的概率估计，并且朴素贝叶斯会产生良好的结果。</p>
<h4 id="数值预测变量"><strong>数值预测变量</strong></h4>
<p>Numeric Predictor Variables</p>
<p>贝叶斯分类器只适用于<strong>分类（因子）预测变量</strong>，例如在垃圾邮件分类中，预测任务的核心在于单词、短语、字符等的出现与否。如果想将朴素贝叶斯应用于<strong>数值预测变量</strong>，必须采取以下两种方法之一：</p>
<ul>
<li><strong>分箱和转换</strong>：将数值预测变量分箱并转换为分类预测变量，然后应用上一节介绍的算法。</li>
<li><strong>使用概率模型</strong>：例如，使用<strong>正态分布</strong>（参见第69页的“正态分布”）来估计条件概率
<span class="math inline">\(P(X_j | Y=i)\)</span>。</li>
</ul>
<blockquote>
<p><strong>警告：</strong></p>
<p>当训练数据中某个预测变量类别<strong>不存在</strong>时，算法会将新数据中相应结果变量的概率赋值为零，而不是像其他方法那样简单地忽略此变量并利用其他变量的信息。大多数朴素贝叶斯的实现都使用一个<strong>平滑参数</strong>（拉普拉斯平滑）来防止这种情况发生。</p>
</blockquote>
<p><strong>关键思想</strong></p>
<ul>
<li><strong>适用范围</strong>：朴素贝叶斯适用于分类（因子）预测变量和结果。</li>
<li><strong>核心问题</strong>：它会问，“在每个结果类别中，哪个预测变量类别最有可能出现？”</li>
<li><strong>反向推断</strong>：然后，该信息被反向用于估计在给定预测变量值的情况下，结果类别的概率。</li>
</ul>
<h3 id="判别分析"><strong>判别分析</strong></h3>
<p>Discriminant Analysis</p>
<p>判别分析是最早出现的统计分类器，由R. A.
Fisher于1936年在《优生学年鉴》杂志上发表的一篇文章中提出。</p>
<p><strong>判别分析的关键术语</strong></p>
<ul>
<li><p><strong>协方差 (Covariance)</strong>
衡量一个变量与另一个变量<strong>共同变化</strong>的程度（即，相似的大小和方向）。</p></li>
<li><p><strong>判别函数 (Discriminant function)</strong>
应用于预测变量时，能<strong>最大化类别间分离度</strong>的函数。</p></li>
<li><p><strong>判别权重 (Discriminant weights)</strong>
应用判别函数后得到的分数，用于估计记录属于某个类别的概率。</p></li>
</ul>
<p>尽管判别分析涵盖了多种技术，但最常用的是<strong>线性判别分析（Linear
Discriminant Analysis,
LDA）</strong>。Fisher最初提出的方法与今天的LDA略有不同，但其机制基本相同。随着树模型和逻辑回归等更复杂技术的出现，LDA的应用现在已不如从前广泛。</p>
<p>然而，在某些应用中你仍然可能遇到LDA，而且它与其他更广泛使用的方法（如<strong>主成分分析</strong>；参见第284页的“主成分分析”）存在联系。</p>
<blockquote>
<p><strong>警告：</strong></p>
<p>线性判别分析（LDA）不应与<strong>潜在狄利克雷分配（Latent Dirichlet
Allocation,
LDA）</strong>混淆。后者用于文本和自然语言处理，与线性判别分析无关。</p>
</blockquote>
<h4 id="协方差矩阵"><strong>协方差矩阵</strong></h4>
<p>Covariance Matrix</p>
<p>要理解判别分析，首先有必要引入两个或多个变量之间的<strong>协方差</strong>概念。协方差衡量了两个变量
<code>x</code> 和 <code>z</code> 之间的关系。用 <span
class="math inline">\(\bar{x}\)</span> 和 <span
class="math inline">\(\bar{z}\)</span>
表示每个变量的均值（参见第9页的“均值”）。<span
class="math inline">\(x\)</span> 和 <span
class="math inline">\(z\)</span> 之间的协方差 <span
class="math inline">\(s_{x, z}\)</span> 由以下公式给出：</p>
<p><span class="math display">\[
s_{x, z} = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(z_i - \bar{z})}{n - 1}
\]</span></p>
<p>其中，<span class="math inline">\(n\)</span> 是记录的数量（注意我们用
<span class="math inline">\(n-1\)</span> 而不是 <span
class="math inline">\(n\)</span> 来除；参见第15页的“自由度，以及 <span
class="math inline">\(n\)</span> 还是 <span
class="math inline">\(n-1\)</span>？”）。</p>
<p>与<strong>相关系数</strong>（参见第30页的“相关性”）一样，正值表示正向关系，负值表示负向关系。然而，相关系数被限制在-1和1之间，而协方差的尺度取决于变量
<span class="math inline">\(x\)</span> 和 <span
class="math inline">\(z\)</span> 的尺度。由 <span
class="math inline">\(x\)</span> 和 <span
class="math inline">\(z\)</span> 组成的<strong>协方差矩阵</strong> <span
class="math inline">\(\Sigma\)</span>
的对角线上（行和列是同一变量）是各个变量的方差 <span
class="math inline">\(s_x^2\)</span> 和 <span
class="math inline">\(s_z^2\)</span>，非对角线上是变量对之间的协方差：</p>
<p><span class="math display">\[
\Sigma = \begin{pmatrix} s_x^2 &amp; s_{x,z} \\ s_{z,x} &amp; s_z^2
\end{pmatrix}
\]</span></p>
<blockquote>
<p><strong>通用注解：</strong></p>
<p>回想一下，<strong>标准差</strong>用于将一个变量标准化为<strong>z分数</strong>；协方差矩阵则用于这种标准化过程的<strong>多元扩展</strong>。这被称为<strong>马哈拉诺比斯距离</strong>（Mahalanobis
distance）（参见第242页的“其他距离度量”），并与 LDA 函数相关。</p>
</blockquote>
<h4 id="费舍尔线性判别"><strong>费舍尔线性判别</strong></h4>
<p>Fisher’s Linear Discriminant</p>
<p>为了简化，我们关注一个分类问题：使用两个连续数值变量 <span
class="math inline">\(x\)</span> 和 <span
class="math inline">\(z\)</span> 来预测一个二元结果 <span
class="math inline">\(y\)</span>。从技术上讲，判别分析假定预测变量是正态分布的连续变量，但实际上，即使在非极端的非正态性或二元预测变量情况下，该方法也表现良好。费舍尔线性判别将<strong>组间变异</strong>与<strong>组内变异</strong>区分开来。具体来说，为了将记录分为两组，线性判别分析（LDA）侧重于最大化“组间”平方和
<span
class="math inline">\(SS_{between}\)</span>（衡量两组之间的变异）相对于“组内”平方和
<span
class="math inline">\(SS_{within}\)</span>（衡量组内变异）的比率。在这里，这两组对应于
<span class="math inline">\(y=0\)</span> 的记录（<span
class="math inline">\(x_0, z_0\)</span>）和 <span
class="math inline">\(y=1\)</span> 的记录（<span
class="math inline">\(x_1, z_1\)</span>）。</p>
<p>该方法找到能够最大化平方和比率的线性组合 <span
class="math inline">\(w_x x + w_z z\)</span>：</p>
<p><span class="math display">\[
\frac{SS_{between}}{SS_{within}}
\]</span></p>
<p><strong>组间平方和</strong>是两个组均值之间的平方距离，而<strong>组内平方和</strong>是每组内部围绕均值的扩散程度，并由协方差矩阵加权。直观地，通过最大化组间平方和并最小化组内平方和，该方法实现了两组之间最大的分离度。</p>
<h4 id="一个简单的例子"><strong>一个简单的例子</strong></h4>
<p>与书籍《Modern Applied Statistics with S》（作者：W. N. Venables 和
B. D. Ripley，1994年，Springer出版社）相关的 <code>MASS</code>
包提供了在 R 中进行 LDA
的函数。以下代码将该函数应用于一个贷款样本数据，使用了两个预测变量
<code>borrower_score</code> 和
<code>payment_inc_ratio</code>，并打印出估计的线性判别权重：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">library<span class="punctuation">(</span>MASS<span class="punctuation">)</span></span><br><span class="line">loan_lda <span class="operator">&lt;-</span> lda<span class="punctuation">(</span>outcome <span class="operator">~</span> borrower_score <span class="operator">+</span> payment_inc_ratio<span class="punctuation">,</span></span><br><span class="line">data<span class="operator">=</span>loan3000<span class="punctuation">)</span></span><br><span class="line">loan_lda<span class="operator">$</span>scaling</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">LD1</span><br><span class="line">borrower_score     7.17583880</span><br><span class="line">payment_inc_ratio -0.09967559</span><br></pre></td></tr></table></figure>
<p>在 Python 中，我们可以使用 <code>sklearn.discriminant_analysis</code>
中的 <code>LinearDiscriminantAnalysis</code>。<code>scalings_</code>
属性给出了估计的权重：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">loan3000.outcome = loan3000.outcome.astype(<span class="string">&#x27;category&#x27;</span>)</span><br><span class="line">predictors = [<span class="string">&#x27;borrower_score&#x27;</span>, <span class="string">&#x27;payment_inc_ratio&#x27;</span>]</span><br><span class="line">outcome = <span class="string">&#x27;outcome&#x27;</span></span><br><span class="line">X = loan3000[predictors]</span><br><span class="line">y = loan3000[outcome]</span><br><span class="line">loan_lda = LinearDiscriminantAnalysis()</span><br><span class="line">loan_lda.fit(X, y)</span><br><span class="line">pd.DataFrame(loan_lda.scalings_, index=X.columns)</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>通用注解：</strong></p>
<p><strong>使用判别分析进行特征选择</strong>：如果在运行 LDA
之前对预测变量进行标准化，那么判别权重可以作为<strong>变量重要性</strong>的度量，从而提供一种计算高效的<strong>特征选择</strong>方法。</p>
</blockquote>
<p><code>lda</code> 函数可以预测“违约”与“已还清”的概率：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pred <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>loan_lda<span class="punctuation">)</span></span><br><span class="line">head<span class="punctuation">(</span>pred<span class="operator">$</span>posterior<span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">paid off  default</span><br><span class="line">1 0.4464563 0.5535437</span><br><span class="line">2 0.4410466 0.5589534</span><br><span class="line">3 0.7273038 0.2726962</span><br><span class="line">4 0.4937462 0.5062538</span><br><span class="line">5 0.3900475 0.6099525</span><br><span class="line">6 0.5892594 0.4107406</span><br></pre></td></tr></table></figure>
<p>拟合模型的 <code>predict_proba</code>
方法返回“违约”和“已还清”结果的概率：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pred = pd.DataFrame(loan_lda.predict_proba(loan3000[predictors]),</span><br><span class="line">columns=loan_lda.classes_)</span><br><span class="line">pred.head()</span><br></pre></td></tr></table></figure>
<p>绘制预测结果的图表有助于说明 LDA 的工作原理。使用
<code>predict</code>
函数的输出，可以按如下方式生成违约概率的估计图：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">center <span class="operator">&lt;-</span> 0.5 <span class="operator">*</span> <span class="punctuation">(</span>loan_lda<span class="operator">$</span>mean<span class="punctuation">[</span><span class="number">1</span><span class="punctuation">,</span> <span class="punctuation">]</span> <span class="operator">+</span> loan_lda<span class="operator">$</span>mean<span class="punctuation">[</span><span class="number">2</span><span class="punctuation">,</span> <span class="punctuation">]</span><span class="punctuation">)</span></span><br><span class="line">slope <span class="operator">&lt;-</span> <span class="operator">-</span>loan_lda<span class="operator">$</span>scaling<span class="punctuation">[</span><span class="number">1</span><span class="punctuation">]</span> <span class="operator">/</span> loan_lda<span class="operator">$</span>scaling<span class="punctuation">[</span><span class="number">2</span><span class="punctuation">]</span></span><br><span class="line">intercept <span class="operator">&lt;-</span> center<span class="punctuation">[</span><span class="number">2</span><span class="punctuation">]</span> <span class="operator">-</span> center<span class="punctuation">[</span><span class="number">1</span><span class="punctuation">]</span> <span class="operator">*</span> slope</span><br><span class="line">ggplot<span class="punctuation">(</span>data<span class="operator">=</span>lda_df<span class="punctuation">,</span> aes<span class="punctuation">(</span>x<span class="operator">=</span>borrower_score<span class="punctuation">,</span> y<span class="operator">=</span>payment_inc_ratio<span class="punctuation">,</span></span><br><span class="line">color<span class="operator">=</span>prob_default<span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">geom_point<span class="punctuation">(</span>alpha<span class="operator">=</span><span class="number">.6</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">scale_color_gradientn<span class="punctuation">(</span>colors<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;#ca0020&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;#f7f7f7&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;#0571b0&#x27;</span><span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">scale_x_continuous<span class="punctuation">(</span>expand<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">0</span><span class="punctuation">,</span><span class="number">0</span><span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">scale_y_continuous<span class="punctuation">(</span>expand<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">0</span><span class="punctuation">,</span><span class="number">0</span><span class="punctuation">)</span><span class="punctuation">,</span> lim<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">0</span><span class="punctuation">,</span> <span class="number">20</span><span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">geom_abline<span class="punctuation">(</span>slope<span class="operator">=</span>slope<span class="punctuation">,</span> intercept<span class="operator">=</span>intercept<span class="punctuation">,</span> color<span class="operator">=</span><span class="string">&#x27;darkgreen&#x27;</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<p>在 Python 中，使用以下代码创建类似的图：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Use scalings and center of means to determine decision boundary</span></span><br><span class="line">center = np.mean(loan_lda.means_, axis=<span class="number">0</span>)</span><br><span class="line">slope = - loan_lda.scalings_[<span class="number">0</span>] / loan_lda.scalings_[<span class="number">1</span>]</span><br><span class="line">intercept = center[<span class="number">1</span>] - center[<span class="number">0</span>] * slope</span><br><span class="line"><span class="comment"># payment_inc_ratio for borrower_score of 0 and 20</span></span><br><span class="line">x_0 = (<span class="number">0</span> - intercept) / slope</span><br><span class="line">x_20 = (<span class="number">20</span> - intercept) / slope</span><br><span class="line">lda_df = pd.concat([loan3000, pred[<span class="string">&#x27;default&#x27;</span>]], axis=<span class="number">1</span>)</span><br><span class="line">lda_df.head()</span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">g = sns.scatterplot(x=<span class="string">&#x27;borrower_score&#x27;</span>, y=<span class="string">&#x27;payment_inc_ratio&#x27;</span>,</span><br><span class="line">hue=<span class="string">&#x27;default&#x27;</span>, data=lda_df,</span><br><span class="line">palette=sns.diverging_palette(<span class="number">240</span>, <span class="number">10</span>, n=<span class="number">9</span>, as_cmap=<span class="literal">True</span>),</span><br><span class="line">ax=ax, legend=<span class="literal">False</span>)</span><br><span class="line">ax.set_ylim(<span class="number">0</span>, <span class="number">20</span>)</span><br><span class="line">ax.set_xlim(<span class="number">0.15</span>, <span class="number">0.8</span>)</span><br><span class="line">ax.plot((x_0, x_20), (<span class="number">0</span>, <span class="number">20</span>), linewidth=<span class="number">3</span>)</span><br><span class="line">ax.plot(*loan_lda.means_.transpose())</span><br></pre></td></tr></table></figure>
<p>由此产生的图表如图5-1所示。对角线左侧的数据点被预测为违约（概率大于0.5）。</p>
<p><img src="/img3/面向数据科学家的实用统计学/F5.1.png" alt="F5.1" style="zoom:50%;" /></p>
<p>使用判别函数权重，LDA
将预测变量空间分为两个区域，如图中的实线所示。离这条线越远（两个方向上），预测的置信度越高（即，概率越远离0.5）。</p>
<blockquote>
<p>通用注解：</p>
<p><strong>判别分析的扩展</strong>Extensions of Discriminant
Analysis<strong>更多预测变量</strong>：尽管本节的文字和示例只使用了两个预测变量，但
LDA
对于多于两个预测变量的情况同样有效。唯一的限制因素是<strong>记录的数量</strong>（估计协方差矩阵需要每个变量有足够的记录，这在数据科学应用中通常不是问题）。</p>
<p>判别分析还有其他变体。其中最著名的是<strong>二次判别分析（Quadratic
Discriminant Analysis, QDA）</strong>。尽管其名称如此，QDA
仍然是一种线性判别函数。主要区别在于，在 LDA 中，我们假定对应于 <span
class="math inline">\(Y=0\)</span> 和 <span
class="math inline">\(Y=1\)</span>
的两组拥有<strong>相同的协方差矩阵</strong>。而在 QDA
中，允许这两组拥有<strong>不同的协方差矩阵</strong>。在实践中，这种差异在大多数应用中并不重要。</p>
</blockquote>
<p><strong>关键思想</strong></p>
<ul>
<li><strong>适用范围</strong>：判别分析适用于连续或分类预测变量，以及分类结果。</li>
<li><strong>工作原理</strong>：它使用<strong>协方差矩阵</strong>计算一个<strong>线性判别函数</strong>，该函数用于区分属于一个类别的记录和属于另一个类别的记录。</li>
<li><strong>结果</strong>：该函数应用于每条记录，以推导出<strong>权重或分数</strong>（每个可能的类别一个），从而确定其估计的类别。</li>
</ul>
<h3 id="逻辑回归"><strong>逻辑回归</strong></h3>
<p>Logistic Regression</p>
<p>逻辑回归与多元线性回归（参见第4章）类似，但其<strong>结果是二元的</strong>。它采用各种转换方法，将问题转化为一个可以拟合线性模型的形式。与K-最近邻和朴素贝叶斯不同，逻辑回归是一种<strong>结构化模型方法</strong>（a
structured model approach），而非以数据为中心的方法（a data-centric
approach）。由于其计算速度快，且输出的模型有助于快速对新数据进行评分，因此它是一种非常流行的方法。</p>
<p><strong>逻辑回归的关键术语</strong></p>
<ul>
<li><p><strong>Logit（逻辑函数）</strong> 将类别成员资格的概率映射到
<span class="math inline">\(\pm \infty\)</span>
范围（而不是0到1）的函数。
同义词：<strong>对数几率</strong>（见下文）</p></li>
<li><p><strong>Odds（几率）</strong>
“成功”（1）与“不成功”（0）的比率。</p></li>
<li><p><strong>Log odds（对数几率）</strong>
转换后模型中的响应变量（现在是线性的），该值可以被映射回概率。</p></li>
</ul>
<h4 id="逻辑响应函数与-logit"><strong>逻辑响应函数与 Logit</strong></h4>
<p>Logistic Response Function and Logit</p>
<p>逻辑回归的关键要素是<strong>逻辑响应函数</strong>和
<strong>logit</strong>，通过它们我们将概率（在0-1的尺度上）映射到一个更广的、适合线性建模的尺度。</p>
<p>第一步是将结果变量视为<strong>标签为“1”的概率 <span
class="math inline">\(p\)</span></strong>。直观地，我们可能会试图将
<span class="math inline">\(p\)</span> 建模为预测变量的线性函数：</p>
<p><span class="math display">\[
p = \beta_0 + \beta_1x_1 + \beta_2x_2 + \dots + \beta_qx_q
\]</span> 然而，拟合这个模型并不能确保 <span
class="math inline">\(p\)</span>
最终会落在0到1之间，而概率必须如此。</p>
<p>相反，我们通过对预测变量应用<strong>逻辑响应</strong>或<strong>逆
Logit 函数</strong>来建模 <span class="math inline">\(p\)</span>：</p>
<p><span class="math display">\[
p = \frac{1}{1 + e^{-(\beta_0 + \beta_1x_1 + \beta_2x_2 + \dots +
\beta_qx_q)}}
\]</span> 这个转换确保了 <span class="math inline">\(p\)</span>
始终在0到1之间。</p>
<p>为了摆脱分母中的指数表达式，我们考虑<strong>几率（Odds）</strong>而非概率。几率是“成功”（1）与“不成功”（0）的比率，这对于任何地方的押注者来说都很熟悉。用概率表示，几率是一个事件发生的概率除以该事件不发生的概率。例如，如果一匹马获胜的概率是0.5，那么“不会获胜”的概率是
(1-0.5)=0.5，几率就是1.0：</p>
<p><span class="math display">\[
\text{Odds}(Y=1) = \frac{p}{1-p}
\]</span> 我们可以使用<strong>逆几率函数</strong>从几率中获得概率：</p>
<p><span class="math display">\[
p = \frac{\text{Odds}}{1 + \text{Odds}}
\]</span> 我们将此与前面展示的逻辑响应函数结合起来，得到：</p>
<p><span class="math display">\[
\text{Odds}(Y=1) = e^{\beta_0 + \beta_1x_1 + \beta_2x_2 + \dots +
\beta_qx_q}
\]</span>
最后，对两边取对数，我们得到一个涉及预测变量线性函数的表达式：</p>
<p><span class="math display">\[
\log(\text{Odds}(Y=1)) = \beta_0 + \beta_1x_1 + \beta_2x_2 + \dots +
\beta_qx_q
\]</span> <strong>对数几率函数</strong>，也称为 <strong>logit
函数</strong>，将概率 <span class="math inline">\(p\)</span> 从 [0, 1]
映射到任意值 <span class="math inline">\([-\infty,
+\infty]\)</span>（见图5-2）。转换的循环完成；我们使用一个<strong>线性模型来预测一个概率</strong>，然后我们可以通过应用一个截止规则，将该概率映射回一个<strong>类别标签</strong>——任何概率大于截止点的记录都被归类为1。</p>
<p><img src="/img3/面向数据科学家的实用统计学/F5.2.png" alt="F5.2" style="zoom:33%;" /></p>
<h4 id="逻辑回归与广义线性模型-glm"><strong>逻辑回归与广义线性模型
(GLM)</strong></h4>
<p>Logistic Regression and the GLM</p>
<p>逻辑回归公式中的响应变量是二元结果为1的<strong>对数几率</strong>。我们只观察到二元结果本身，而不是对数几率，因此需要特殊的统计方法来拟合方程。逻辑回归是广义线性模型（GLM）的一个特例，该模型旨在将线性回归扩展到其他情境。</p>
<p>在 R 中，要拟合逻辑回归，需要使用 <code>glm</code> 函数并将
<code>family</code> 参数设置为
<code>binomial</code>。以下代码使用第238页“K-最近邻”中引入的个人贷款数据来拟合逻辑回归：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">logistic_model <span class="operator">&lt;-</span> glm<span class="punctuation">(</span>outcome <span class="operator">~</span> payment_inc_ratio <span class="operator">+</span> purpose_ <span class="operator">+</span></span><br><span class="line">home_ <span class="operator">+</span> emp_len_ <span class="operator">+</span> borrower_score<span class="punctuation">,</span></span><br><span class="line">data<span class="operator">=</span>loan_data<span class="punctuation">,</span> family<span class="operator">=</span><span class="string">&#x27;binomial&#x27;</span><span class="punctuation">)</span></span><br><span class="line">logistic_model</span><br><span class="line">Call<span class="operator">:</span> glm<span class="punctuation">(</span>formula <span class="operator">=</span> outcome <span class="operator">~</span> payment_inc_ratio <span class="operator">+</span> purpose_ <span class="operator">+</span> home_ <span class="operator">+</span></span><br><span class="line">emp_len_ <span class="operator">+</span> borrower_score<span class="punctuation">,</span> family <span class="operator">=</span> <span class="string">&quot;binomial&quot;</span><span class="punctuation">,</span> data <span class="operator">=</span> loan_data<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line">Coefficients<span class="operator">:</span></span><br><span class="line"><span class="punctuation">(</span>Intercept<span class="punctuation">)</span> payment_inc_ratio</span><br><span class="line"><span class="number">1.63809</span> <span class="number">0.07974</span></span><br><span class="line">purpose_debt_consolidation purpose_home_improvement</span><br><span class="line"><span class="number">0.24937</span> <span class="number">0.40774</span></span><br><span class="line">purpose_major_purchase purpose_medical</span><br><span class="line"><span class="number">0.22963</span> <span class="number">0.51048</span></span><br><span class="line">purpose_other purpose_small_business</span><br><span class="line"><span class="number">0.62066</span> <span class="number">1.21526</span></span><br><span class="line">home_OWN home_RENT</span><br><span class="line"><span class="number">0.04833</span> <span class="number">0.15732</span></span><br><span class="line">emp_len_ <span class="operator">&gt;</span> <span class="number">1</span> Year borrower_score</span><br><span class="line"><span class="operator">-</span><span class="number">0.35673</span> <span class="operator">-</span><span class="number">4.61264</span></span><br><span class="line">Degrees of Freedom<span class="operator">:</span> <span class="number">45341</span> Total <span class="punctuation">(</span>i.e. Null<span class="punctuation">)</span>; <span class="number">45330</span> Residual</span><br><span class="line">Null Deviance<span class="operator">:</span> <span class="number">62860</span></span><br><span class="line">Residual Deviance<span class="operator">:</span> <span class="number">57510</span> AIC<span class="operator">:</span> <span class="number">57540</span></span><br></pre></td></tr></table></figure>
<p>响应变量是
<code>outcome</code>，如果贷款已还清则为0，如果违约则为1。<code>purpose_</code>
和 <code>home_</code>
是因子变量，分别代表贷款目的和房屋所有权状态。与线性回归一样，一个有 P
个水平的因子变量用 P-1 列来表示。在 R
中，默认使用<strong>参考编码</strong>，所有水平都与参考水平进行比较（参见第163页的“回归中的因子变量”）。这些因子的参考水平分别是
<code>credit_card</code> 和
<code>MORTGAGE</code>。<code>borrower_score</code>
变量是一个从0到1的分数，代表借款人的信用度（从差到优秀）。这个变量是使用K-最近邻从其他几个变量创建的——参见第247页的“KNN
作为特征工程”。</p>
<p>在 Python 中，我们使用 <code>sklearn.linear_model</code> 中的
<code>LogisticRegression</code> 类。<code>penalty</code> 和
<code>C</code> 参数用于通过 <strong>L1 或 L2
正则化</strong>防止过拟合。正则化默认是开启的。为了在不进行正则化的情况下拟合，我们将
<code>C</code> 设置为一个非常大的值。<code>solver</code>
参数选择使用的优化器；<code>liblinear</code> 方法是默认的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">predictors = [<span class="string">&#x27;payment_inc_ratio&#x27;</span>, <span class="string">&#x27;purpose_&#x27;</span>, <span class="string">&#x27;home_&#x27;</span>, <span class="string">&#x27;emp_len_&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;borrower_score&#x27;</span>]</span><br><span class="line">outcome = <span class="string">&#x27;outcome&#x27;</span></span><br><span class="line">X = pd.get_dummies(loan_data[predictors], prefix=</span><br><span class="line"><span class="string">&#x27;&#x27;</span></span><br><span class="line">, prefix_sep=</span><br><span class="line"><span class="string">&#x27;&#x27;</span></span><br><span class="line">,</span><br><span class="line">drop_first=<span class="literal">True</span>)</span><br><span class="line">y = loan_data[outcome]</span><br><span class="line">logit_reg = LogisticRegression(penalty=<span class="string">&#x27;l2&#x27;</span>, C=<span class="number">1e42</span>, solver=<span class="string">&#x27;liblinear&#x27;</span>)</span><br><span class="line">logit_reg.fit(X, y)</span><br></pre></td></tr></table></figure>
<p>与 R 不同，<code>scikit-learn</code> 从 <code>y</code>
中的唯一值（<code>paid off</code> 和
<code>default</code>）派生出类别。在内部，这些类别按字母顺序排序。由于这与
R 中使用的因子顺序相反，你会发现系数是相反的。<code>predict</code>
方法返回类别标签，而 <code>predict_proba</code> 返回从
<code>logit_reg.classes_</code> 属性中可用的顺序的概率。</p>
<h4 id="广义线性模型"><strong>广义线性模型</strong></h4>
<p>Generalized Linear Models</p>
<p>广义线性模型（Generalized Linear Models,
GLMs）由两个主要部分构成：</p>
<ul>
<li><strong>一个概率分布或族</strong>：在逻辑回归中是二项分布。</li>
<li><strong>一个链接函数</strong>：将响应变量映射到预测变量的转换函数，在逻辑回归中是
<strong>logit</strong>。</li>
</ul>
<p>逻辑回归是迄今为止最常见的 GLM 形式。数据科学家也会遇到其他类型的
GLM。有时会使用 <strong>log 链接函数</strong>而不是
logit；在实践中，对于大多数应用，使用 log
链接函数不太可能导致非常不同的结果。<strong>泊松分布</strong>通常用于建模计数数据（例如，一个用户在特定时间内访问网页的次数）。其他族包括<strong>负二项分布</strong>和<strong>伽马分布</strong>，它们常用于建模经过的时间（例如，到故障的时间）。与逻辑回归相比，使用这些模型的
GLM
应用更为细致，需要更谨慎。除非你熟悉并理解这些方法的效用和陷阱，否则最好避免使用它们。</p>
<h4 id="逻辑回归的预测值"><strong>逻辑回归的预测值</strong></h4>
<p>Predicted Values from Logistic Regression</p>
<p>逻辑回归的预测值是<strong>对数几率</strong>：<span
class="math inline">\(\hat{Y }=
\log(\text{Odds}(Y=1))\)</span>。预测概率由<strong>逻辑响应函数</strong>给出：</p>
<p><span class="math display">\[
\hat{p} = \frac{1}{1 + e^{- \hat{Y}}}
\]</span> 例如，查看 R 中 <code>logistic_model</code> 的预测：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pred <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>logistic_model<span class="punctuation">)</span></span><br><span class="line">summary<span class="punctuation">(</span>pred<span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Min. 1st Qu. Median Mean 3rd Qu. Max.</span><br><span class="line">-2.704774 -0.518825 -0.008539 0.002564 0.505061 3.509606</span><br></pre></td></tr></table></figure>
<p>在 Python 中，我们可以将概率转换为数据框，并使用
<code>describe</code> 方法来获取分布的这些特征：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pred = pd.DataFrame(logit_reg.predict_log_proba(X),</span><br><span class="line">columns=loan_data[outcome].cat.categories)</span><br><span class="line">pred.describe()</span><br></pre></td></tr></table></figure>
<p>将这些值转换为概率是一个简单的转换：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">prob <span class="operator">&lt;-</span> 1<span class="operator">/</span><span class="punctuation">(</span><span class="number">1</span> <span class="operator">+</span> <span class="built_in">exp</span><span class="punctuation">(</span><span class="operator">-</span>pred<span class="punctuation">)</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; summary(prob)</span><br><span class="line">Min. 1st Qu. Median Mean 3rd Qu. Max.</span><br><span class="line">0.06269 0.37313 0.49787 0.50000 0.62365 0.97096</span><br></pre></td></tr></table></figure>
<p><code>scikit-learn</code> 中的 <code>predict_proba</code>
方法可以直接获得概率：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pred = pd.DataFrame(logit_reg.predict_proba(X),</span><br><span class="line">columns=loan_data[outcome].cat.categories)</span><br><span class="line">pred.describe()</span><br></pre></td></tr></table></figure>
<p>这些值在0到1的范围内，但它们尚未声明预测值是违约还是已还清。我们可以将任何大于0.5的值声明为违约。在实践中，如果目标是识别<strong>罕见类别</strong>的成员，通常采用较低的截止点是合适的（参见第223页的“罕见类别问题”）。</p>
<h4 id="系数与几率比的解释"><strong>系数与几率比的解释</strong></h4>
<p>Interpreting the Coefficients and Odds Ratios</p>
<p>逻辑回归的一个优点是，它产生的模型可以快速地在新数据上进行评分，无需重新计算。另一个优点是，与其他分类方法相比，它相对更容易解释。其核心概念是理解<strong>几率比</strong>(odds
ratio)。对于一个二元因子变量 <span class="math inline">\(X\)</span>
来说，几率比最容易理解：</p>
<p><span class="math display">\[
\text{几率比} = \frac{\text{Odds}(Y=1 | X=1)}{\text{Odds}(Y=1 | X=0)}
\]</span> 这被解释为当 <span class="math inline">\(X=1\)</span>
时的几率与当 <span class="math inline">\(X=0\)</span>
时的几率之比。如果几率比是2，那么当 <span
class="math inline">\(X=1\)</span> 时 <span
class="math inline">\(Y=1\)</span> 的几率是当 <span
class="math inline">\(X=0\)</span> 时的两倍。</p>
<p>为什么我们要用几率比而不是概率？我们使用几率是因为，在逻辑回归中，系数
<span class="math inline">\(\beta_j\)</span> 是 <span
class="math inline">\(X_j\)</span> <strong>几率比的对数</strong>。</p>
<p>一个例子可以更清楚地说明这一点。对于第210页“逻辑回归与
GLM”中拟合的模型，<code>purpose_small_business</code>
的回归系数是1.21526。这意味着，与用于偿还信用卡债务的贷款相比，用于创建或扩展小企业的贷款，其违约与已还清的几率比<strong>减少了</strong>
<span class="math inline">\(e^{1.21526} \approx 3.4\)</span>
倍。显然，用于创建或扩展小企业的贷款比其他类型的贷款风险高得多。</p>
<p><img src="/img3/面向数据科学家的实用统计学/F5.3.png" alt="F5.3" style="zoom:33%;" /></p>
<p>图5-3显示了当几率比大于1时，几率比和对数几率比之间的关系。由于系数在对数尺度上，系数增加1，会导致几率比增加
<span class="math inline">\(e^1 \approx 2.72\)</span> 倍。</p>
<p>数值变量 <span class="math inline">\(X\)</span>
的几率比可以类似地解释：它们衡量 <span class="math inline">\(X\)</span>
变化一个单位时几率比的变化。例如，付款收入比从5增加到6，将使贷款违约的几率增加
<span class="math inline">\(e^{0.08244} \approx 1.09\)</span> 倍。变量
<code>borrower_score</code>
是衡量借款人信用度的分数，范围从0（低）到1（高）。信用度最好的借款人相对于最差的借款人违约的几率比，要小
<span class="math inline">\(e^{-4.61264} \approx 0.01\)</span>
倍。换句话说，信用度最差的借款人的违约风险是信用度最好的借款人的100倍！</p>
<h4
id="线性回归与逻辑回归相似点与差异"><strong>线性回归与逻辑回归：相似点与差异</strong></h4>
<p>Linear and Logistic Regression: Similarities and Differences</p>
<p>线性回归和逻辑回归有许多共同点。两者都假设预测变量与响应变量之间存在<strong>参数化的线性形式</strong>。探索和寻找最佳模型的方式也十分相似。线性模型的扩展，例如使用样条变换预测变量（参见第189页的“样条回归”），同样适用于逻辑回归。</p>
<p>然而，逻辑回归在两个根本方面有所不同： *
<strong>模型的拟合方式</strong>（不适用最小二乘法）。 *
<strong>模型残差的性质和分析</strong>。</p>
<p><strong>模型拟合</strong></p>
<p>线性回归使用<strong>最小二乘法</strong>进行拟合，其拟合质量通过
<strong>RMSE</strong> 和 <strong>R-squared</strong>
统计量进行评估。在逻辑回归中（与线性回归不同），没有闭合形式的解，模型必须使用<strong>最大似然估计（MLE）</strong>进行拟合。最大似然估计是一个试图找到最有可能产生我们所观察到的数据的模型的过程。在逻辑回归方程中，响应变量不是0或1，而是对响应为1的对数几率的估计。最大似然估计找到的解，使得估计的对数几率能够最好地描述观察到的结果。该算法的机制涉及一种<strong>拟牛顿优化</strong>，它在基于当前参数的评分步骤（费舍尔评分）和更新参数以改善拟合之间进行迭代。</p>
<p><strong>最大似然估计（MLE）</strong></p>
<p>如果你对统计符号感兴趣，这里有更多细节：假设有一组数据 <span
class="math inline">\(X_1, X_2, \dots, X_n\)</span> 和一个依赖于一组参数
<span class="math inline">\(\theta\)</span> 的概率模型 <span
class="math inline">\(P_\theta(X_1, X_2, \dots,
X_n)\)</span>。<strong>MLE 的目标是找到能够最大化 <span
class="math inline">\(P_\theta(X_1, X_2, \dots, X_n)\)</span> 值的参数集
<span
class="math inline">\(\theta\)</span></strong>；也就是说，它最大化在给定模型
<span class="math inline">\(P\)</span> 的情况下观察到 <span
class="math inline">\(X_1, X_2, \dots, X_n\)</span>
的概率。在拟合过程中，模型使用一个称为<strong>偏差（deviance）</strong>的度量进行评估：</p>
<p><span class="math display">\[
\text{偏差} = -2 \log P_\theta(X_1, X_2, \dots, X_n)
\]</span> 偏差越低 (个人注：因为有负号) ，拟合效果越好。</p>
<p>幸运的是，大多数实践者不需要关注拟合算法的细节，因为这些都由软件自动处理。大多数数据科学家无需担心拟合方法，只需理解它是在特定假设下找到一个好模型的方式即可。</p>
<blockquote>
<p>警告：</p>
<p><strong>处理因子变量</strong>（Handling Factor Variables）</p>
<p>在逻辑回归中，因子变量应像在线性回归中一样进行编码；参见第163页的“回归中的因子变量”。在
R
和其他软件中，这通常是自动处理的，并且通常使用<strong>参考编码</strong>。</p>
<p>本章涵盖的所有其他分类方法通常使用<strong>独热编码</strong>表示（参见第242页的“独热编码”）。在
Python 的 <code>scikit-learn</code>
中，最容易使用独热编码，这意味着在回归中只能使用由此产生的 <span
class="math inline">\(n-1\)</span> 个虚拟变量。</p>
</blockquote>
<h4 id="模型评估"><strong>模型评估</strong></h4>
<p>Assessing the Model</p>
<p>像其他分类方法一样，逻辑回归模型的评估标准是其对新数据的分类准确性（参见第219页的“评估分类模型”）。与线性回归一样，一些标准的统计工具可用于检查和改进模型。除了估计的系数外，R
还会报告系数的标准误（SE）、z 值和 p 值：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">summary<span class="punctuation">(</span>logistic_model<span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">Call:</span><br><span class="line">glm(formula = outcome ~ payment_inc_ratio + purpose_ + home_ +</span><br><span class="line">emp_len_ + borrower_score, family = &quot;binomial&quot;, data = loan_data)</span><br><span class="line">Deviance Residuals:</span><br><span class="line">Min       1Q   Median       3Q      Max</span><br><span class="line">-2.51951 -1.06908 -0.05853  1.07421  2.15528</span><br><span class="line">Coefficients:</span><br><span class="line">                           Estimate  Std. Error z value Pr(&gt;|z|)</span><br><span class="line">(Intercept)                 1.638092   0.073708  22.224  &lt; 2e-16 ***</span><br><span class="line">payment_inc_ratio           0.079737   0.002487  32.058  &lt; 2e-16 ***</span><br><span class="line">purpose_debt_consolidation  0.249373   0.027615   9.030  &lt; 2e-16 ***</span><br><span class="line">purpose_home_improvement    0.407743   0.046615   8.747  &lt; 2e-16 ***</span><br><span class="line">purpose_major_purchase      0.229628   0.053683   4.277  1.89e-05 ***</span><br><span class="line">purpose_medical             0.510479   0.086780   5.882  4.04e-09 ***</span><br><span class="line">purpose_other               0.620663   0.039436  15.738  &lt; 2e-16 ***</span><br><span class="line">purpose_small_business      1.215261   0.063320  19.192  &lt; 2e-16 ***</span><br><span class="line">home_OWN                    0.048330   0.038036   1.271  0.204</span><br><span class="line">home_RENT                   0.157320   0.021203   7.420  1.17e-13 ***</span><br><span class="line">emp_len_ &gt; 1 Year           -0.356731   0.052622  -6.779  1.21e-11 ***</span><br><span class="line">borrower_score              -4.612638   0.083558 -55.203  &lt; 2e-16 ***</span><br><span class="line">---</span><br><span class="line">Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1</span><br><span class="line">(Dispersion parameter for binomial family taken to be 1)</span><br><span class="line">Null deviance: 62857  on 45341  degrees of freedom</span><br><span class="line">Residual deviance: 57515  on 45330  degrees of freedom</span><br><span class="line">AIC: 57539</span><br><span class="line">Number of Fisher Scoring iterations: 4</span><br></pre></td></tr></table></figure>
<p><code>statsmodels</code>
包有一个广义线性模型（GLM）的实现，它提供了类似详细的信息：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">y_numbers = [<span class="number">1</span> <span class="keyword">if</span> yi == <span class="string">&#x27;default&#x27;</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> yi <span class="keyword">in</span> y]</span><br><span class="line">logit_reg_sm = sm.GLM(y_numbers, X.assign(const=<span class="number">1</span>),</span><br><span class="line">family=sm.families.Binomial())</span><br><span class="line">logit_result = logit_reg_sm.fit()</span><br><span class="line">logit_result.summary()</span><br></pre></td></tr></table></figure>
<p><strong>p
值的解释与回归中的情况具有相同的警告，应将其更多地视为变量重要性的相对指标（参见第153页的“评估模型”），而不是作为统计显著性的正式度量。</strong>一个具有二元响应的逻辑回归模型<strong>没有</strong>相关的
RMSE 或
R-squared。相反，逻辑回归模型通常使用更通用的<strong>分类指标</strong>进行评估；参见第219页的“评估分类模型”。</p>
<p>许多线性回归的概念也适用于逻辑回归（以及其他
GLM）。例如，你可以使用<strong>逐步回归</strong>、拟合<strong>交互项</strong>，或包含<strong>样条项</strong>。关于<strong>混杂变量</strong>和<strong>相关变量</strong>的相同问题也适用于逻辑回归（参见第169页的“解释回归方程”）。你可以使用
R 中的 <code>mgcv</code>
包拟合广义加性模型（参见第192页的“广义加性模型”）：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">logistic_gam <span class="operator">&lt;-</span> gam<span class="punctuation">(</span>outcome <span class="operator">~</span> s<span class="punctuation">(</span>payment_inc_ratio<span class="punctuation">)</span> <span class="operator">+</span> purpose_ <span class="operator">+</span></span><br><span class="line">home_ <span class="operator">+</span> emp_len_ <span class="operator">+</span> s<span class="punctuation">(</span>borrower_score<span class="punctuation">)</span><span class="punctuation">,</span></span><br><span class="line">data<span class="operator">=</span>loan_data<span class="punctuation">,</span> family<span class="operator">=</span><span class="string">&#x27;binomial&#x27;</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<p><code>statsmodels</code> 的公式接口在 Python 中也支持这些扩展：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> statsmodels.formula.api <span class="keyword">as</span> smf</span><br><span class="line">formula = (<span class="string">&#x27;outcome ~ bs(payment_inc_ratio, df=4) + purpose_ + &#x27;</span> +</span><br><span class="line"><span class="string">&#x27;home_ + emp_len_ + bs(borrower_score, df=4)&#x27;</span>)</span><br><span class="line">model = smf.glm(formula=formula, data=loan_data, family=sm.families.Binomial())</span><br><span class="line">results = model.fit()</span><br></pre></td></tr></table></figure>
<p><strong>残差分析</strong></p>
<p>Analysis of residuals</p>
<p>残差分析是逻辑回归与线性回归的一个不同之处。与线性回归一样（参见图4-9），在
R 中计算<strong>偏残差</strong>非常简单：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">terms <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>logistic_gam<span class="punctuation">,</span> type<span class="operator">=</span><span class="string">&#x27;terms&#x27;</span><span class="punctuation">)</span></span><br><span class="line">partial_resid <span class="operator">&lt;-</span> resid<span class="punctuation">(</span>logistic_model<span class="punctuation">)</span> <span class="operator">+</span> terms</span><br><span class="line">df <span class="operator">&lt;-</span> data.frame<span class="punctuation">(</span>payment_inc_ratio <span class="operator">=</span> loan_data<span class="punctuation">[</span><span class="punctuation">,</span> <span class="string">&#x27;payment_inc_ratio&#x27;</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">terms <span class="operator">=</span> terms<span class="punctuation">[</span><span class="punctuation">,</span> <span class="string">&#x27;s(payment_inc_ratio)&#x27;</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">partial_resid <span class="operator">=</span> partial_resid<span class="punctuation">[</span><span class="punctuation">,</span> <span class="string">&#x27;s(payment_inc_ratio)&#x27;</span><span class="punctuation">]</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line">ggplot<span class="punctuation">(</span>df<span class="punctuation">,</span> aes<span class="punctuation">(</span>x<span class="operator">=</span>payment_inc_ratio<span class="punctuation">,</span> y<span class="operator">=</span>partial_resid<span class="punctuation">,</span> solid <span class="operator">=</span> <span class="literal">FALSE</span><span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">geom_point<span class="punctuation">(</span>shape<span class="operator">=</span><span class="number">46</span><span class="punctuation">,</span> alpha<span class="operator">=</span><span class="number">0.4</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">geom_line<span class="punctuation">(</span>aes<span class="punctuation">(</span>x<span class="operator">=</span>payment_inc_ratio<span class="punctuation">,</span> y<span class="operator">=</span>terms<span class="punctuation">)</span><span class="punctuation">,</span></span><br><span class="line">color<span class="operator">=</span><span class="string">&#x27;red&#x27;</span><span class="punctuation">,</span> alpha<span class="operator">=</span><span class="number">0.5</span><span class="punctuation">,</span> size<span class="operator">=</span><span class="number">1.5</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">labs<span class="punctuation">(</span>y<span class="operator">=</span><span class="string">&#x27;Partial Residual&#x27;</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<p>由此产生的图表如图5-4所示。</p>
<p><img src="/img3/面向数据科学家的实用统计学/F5.4.png" alt="F5.4" style="zoom:50%;" /></p>
<p>图中所示的估计拟合线穿过两组点云之间。顶部的点云对应于响应为1（违约贷款），底部的点云对应于响应为0（已还清贷款）。这对于逻辑回归的残差来说是非常典型的，因为其输出是二元的。预测值是以
<strong>logit</strong>（对数几率）来衡量的，它始终是一个有限值。而实际值（绝对的0或1）对应于<strong>无限的
logit</strong>（正或负），因此残差（被加到拟合值上）永远不会等于0。因此，在偏残差图中，绘制的点云始终位于拟合线上方或下方。尽管逻辑回归中的偏残差不如线性回归中那么有价值，但它们仍可用于确认非线性行为并识别<strong>高影响力的记录</strong>。</p>
<p>目前，在任何主要的 Python
包中都没有偏残差的实现。我们在随附的源代码仓库中提供了用于创建偏残差图的
Python 代码。</p>
<blockquote>
<p>警告：</p>
<p><code>summary</code>
函数的部分输出可以被忽略。<strong>离散参数</strong>不适用于逻辑回归，它是为其他类型的
GLM
准备的。<strong>残差偏差</strong>和<strong>评分迭代次数</strong>与最大似然拟合方法相关；参见第215页的“最大似然估计”。</p>
</blockquote>
<p><strong>关键思想</strong></p>
<ul>
<li><strong>相似性</strong>：逻辑回归与线性回归类似，但其结果变量是<strong>二元</strong>的。</li>
<li><strong>模型转换</strong>：需要进行几种转换，才能将模型转换为可以作为线性模型进行拟合的形式，其中响应变量是<strong>几率比的对数</strong>。</li>
<li><strong>反向映射</strong>：在模型（通过迭代过程）拟合后，对数几率被映射回一个<strong>概率</strong>。</li>
<li><strong>流行原因</strong>：逻辑回归之所以流行，是因为它计算速度快，并且产生的模型只需少量算术运算即可在新数据上进行评分。</li>
</ul>
<h3 id="评估分类模型">评估分类模型</h3>
<p>Evaluating Classification Models</p>
<p>对分类模型进行评估，通常是训练几个不同的模型，将它们分别应用于一个<strong>保留样本</strong>，并评估其性能。有时，在评估和调优了多个模型后，如果数据足够，会使用<strong>第三个</strong>、之前未使用的保留样本来估计所选模型在新数据上的表现。不同的学科和从业者也会使用<strong>验证（validation）</strong>和<strong>测试（test）</strong>这两个术语来指代保留样本。从根本上说，评估过程旨在确定哪个模型能产生最准确和最有用的预测。</p>
<p><strong>评估分类模型的关键术语</strong></p>
<ul>
<li><p><strong>准确率（Accuracy）</strong>
正确分类的案例所占的百分比（或比例）。</p></li>
<li><p><strong>混淆矩阵（Confusion matrix）</strong>
一个表格显示（二元情况下为2×2），按预测和实际分类状态列出记录数量。</p></li>
<li><p><strong>灵敏度（Sensitivity）</strong>
所有实际为1的案例中，被正确分类为1的百分比（或比例）。
同义词：<strong>召回率（Recall）</strong></p></li>
<li><p><strong>特异度（Specificity）</strong>
所有实际为0的案例中，被正确分类为0的百分比（或比例）。</p></li>
<li><p><strong>精确率（Precision）</strong>
所有被预测为1的案例中，实际为1的百分比（或比例）。</p></li>
<li><p><strong>ROC曲线（ROC curve）</strong>
绘制灵敏度与特异度关系的图。</p></li>
<li><p><strong>提升度（Lift）</strong>
衡量模型在不同概率截止点下识别（相对罕见的）1的能力有多有效。</p></li>
</ul>
<p>衡量分类性能的一个简单方法是计算正确预测的比例，即测量<strong>准确率（Accuracy）</strong>。准确率只是一个总误差的度量：</p>
<p><span class="math display">\[
\text{准确率} = \frac{\sum\text{True Positive} + \sum\text{True
Negative}}{\text{样本量}}
\]</span>
在大多数分类算法中，每个案例都被分配一个“<strong>估计为1的概率</strong>”。默认的决策点，或<strong>截止点（cutoff）</strong>，通常是0.50或50%。如果概率高于0.5，则分类为“1”；否则为“0”。另一个替代的默认截止点是数据中1的<strong>普遍概率</strong>。</p>
<h4 id="混淆矩阵"><strong>混淆矩阵</strong></h4>
<p>Confusion Matrix</p>
<p>混淆矩阵是分类指标的核心。它是一个表格，按预测和实际结果的类别显示正确和不正确预测的数量。在
R 和 Python
中有许多可用的包来计算混淆矩阵，但在二元情况下，手动计算一个也很简单。</p>
<p>为了说明混淆矩阵，考虑在包含相同数量的违约和已还清贷款的平衡数据集上训练的
<code>logistic_gam</code> 模型（参见图5-4）。按照惯例，<strong><span
class="math inline">\(Y=1\)</span>
对应于目标事件</strong>（例如，违约），而 <strong><span
class="math inline">\(Y=0\)</span>
对应于负面（或常规）事件</strong>（例如，已还清）。以下代码在 R
中计算了应用于整个（不平衡）训练集的 <code>logistic_gam</code>
模型的混淆矩阵：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">pred <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>logistic_gam<span class="punctuation">,</span> newdata<span class="operator">=</span>train_set<span class="punctuation">)</span></span><br><span class="line">pred_y <span class="operator">&lt;-</span> <span class="built_in">as.numeric</span><span class="punctuation">(</span>pred <span class="operator">&gt;</span> <span class="number">0</span><span class="punctuation">)</span></span><br><span class="line">true_y <span class="operator">&lt;-</span> <span class="built_in">as.numeric</span><span class="punctuation">(</span>train_set<span class="operator">$</span>outcome<span class="operator">==</span><span class="string">&#x27;default&#x27;</span><span class="punctuation">)</span></span><br><span class="line">true_pos <span class="operator">&lt;-</span> <span class="punctuation">(</span>true_y<span class="operator">==</span><span class="number">1</span><span class="punctuation">)</span> <span class="operator">&amp;</span> <span class="punctuation">(</span>pred_y<span class="operator">==</span><span class="number">1</span><span class="punctuation">)</span></span><br><span class="line">true_neg <span class="operator">&lt;-</span> <span class="punctuation">(</span>true_y<span class="operator">==</span><span class="number">0</span><span class="punctuation">)</span> <span class="operator">&amp;</span> <span class="punctuation">(</span>pred_y<span class="operator">==</span><span class="number">0</span><span class="punctuation">)</span></span><br><span class="line">false_pos <span class="operator">&lt;-</span> <span class="punctuation">(</span>true_y<span class="operator">==</span><span class="number">0</span><span class="punctuation">)</span> <span class="operator">&amp;</span> <span class="punctuation">(</span>pred_y<span class="operator">==</span><span class="number">1</span><span class="punctuation">)</span></span><br><span class="line">false_neg <span class="operator">&lt;-</span> <span class="punctuation">(</span>true_y<span class="operator">==</span><span class="number">1</span><span class="punctuation">)</span> <span class="operator">&amp;</span> <span class="punctuation">(</span>pred_y<span class="operator">==</span><span class="number">0</span><span class="punctuation">)</span></span><br><span class="line">conf_mat <span class="operator">&lt;-</span> matrix<span class="punctuation">(</span><span class="built_in">c</span><span class="punctuation">(</span><span class="built_in">sum</span><span class="punctuation">(</span>true_pos<span class="punctuation">)</span><span class="punctuation">,</span> <span class="built_in">sum</span><span class="punctuation">(</span>false_pos<span class="punctuation">)</span><span class="punctuation">,</span></span><br><span class="line"><span class="built_in">sum</span><span class="punctuation">(</span>false_neg<span class="punctuation">)</span><span class="punctuation">,</span> <span class="built_in">sum</span><span class="punctuation">(</span>true_neg<span class="punctuation">)</span><span class="punctuation">)</span><span class="punctuation">,</span> <span class="number">2</span><span class="punctuation">,</span> <span class="number">2</span><span class="punctuation">)</span></span><br><span class="line">colnames<span class="punctuation">(</span>conf_mat<span class="punctuation">)</span> <span class="operator">&lt;-</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;Yhat = 1&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;Yhat = 0&#x27;</span><span class="punctuation">)</span></span><br><span class="line">rownames<span class="punctuation">(</span>conf_mat<span class="punctuation">)</span> <span class="operator">&lt;-</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;Y = 1&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;Y = 0&#x27;</span><span class="punctuation">)</span></span><br><span class="line">conf_mat</span><br><span class="line"></span><br><span class="line">Yhat <span class="operator">=</span> <span class="number">1</span> Yhat <span class="operator">=</span> <span class="number">0</span></span><br><span class="line">Y <span class="operator">=</span> <span class="number">1</span> <span class="number">14295</span>    <span class="number">8376</span></span><br><span class="line">Y <span class="operator">=</span> <span class="number">0</span> <span class="number">8052</span>    <span class="number">14619</span></span><br></pre></td></tr></table></figure>
<p>在 Python 中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">pred = logit_reg.predict(X)</span><br><span class="line">pred_y = logit_reg.predict(X) == <span class="string">&#x27;default&#x27;</span></span><br><span class="line">true_y = y == <span class="string">&#x27;default&#x27;</span></span><br><span class="line">true_pos = true_y &amp; pred_y</span><br><span class="line">true_neg = ~true_y &amp; ~pred_y</span><br><span class="line">false_pos = ~true_y &amp; pred_y</span><br><span class="line">false_neg = true_y &amp; ~pred_y</span><br><span class="line">conf_mat = pd.DataFrame([[np.<span class="built_in">sum</span>(true_pos), np.<span class="built_in">sum</span>(false_neg)],</span><br><span class="line">[np.<span class="built_in">sum</span>(false_pos), np.<span class="built_in">sum</span>(true_neg)]],</span><br><span class="line">index=[<span class="string">&#x27;Y = default&#x27;</span>, <span class="string">&#x27;Y = paid off&#x27;</span>],</span><br><span class="line">columns=[<span class="string">&#x27;Yhat = default&#x27;</span>, <span class="string">&#x27;Yhat = paid off&#x27;</span>])</span><br><span class="line">conf_mat</span><br></pre></td></tr></table></figure>
<p>预测结果是列，实际结果是行。矩阵的对角线元素显示了<strong>正确预测的数量</strong>，非对角线元素显示了<strong>不正确预测的数量</strong>。例如，有14,295笔违约贷款被正确预测为违约，但有8,376笔违约贷款被错误地预测为已还清。</p>
<p>图5-5显示了二元响应 <span class="math inline">\(Y\)</span>
的混淆矩阵与不同指标之间的关系（关于这些指标的更多信息，请参阅第223页的“精确率、召回率和特异度”）。与贷款数据的示例一样，<strong>实际响应沿行排列，预测响应沿列排列</strong>。对角线上的框（左上、右下）显示了预测
<span class="math inline">\(Y\)</span> 正确预测响应的情况。</p>
<p><img src="/img3/面向数据科学家的实用统计学/F5.5.png" alt="F5.5" style="zoom:50%;" /></p>
<p>一个没有明确提及的重要指标是<strong>假阳性率</strong>（与精确率镜像）。当1s是<strong>罕见</strong>的类别时，假阳性与所有预测为阳性的比率可能很高，这导致一种不直观的情况：被预测为1的案例很可能实际上是0。这个问题困扰着广泛应用的医疗筛查测试（例如乳房X光检查）：由于病症相对罕见，阳性测试结果<strong>很可能并不意味着患有乳腺癌</strong>。这给公众带来了很大的困惑。</p>
<blockquote>
<p>警告：</p>
<p>在这里，我们将实际响应沿行排列，预测响应沿列排列，但将其反转也很常见。一个值得注意的例子是
R 中流行的 <code>caret</code> 包。</p>
</blockquote>
<h4 id="罕见类别问题"><strong>罕见类别问题</strong></h4>
<p>The Rare Class Problem</p>
<p>在许多情况下，待预测的类别存在<strong>不平衡</strong>，其中一个类别比另一个更普遍，例如，合法的保险索赔与欺诈性索赔，或网站上的浏览者与购买者。<strong>罕见类别</strong>（例如，欺诈性索赔）通常是更受关注的类别，通常被指定为1，与更普遍的0形成对比。在典型场景中，1是更重要的案例，因为将其错误地分类为0的成本比将0错误地分类为1的成本更高。例如，正确识别一次欺诈性保险索赔可能节省数千美元。另一方面，正确识别一次非欺诈性索赔仅节省了你手动进行更仔细审查的成本和精力（如果该索赔被标记为“欺诈性”，你就会这样做）。</p>
<p>在这种情况下，除非类别很容易分离，否则<strong>最准确的分类模型可能是一个简单地将所有东西都分类为0的模型</strong>。例如，如果一个网站上只有0.1%的浏览者最终购买，那么一个预测每个浏览者都会离开而不会购买的模型将有99.9%的准确率。然而，它将毫无用处。相反，我们会乐于接受一个整体准确率较低，但善于识别购买者的模型，即使它在这个过程中错误地分类了一些非购买者。</p>
<h4
id="精确率召回率和特异度"><strong>精确率、召回率和特异度</strong></h4>
<p>Precision, Recall, and Specificity</p>
<p>除了纯粹的准确率之外，还有一些更细致的指标常用于评估分类模型。其中一些在统计学——特别是生物统计学——中有很长的历史，它们被用来描述诊断测试的预期性能。<strong>精确率（Precision）衡量预测为正向结果的准确性</strong>（参见图5-5）：
<span class="math display">\[
\text{精确率} = \frac{\sum\text{True Positive}}{\sum\text{True Positive}
+ \sum\text{False Positive}}
\]</span>
<strong>召回率（Recall）</strong>，也称为<strong>灵敏度（Sensitivity）</strong>，衡量模型<strong>预测正向结果的能力</strong>——它正确识别的1的比例（参见图5-5）。在生物统计学和医学诊断中，“灵敏度”这个术语使用得更多，而在机器学习社区中，“召回率”使用得更多。召回率的定义是：
<span class="math display">\[
\text{召回率} = \frac{\sum\text{True Positive}}{\sum\text{True Positive}
+ \sum\text{False Negative}}
\]</span>
另一个使用的指标是<strong>特异度（Specificity）</strong>，它衡量模型<strong>预测负向结果的能力</strong>：
<span class="math display">\[
\text{特异度} = \frac{\sum\text{True Negative}}{\sum\text{True Negative}
+ \sum\text{False Positive}}
\]</span> 我们可以从 R 中的 <code>conf_mat</code> 计算这三个指标：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># precision</span></span><br><span class="line">conf_mat<span class="punctuation">[</span><span class="number">1</span><span class="punctuation">,</span> <span class="number">1</span><span class="punctuation">]</span> <span class="operator">/</span> <span class="built_in">sum</span><span class="punctuation">(</span>conf_mat<span class="punctuation">[</span><span class="punctuation">,</span><span class="number">1</span><span class="punctuation">]</span><span class="punctuation">)</span></span><br><span class="line"><span class="comment"># recall</span></span><br><span class="line">conf_mat<span class="punctuation">[</span><span class="number">1</span><span class="punctuation">,</span> <span class="number">1</span><span class="punctuation">]</span> <span class="operator">/</span> <span class="built_in">sum</span><span class="punctuation">(</span>conf_mat<span class="punctuation">[</span><span class="number">1</span><span class="punctuation">,</span><span class="punctuation">]</span><span class="punctuation">)</span></span><br><span class="line"><span class="comment"># specificity</span></span><br><span class="line">conf_mat<span class="punctuation">[</span><span class="number">2</span><span class="punctuation">,</span> <span class="number">2</span><span class="punctuation">]</span> <span class="operator">/</span> <span class="built_in">sum</span><span class="punctuation">(</span>conf_mat<span class="punctuation">[</span><span class="number">2</span><span class="punctuation">,</span><span class="punctuation">]</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<p>以下是 Python 中计算这些指标的等效代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">conf_mat = confusion_matrix(y, logit_reg.predict(X))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Precision&#x27;</span>, conf_mat[<span class="number">0</span>, <span class="number">0</span>] / <span class="built_in">sum</span>(conf_mat[:, <span class="number">0</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Recall&#x27;</span>, conf_mat[<span class="number">0</span>, <span class="number">0</span>] / <span class="built_in">sum</span>(conf_mat[<span class="number">0</span>, :]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Specificity&#x27;</span>, conf_mat[<span class="number">1</span>, <span class="number">1</span>] / <span class="built_in">sum</span>(conf_mat[<span class="number">1</span>, :]))</span><br><span class="line">precision_recall_fscore_support(y, logit_reg.predict(X),</span><br><span class="line">labels=[<span class="string">&#x27;default&#x27;</span>, <span class="string">&#x27;paid off&#x27;</span>])</span><br></pre></td></tr></table></figure>
<p><code>scikit-learn</code> 有一个自定义方法
<code>precision_recall_fscore_support</code>，可以一次性计算出精确率和召回率/特异度。</p>
<h4 id="roc曲线"><strong>ROC曲线</strong></h4>
<p>ROC Curve</p>
<p>你可以看到，召回率（recall）<strong>和</strong>特异度（specificity）之间存在一个权衡。捕获更多的1通常意味着将更多的0错误分类为1。一个理想的分类器应该在很好地分类1的同时，不会将更多的0错误分类为1。</p>
<p>捕捉这种权衡的指标是“<strong>受试者工作特征（Receiver Operating
Characteristics）</strong>”曲线，通常简称为<strong>ROC曲线</strong>。ROC曲线将召回率（灵敏度）绘制在y轴上，特异度绘制在x轴上。ROC曲线显示了当你改变截止点来决定如何分类一条记录时，召回率和特异度之间的权衡关系。灵敏度（召回率）绘制在y轴上，而你可能会遇到两种形式的x轴标注：</p>
<ul>
<li><strong>x轴绘制特异度</strong>，左边是1，右边是0。</li>
<li><strong>x轴绘制1-特异度</strong>，左边是0，右边是1。</li>
</ul>
<p>无论哪种方式，曲线看起来都完全相同。计算ROC曲线的过程是：</p>
<ol type="1">
<li><strong>排序</strong>：根据预测为1的概率对记录进行排序，从最有可能的开始，到最不可能的结束。</li>
<li><strong>计算</strong>：基于排序后的记录，计算累积的特异度和召回率。</li>
</ol>
<p>在 R 中计算 ROC 曲线非常简单。以下代码计算贷款数据的 ROC：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">idx <span class="operator">&lt;-</span> order<span class="punctuation">(</span><span class="operator">-</span>pred<span class="punctuation">)</span></span><br><span class="line">recall <span class="operator">&lt;-</span> <span class="built_in">cumsum</span><span class="punctuation">(</span>true_y<span class="punctuation">[</span>idx<span class="punctuation">]</span> <span class="operator">==</span> <span class="number">1</span><span class="punctuation">)</span> <span class="operator">/</span> <span class="built_in">sum</span><span class="punctuation">(</span>true_y <span class="operator">==</span> <span class="number">1</span><span class="punctuation">)</span></span><br><span class="line">specificity <span class="operator">&lt;-</span> <span class="punctuation">(</span><span class="built_in">sum</span><span class="punctuation">(</span>true_y <span class="operator">==</span> <span class="number">0</span><span class="punctuation">)</span> <span class="operator">-</span> <span class="built_in">cumsum</span><span class="punctuation">(</span>true_y<span class="punctuation">[</span>idx<span class="punctuation">]</span> <span class="operator">==</span> <span class="number">0</span><span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">/</span> <span class="built_in">sum</span><span class="punctuation">(</span>true_y <span class="operator">==</span> <span class="number">0</span><span class="punctuation">)</span></span><br><span class="line">roc_df <span class="operator">&lt;-</span> data.frame<span class="punctuation">(</span>recall <span class="operator">=</span> recall<span class="punctuation">,</span> specificity <span class="operator">=</span> specificity<span class="punctuation">)</span></span><br><span class="line">ggplot<span class="punctuation">(</span>roc_df<span class="punctuation">,</span> aes<span class="punctuation">(</span>x<span class="operator">=</span>specificity<span class="punctuation">,</span> y<span class="operator">=</span>recall<span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">geom_line<span class="punctuation">(</span>color<span class="operator">=</span><span class="string">&#x27;blue&#x27;</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">scale_x_reverse<span class="punctuation">(</span>expand<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">0</span><span class="punctuation">,</span> <span class="number">0</span><span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">scale_y_continuous<span class="punctuation">(</span>expand<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">0</span><span class="punctuation">,</span> <span class="number">0</span><span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">geom_line<span class="punctuation">(</span>data<span class="operator">=</span>data.frame<span class="punctuation">(</span>x<span class="operator">=</span><span class="punctuation">(</span><span class="number">0</span><span class="operator">:</span><span class="number">100</span><span class="punctuation">)</span> <span class="operator">/</span> <span class="number">100</span><span class="punctuation">)</span><span class="punctuation">,</span> aes<span class="punctuation">(</span>x<span class="operator">=</span>x<span class="punctuation">,</span> y<span class="operator">=</span><span class="number">1</span><span class="operator">-</span>x<span class="punctuation">)</span><span class="punctuation">,</span></span><br><span class="line">linetype<span class="operator">=</span><span class="string">&#x27;dotted&#x27;</span><span class="punctuation">,</span> color<span class="operator">=</span><span class="string">&#x27;red&#x27;</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<p>在 Python 中，我们可以使用 <code>scikit-learn</code> 函数
<code>sklearn.metrics.roc_curve</code> 来计算 ROC
曲线所需的信息。你也可以找到类似的 R 包，例如 <code>ROCR</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">fpr, tpr, thresholds = roc_curve(y, logit_reg.predict_proba(X)[:,<span class="number">0</span>],</span><br><span class="line">pos_label=<span class="string">&#x27;default&#x27;</span>)</span><br><span class="line">roc_df = pd.DataFrame(&#123;<span class="string">&#x27;recall&#x27;</span>: tpr, <span class="string">&#x27;specificity&#x27;</span>: <span class="number">1</span> - fpr&#125;)</span><br><span class="line">ax = roc_df.plot(x=<span class="string">&#x27;specificity&#x27;</span>, y=<span class="string">&#x27;recall&#x27;</span>, figsize=(<span class="number">4</span>, <span class="number">4</span>), legend=<span class="literal">False</span>)</span><br><span class="line">ax.set_ylim(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">ax.set_xlim(<span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">ax.plot((<span class="number">1</span>, <span class="number">0</span>), (<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">ax.set_xlabel(<span class="string">&#x27;specificity&#x27;</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&#x27;recall&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>结果如图5-6所示。虚线对角线对应于<strong>不比随机猜测好</strong>的分类器。一个<strong>极其有效</strong>的分类器（或者在医疗情境中，一个极其有效的诊断测试）的
ROC
曲线将<strong>紧贴左上角</strong>——它将正确识别大量的1，同时不会将大量的0错误分类为1。对于这个模型，如果我们想要一个特异度至少为50%的分类器，那么召回率约为75%。</p>
<p><img src="/img3/面向数据科学家的实用统计学/F5.6.png" alt="F5.6" style="zoom:33%;" /></p>
<blockquote>
<p>通用注解：</p>
<p><strong>精确率-召回率曲线</strong>(Precision-Recall Curve)</p>
<p>除了 ROC 曲线之外，检查<strong>精确率-召回率（Precision-Recall,
PR）曲线</strong>也很有启发性。PR
曲线以类似的方式计算，不同之处在于数据从最不可能到最有可能进行排序，并计算累积的精确率和召回率统计数据。PR
曲线在评估具有<strong>高度不平衡结果</strong>的数据时特别有用。</p>
</blockquote>
<h4 id="auc-曲线下面积"><strong>AUC (曲线下面积)</strong></h4>
<p>ROC曲线是一个有价值的图形工具，但它本身并不能构成衡量分类器性能的单一指标。然而，ROC曲线可以用来计算曲线下面积（Area
Under the Curve,
AUC）指标。AUC就是ROC曲线下的总面积。<strong>AUC的值越大，分类器越有效</strong>。一个AUC为1的分类器是完美的：它能正确分类所有1，并且不会将任何0错误地分类为1。一个完全无效的分类器——对角线——的AUC值为0.5。</p>
<p><img src="/img3/面向数据科学家的实用统计学/F5.7.png" alt="F5.7" style="zoom:33%;" /></p>
<p>图5-7显示了贷款模型的ROC曲线下面积。在 R
中可以通过数值积分计算AUC值：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sum</span><span class="punctuation">(</span>roc_df<span class="operator">$</span>recall<span class="punctuation">[</span><span class="operator">-</span><span class="number">1</span><span class="punctuation">]</span> <span class="operator">*</span> diff<span class="punctuation">(</span><span class="number">1</span> <span class="operator">-</span> roc_df<span class="operator">$</span>specificity<span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line"><span class="punctuation">[</span><span class="number">1</span><span class="punctuation">]</span> <span class="number">0.6926172</span></span><br></pre></td></tr></table></figure>
<p>在 Python 中，我们可以像 R 中那样计算准确率，也可以使用
<code>scikit-learn</code> 的 <code>sklearn.metrics.roc_auc_score</code>
函数。你需要提供0或1的期望值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(np.<span class="built_in">sum</span>(roc_df.recall[:-<span class="number">1</span>] * np.diff(<span class="number">1</span> - roc_df.specificity)))</span><br><span class="line"><span class="built_in">print</span>(roc_auc_score([<span class="number">1</span> <span class="keyword">if</span> yi == <span class="string">&#x27;default&#x27;</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> yi <span class="keyword">in</span> y],</span><br><span class="line">logit_reg.predict_proba(X)[:, <span class="number">0</span>]))</span><br></pre></td></tr></table></figure>
<p>该模型的AUC约为0.69，这表明它是一个相对较弱的分类器。</p>
<blockquote>
<p>警告：</p>
<p><strong>假阳性率的混淆</strong>（False Positive Rate Confusion）</p>
<p>假阳性/假阴性率常常与特异度或灵敏度混淆或混为一谈（甚至在出版物和软件中也是如此！）。有时，假阳性率被定义为<strong>测试结果为阳性的真阴性样本的比例</strong>。在许多情况下（例如网络入侵检测），该术语被用来指代<strong>真阴性的阳性信号的比例</strong>。</p>
</blockquote>
<h4 id="提升度"><strong>提升度</strong></h4>
<p>Lift</p>
<p>使用 AUC
作为评估模型的指标，比简单的准确率有所改进，因为它能评估分类器在整体准确率和识别更重要的
1
之间的权衡处理得如何。但它没有完全解决<strong>罕见类别问题</strong>，在这种情况下，你需要将模型的概率截止点降低到0.5以下，以避免所有记录都被分类为0。在这种情况下，一条记录被分类为1的概率可能低至0.4、0.3甚至更低就足够了。实际上，我们最终会过度识别1，以体现它们更高的重要性。</p>
<p>改变这个截止点会增加你捕捉到1的机会（代价是将更多的0错误地分类为1）。但最佳的截止点在哪里呢？</p>
<p><strong>提升度（Lift）</strong>的概念让你能够<strong>推迟回答这个问题</strong>。相反，它让你按照记录被预测为1的概率顺序来考虑它们。例如，在被分类为1的记录中，如果取概率最高的10%作为子集，那么与<strong>盲目随机挑选</strong>的基准相比，算法的表现要好多少？如果你在这个最高的十分位数中能得到0.3%的响应率，而不是随机挑选得到的0.1%的整体响应率，那么该算法在这个十分位数中的提升度（也称为<strong>增益</strong>）就是3。<strong>提升度图（增益图）</strong>将这种效果量化到整个数据范围内。它可以按十分位数生成，也可以在数据的整个范围内连续生成。</p>
<p>要计算提升度图，首先要生成一个<strong>累积增益图（cumulative gains
chart）</strong>，该图将召回率绘制在y轴上，记录总数绘制在x轴上。<strong>提升度曲线是累积增益与代表随机选择的对角线之间的比率</strong>。十分位增益图是预测建模中最古老的技术之一，其历史可以追溯到互联网商业出现之前。它们在直邮营销专业人士中特别受欢迎。如果无差别地应用，直邮是一种昂贵的广告方式，因此广告商会使用预测模型（在早期非常简单）来识别最有可能获得回报的潜在客户。</p>
<blockquote>
<p>警告：</p>
<p><strong>Uplift</strong></p>
<p>有时，<strong>uplift</strong> 一词与 <strong>lift</strong>
含义相同。但在一个更受限的情境中，它有一个不同的含义，即当进行 A/B
测试，并将处理方式（A或B）用作预测模型中的预测变量时。<strong>Uplift
是预测模型对于单个案例使用处理A与处理B相比，其响应的改进程度</strong>。这是通过对单个案例进行两次评分来确定的：第一次将预测变量设置为A，然后再次将其切换为B。营销人员和政治竞选顾问使用这种方法来决定哪种信息处理方式应该用于哪些客户或选民。</p>
</blockquote>
<p>提升度曲线让你能够观察<strong>设定不同概率截止点来将记录分类为1所带来的后果</strong>。它可以作为确定适当截止水平的中间步骤。例如，税务机关可能只有一定数量的资源用于税务审计，并希望将这些资源用于最有可能偷税漏税的人。考虑到资源限制，该机构会使用提升度图来估算在哪里划定界限，决定哪些报税表将被选中进行审计，哪些将被放过。</p>
<p><strong>关键思想</strong></p>
<ul>
<li><strong>准确率（正确分类的百分比）</strong>仅仅是评估模型的第一步。</li>
<li>其他指标（召回率、特异度和精确率）侧重于更具体的性能特征（例如，<strong>召回率</strong>衡量模型正确识别1的能力）。</li>
<li><strong>AUC（ROC曲线下面积）</strong>是衡量模型区分1和0能力的一个常用指标。</li>
<li>同样，<strong>提升度</strong>衡量模型识别1的有效性，通常按十分位数计算，从最有可能的1开始。</li>
</ul>
<h3 id="不平衡数据的策略"><strong>不平衡数据的策略</strong></h3>
<p>Strategies for Imbalanced Data</p>
<p>上一节讨论了使用<strong>超越简单准确率</strong>的指标来评估分类模型，这些指标适用于<strong>不平衡数据</strong>——即目标结果（网站购买、保险欺诈等）非常罕见的数据。本节将探讨可用于<strong>改善不平衡数据下预测建模性能的其他策略</strong>。</p>
<p><strong>不平衡数据的关键术语</strong></p>
<ul>
<li><p><strong>欠采样（Undersample）</strong>
在分类模型中使用较少数量的普遍类别记录。
同义词：<strong>降采样（Downsample）</strong></p></li>
<li><p><strong>过采样（Oversample）</strong>
在分类模型中使用更多数量的罕见类别记录，如果需要，可通过自举法实现。
同义词：<strong>升采样（Upsample）</strong></p></li>
<li><p><strong>上加权或下加权（Up weight or down weight）</strong>
在模型中对罕见（或普遍）类别赋予更多（或更少）的权重。</p></li>
<li><p><strong>数据生成（Data generation）</strong>
类似于自举法，但每个新的自举记录都与原始记录略有不同。</p></li>
<li><p><strong>z分数（z-score）</strong> 标准化后得到的值。</p></li>
<li><p><strong>K</strong> 在最近邻计算中考虑的邻居数量。</p></li>
</ul>
<h4 id="欠采样"><strong>欠采样</strong></h4>
<p>Undersampling</p>
<p>如果你有足够的数据，就像贷款数据一样，一种解决方案是<strong>对普遍类别进行欠采样（或降采样）</strong>，这样建模数据在0和1之间会更平衡。欠采样的基本思想是，主导类的数据包含许多<strong>冗余记录</strong>。处理一个更小、更平衡的数据集，有助于提高模型性能，并使数据准备、模型探索和试运行变得更容易。</p>
<p>多少数据才算足够？这取决于应用，但通常来说，对于较不主导的类别，拥有数万条记录就足够了。<strong>1与0之间越容易区分，所需的数据就越少</strong>。</p>
<p>第208页“逻辑回归”中分析的贷款数据基于一个<strong>平衡的训练集</strong>：一半贷款已还清，另一半已违约。预测值也类似：一半的概率小于0.5，另一半大于0.5。在完整的（不平衡）数据集中，只有大约19%的贷款是违约的，如R代码所示：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mean<span class="punctuation">(</span>full_train_set<span class="operator">$</span>outcome<span class="operator">==</span><span class="string">&#x27;default&#x27;</span><span class="punctuation">)</span></span><br><span class="line"><span class="punctuation">[</span><span class="number">1</span><span class="punctuation">]</span> <span class="number">0.1889455</span></span><br></pre></td></tr></table></figure>
<p>在Python中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;percentage of loans in default: &#x27;</span>,</span><br><span class="line"><span class="number">100</span> * np.mean(full_train_set.outcome == <span class="string">&#x27;default&#x27;</span>))</span><br></pre></td></tr></table></figure>
<p>如果我们使用完整的数据集来训练模型，会发生什么？让我们看看在R中会是什么样子：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">full_model <span class="operator">&lt;-</span> glm<span class="punctuation">(</span>outcome <span class="operator">~</span> payment_inc_ratio <span class="operator">+</span> purpose_ <span class="operator">+</span> home_ <span class="operator">+</span></span><br><span class="line">emp_len_<span class="operator">+</span> dti <span class="operator">+</span> revol_bal <span class="operator">+</span> revol_util<span class="punctuation">,</span></span><br><span class="line">data<span class="operator">=</span>full_train_set<span class="punctuation">,</span> family<span class="operator">=</span><span class="string">&#x27;binomial&#x27;</span><span class="punctuation">)</span></span><br><span class="line">pred <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>full_model<span class="punctuation">)</span></span><br><span class="line">mean<span class="punctuation">(</span>pred <span class="operator">&gt;</span> <span class="number">0</span><span class="punctuation">)</span></span><br><span class="line"><span class="punctuation">[</span><span class="number">1</span><span class="punctuation">]</span> <span class="number">0.003942094</span></span><br></pre></td></tr></table></figure>
<p>在Python中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">predictors = [<span class="string">&#x27;payment_inc_ratio&#x27;</span>, <span class="string">&#x27;purpose_&#x27;</span>, <span class="string">&#x27;home_&#x27;</span>, <span class="string">&#x27;emp_len_&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;dti&#x27;</span>, <span class="string">&#x27;revol_bal&#x27;</span>, <span class="string">&#x27;revol_util&#x27;</span>]</span><br><span class="line">outcome = <span class="string">&#x27;outcome&#x27;</span></span><br><span class="line">X = pd.get_dummies(full_train_set[predictors], prefix=</span><br><span class="line">drop_first=<span class="literal">True</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;</span></span><br><span class="line">, prefix_sep=</span><br><span class="line"><span class="string">&#x27;&#x27;</span></span><br><span class="line">,</span><br><span class="line">y = full_train_set[outcome]</span><br><span class="line">full_model = LogisticRegression(penalty=<span class="string">&#x27;l2&#x27;</span>, C=<span class="number">1e42</span>, solver=<span class="string">&#x27;liblinear&#x27;</span>)</span><br><span class="line">full_model.fit(X, y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;percentage of loans predicted to default: &#x27;</span>,</span><br><span class="line"><span class="number">100</span> * np.mean(full_model.predict(X) == <span class="string">&#x27;default&#x27;</span>))</span><br></pre></td></tr></table></figure>
<p>只有0.39%的贷款被预测为违约，不到预期数量的1/47。已还清的贷款<strong>压倒性地盖过了</strong>违约贷款，因为模型在训练时对所有数据给予了同等的权重。直观地思考，存在如此多的未违约贷款，加上预测变量数据中不可避免的变异性，这意味着，即使对于一笔违约贷款，模型也可能偶然找到一些与它相似的未违约贷款。当使用平衡样本时，大约50%的贷款被预测为违约。</p>
<h4 id="过采样和上下加权"><strong>过采样和上/下加权</strong></h4>
<p>Oversampling and Up/Down Weighting</p>
<p>对<strong>欠采样</strong>方法的一种批评是，它<strong>丢弃了数据</strong>，没有利用手头的所有信息。如果你的数据集相对较小，且稀有类别只包含几百或几千条记录，那么对主导类别进行欠采样可能会丢失有用的信息。在这种情况下，你应该过采样（或升采样）<strong>稀有类别，通过</strong>带放回抽样（自举法）来增加额外的行。</p>
<p>你也可以通过<strong>加权数据</strong>来达到类似的效果。许多分类算法都接受一个权重参数，允许你对数据进行上/下加权。例如，在
R 中使用 <code>glm</code> 函数的 <code>weight</code>
参数，对贷款数据应用一个权重向量：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">wt <span class="operator">&lt;-</span> ifelse<span class="punctuation">(</span>full_train_set<span class="operator">$</span>outcome<span class="operator">==</span><span class="string">&#x27;default&#x27;</span><span class="punctuation">,</span></span><br><span class="line"><span class="number">1</span> <span class="operator">/</span> mean<span class="punctuation">(</span>full_train_set<span class="operator">$</span>outcome <span class="operator">==</span> <span class="string">&#x27;default&#x27;</span><span class="punctuation">)</span><span class="punctuation">,</span> <span class="number">1</span><span class="punctuation">)</span></span><br><span class="line">full_model <span class="operator">&lt;-</span> glm<span class="punctuation">(</span>outcome <span class="operator">~</span> payment_inc_ratio <span class="operator">+</span> purpose_ <span class="operator">+</span> home_ <span class="operator">+</span></span><br><span class="line">emp_len_<span class="operator">+</span> dti <span class="operator">+</span> revol_bal <span class="operator">+</span> revol_util<span class="punctuation">,</span></span><br><span class="line">data<span class="operator">=</span>full_train_set<span class="punctuation">,</span> weight<span class="operator">=</span>wt<span class="punctuation">,</span> family<span class="operator">=</span><span class="string">&#x27;quasibinomial&#x27;</span><span class="punctuation">)</span></span><br><span class="line">pred <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>full_model<span class="punctuation">)</span></span><br><span class="line">mean<span class="punctuation">(</span>pred <span class="operator">&gt;</span> <span class="number">0</span><span class="punctuation">)</span></span><br><span class="line"><span class="punctuation">[</span><span class="number">1</span><span class="punctuation">]</span> <span class="number">0.5767208</span></span><br></pre></td></tr></table></figure>
<p>大多数 <code>scikit-learn</code> 方法允许在 <code>fit</code>
函数中使用 <code>sample_weight</code> 关键字参数来指定权重：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">default_wt = <span class="number">1</span> / np.mean(full_train_set.outcome == <span class="string">&#x27;default&#x27;</span>)</span><br><span class="line">wt = [default_wt <span class="keyword">if</span> outcome == <span class="string">&#x27;default&#x27;</span> <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> outcome <span class="keyword">in</span> full_train_set.outcome]</span><br><span class="line">full_model = LogisticRegression(penalty=<span class="string">&quot;l2&quot;</span>, C=<span class="number">1e42</span>, solver=<span class="string">&#x27;liblinear&#x27;</span>)</span><br><span class="line">full_model.fit(X, y, sample_weight=wt)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;percentage of loans predicted to default (weighting): &#x27;</span>,</span><br><span class="line"><span class="number">100</span> * np.mean(full_model.predict(X) == <span class="string">&#x27;default&#x27;</span>))</span><br></pre></td></tr></table></figure>
<p>对于违约贷款，权重被设置为 <span
class="math inline">\(1/p\)</span>，其中 <span
class="math inline">\(p\)</span>
是违约的概率。未违约贷款的权重为1。违约贷款和未违约贷款的<strong>权重总和大致相等</strong>。现在预测值的均值约为58%，而不是0.39%。</p>
<p>请注意，加权提供了<strong>过采样稀有类别和欠采样主导类别</strong>的替代方案。</p>
<blockquote>
<p>通用注解：</p>
<p><strong>调整损失函数</strong>（Adapting the Loss Function）</p>
<p>许多分类和回归算法都优化某个特定标准或<strong>损失函数</strong>。例如，逻辑回归试图最小化<strong>偏差（deviance）</strong>。在文献中，有人提出修改损失函数以避免由稀有类别引起的问题。在实践中，这很难做到：分类算法可能很复杂且难以修改。<strong>加权</strong>是一种改变损失函数的简单方法，它<strong>降低了低权重记录的错误对模型的影响，从而倾向于高权重记录</strong>。</p>
</blockquote>
<h4 id="数据生成"><strong>数据生成</strong></h4>
<p>Data Generation</p>
<p><strong>数据生成</strong>是<strong>通过扰动现有记录来创建新记录</strong>，是利用自举法进行过采样的一种变体（参见第232页的“过采样和上/下加权”）。这一想法的直觉在于，由于我们只观察到有限的实例集，算法没有足够丰富的信息来构建分类“规则”。通过创建<strong>与现有记录相似但不完全相同</strong>的新记录，算法有机会学习到一套更健壮的规则。这种思想在精神上类似于<strong>集成统计模型</strong>（ensemble
statistical
models），例如<strong>提升法（boosting）</strong>和<strong>装袋法（bagging）</strong>（参见第6章）。</p>
<p>随着 SMOTE
算法（<strong>“合成少数类过采样技术”</strong>的缩写）的发布，这一想法得到了推广。SMOTE
算法会找到一条与被过采样的记录相似的记录（参见第238页的“K-最近邻”），并创建一个<strong>合成记录</strong>，该记录是原始记录和相邻记录的随机加权平均值，其中权重是针对每个预测变量单独生成的。创建的合成过采样记录的数量取决于所需的过采样比率，以使数据集在结果类别上达到大致平衡。</p>
<p>在 R 中有几个 SMOTE 的实现。处理不平衡数据最全面的包是
<code>unbalanced</code>。它提供了多种技术，包括一个用于选择最佳方法的“Racing”算法。然而，SMOTE
算法本身足够简单，可以使用 <code>FNN</code> 包在 R 中直接实现。</p>
<p>Python 包 <code>imbalanced-learn</code> 实现了一系列方法，其 API 与
<code>scikit-learn</code>
兼容。它提供了各种过采样和欠采样的方法，并支持将这些技术与提升法和装袋法分类器结合使用。</p>
<h4 id="基于成本的分类"><strong>基于成本的分类</strong></h4>
<p>Cost-Based Classification</p>
<p>在实践中，准确率和 AUC
是选择分类规则的<strong>一种简陋方式</strong>。通常，可以为<strong>假阳性与假阴性</strong>分配一个<strong>估计成本</strong>，更合适的方法是结合这些成本来确定分类1和0时的<strong>最佳截止点</strong>。例如，假设一笔新贷款违约的预期成本为
<span class="math inline">\(C\)</span>，而一笔已还清贷款的预期回报为
<span class="math inline">\(R\)</span>。那么该笔贷款的预期回报（expected
return）是：</p>
<p><span class="math display">\[
预期回报 = P(Y=0) \times R + P(Y=1) \times C
\]</span>
与其简单地将一笔贷款标记为违约或已还清，或者确定违约概率，更有意义的是<strong>确定该笔贷款是否具有正的预期回报</strong>。预测的违约概率是一个中间步骤，必须将其与贷款的总价值结合起来，以确定<strong>预期利润</strong>，这才是业务的最终规划指标。例如，一笔价值较小的贷款可能会被放弃，而选择一笔价值较大但预测违约概率稍高的贷款。</p>
<h4 id="探索预测结果"><strong>探索预测结果</strong></h4>
<p>Exploring the Predictions</p>
<p>仅仅一个单一的指标，例如AUC，无法评估模型对特定情况的适用性的所有方面。图5-8展示了针对贷款数据，仅使用两个预测变量：<code>borrower_score</code>
和
<code>payment_inc_ratio</code>，拟合的四种不同模型的决策规则。这些模型包括：线性判别分析（LDA）、逻辑线性回归、使用广义加性模型（GAM）拟合的逻辑回归，以及一个树模型（参见第249页的“树模型”）。图中线左上方的区域对应于<strong>预测违约</strong>。在这种情况下，LDA
和逻辑线性回归的结果几乎相同。树模型产生了最不规则的规则，呈现出两个台阶。最后，逻辑回归的
GAM 拟合代表了树模型和线性模型之间的折中。</p>
<p><img src="/img3/面向数据科学家的实用统计学/F5.8.png" alt="F5.8" style="zoom:50%;" /></p>
<p>在更高维度中，或者在 GAM
和树模型的情况下，即使是生成这些规则的区域，也<strong>不容易可视化</strong>。</p>
<p>无论如何，对预测值进行探索性分析总是值得的。</p>
<p><strong>关键思想</strong></p>
<ul>
<li><strong>高度不平衡的数据</strong>（即，感兴趣的结果，即1，很罕见）对于分类算法来说是<strong>有问题的</strong>。</li>
<li>处理不平衡数据的一种策略是，通过<strong>欠采样</strong>多数类别（或<strong>过采样</strong>稀有类别）来平衡训练数据。</li>
<li>如果使用了所有1s后仍然太少，你可以对稀有类别进行<strong>自举</strong>，或使用
<strong>SMOTE</strong>
来创建与现有稀有案例相似的<strong>合成数据</strong>。</li>
<li>不平衡数据通常意味着正确分类某个类别（即1s）具有更高的价值，而这种价值比率应该被纳入评估指标中。</li>
</ul>
<h3 id="总结"><strong>总结</strong></h3>
<p><strong>分类</strong>，即预测一条记录属于两个或多个类别中的哪一个，是预测分析的一个基本工具。一笔贷款会违约吗（是或否）？它会提前还款吗？一个网站访问者会点击链接吗？他们会购买东西吗？一笔保险索赔是欺诈性的吗？在分类问题中，通常有一个类别是主要关注的（例如，欺诈性的保险索赔），在二元分类中，这个类别被指定为1，而另一个更普遍的类别为0。通常，这个过程的一个关键部分是<strong>估计倾向得分</strong>，即属于目标类别的概率。一个常见的情景是，感兴趣的类别相对<strong>罕见</strong>。在评估分类器时，有多种<strong>超越简单准确率</strong>的模型评估指标；当所有记录都分类为0也能产生高准确率时，这些指标在稀有类别情境下尤为重要。</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/AI/" rel="tag"># AI</a>
              <a href="/tags/Python/" rel="tag"># Python</a>
              <a href="/tags/%E7%BB%9F%E8%AE%A1/" rel="tag"># 统计</a>
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a>
              <a href="/tags/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6/" rel="tag"># 数据科学</a>
              <a href="/tags/R/" rel="tag"># R</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2025/09/25/%E7%AC%AC4%E7%AB%A0%20%E5%9B%9E%E5%BD%92%E4%B8%8E%E9%A2%84%E6%B5%8B/" rel="prev" title="第4章 回归与预测">
                  <i class="fa fa-chevron-left"></i> 第4章 回归与预测
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2025/09/25/%E7%AC%AC3%E7%AB%A0%20%E7%BB%9F%E8%AE%A1%E5%AE%9E%E9%AA%8C%E4%B8%8E%E6%98%BE%E8%91%97%E6%80%A7%E6%A3%80%E9%AA%8C/" rel="next" title="第3章 统计实验与显著性检验">
                  第3章 统计实验与显著性检验 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Rayman.hung</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
<script src="https://cdn.jsdelivr.net/npm/hexo-generator-searchdb@1.4.0/dist/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js","integrity":"sha256-r+3itOMtGGjap0x+10hu6jW/gZCzxHsoKrOd7gyRSGY="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>

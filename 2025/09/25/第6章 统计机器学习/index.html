<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.1.1">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" integrity="sha256-DfWjNxDkM94fVBWx1H5BMMp0Zq7luBlV8QRcSES7s+0=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"hongyitong.github.io","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.11.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="《Practical Statistics for Data Scientists》书籍英文版 《面向数据科学家的实用统计学》中文版书籍 第6章 统计机器学习 Statistical Machine Learning 统计学在近期发展中，致力于开发更强大、更自动化的预测建模技术，涵盖了回归和分类。这些方法与上一章讨论的方法一样，都是有监督学习——它们通过在已知结果的数据上进行训练，来学习如何预测新">
<meta property="og:type" content="article">
<meta property="og:title" content="第6章 统计机器学习">
<meta property="og:url" content="http://hongyitong.github.io/2025/09/25/%E7%AC%AC6%E7%AB%A0%20%E7%BB%9F%E8%AE%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="忆桐之家的博客">
<meta property="og:description" content="《Practical Statistics for Data Scientists》书籍英文版 《面向数据科学家的实用统计学》中文版书籍 第6章 统计机器学习 Statistical Machine Learning 统计学在近期发展中，致力于开发更强大、更自动化的预测建模技术，涵盖了回归和分类。这些方法与上一章讨论的方法一样，都是有监督学习——它们通过在已知结果的数据上进行训练，来学习如何预测新">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E9%9D%A2%E5%90%91%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%AE%B6%E7%9A%84%E5%AE%9E%E7%94%A8%E7%BB%9F%E8%AE%A1%E5%AD%A6/F6.1.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E9%9D%A2%E5%90%91%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%AE%B6%E7%9A%84%E5%AE%9E%E7%94%A8%E7%BB%9F%E8%AE%A1%E5%AD%A6/T6.1.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E9%9D%A2%E5%90%91%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%AE%B6%E7%9A%84%E5%AE%9E%E7%94%A8%E7%BB%9F%E8%AE%A1%E5%AD%A6/F6.2.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E9%9D%A2%E5%90%91%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%AE%B6%E7%9A%84%E5%AE%9E%E7%94%A8%E7%BB%9F%E8%AE%A1%E5%AD%A6/T6.2.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E9%9D%A2%E5%90%91%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%AE%B6%E7%9A%84%E5%AE%9E%E7%94%A8%E7%BB%9F%E8%AE%A1%E5%AD%A6/F6.3.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E9%9D%A2%E5%90%91%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%AE%B6%E7%9A%84%E5%AE%9E%E7%94%A8%E7%BB%9F%E8%AE%A1%E5%AD%A6/F6.4.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E9%9D%A2%E5%90%91%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%AE%B6%E7%9A%84%E5%AE%9E%E7%94%A8%E7%BB%9F%E8%AE%A1%E5%AD%A6/F6.5.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E9%9D%A2%E5%90%91%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%AE%B6%E7%9A%84%E5%AE%9E%E7%94%A8%E7%BB%9F%E8%AE%A1%E5%AD%A6/F6.6.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E9%9D%A2%E5%90%91%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%AE%B6%E7%9A%84%E5%AE%9E%E7%94%A8%E7%BB%9F%E8%AE%A1%E5%AD%A6/F6.7.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E9%9D%A2%E5%90%91%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%AE%B6%E7%9A%84%E5%AE%9E%E7%94%A8%E7%BB%9F%E8%AE%A1%E5%AD%A6/F6.8.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E9%9D%A2%E5%90%91%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%AE%B6%E7%9A%84%E5%AE%9E%E7%94%A8%E7%BB%9F%E8%AE%A1%E5%AD%A6/F6.9.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E9%9D%A2%E5%90%91%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%AE%B6%E7%9A%84%E5%AE%9E%E7%94%A8%E7%BB%9F%E8%AE%A1%E5%AD%A6/F6.10.png">
<meta property="article:published_time" content="2025-09-24T16:00:00.000Z">
<meta property="article:modified_time" content="2025-09-25T03:05:44.542Z">
<meta property="article:author" content="Rayman.hung">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="Python">
<meta property="article:tag" content="统计">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="数据科学">
<meta property="article:tag" content="R">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://hongyitong.github.io/img3/%E9%9D%A2%E5%90%91%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%AE%B6%E7%9A%84%E5%AE%9E%E7%94%A8%E7%BB%9F%E8%AE%A1%E5%AD%A6/F6.1.png">


<link rel="canonical" href="http://hongyitong.github.io/2025/09/25/%E7%AC%AC6%E7%AB%A0%20%E7%BB%9F%E8%AE%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://hongyitong.github.io/2025/09/25/%E7%AC%AC6%E7%AB%A0%20%E7%BB%9F%E8%AE%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/","path":"2025/09/25/第6章 统计机器学习/","title":"第6章 统计机器学习"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>第6章 统计机器学习 | 忆桐之家的博客</title>
  





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">忆桐之家的博客</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Rayman&Tony</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section">首页</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section">分类</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section">关于</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section">归档</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section">标签</a></li><li class="menu-item menu-item-commonweal"><a href="/404.html" rel="section">公益 404</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC6%E7%AB%A0-%E7%BB%9F%E8%AE%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"><span class="nav-number">1.</span> <span class="nav-text">第6章 统计机器学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#k-%E6%9C%80%E8%BF%91%E9%82%BB"><span class="nav-number">1.1.</span> <span class="nav-text">K-最近邻</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%80%E4%B8%AA%E5%B0%8F%E4%BE%8B%E5%AD%90%E9%A2%84%E6%B5%8B%E8%B4%B7%E6%AC%BE%E8%BF%9D%E7%BA%A6"><span class="nav-number">1.1.1.</span> <span class="nav-text">一个小例子：预测贷款违约</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F"><span class="nav-number">1.1.2.</span> <span class="nav-text">距离度量</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%8B%AC%E7%83%AD%E7%BC%96%E7%A0%81"><span class="nav-number">1.1.3.</span> <span class="nav-text">独热编码</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A0%87%E5%87%86%E5%8C%96%E5%BD%92%E4%B8%80%E5%8C%96z-%E5%88%86%E6%95%B0"><span class="nav-number">1.1.4.</span> <span class="nav-text">标准化（归一化，z-分数）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#k%E7%9A%84%E9%80%89%E6%8B%A9"><span class="nav-number">1.1.5.</span> <span class="nav-text">K的选择</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#knn%E4%BD%9C%E4%B8%BA%E7%89%B9%E5%BE%81%E5%BC%95%E6%93%8E"><span class="nav-number">1.1.6.</span> <span class="nav-text">KNN作为特征引擎</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%91%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.2.</span> <span class="nav-text">树模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%A4%BA%E4%BE%8B"><span class="nav-number">1.2.1.</span> <span class="nav-text">一个简单示例</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%80%92%E5%BD%92%E5%88%92%E5%88%86%E7%AE%97%E6%B3%95"><span class="nav-number">1.2.2.</span> <span class="nav-text">递归划分算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B5%8B%E9%87%8F%E5%90%8C%E8%B4%A8%E6%80%A7%E6%88%96%E4%B8%8D%E7%BA%AF%E5%BA%A6"><span class="nav-number">1.2.3.</span> <span class="nav-text">测量同质性或不纯度</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%98%BB%E6%AD%A2%E6%A0%91%E7%BB%A7%E7%BB%AD%E7%94%9F%E9%95%BF"><span class="nav-number">1.2.4.</span> <span class="nav-text">阻止树继续生长</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%A2%84%E6%B5%8B%E8%BF%9E%E7%BB%AD%E5%80%BC"><span class="nav-number">1.2.5.</span> <span class="nav-text">预测连续值</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A0%91%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%BA%94%E7%94%A8%E6%96%B9%E5%BC%8F"><span class="nav-number">1.2.6.</span> <span class="nav-text">树模型的应用方式</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A3%85%E8%A2%8B%E6%B3%95%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97"><span class="nav-number">1.3.</span> <span class="nav-text">装袋法与随机森林</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#bagging-%E6%96%B9%E6%B3%95"><span class="nav-number">1.3.1.</span> <span class="nav-text">Bagging 方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97"><span class="nav-number">1.3.2.</span> <span class="nav-text">随机森林</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%98%E9%87%8F%E9%87%8D%E8%A6%81%E6%80%A7"><span class="nav-number">1.3.3.</span> <span class="nav-text">变量重要性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%B6%85%E5%8F%82%E6%95%B0"><span class="nav-number">1.3.4.</span> <span class="nav-text">超参数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8F%90%E5%8D%87%E6%B3%95"><span class="nav-number">1.4.</span> <span class="nav-text">提升法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8F%90%E5%8D%87%E7%AE%97%E6%B3%95"><span class="nav-number">1.4.1.</span> <span class="nav-text">提升算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#xgboost"><span class="nav-number">1.4.2.</span> <span class="nav-text">XGBoost</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96%E9%81%BF%E5%85%8D%E8%BF%87%E6%8B%9F%E5%90%88"><span class="nav-number">1.4.3.</span> <span class="nav-text">正则化：避免过拟合</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%B6%85%E5%8F%82%E6%95%B0%E5%92%8C%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81"><span class="nav-number">1.4.4.</span> <span class="nav-text">超参数和交叉验证</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B0%8F%E7%BB%93"><span class="nav-number">1.5.</span> <span class="nav-text">小结</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Rayman.hung</p>
  <div class="site-description" itemprop="description">技术分享、读书心得、亲子时刻</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives">
          <span class="site-state-item-count">903</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags">
        <span class="site-state-item-count">1149</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/hongyitong" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;hongyitong" rel="noopener" target="_blank">GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="http://blog.sina.com.cn/yitonghong" title="忆桐之家 → http:&#x2F;&#x2F;blog.sina.com.cn&#x2F;yitonghong" rel="noopener" target="_blank">忆桐之家</a>
      </span>
      <span class="links-of-author-item">
        <a href="http://douban.com/people/2780741" title="豆瓣 → http:&#x2F;&#x2F;douban.com&#x2F;people&#x2F;2780741" rel="noopener" target="_blank">豆瓣</a>
      </span>
      <span class="links-of-author-item">
        <a href="http://www.zhihu.com/people/rayman-36" title="知乎 → http:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;rayman-36" rel="noopener" target="_blank">知乎</a>
      </span>
      <span class="links-of-author-item">
        <a href="http://www.liaoxuefeng.com/" title="廖雪峰的官方网站 → http:&#x2F;&#x2F;www.liaoxuefeng.com&#x2F;" rel="noopener" target="_blank">廖雪峰的官方网站</a>
      </span>
      <span class="links-of-author-item">
        <a href="http://www.vaikan.com/" title="外刊IT评论 → http:&#x2F;&#x2F;www.vaikan.com&#x2F;" rel="noopener" target="_blank">外刊IT评论</a>
      </span>
  </div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://hongyitong.github.io/2025/09/25/%E7%AC%AC6%E7%AB%A0%20%E7%BB%9F%E8%AE%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Rayman.hung">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="忆桐之家的博客">
      <meta itemprop="description" content="技术分享、读书心得、亲子时刻">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="第6章 统计机器学习 | 忆桐之家的博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          第6章 统计机器学习
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2025-09-25 00:00:00 / 修改时间：11:05:44" itemprop="dateCreated datePublished" datetime="2025-09-25T00:00:00+08:00">2025-09-25</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/" itemprop="url" rel="index"><span itemprop="name">读书心得</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p><a
href="/img3/面向数据科学家的实用统计学/Practical-Statistics-for-Data-Scientists.pdf">《Practical
Statistics for Data Scientists》书籍英文版</a><br />
<a
href="/img3/面向数据科学家的实用统计学/面向数据科学家的实用统计学.pdf">《面向数据科学家的实用统计学》中文版书籍</a></p>
<h2 id="第6章-统计机器学习"><strong>第6章 统计机器学习</strong></h2>
<p>Statistical Machine Learning</p>
<p>统计学在近期发展中，致力于开发更强大、更自动化的<strong>预测建模技术</strong>，涵盖了<strong>回归</strong>和<strong>分类</strong>。这些方法与上一章讨论的方法一样，都是<strong>有监督学习</strong>——它们通过在已知结果的数据上进行训练，来学习如何预测新数据的结果。它们属于<strong>统计机器学习</strong>的范畴，与经典统计方法不同之处在于，它们是<strong>数据驱动</strong>的，并且不试图对数据强加线性的或其他整体结构。例如，<strong>K-最近邻</strong>(K-Nearest
Neighbors)方法非常简单：根据相似记录的分类方式来对一条记录进行分类。最成功和应用最广泛的技术是基于<strong>集成学习</strong>(ensemble
learning)并应用于<strong>决策树</strong>(decision
trees.)的方法。集成学习的基本思想是<strong>使用多个模型</strong>来形成预测，而不是仅仅使用一个单一模型。决策树是一种灵活且自动化的技术，用于学习预测变量和结果变量之间关系的规则。事实证明，将集成学习与决策树相结合，可以产生一些性能最佳的现成预测建模技术。</p>
<p>许多统计机器学习技术的发展，可以追溯到加州大学伯克利分校的统计学家
Leo Breiman（参见图6-1）和斯坦福大学的 Jerry Friedman。
他们的工作，以及伯克利和斯坦福其他研究人员的工作，始于1984年对<strong>树模型</strong>的开发。随后在20世纪90年代开发的<strong>装袋法（bagging）</strong>和<strong>提升法（boosting）</strong>等集成方法，奠定了统计机器学习的基础。
<span id="more"></span></p>
<p><img src="/img3/面向数据科学家的实用统计学/F6.1.png" alt="F6.1" style="zoom:50%;" /></p>
<blockquote>
<p>通用注解：</p>
<p><strong>机器学习与统计学</strong>（Machine Learning Versus
Statistics）</p>
<p>在预测建模的背景下，<strong>机器学习和统计学有什么区别</strong>？这两个学科之间没有明确的界限。机器学习更倾向于开发<strong>高效算法</strong>，以处理大规模数据来优化预测模型。而统计学通常更关注<strong>概率理论</strong>和模型的<strong>底层结构</strong>。<strong>装袋法</strong>和<strong>随机森林</strong>（参见第259页的“装袋法与随机森林”）稳固地属于统计学阵营。另一方面，<strong>提升法</strong>（参见第270页的“提升法”）在这两个学科中都有发展，但在机器学习方面获得了更多关注。不管历史如何，提升法所展现的潜力确保了它将在统计学和机器学习中都蓬勃发展。</p>
</blockquote>
<h3 id="k-最近邻"><strong>K-最近邻</strong></h3>
<p>K-Nearest Neighbors</p>
<p>K-最近邻（KNN）背后的思想非常简单。对于每一条待分类或预测的记录：</p>
<ol type="1">
<li>找到<strong>K</strong>个特征相似（即，预测变量值相似）的记录。</li>
<li>对于<strong>分类</strong>，找出这些相似记录中<strong>哪一类是多数</strong>，并将该类别分配给新记录。</li>
<li>对于<strong>预测</strong>（也称作 <strong>KNN
回归</strong>），求出这些相似记录的<strong>平均值</strong>，并用该平均值作为新记录的预测值。</li>
</ol>
<p><strong>K-最近邻的关键术语</strong></p>
<ul>
<li><p><strong>邻居（Neighbor）</strong>
与另一条记录有相似预测变量值的记录。</p></li>
<li><p><strong>距离度量（Distance metrics）</strong>
用一个单一数值来衡量一条记录与另一条记录相距多远。</p></li>
<li><p><strong>标准化（Standardization）</strong> 减去均值并除以标准差。
同义词：<strong>归一化（Normalization）</strong></p></li>
<li><p><strong>z分数（z-score）</strong> 标准化后得到的值。</p></li>
<li><p><strong>K</strong> 在最近邻计算中考虑的邻居数量。</p></li>
</ul>
<p>KNN
是最简单的预测/分类技术之一：它<strong>不需要拟合模型</strong>（如回归中那样）。但这并不意味着使用
KNN
是一个自动化的过程。预测结果取决于<strong>特征如何缩放</strong>、<strong>如何衡量相似性</strong>以及<strong>K设置得有多大</strong>。此外，所有预测变量都必须是<strong>数值形式</strong>。我们将通过一个分类示例来演示如何使用
KNN 方法。</p>
<h4
id="一个小例子预测贷款违约"><strong>一个小例子：预测贷款违约</strong></h4>
<p>表6-1显示了来自 LendingClub 的一些个人贷款数据记录。LendingClub
是P2P借贷领域的领导者，投资者群体向个人提供贷款。分析的目标是预测一笔新的潜在贷款的结果：<strong>已还清</strong>还是<strong>违约</strong>。</p>
<p><img src="/img3/面向数据科学家的实用统计学/T6.1.png" alt="T6.1" style="zoom:50%;" /></p>
<p>考虑一个非常简单的模型，只有两个预测变量：<strong>dti</strong>，即债务支付（不包括抵押贷款）与收入的比率；以及
<strong>payment_inc_ratio</strong>，即贷款还款额与收入的比率。两个比率都乘以100。</p>
<p>使用一个包含200笔贷款的小数据集
<code>loan200</code>，其中包含已知的二元结果（违约或未违约，在预测变量
<code>outcome200</code> 中指定），并将 K
设置为20。对于一笔待预测的新贷款 <code>newloan</code>，其
dti=22.5，payment_inc_ratio=9，KNN 估计值在 R 中可计算如下：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">newloan <span class="operator">&lt;-</span> loan200<span class="punctuation">[</span><span class="number">1</span><span class="punctuation">,</span> <span class="number">2</span><span class="operator">:</span><span class="number">3</span><span class="punctuation">,</span> drop<span class="operator">=</span><span class="literal">FALSE</span><span class="punctuation">]</span></span><br><span class="line">knn_pred <span class="operator">&lt;-</span> knn<span class="punctuation">(</span>train<span class="operator">=</span>loan200<span class="punctuation">[</span><span class="operator">-</span><span class="number">1</span><span class="punctuation">,</span> <span class="number">2</span><span class="operator">:</span><span class="number">3</span><span class="punctuation">]</span><span class="punctuation">,</span> test<span class="operator">=</span>newloan<span class="punctuation">,</span> cl<span class="operator">=</span>loan200<span class="punctuation">[</span><span class="operator">-</span><span class="number">1</span><span class="punctuation">,</span> <span class="number">1</span><span class="punctuation">]</span><span class="punctuation">,</span> k<span class="operator">=</span><span class="number">20</span><span class="punctuation">)</span></span><br><span class="line">knn_pred <span class="operator">==</span> <span class="string">&#x27;paid off&#x27;</span></span><br><span class="line"><span class="punctuation">[</span><span class="number">1</span><span class="punctuation">]</span> <span class="literal">TRUE</span></span><br></pre></td></tr></table></figure>
<p>KNN 的预测结果是该贷款将会违约。</p>
<p>虽然 R 有一个原生的 <code>knn</code> 函数，但贡献的 R 包
<strong><code>FNN</code></strong>（意为 Fast Nearest
Neighbor，快速最近邻）能更有效地扩展到大数据，并提供更大的灵活性。</p>
<p><code>scikit-learn</code> 包在 Python 中提供了 KNN
的快速高效实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">predictors = [<span class="string">&#x27;payment_inc_ratio&#x27;</span>, <span class="string">&#x27;dti&#x27;</span>]</span><br><span class="line">outcome = <span class="string">&#x27;outcome&#x27;</span></span><br><span class="line">newloan = loan200.loc[<span class="number">0</span>:<span class="number">0</span>, predictors]</span><br><span class="line">X = loan200.loc[<span class="number">1</span>:, predictors]</span><br><span class="line">y = loan200.loc[<span class="number">1</span>:, outcome]</span><br><span class="line">knn = KNeighborsClassifier(n_neighbors=<span class="number">20</span>)</span><br><span class="line">knn.fit(X, y)</span><br><span class="line">knn.predict(newloan)</span><br></pre></td></tr></table></figure>
<p>图6-2给出了这个例子的可视化展示。 .
中间的<strong>十字</strong>是待预测的新贷款。<strong>方块</strong>（已还清）和<strong>圆圈</strong>（违约）是训练数据。<strong>大黑圈</strong>显示了最近20个点的边界。在这个例子中，圆圈内有9笔违约贷款和11笔已还清贷款。因此，预测的贷款结果是<strong>已还清</strong>。注意，如果我们只考虑3个最近邻，预测结果将是贷款违约。</p>
<p><img src="/img3/面向数据科学家的实用统计学/F6.2.png" alt="F6.2" style="zoom:50%;" /></p>
<blockquote>
<p>通用注解：</p>
<p>KNN
分类的输出通常是一个二元决策，例如贷款数据中的<strong>违约</strong>或<strong>已还清</strong>，但KNN程序通常也提供输出一个介于0到1之间的<strong>概率（倾向性）</strong>。这个概率是基于K个最近邻居中某一类别所占的比例。在前面的例子中，违约的概率估计为<span
class="math inline">\({9}/{20}\)</span>，即0.45。</p>
<p>使用概率得分可以让您使用<strong>不同于简单多数投票（概率0.5）的分类规则</strong>。这在处理<strong>不平衡类别</strong>问题时尤为重要；参见第230页的“不平衡数据的策略”。例如，如果目标是识别一个<strong>罕见类别</strong>的成员，截止点通常会设置在50%以下。一种常见的方法是将截止点设置为罕见事件的发生概率。</p>
</blockquote>
<h4 id="距离度量"><strong>距离度量</strong></h4>
<p>Distance Metrics</p>
<p><strong>相似性（接近度）</strong>是使用<strong>距离度量</strong>来确定的，距离度量是一个函数，它测量两条记录
<span class="math inline">\((x_1, x_2, \dots, x_p)\)</span> 和 <span
class="math inline">\((u_1, u_2, \dots, u_p)\)</span> 之间的距离。</p>
<p>两个向量之间最流行的距离度量是<strong>欧几里得距离</strong>（Euclidean
distance）。要测量两个向量之间的欧几里得距离，需要将一个向量的对应分量减去另一个向量的对应分量，将差值平方，然后求和，最后取平方根：</p>
<p><span class="math display">\[
\sqrt{(x_1 - u_1)^2 + (x_2 - u_2)^2 + \cdots + (x_p - u_p)^2}
\]</span>
另一种用于数值数据的常见距离度量是<strong>曼哈顿距离</strong>（Manhattan
distance）：</p>
<p><span class="math display">\[
|x_1 - u_1| + |x_2 - u_2| + \cdots + |x_p - u_p|
\]</span>
<strong>欧几里得距离</strong>对应于两点之间的直线距离（例如，像乌鸦飞行的距离）。<strong>曼哈顿距离</strong>是沿着单一方向一次移动一段距离来遍历两点之间的距离（例如，沿着矩形城市街区行进）。因此，如果相似性被定义为点到点的旅行时间，曼哈顿距离是一个有用的近似值。</p>
<p>在测量两个向量之间的距离时，<strong>测量尺度相对较大的变量（特征）将主导整个度量</strong>。例如，对于贷款数据，距离几乎完全取决于以成千上万计的<strong>收入</strong>和<strong>贷款金额</strong>变量。相比之下，比率变量的作用几乎可以忽略不计。我们通过<strong>标准化数据</strong>来解决这个问题；参见第243页的“标准化（归一化、z-分数）”。</p>
<blockquote>
<p>通用注解：</p>
<p><strong>其他距离度量</strong>（Other Distance Metrics）</p>
<p>有许多其他的度量方法可以用来测量向量之间的距离。<strong>对于数值数据，马氏距离（Mahalanobis
distance）很有吸引力，因为它考虑了两个变量之间的相关性。这一点很有用，因为如果两个变量高度相关，马氏距离在计算时会本质上将它们视为一个单一变量。而欧几里得距离和曼哈顿距离不考虑相关性</strong>，实际上会更多地加权那些作为特征基础的属性。马氏距离是主成分（参见第284页的“主成分分析”）之间的欧几里得距离。使用马氏距离的缺点是<strong>增加了计算量和复杂性</strong>；它需要使用<strong>协方差矩阵</strong>进行计算（参见第202页的“协方差矩阵”）。</p>
</blockquote>
<h4 id="独热编码"><strong>独热编码</strong></h4>
<p>One Hot Encoder</p>
<p>表6-1中的贷款数据包含几个因子（字符串）变量。大多数统计和机器学习模型要求将这类变量转换为一系列传达相同信息的<strong>二元虚拟变量</strong>，如表6-2所示。</p>
<p><img src="/img3/面向数据科学家的实用统计学/T6.2.png" alt="T6.2" style="zoom:50%;" /></p>
<p>原本一个表示<strong>房产居住状态</strong>的单一变量，可以是“有抵押贷款自有房”、“无抵押贷款自有房”、“租房”或“其他”，现在我们得到了四个二元变量。第一个将是“有抵押贷款自有房
- 是/否”，第二个将是“无抵押贷款自有房 -
是/否”，以此类推。因此，这个名为“房产居住状态”的预测变量会产生一个包含一个1和三个0的向量，可用于统计和机器学习算法。独热编码（one
hot
encoding）这个短语源于数字电路术语，它描述的是一种电路设置，其中只有一个位被允许为正值（热）。</p>
<blockquote>
<p>通用注解：</p>
<p>在线性回归和逻辑回归中，独热编码会引起<strong>多重共线性</strong>问题；参见第172页的“多重共线性”。在这种情况下，一个虚拟变量会被省略（它的值可以从其他变量推断出来）。但在KNN和本书讨论的其他方法中，这不是一个问题。</p>
</blockquote>
<h4
id="标准化归一化z-分数"><strong>标准化（归一化，z-分数）</strong></h4>
<p>Standardization (Normalization, z-Scores)</p>
<p>在测量中，我们通常不那么关心“有多少”，而更关心“<strong>与平均值有多大差异</strong>”。<strong>标准化（Standardization）</strong>，也称为<strong>归一化（Normalization）</strong>，通过减去均值并除以标准差，将所有变量置于相似的尺度上；这样，我们确保一个变量不会仅仅因为其原始测量的尺度而过度影响模型：</p>
<p><span class="math display">\[
z = \frac{x - \bar{x}}{s}
\]</span> 这种转换的结果通常被称为
<strong>z-分数</strong>。测量值随后以“<strong>偏离均值的标准差</strong>”来表示。</p>
<blockquote>
<p>警告：</p>
<p>在这种统计背景下的“归一化”不应与数据库归一化相混淆，后者是移除冗余数据和验证数据依赖关系的过程。</p>
</blockquote>
<p>对于KNN和一些其他程序（例如主成分分析和聚类），在应用程序之前<strong>对数据进行标准化至关重要</strong>。为了说明这个想法，我们使用
<code>dti</code> 和
<code>payment_inc_ratio</code>（参见第239页的“一个小例子：预测贷款违约”）以及另外两个变量来对贷款数据应用KNN：<code>revol_bal</code>，申请人可用的总循环信贷余额（以美元计），以及
<code>revol_util</code>，已使用的信贷百分比。待预测的新记录如下所示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">newloan</span><br><span class="line">payment_inc_ratio dti revol_bal revol_util</span><br><span class="line">1 2.3932 1 1687 9.4</span><br></pre></td></tr></table></figure>
<p>在<code>revol_bal</code>这个以美元计量的变量上，其量级远大于其他变量。<strong><code>knn</code></strong>函数返回最近邻居的索引作为一个属性<strong><code>nn.index</code></strong>，我们可以用它来展示<code>loan_df</code>中五个最接近的行：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">loan_df <span class="operator">&lt;-</span> model.matrix<span class="punctuation">(</span><span class="operator">~</span> <span class="operator">-</span><span class="number">1</span> <span class="operator">+</span> payment_inc_ratio <span class="operator">+</span> dti <span class="operator">+</span> revol_bal <span class="operator">+</span></span><br><span class="line">revol_util<span class="punctuation">,</span> data<span class="operator">=</span>loan_data<span class="punctuation">)</span></span><br><span class="line">newloan <span class="operator">&lt;-</span> loan_df<span class="punctuation">[</span><span class="number">1</span><span class="punctuation">,</span> <span class="punctuation">,</span> drop<span class="operator">=</span><span class="literal">FALSE</span><span class="punctuation">]</span></span><br><span class="line">loan_df <span class="operator">&lt;-</span> loan_df<span class="punctuation">[</span><span class="operator">-</span><span class="number">1</span><span class="punctuation">,</span><span class="punctuation">]</span></span><br><span class="line">outcome <span class="operator">&lt;-</span> loan_data<span class="punctuation">[</span><span class="operator">-</span><span class="number">1</span><span class="punctuation">,</span> <span class="number">1</span><span class="punctuation">]</span></span><br><span class="line">knn_pred <span class="operator">&lt;-</span> knn<span class="punctuation">(</span>train<span class="operator">=</span>loan_df<span class="punctuation">,</span> test<span class="operator">=</span>newloan<span class="punctuation">,</span> cl<span class="operator">=</span>outcome<span class="punctuation">,</span> k<span class="operator">=</span><span class="number">5</span><span class="punctuation">)</span></span><br><span class="line">loan_df<span class="punctuation">[</span><span class="built_in">attr</span><span class="punctuation">(</span>knn_pred<span class="punctuation">,</span> <span class="string">&quot;nn.index&quot;</span><span class="punctuation">)</span><span class="punctuation">,</span><span class="punctuation">]</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">   payment_inc_ratio   dti   revol_bal   revol_util</span><br><span class="line">35537          1.47212  1.46        1686       10.0</span><br><span class="line">33652          3.38178  6.37        1688        8.4</span><br><span class="line">25864          2.36303  1.39        1691        3.5</span><br><span class="line">42954          1.28160  7.14        1684        3.9</span><br><span class="line">43600          4.12244  8.98        1684        7.2</span><br></pre></td></tr></table></figure>
<p>在模型拟合后，我们可以使用<strong><code>scikit-learn</code></strong>的<strong><code>kneighbors</code></strong>方法来识别训练集中五个最接近的行：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">predictors = [<span class="string">&#x27;payment_inc_ratio&#x27;</span>, <span class="string">&#x27;dti&#x27;</span>, <span class="string">&#x27;revol_bal&#x27;</span>, <span class="string">&#x27;revol_util&#x27;</span>]</span><br><span class="line">outcome = <span class="string">&#x27;outcome&#x27;</span></span><br><span class="line">newloan = loan_data.loc[<span class="number">0</span>:<span class="number">0</span>, predictors]</span><br><span class="line">X = loan_data.loc[<span class="number">1</span>:, predictors]</span><br><span class="line">y = loan_data.loc[<span class="number">1</span>:, outcome]</span><br><span class="line">knn = KNeighborsClassifier(n_neighbors=<span class="number">5</span>)</span><br><span class="line">knn.fit(X, y)</span><br><span class="line">nbrs = knn.kneighbors(newloan)</span><br><span class="line">X.iloc[nbrs[<span class="number">1</span>][<span class="number">0</span>], :]</span><br></pre></td></tr></table></figure>
<p>在这些邻居中，<code>revol_bal</code>的值与新记录中的值非常接近，但其他预测变量的值则完全不相干，基本上没有在确定邻居的过程中起到任何作用。</p>
<p>现在，我们比较一下将KNN应用于<strong>标准化后</strong>的数据，使用R的<strong><code>scale</code></strong>函数，该函数计算每个变量的z-分数：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">loan_df <span class="operator">&lt;-</span> model.matrix<span class="punctuation">(</span><span class="operator">~</span> <span class="operator">-</span><span class="number">1</span> <span class="operator">+</span> payment_inc_ratio <span class="operator">+</span> dti <span class="operator">+</span> revol_bal <span class="operator">+</span></span><br><span class="line">revol_util<span class="punctuation">,</span> data<span class="operator">=</span>loan_data<span class="punctuation">)</span></span><br><span class="line">loan_std <span class="operator">&lt;-</span> scale<span class="punctuation">(</span>loan_df<span class="punctuation">)</span></span><br><span class="line">newloan_std <span class="operator">&lt;-</span> loan_std<span class="punctuation">[</span><span class="number">1</span><span class="punctuation">,</span> <span class="punctuation">,</span> drop<span class="operator">=</span><span class="literal">FALSE</span><span class="punctuation">]</span></span><br><span class="line">loan_std <span class="operator">&lt;-</span> loan_std<span class="punctuation">[</span><span class="operator">-</span><span class="number">1</span><span class="punctuation">,</span><span class="punctuation">]</span></span><br><span class="line">loan_df <span class="operator">&lt;-</span> loan_df<span class="punctuation">[</span><span class="operator">-</span><span class="number">1</span><span class="punctuation">,</span><span class="punctuation">]</span></span><br><span class="line">outcome <span class="operator">&lt;-</span> loan_data<span class="punctuation">[</span><span class="operator">-</span><span class="number">1</span><span class="punctuation">,</span> <span class="number">1</span><span class="punctuation">]</span></span><br><span class="line">knn_pred <span class="operator">&lt;-</span> knn<span class="punctuation">(</span>train<span class="operator">=</span>loan_std<span class="punctuation">,</span> test<span class="operator">=</span>newloan_std<span class="punctuation">,</span> cl<span class="operator">=</span>outcome<span class="punctuation">,</span> k<span class="operator">=</span><span class="number">5</span><span class="punctuation">)</span></span><br><span class="line">loan_df<span class="punctuation">[</span><span class="built_in">attr</span><span class="punctuation">(</span>knn_pred<span class="punctuation">,</span> <span class="string">&quot;nn.index&quot;</span><span class="punctuation">)</span><span class="punctuation">,</span><span class="punctuation">]</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">   payment_inc_ratio    dti    revol_bal   revol_util</span><br><span class="line">2081          2.61091   1.03         1218        9.7</span><br><span class="line">1439          2.34343   0.51          278        9.9</span><br><span class="line">30216         2.71200   1.34         1075        8.5</span><br><span class="line">28543         2.39760   0.74         2917        7.4</span><br><span class="line">44738         2.34309   1.37          488        7.2</span><br></pre></td></tr></table></figure>
<p>我们还需要移除<strong><code>loan_df</code></strong>的第一行，以使行号相互对应。</p>
<p><strong><code>sklearn.preprocessing.StandardScaler</code></strong>方法首先使用预测变量进行训练，然后用于在训练KNN模型之前转换数据集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">newloan = loan_data.loc[<span class="number">0</span>:<span class="number">0</span>, predictors]</span><br><span class="line">X = loan_data.loc[<span class="number">1</span>:, predictors]</span><br><span class="line">y = loan_data.loc[<span class="number">1</span>:, outcome]</span><br><span class="line">scaler = preprocessing.StandardScaler()</span><br><span class="line">scaler.fit(X * <span class="number">1.0</span>)</span><br><span class="line">X_std = scaler.transform(X * <span class="number">1.0</span>)</span><br><span class="line">newloan_std = scaler.transform(newloan * <span class="number">1.0</span>)</span><br><span class="line">knn = KNeighborsClassifier(n_neighbors=<span class="number">5</span>)</span><br><span class="line">knn.fit(X_std, y)</span><br><span class="line">nbrs = knn.kneighbors(newloan_std)</span><br><span class="line">X.iloc[nbrs[<span class="number">1</span>][<span class="number">0</span>], :]</span><br></pre></td></tr></table></figure>
<p>五个最近的邻居在所有变量上都更加相似，这提供了一个更合理的结果。请注意，结果是<strong>以原始尺度显示</strong>的，但KNN是<strong>应用于缩放后的数据</strong>和待预测的新贷款的。</p>
<blockquote>
<p>知识点：</p>
<p>使用z-分数只是重新缩放变量的一种方式。除了均值，我们还可以使用更稳健的<strong>位置估计量</strong>，例如<strong>中位数</strong>。同样，可以使用<strong>四分位距</strong>等不同的<strong>尺度估计量</strong>来代替标准差。有时，变量会被“压扁”到0-1的范围内。同样重要的是要意识到，将每个变量缩放到具有单位方差在某种程度上是<strong>武断</strong>的。这暗示着每个变量在预测能力上被认为具有相同的<strong>重要性</strong>。如果你主观上知道某些变量比其他变量更重要，那么可以对这些变量进行放大。例如，对于贷款数据，可以合理地预期<strong>还款与收入的比率</strong>非常重要。</p>
</blockquote>
<blockquote>
<p>通用注解：</p>
<p><strong>标准化（归一化）并不会改变数据的分布形状</strong>；如果数据本身不呈正态分布，标准化也不会使其变为正态分布（参见第69页的“正态分布”）。</p>
</blockquote>
<h4 id="k的选择"><strong>K的选择</strong></h4>
<p>Choosing K</p>
<p><strong>K的选择对于KNN的性能非常重要</strong>。最简单的选择是设置<strong>K
=
1</strong>，这被称为<strong>1-最近邻分类器</strong>。它的预测很直观：基于在训练集中找到与待预测新记录最相似的数据记录。但是，将K设置为1很少是最佳选择；通常情况下，使用K&gt;1的最近邻居会获得更好的性能。</p>
<p>一般来说，如果<strong>K过低</strong>，我们可能会<strong>过拟合</strong>：包含了数据中的噪声。<strong>较高的K值</strong>提供了平滑效果，可以降低训练数据过拟合的风险。另一方面，如果<strong>K过高</strong>，我们可能会<strong>过度平滑</strong>数据，并错过KNN捕捉数据<strong>局部结构</strong>的能力，而这正是其主要优势之一。</p>
<p>在<strong>过拟合和过度平滑之间取得最佳平衡的K值</strong>通常通过<strong>准确率指标</strong>来确定，特别是使用<strong>保留（holdout）或验证数据</strong>时的准确率。关于最佳K值没有普适的规则——它很大程度上取决于数据的性质。对于<strong>结构高度清晰且噪声较小</strong>的数据，较小的K值效果最好。借用信号处理领域的一个术语，这类数据有时被称为具有<strong>高信噪比（SNR）</strong>。手写和语音识别数据集就是通常具有高信噪比的例子。对于<strong>噪声大且结构较少</strong>的数据（低信噪比数据），例如贷款数据，较大的K值更合适。通常，<strong>K值在1到20之间</strong>。为了避免平局，通常会选择一个奇数。</p>
<blockquote>
<p>通用注解：</p>
<p><strong>偏差-方差权衡</strong>（Bias-Variance Trade-off）</p>
<p><strong>过度平滑与过拟合之间的矛盾</strong>是<strong>偏差-方差权衡</strong>的一个例子，这是统计模型拟合中一个普遍存在的问题。</p>
<ul>
<li><strong>方差</strong>指的是由于选择<strong>训练数据</strong>而产生的建模误差；也就是说，如果你选择一组不同的训练数据，得到的模型也会不同。</li>
<li><strong>偏差</strong>指的是由于你没有正确识别底层的<strong>真实世界情景</strong>而产生的建模误差；即使你简单地增加更多的训练数据，这种误差也不会消失。</li>
</ul>
<p>当一个灵活的模型<strong>过拟合</strong>时，<strong>方差会增加</strong>。你可以通过使用一个更简单的模型来减少方差，但由于模型在模拟真实底层情况时失去了灵活性，<strong>偏差可能会增加</strong>。处理这种权衡的一种通用方法是<strong>交叉验证</strong>。有关更多细节，请参见第155页的“交叉验证”。</p>
</blockquote>
<h4 id="knn作为特征引擎"><strong>KNN作为特征引擎</strong></h4>
<p>KNN as a Feature Engine</p>
<p>KNN因其简单和直观的特性而受到欢迎。但在性能方面，<strong>KNN本身通常无法与更复杂的分类技术竞争</strong>。然而，在实际模型拟合中，可以通过<strong>分阶段的过程</strong>将KNN与其他分类技术结合，以添加<strong>“局部知识”</strong>：</p>
<ol type="1">
<li>对数据运行KNN，为每条记录得出一个分类（或一个类别的准概率）。</li>
<li>将这个结果作为一个<strong>新特征</strong>添加到记录中，然后用另一种分类方法对数据进行处理。这样，原始的预测变量就被使用了两次。</li>
</ol>
<p>你可能首先会想，这个过程是否会因为某些预测变量被使用两次而导致<strong>多重共线性</strong>问题（参见第172页的“多重共线性”）。这不是一个问题，因为被纳入第二阶段模型的信息是<strong>高度局部的</strong>，它只来自少数几条附近的记录，因此是<strong>附加信息而非冗余信息</strong>。</p>
<blockquote>
<p>通用注解：</p>
<p>你可以将这种分阶段使用KNN的方式看作是<strong>集成学习</strong>的一种形式，其中多个预测建模方法被结合在一起使用。它也可以被认为是<strong>特征工程</strong>的一种形式，其目的是推导出具有预测能力的特征（预测变量）。这通常需要手动审查数据；而KNN提供了一种<strong>相当自动化的方法</strong>来完成此任务。</p>
</blockquote>
<p>例如，考虑金县（King
County）的住房数据。在为待售房屋定价时，房地产经纪人会根据最近售出的类似房屋的价格来定价，这被称为“<strong>comps</strong>”（可比房屋）。从本质上讲，房地产经纪人正在进行<strong>手动版本的KNN</strong>：通过查看类似房屋的销售价格，他们可以估算出某一套房屋的售价。我们可以通过对最近的销售数据应用KNN，来为统计模型创建一个新特征，以模仿房地产专业人士的做法。预测值是销售价格，现有的预测变量可以包括位置、总平方英尺、结构类型、地块大小以及卧室和浴室的数量。我们通过KNN添加的新预测变量（特征）是每条记录的KNN预测值（类似于房地产经纪人的“comps”）。由于我们正在预测一个数值，这里使用的是K-最近邻的<strong>平均值</strong>而不是多数投票（这被称为<strong>KNN回归</strong>）。</p>
<p>类似地，对于贷款数据，我们可以创建代表贷款流程不同方面的新特征。例如，以下R代码将构建一个代表借款人<strong>信用度</strong>的特征：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">borrow_df <span class="operator">&lt;-</span> model.matrix<span class="punctuation">(</span><span class="operator">~</span> <span class="operator">-</span><span class="number">1</span> <span class="operator">+</span> dti <span class="operator">+</span> revol_bal <span class="operator">+</span> revol_util <span class="operator">+</span> open_acc <span class="operator">+</span></span><br><span class="line">delinq_2yrs_zero <span class="operator">+</span> pub_rec_zero<span class="punctuation">,</span> data<span class="operator">=</span>loan_data<span class="punctuation">)</span></span><br><span class="line">borrow_knn <span class="operator">&lt;-</span> knn<span class="punctuation">(</span>borrow_df<span class="punctuation">,</span> test<span class="operator">=</span>borrow_df<span class="punctuation">,</span> cl<span class="operator">=</span>loan_data<span class="punctuation">[</span><span class="punctuation">,</span> <span class="string">&#x27;outcome&#x27;</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">prob<span class="operator">=</span><span class="literal">TRUE</span><span class="punctuation">,</span> k<span class="operator">=</span><span class="number">20</span><span class="punctuation">)</span></span><br><span class="line">prob <span class="operator">&lt;-</span> <span class="built_in">attr</span><span class="punctuation">(</span>borrow_knn<span class="punctuation">,</span> <span class="string">&quot;prob&quot;</span><span class="punctuation">)</span></span><br><span class="line">borrow_feature <span class="operator">&lt;-</span> ifelse<span class="punctuation">(</span>borrow_knn <span class="operator">==</span> <span class="string">&#x27;default&#x27;</span><span class="punctuation">,</span> prob<span class="punctuation">,</span> <span class="number">1</span> <span class="operator">-</span> prob<span class="punctuation">)</span></span><br><span class="line">summary<span class="punctuation">(</span>borrow_feature<span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">   Min. 1st Qu. Median Mean 3rd Qu. Max.</span><br><span class="line">0.000  0.400  0.500 0.501  0.600  0.950</span><br></pre></td></tr></table></figure>
<p>使用
<strong><code>scikit-learn</code></strong>，我们使用训练模型的<strong><code>predict_proba</code></strong>方法来获取概率：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">predictors = [<span class="string">&#x27;dti&#x27;</span>, <span class="string">&#x27;revol_bal&#x27;</span>, <span class="string">&#x27;revol_util&#x27;</span>, <span class="string">&#x27;open_acc&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;delinq_2yrs_zero&#x27;</span>, <span class="string">&#x27;pub_rec_zero&#x27;</span>]</span><br><span class="line">outcome = <span class="string">&#x27;outcome&#x27;</span></span><br><span class="line">X = loan_data[predictors]</span><br><span class="line">y = loan_data[outcome]</span><br><span class="line">knn = KNeighborsClassifier(n_neighbors=<span class="number">20</span>)</span><br><span class="line">knn.fit(X, y)</span><br><span class="line">loan_data[<span class="string">&#x27;borrower_score&#x27;</span>] = knn.predict_proba(X)[:, <span class="number">1</span>]</span><br><span class="line">loan_data[<span class="string">&#x27;borrower_score&#x27;</span>].describe()</span><br></pre></td></tr></table></figure>
<p>其结果是一个新特征，它根据借款人的信用历史来预测其违约的可能性。</p>
<p><strong>关键思想</strong></p>
<ul>
<li><strong>K-最近邻（KNN）</strong>通过将一条记录分配给<strong>与其相似的记录所属的类别</strong>来进行分类。</li>
<li>相似性（距离）是通过<strong>欧几里得距离</strong>或其他相关度量来确定的。</li>
<li>用于比较记录的最近邻居数量
<strong>K</strong>，是通过使用不同的K值来衡量算法在训练数据上的表现来确定的。</li>
<li>通常，预测变量会进行<strong>标准化</strong>，以确保尺度较大的变量不会主导距离度量。</li>
<li>KNN常被用作<strong>预测建模的第一阶段</strong>，其预测值作为<strong>第二阶段（非KNN）建模的预测变量</strong>重新添加到数据中。</li>
</ul>
<h3 id="树模型"><strong>树模型</strong></h3>
<p>Tree Models</p>
<p><strong>树模型</strong>，也称为<strong>分类与回归树（CART）</strong>、<strong>决策树</strong>，或简称<strong>树</strong>，是由
Leo Breiman
等人在1984年首次开发的一种有效且流行的分类（和回归）方法。树模型及其更强大的后代——<strong>随机森林</strong>和<strong>提升树</strong>（参见第259页的“装袋法与随机森林”和第270页的“提升法”）——构成了数据科学中最广泛使用和最强大的回归与分类预测建模工具的基础。</p>
<p><strong>树的关键术语</strong></p>
<ul>
<li><p><strong>递归划分（Recursive partitioning）</strong>
反复地将数据进行划分和再划分，目的是使每个最终子划分中的结果尽可能<strong>同质（homogeneous）</strong>。</p></li>
<li><p><strong>分割值（Split value）</strong>
一个预测变量的值，它将记录分为两组：一组是该预测变量的值<strong>小于</strong>分割值的记录，另一组是<strong>大于</strong>分割值的记录。</p></li>
<li><p><strong>节点（Node）</strong>
在决策树或相应的分支规则集中，节点是分割值的图形或规则表示。</p></li>
<li><p><strong>叶子（Leaf）</strong>
一组if-then规则或树的分支的末端——将你带到该叶子的规则为树中的任何记录提供了一条分类规则。</p></li>
<li><p><strong>损失（Loss）</strong>
在分割过程的某个阶段中，错误分类的数量；损失越多，<strong>不纯度（impurity）</strong>越高。</p></li>
<li><p><strong>不纯度（Impurity）</strong>
数据子分区中各类别的混合程度（混合程度越高，不纯度越高）。
同义词：<strong>异质性（Heterogeneity）</strong>
反义词：<strong>同质性（Homogeneity）</strong>、<strong>纯度（purity）</strong></p></li>
<li><p><strong>剪枝（Pruning）</strong>
对一棵完全生长的树进行逐步修剪其分支的过程，以<strong>减少过拟合</strong>。</p></li>
</ul>
<p>树模型是一组“如果-那么-否则”的规则，易于理解和实现。与线性和逻辑回归相比，树模型能够<strong>发现数据中对应于复杂交互的隐藏模式</strong>。然而，与KNN或朴素贝叶斯不同的是，简单的树模型可以用易于解释的预测变量关系来表达。</p>
<blockquote>
<p>警告：</p>
<p><strong>运筹学中的决策树</strong>（Decision Trees in Operations
Research）</p>
<p>在<strong>决策科学</strong>和<strong>运筹学</strong>中，“决策树”这个术语有一个不同（且更古老）的含义，它指的是一种<strong>人类决策分析过程</strong>。在这个含义下，决策点、可能的结果及其估计概率被呈现在一个分支图中，并选择具有<strong>最大预期价值</strong>的决策路径。</p>
</blockquote>
<h4 id="一个简单示例"><strong>一个简单示例</strong></h4>
<p>A Simple Example</p>
<p>在 R 中拟合树模型主要有两个包：<code>rpart</code> 和
<code>tree</code>。使用 <code>rpart</code>
包，我们可以对3000条贷款数据记录的样本进行模型拟合，使用
<code>payment_inc_ratio</code> 和 <code>borrower_score</code>
变量（数据描述参见第238页的“K-最近邻”）：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">library<span class="punctuation">(</span>rpart<span class="punctuation">)</span></span><br><span class="line">loan_tree <span class="operator">&lt;-</span> rpart<span class="punctuation">(</span>outcome <span class="operator">~</span> borrower_score <span class="operator">+</span> payment_inc_ratio<span class="punctuation">,</span></span><br><span class="line">data<span class="operator">=</span>loan3000<span class="punctuation">,</span> control<span class="operator">=</span>rpart.control<span class="punctuation">(</span>cp<span class="operator">=</span><span class="number">0.005</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">plot<span class="punctuation">(</span>loan_tree<span class="punctuation">,</span> uniform<span class="operator">=</span><span class="literal">TRUE</span><span class="punctuation">,</span> margin<span class="operator">=</span><span class="number">0.05</span><span class="punctuation">)</span></span><br><span class="line">text<span class="punctuation">(</span>loan_tree<span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<p><code>sklearn.tree.DecisionTreeClassifier</code>
提供了决策树的实现。<code>dmba</code>
包提供了一个方便的函数，用于在Jupyter Notebook中创建可视化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">predictors = [<span class="string">&#x27;borrower_score&#x27;</span>, <span class="string">&#x27;payment_inc_ratio&#x27;</span>]</span><br><span class="line">outcome = <span class="string">&#x27;outcome&#x27;</span></span><br><span class="line">X = loan3000[predictors]</span><br><span class="line">y = loan3000[outcome]</span><br><span class="line">loan_tree = DecisionTreeClassifier(random_state=<span class="number">1</span>, criterion=<span class="string">&#x27;entropy&#x27;</span>,</span><br><span class="line">min_impurity_decrease=<span class="number">0.003</span>)</span><br><span class="line">loan_tree.fit(X, y)</span><br><span class="line">plotDecisionTree(loan_tree, feature_names=predictors,</span><br><span class="line">class_names=loan_tree.classes_)</span><br></pre></td></tr></table></figure>
<p>生成的树如图6-3所示。由于实现方式不同，你会发现 R 和 Python
的结果不完全相同；这是正常的。 .
这些分类规则是通过遍历一棵<strong>分层树</strong>来确定的，从<strong>根节点</strong>开始，如果节点条件为真则向左移动，否则向右移动，直到到达<strong>叶子节点</strong>。通常，树是倒置绘制的，根节点在顶部，叶子节点在底部。例如，如果一笔贷款的
<code>borrower_score</code> 为0.6，<code>payment_inc_ratio</code>
为8.0，我们最终会到达最左边的叶子节点，并预测该贷款将<strong>已还清</strong>。</p>
<p><img src="/img3/面向数据科学家的实用统计学/F6.3.png" alt="F6.3" style="zoom:50%;" /></p>
<p>在 R 中也可以轻松生成一个格式优美的树文本版本：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loan_tree</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">n= 3000</span><br><span class="line"></span><br><span class="line">node), split, n, loss, yval, (yprob)</span><br><span class="line">      * denotes terminal node</span><br><span class="line">1) root 3000 1445 paid off (0.5183333 0.4816667)</span><br><span class="line">2) borrower_score&gt;=0.575 878 261 paid off (0.7027335 0.2972665) *</span><br><span class="line">3) borrower_score&lt; 0.575 2122 938 default (0.4420358 0.5579642)</span><br><span class="line">6) borrower_score&gt;=0.375 1639 802 default (0.4893228 0.5106772)</span><br><span class="line">12) payment_inc_ratio&lt; 10.42265 1157 547 paid off (0.5272256 0.4727744)</span><br><span class="line">24) payment_inc_ratio&lt; 4.42601 334 139 paid off (0.5838323 0.4161677) *</span><br><span class="line">25) payment_inc_ratio&gt;=4.42601 823 408 paid off (0.5042527 0.4957473)</span><br><span class="line">50) borrower_score&gt;=0.475 418 190 paid off (0.5454545 0.4545455) *</span><br><span class="line">51) borrower_score&lt; 0.475 405 187 default (0.4617284 0.5382716) *</span><br><span class="line">13) payment_inc_ratio&gt;=10.42265 482 192 default (0.3983402 0.6016598) *</span><br><span class="line">7) borrower_score&lt; 0.375 483 136 default (0.2815735 0.7184265) *</span><br></pre></td></tr></table></figure>
<p>树的深度由缩进表示。每个节点对应于由该分区中主要结果决定的<strong>临时分类</strong>。<strong>“损失”（loss）</strong>是在一个分区中，由该临时分类产生的错误分类数量。例如，在节点2中，总共878条记录中有261条是错误分类的。括号中的值分别对应于已还清贷款和违约贷款的比例。例如，在预测为违约的节点13中，超过60%的记录是违约贷款。</p>
<p><code>scikit-learn</code>
文档描述了如何创建决策树模型的文本表示。我们在 <code>dmba</code>
包中包含了一个方便的函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(textDecisionTree(loan_tree))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">--</span><br><span class="line">node=0 test node: go to node 1 if 0 &lt;= 0.5750000178813934 else to node 6</span><br><span class="line">node=1 test node: go to node 2 if 0 &lt;= 0.32500000298023224 else to node 3</span><br><span class="line">node=2 leaf node: [[0.785, 0.215]]</span><br><span class="line">node=3 test node: go to node 4 if 1 &lt;= 10.42264986038208 else to node 5</span><br><span class="line">node=4 leaf node: [[0.488, 0.512]]</span><br><span class="line">node=5 leaf node: [[0.613, 0.387]]</span><br><span class="line">node=6 test node: go to node 7 if 1 &lt;= 9.19082498550415 else to node 10</span><br><span class="line">node=7 test node: go to node 8 if 0 &lt;= 0.7249999940395355 else to node 9</span><br><span class="line">node=8 leaf node: [[0.247, 0.753]]</span><br><span class="line">node=9 leaf node: [[0.073, 0.927]]</span><br><span class="line">node=10 leaf node: [[0.457, 0.543]]</span><br></pre></td></tr></table></figure>
<h4 id="递归划分算法"><strong>递归划分算法</strong></h4>
<p>The Recursive Partitioning Algorithm</p>
<p>用于构建决策树的算法，称为<strong>递归划分</strong>，简单直观。该算法反复使用<strong>最能将数据划分为相对同质分区的预测变量值</strong>来对数据进行划分。图6-4展示了图6-3中树创建的分区。第一个规则（用规则1表示）是
<code>borrower_score &gt;= 0.575</code>，它将图的右侧部分分割出来。第二个规则是
<code>borrower_score &lt; 0.375</code>，它将左侧部分分割出来。</p>
<p><img src="/img3/面向数据科学家的实用统计学/F6.4.png" alt="F6.4" style="zoom:50%;" /></p>
<p>假设我们有一个响应变量 <span class="math inline">\(Y\)</span> 和一组
<span class="math inline">\(P\)</span> 个预测变量 <span
class="math inline">\(X_j\)</span> (其中 <span
class="math inline">\(j=1, \dots, P\)</span>)。对于一个记录分区 <span
class="math inline">\(A\)</span>，递归划分会找到最佳方式将其划分为两个子分区：</p>
<ol type="1">
<li>对于每个预测变量 <span class="math inline">\(X_j\)</span>：
<ol type="a">
<li>对于 <span class="math inline">\(X_j\)</span> 的每个值 <span
class="math inline">\(s_j\)</span>：
<ol type="i">
<li>将分区 <span class="math inline">\(A\)</span> 中 <span
class="math inline">\(X_j\)</span> 值小于 <span
class="math inline">\(s_j\)</span> 的记录作为一个分区，其余 <span
class="math inline">\(X_j\)</span> 值大于或等于 <span
class="math inline">\(s_j\)</span> 的记录作为另一个分区。</li>
<li>测量 <span class="math inline">\(A\)</span>
的每个子分区内类别的<strong>同质性</strong>。</li>
</ol></li>
<li>选择能产生<strong>最大分区内类别同质性</strong>的 <span
class="math inline">\(s_j\)</span> 值。</li>
</ol></li>
<li>选择能产生<strong>最大分区内类别同质性</strong>的变量 <span
class="math inline">\(X_j\)</span> 和分割值 <span
class="math inline">\(s_j\)</span>。</li>
</ol>
<p>现在是<strong>递归</strong>部分： 1. 用整个数据集初始化 <span
class="math inline">\(A\)</span>。 2. 应用划分算法将 <span
class="math inline">\(A\)</span> 划分为两个子分区 <span
class="math inline">\(A_1\)</span> 和 <span
class="math inline">\(A_2\)</span>。 3. 对子分区 <span
class="math inline">\(A_1\)</span> 和 <span
class="math inline">\(A_2\)</span> 重复步骤2。 4.
当无法再进行能够充分改善分区同质性的划分时，算法终止。</p>
<p>最终结果是对数据的划分，如在<span
class="math inline">\(P\)</span>维空间中的图6-4所示，每个分区根据该分区中响应变量的<strong>多数投票</strong>来预测结果为0或1。</p>
<blockquote>
<p>通用注解：</p>
<p>除了二元0/1预测，树模型还可以根据分区中0和1的数量来产生<strong>概率估计</strong>。该估计值就是分区中1的数量除以该分区中的观察记录总数：
<span class="math display">\[
P(Y=1) = \frac{分区中1的数量}{分区的大小}
\]</span> 然后，估计的 <span class="math inline">\(P(Y=1)\)</span>
可以转换为二元决策；例如，如果 <span class="math inline">\(P(Y=1) &gt;
0.5\)</span>，则将估计值设为1。</p>
</blockquote>
<h4 id="测量同质性或不纯度"><strong>测量同质性或不纯度</strong></h4>
<p>Measuring Homogeneity or Impurity</p>
<p>树模型会递归地创建分区（记录集）<span
class="math inline">\(A\)</span>，并预测结果 <span
class="math inline">\(Y=0\)</span> 或 <span
class="math inline">\(Y=1\)</span>。从前面的算法中可以看出，我们需要一种方法来测量分区内的<strong>同质性</strong>，也称为<strong>类别纯度</strong>。或者等价地，我们需要测量分区中的<strong>不纯度</strong>。预测的准确率是该分区内被错误分类的记录比例
<span
class="math inline">\(p\)</span>，其范围从0（完美）到0.5（纯随机猜测）。</p>
<p>事实证明，<strong>准确率不是一个好的不纯度度量</strong>。相反，两个常见的不纯度度量是
<strong>Gini不纯度（Gini impurity）</strong>和<strong>信息熵（entropy of
information）</strong>。虽然这些（以及其他）不纯度度量也适用于具有两个以上类别的分类问题，但我们这里重点关注<strong>二元情况</strong>。</p>
<p>一个记录集 <span class="math inline">\(A\)</span> 的
Gini不纯度是：</p>
<p><span class="math display">\[
I_G(A) = p(1 - p)
\]</span></p>
<p>熵度量由下式给出：</p>
<p><span class="math display">\[
I_E(A) = -p \log_2 p - (1-p) \log_2 (1-p)
\]</span></p>
<p>图6-5显示，<strong>基尼不纯度</strong>（重新缩放后）和<strong>熵</strong>度量是相似的，但<strong>熵</strong>对于中等到高准确率会给出<strong>更高</strong>的不纯度分数。</p>
<p><img src="/img3/面向数据科学家的实用统计学/F6.5.png" alt="F6.5" style="zoom:33%;" /></p>
<blockquote>
<p>警告：</p>
<p><strong>Gini系数</strong></p>
<p>Gini不纯度不应与<strong>Gini系数</strong>相混淆。它们代表相似的概念，但<strong>Gini系数</strong>仅限于二元分类问题，并且与<strong>AUC指标</strong>相关（参见第226页的“AUC”）。</p>
</blockquote>
<p>不纯度度量被用于前面描述的<strong>分割算法</strong>中。对于数据的每一个提议分区，都会测量由该分割所产生的每个分区的不纯度。然后计算一个<strong>加权平均值</strong>，并在每个阶段选择产生最低加权平均值的分区。</p>
<h4 id="阻止树继续生长"><strong>阻止树继续生长</strong></h4>
<p>Stopping the Tree from Growing</p>
<p>随着树变得越来越大，其分割规则也变得越来越详细，树会逐渐从识别数据中真实可靠的“大”规则，转变为反映噪声的“微小”规则。一棵<strong>完全生长的树</strong>会导致<strong>完全纯净的叶子</strong>，因此在分类其所训练的数据时会达到100%的准确率。当然，这种准确率是<strong>虚幻的</strong>——我们<strong>过拟合</strong>了数据（参见第247页的“偏差-方差权衡”），拟合的是训练数据中的噪声，而不是我们想要在新数据中识别的信号。</p>
<p>我们需要某种方法来决定<strong>何时停止树的生长</strong>，使其处于一个能够<strong>泛化到新数据</strong>的阶段。在R和Python中有多种方法可以阻止分割：</p>
<ul>
<li><strong>如果生成的子分区太小，或终端叶子太小，就避免进行分割。</strong>
在 <code>rpart</code> (R) 中，这些约束分别由参数 <code>minsplit</code>
和 <code>minbucket</code> 控制，默认值分别为20和7。在 Python 的
<code>DecisionTreeClassifier</code> 中，我们可以使用参数
<code>min_samples_split</code> (默认2) 和 <code>min_samples_leaf</code>
(默认1) 来控制。</li>
<li><strong>如果新的分区不能“显著”减少不纯度，则不进行分割。</strong> 在
<code>rpart</code> 中，这由<strong>复杂度参数</strong> <code>cp</code>
控制，<code>cp</code> 是衡量树复杂度的指标——复杂度越高，<code>cp</code>
值越大。实际上，<code>cp</code>
被用来通过对树中额外的复杂度（分割）施加惩罚来限制树的生长。<code>DecisionTreeClassifier</code>
(Python) 有一个参数
<code>min_impurity_decrease</code>，它根据加权不纯度减少值来限制分割。在这个参数中，较小的值将导致更复杂的树。</li>
</ul>
<p>这些方法涉及<strong>主观规则</strong>，可以用于探索性工作，但我们<strong>不容易确定其最佳值</strong>（即，能最大化在新数据上的预测准确率的值）。我们需要将<strong>交叉验证</strong>与<strong>系统性地改变模型参数</strong>或通过<strong>剪枝</strong>来修改树的方法相结合。</p>
<p><strong>在 R 中控制树的复杂度</strong></p>
<p>利用复杂度参数
<strong>cp</strong>，我们可以估计出哪种大小的树在新数据上表现最佳。如果
<strong>cp</strong>
太小，树就会<strong>过拟合</strong>数据，拟合了噪声而非信号。另一方面，如果
<strong>cp</strong>
太大，树就会太小，<strong>预测能力</strong>也会很弱。<code>rpart</code>
的默认值为0.01，不过对于大型数据集来说，这个值可能太大。在前面的例子中，我们将
<strong>cp</strong>
设置为0.005，因为默认值导致了只有一个分割的树。在探索性分析中，简单地尝试几个值就足够了。</p>
<p>确定最佳 <strong>cp</strong>
值是<strong>偏差-方差权衡</strong>的一个例子。估算一个好的
<strong>cp</strong>
值的最常用方法是通过<strong>交叉验证</strong>（参见第155页的“交叉验证”）：
1.
将数据划分为<strong>训练集</strong>和<strong>验证集</strong>（保留集）。
2. 用训练数据生长树。 3. 逐步修剪它，在每一步记录 <strong>cp</strong>
值（使用训练数据）。 4.
记录对应于验证数据上<strong>最小误差（损失）</strong>的
<strong>cp</strong> 值。 5.
重新将数据划分为训练集和验证集，并重复树的生长、修剪和
<strong>cp</strong> 值记录过程。 6.
反复执行此操作，并对每棵树反映最小误差的 <strong>cp</strong> 值取平均。
7. 回到原始数据或未来的数据上，生长一棵树，并在最佳 <strong>cp</strong>
值处停止。</p>
<p>在 <code>rpart</code> 中，您可以使用参数 <code>cptable</code>
生成一个包含 <strong>cp</strong>
值及其相关<strong>交叉验证误差</strong>（在 R 中为
<strong>xerror</strong>）的表格，您可以从中确定具有最低交叉验证误差的
<strong>cp</strong> 值。</p>
<p><strong>在 Python 中控制树的复杂度</strong></p>
<p>在 <code>scikit-learn</code>
的决策树实现中，<strong>既没有复杂度参数，也没有剪枝功能</strong>。解决方案是使用<strong>网格搜索</strong>来组合不同的参数值。例如，我们可以将
<code>max_depth</code> 的范围设为5到30，<code>min_samples_split</code>
在20到100之间变化。<code>scikit-learn</code> 中的
<strong><code>GridSearchCV</code></strong>
方法是一种方便的方式，可以将穷尽搜索所有组合与交叉验证结合起来。然后，根据交叉验证的模型性能选择<strong>最优参数集</strong>。</p>
<h4 id="预测连续值"><strong>预测连续值</strong></h4>
<p>Predicting a Continuous Value</p>
<p>用树模型预测连续值（也称为<strong>回归</strong>）遵循同样的逻辑和程序，只是<strong>不纯度</strong>的测量方式不同。在每个子分区中，不纯度是通过与<strong>均值的平方偏差</strong>（平方误差）来测量的，并且预测性能是根据每个分区中<strong>均方根误差（RMSE）</strong>的平方根来评估的（参见第153页的“评估模型”）。</p>
<p><code>scikit-learn</code> 提供了
<code>sklearn.tree.DecisionTreeRegressor</code>
方法来训练决策树回归模型。</p>
<h4 id="树模型的应用方式"><strong>树模型的应用方式</strong></h4>
<p>How Trees Are Used</p>
<p>组织中预测建模人员面临的一大障碍是，他们所使用的方法被认为是<strong>“黑箱”</strong>，这会引起组织内其他部门的反对。在这方面，树模型有两个吸引人的优点：</p>
<ul>
<li><strong>树模型提供了一个可视化工具</strong>来探索数据，从而了解哪些变量是重要的，以及它们之间是如何相互关联的。树可以捕捉预测变量之间的<strong>非线性关系</strong>。</li>
<li><strong>树模型提供了一套规则</strong>，可以有效地传达给非专业人士，以便进行实施或“推销”数据挖掘项目。</li>
</ul>
<p>然而，在预测方面，<strong>利用多个树的结果通常比只使用单个树更强大</strong>。特别是，<strong>随机森林</strong>和<strong>提升树</strong>算法几乎总是提供卓越的预测准确性和性能（参见第259页的“装袋法与随机森林”和第270页的“提升法”），但<strong>单个树的上述优点</strong>也随之<strong>丧失</strong>了。</p>
<p><strong>关键思想</strong></p>
<ul>
<li><strong>决策树</strong>生成一系列规则，用于分类或预测结果。</li>
<li>这些规则对应于对数据进行<strong>连续的分区</strong>。</li>
<li>每个分区或分割都引用一个特定的预测变量值，并将数据分为该预测变量值<strong>高于或低于</strong>该分割值的记录。</li>
<li>在每个阶段，树算法选择能够<strong>最小化每个子分区内结果不纯度</strong>的分割点。</li>
<li>当无法再进行分割时，树就<strong>完全生长</strong>了，每个<strong>终端节点</strong>或<strong>叶子节点</strong>都只包含单一类别的记录；遵循该规则（分割）路径的新案例将被分配到该类别。</li>
<li>一棵完全生长的树会<strong>过拟合</strong>数据，必须进行<strong>剪枝</strong>，以使其捕捉<strong>信号</strong>而非<strong>噪声</strong>。</li>
<li><strong>多树算法</strong>，如随机森林和提升树，能提供更好的预测性能，但它们<strong>失去了单棵树基于规则的可沟通能力</strong>。</li>
</ul>
<h3 id="装袋法与随机森林"><strong>装袋法与随机森林</strong></h3>
<p>Bagging and the Random Forest</p>
<p>1906年，统计学家弗朗西斯·高尔顿爵士参观英格兰的一个乡村集市，那里正在举行一场猜测展览公牛净重的比赛。共有800个猜测，尽管单个猜测差异很大，但其平均值和中位数与公牛的真实重量相差不到1%。詹姆斯·苏洛维基在他的著作《群体的智慧》（The
Wisdom of Crowds, Doubleday,
2004）中探讨了这一现象。这一原理也适用于预测模型：<strong>对多个模型进行平均（或多数投票）</strong>——即<strong>模型集成</strong>——结果证明比仅选择一个模型<strong>更为准确</strong>。</p>
<p><strong>装袋法与随机森林的关键术语</strong></p>
<ul>
<li><p><strong>集成（Ensemble）</strong> 通过使用一组模型来形成预测。
同义词：<strong>模型平均（Model averaging）</strong></p></li>
<li><p><strong>装袋法（Bagging）</strong>
一种通过对数据进行自举来形成一组模型的通用技术。
同义词：<strong>自举聚合（Bootstrap aggregation）</strong></p></li>
<li><p><strong>随机森林（Random forest）</strong>
一种基于决策树模型的装袋估计方法。 同义词：<strong>装袋决策树（Bagged
decision trees）</strong></p></li>
<li><p><strong>变量重要性（Variable importance）</strong>
衡量预测变量在模型性能中的重要性的指标。</p></li>
</ul>
<p>集成方法已被应用于许多不同的建模方法，最著名的例子是<strong>Netflix
Prize</strong>，Netflix曾悬赏100万美元，奖励任何能够将预测顾客对电影评分的准确性提高10%的参赛者。集成的简单版本如下：
1. 开发一个预测模型，并记录给定数据集的预测结果。 2.
在相同数据上为多个模型重复此步骤。 3.
对于每条待预测的记录，对其预测结果取<strong>平均值</strong>（或加权平均值，或多数投票）。</p>
<p>集成方法最系统且最有效地应用于<strong>决策树</strong>。集成树模型非常强大，可以帮助我们以相对较少的努力构建出优秀的预测模型。</p>
<p>除了简单的集成算法，集成模型还有两个主要变体：<strong>装袋法</strong>和<strong>提升法</strong>。在集成树模型中，它们分别被称为<strong>随机森林模型</strong>和<strong>提升树模型</strong>。本节重点介绍<strong>装袋法</strong>；提升法将在第270页的“提升法”中进行讨论。</p>
<h4 id="bagging-方法"><strong>Bagging 方法</strong></h4>
<p><strong>Bagging</strong>，是“bootstrap
aggregating”（自助聚合）的缩写，由 Leo Breiman
于1994年提出。假设我们有一个响应变量 <span
class="math inline">\(Y\)</span> 和 <span
class="math inline">\(P\)</span> 个预测变量 <span
class="math inline">\(\mathbf{X} = (X_1, X_2, \dots, X_P)\)</span>，以及
<span class="math inline">\(N\)</span> 条记录。</p>
<p>Bagging
类似于集成学习的基本算法，不同之处在于，<strong>不是将不同的模型拟合到相同的数据上，而是将每个新模型拟合到一个自助（bootstrap）重采样的数据上</strong>。这里更正式地介绍该算法：
1. 初始化要拟合的模型数量 <span class="math inline">\(M\)</span>
和要选择的记录数量 <span class="math inline">\(n\)</span>（<span
class="math inline">\(n &lt; N\)</span>）。设置迭代计数器 <span
class="math inline">\(m=1\)</span>。 2.
从训练数据中<strong>有放回地</strong>抽取 <span
class="math inline">\(n\)</span> 条记录，形成一个子样本 <span
class="math inline">\(Y_m\)</span> 和 <span
class="math inline">\(\mathbf{X}_m\)</span>（即“<strong>包</strong>”）。
3. 使用 <span class="math inline">\(Y_m\)</span> 和 <span
class="math inline">\(\mathbf{X}_m\)</span>
训练一个模型，以创建一组决策规则 <span class="math inline">\(\hat
f_m(\mathbf{X})\)</span>。 4. 增加模型计数器 <span
class="math inline">\(m = m + 1\)</span>。如果 <span
class="math inline">\(m \le M\)</span>，返回步骤2。</p>
<p>在 <span class="math inline">\(\hat f_m\)</span> 预测 <span
class="math inline">\(Y=1\)</span> 的概率的情况下，bagging
估计量由下式给出：</p>
<p><span class="math display">\[
\hat f(\mathbf{X}) = \frac{1}{M} (\hat f_1(\mathbf{X}) + \hat
f_2(\mathbf{X}) + \dots + \hat f_M(\mathbf{X}))
\]</span></p>
<h4 id="随机森林"><strong>随机森林</strong></h4>
<p>Random Forest</p>
<p>随机森林是在决策树上应用<strong>装袋法</strong>的一种重要扩展：除了对记录进行抽样，该算法<strong>也对变量进行抽样</strong>。在传统的决策树中，为了确定如何创建一个子分区
<span
class="math inline">\(A\)</span>，算法会通过最小化<strong>Gini不纯度</strong>等标准来选择变量和分割点（参见第254页的“测量同质性或不纯度”）。而在随机森林中，在算法的每个阶段，变量的选择<strong>被限制在一个随机的变量子集中</strong>。</p>
<p>与基本的树算法（参见第252页的“递归划分算法”）相比，随机森林算法增加了两个步骤：前面讨论过的装袋法（参见第259页的“装袋法与随机森林”），以及在每次分割时对变量进行的<strong>自举抽样</strong>：</p>
<ol type="1">
<li>从记录中进行<strong>自举（有放回）抽样</strong>。</li>
<li>对于第一次分割，<strong>无放回地随机抽取 <span
class="math inline">\(p &lt; P\)</span> 个变量</strong>。</li>
<li>对于每个抽样的变量 <span class="math inline">\(X_{j_1}, X_{j_2},
\dots, X_{j_p}\)</span>，应用分割算法：
<ol type="a">
<li>对于每个 <span class="math inline">\(X_{j_k}\)</span> 的值 <span
class="math inline">\(s_{j_k}\)</span>：</li>
<li>将分区 <span class="math inline">\(A\)</span> 中 <span
class="math inline">\(X_{j_k}\)</span> 值小于 <span
class="math inline">\(s_{j_k}\)</span> 的记录划分为一个分区，其余 <span
class="math inline">\(X_{j_k}\)</span> 值大于或等于 <span
class="math inline">\(s_{j_k}\)</span> 的记录作为另一个分区。</li>
</ol>
<ol start="2" type="i">
<li>测量 <span class="math inline">\(A\)</span>
的每个子分区内的类别<strong>同质性</strong>。</li>
</ol>
<ol start="2" type="a">
<li>选择能产生<strong>最大分区内类别同质性</strong>的 <span
class="math inline">\(s_{j_k}\)</span> 值。</li>
</ol></li>
<li>选择能产生<strong>最大分区内类别同质性</strong>的变量 <span
class="math inline">\(X_{j_k}\)</span> 和分割值 <span
class="math inline">\(s_{j_k}\)</span>。</li>
<li>继续进行下一次分割，从步骤2开始重复之前的步骤。</li>
<li>继续进行额外的分割，遵循相同的程序，直到树完全生长。</li>
<li>回到步骤1，进行另一次自举子抽样，并重新开始整个过程。</li>
</ol>
<p>在每一步中要抽取多少变量？经验法则是选择 <span
class="math inline">\(\sqrt{P}\)</span>，其中 <span
class="math inline">\(P\)</span>
是预测变量的数量。<code>randomForest</code> 包在 R
中实现了随机森林。以下代码将此包应用于贷款数据（数据描述参见第238页的“K-最近邻”）：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rf <span class="operator">&lt;-</span> randomForest<span class="punctuation">(</span>outcome <span class="operator">~</span> borrower_score <span class="operator">+</span> payment_inc_ratio<span class="punctuation">,</span></span><br><span class="line">data<span class="operator">=</span>loan3000<span class="punctuation">)</span></span><br><span class="line">rf</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Call:</span><br><span class="line"> randomForest(formula = outcome ~ borrower_score + payment_inc_ratio,</span><br><span class="line"> data = loan3000)</span><br><span class="line">               Type of random forest: classification</span><br><span class="line">                     Number of trees: 500</span><br><span class="line">No. of variables tried at each split: 1</span><br><span class="line"></span><br><span class="line">        OOB estimate of  error rate: 39.17%</span><br><span class="line">Confusion matrix:</span><br><span class="line">         default paid off class.error</span><br><span class="line">default      873      572   0.39584775</span><br><span class="line">paid off     603      952   0.38778135</span><br></pre></td></tr></table></figure>
<p>在 Python 中，我们使用
<code>sklearn.ensemble.RandomForestClassifier</code> 方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">predictors = [<span class="string">&#x27;borrower_score&#x27;</span>, <span class="string">&#x27;payment_inc_ratio&#x27;</span>]</span><br><span class="line">outcome = <span class="string">&#x27;outcome&#x27;</span></span><br><span class="line">X = loan3000[predictors]</span><br><span class="line">y = loan3000[outcome]</span><br><span class="line">rf = RandomForestClassifier(n_estimators=<span class="number">500</span>, random_state=<span class="number">1</span>, oob_score=<span class="literal">True</span>)</span><br><span class="line">rf.fit(X, y)</span><br></pre></td></tr></table></figure>
<p>默认情况下，会训练500棵树。由于预测变量集中只有两个变量，算法在每个阶段随机选择一个变量进行分割（即，大小为1的自举子样本）。</p>
<p><strong>袋外误差（Out-of-bag, OOB）</strong>
是指训练好的模型应用于<strong>未被用于该树训练</strong>的数据时的<strong>误差率</strong>。利用模型的输出，可以在
R
中绘制<strong>OOB误差</strong>与随机森林中<strong>树的数量</strong>之间的关系图：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">error_df <span class="operator">=</span> data.frame<span class="punctuation">(</span>error_rate<span class="operator">=</span>rf<span class="operator">$</span>err.rate<span class="punctuation">[</span><span class="punctuation">,</span><span class="string">&#x27;OOB&#x27;</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">num_trees<span class="operator">=</span><span class="number">1</span><span class="operator">:</span>rf<span class="operator">$</span>ntree<span class="punctuation">)</span></span><br><span class="line">ggplot<span class="punctuation">(</span>error_df<span class="punctuation">,</span> aes<span class="punctuation">(</span>x<span class="operator">=</span>num_trees<span class="punctuation">,</span> y<span class="operator">=</span>error_rate<span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">geom_line<span class="punctuation">(</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<p><code>RandomForestClassifier</code>
的实现没有简单的方法来获取作为随机森林中树数量函数的袋外估计。我们可以训练一系列树数量递增的分类器，并跟踪
<code>oob_score_</code> 值。然而，这种方法效率不高：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">n_estimator = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">20</span>, <span class="number">510</span>, <span class="number">5</span>))</span><br><span class="line">oobScores = []</span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> n_estimator:</span><br><span class="line">rf = RandomForestClassifier(n_estimators=n, criterion=<span class="string">&#x27;entropy&#x27;</span>,</span><br><span class="line">max_depth=<span class="number">5</span>, random_state=<span class="number">1</span>, oob_score=<span class="literal">True</span>)</span><br><span class="line">rf.fit(X, y)</span><br><span class="line">oobScores.append(rf.oob_score_)</span><br><span class="line">df = pd.DataFrame(&#123; <span class="string">&#x27;n&#x27;</span>: n_estimator, <span class="string">&#x27;oobScore&#x27;</span>: oobScores &#125;)</span><br><span class="line">df.plot(x=<span class="string">&#x27;n&#x27;</span>, y=<span class="string">&#x27;oobScore&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/img3/面向数据科学家的实用统计学/F6.6.png" alt="F6.6" style="zoom:33%;" /></p>
<p>结果如图6-6所示。
误差率从超过0.44迅速下降，随后稳定在0.385左右。预测值可以通过
<code>predict</code> 函数获得，并在 R 中绘制如下：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">pred <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>rf<span class="punctuation">,</span> prob<span class="operator">=</span><span class="literal">TRUE</span><span class="punctuation">)</span></span><br><span class="line">rf_df <span class="operator">&lt;-</span> cbind<span class="punctuation">(</span>loan3000<span class="punctuation">,</span> pred <span class="operator">=</span> pred<span class="punctuation">)</span></span><br><span class="line">ggplot<span class="punctuation">(</span>data<span class="operator">=</span>rf_df<span class="punctuation">,</span> aes<span class="punctuation">(</span>x<span class="operator">=</span>borrower_score<span class="punctuation">,</span> y<span class="operator">=</span>payment_inc_ratio<span class="punctuation">,</span></span><br><span class="line">shape<span class="operator">=</span>pred<span class="punctuation">,</span> color<span class="operator">=</span>pred<span class="punctuation">,</span> size<span class="operator">=</span>pred<span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">geom_point<span class="punctuation">(</span>alpha<span class="operator">=</span><span class="number">.8</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">scale_color_manual<span class="punctuation">(</span>values <span class="operator">=</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;paid off&#x27;</span><span class="operator">=</span><span class="string">&#x27;#b8e186&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;default&#x27;</span><span class="operator">=</span><span class="string">&#x27;#d95f02&#x27;</span><span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">scale_shape_manual<span class="punctuation">(</span>values <span class="operator">=</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;paid off&#x27;</span><span class="operator">=</span><span class="number">0</span><span class="punctuation">,</span> <span class="string">&#x27;default&#x27;</span><span class="operator">=</span><span class="number">1</span><span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">scale_size_manual<span class="punctuation">(</span>values <span class="operator">=</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;paid off&#x27;</span><span class="operator">=</span><span class="number">0.5</span><span class="punctuation">,</span> <span class="string">&#x27;default&#x27;</span><span class="operator">=</span><span class="number">2</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<p>在 Python 中，我们可以创建类似的图：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">predictions = X.copy()</span><br><span class="line">predictions[<span class="string">&#x27;prediction&#x27;</span>] = rf.predict(X)</span><br><span class="line">predictions.head()</span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">predictions.loc[predictions.prediction==<span class="string">&#x27;paid off&#x27;</span>].plot(</span><br><span class="line">x=<span class="string">&#x27;borrower_score&#x27;</span>, y=<span class="string">&#x27;payment_inc_ratio&#x27;</span>, style=<span class="string">&#x27;.&#x27;</span>,</span><br><span class="line">markerfacecolor=<span class="string">&#x27;none&#x27;</span>, markeredgecolor=<span class="string">&#x27;C1&#x27;</span>, ax=ax)</span><br><span class="line">predictions.loc[predictions.prediction==<span class="string">&#x27;default&#x27;</span>].plot(</span><br><span class="line">x=<span class="string">&#x27;borrower_score&#x27;</span>, y=<span class="string">&#x27;payment_inc_ratio&#x27;</span>, style=<span class="string">&#x27;o&#x27;</span>,</span><br><span class="line">markerfacecolor=<span class="string">&#x27;none&#x27;</span>, markeredgecolor=<span class="string">&#x27;C0&#x27;</span>, ax=ax)</span><br><span class="line">ax.legend([<span class="string">&#x27;paid off&#x27;</span>, <span class="string">&#x27;default&#x27;</span>]);</span><br><span class="line">ax.set_xlim(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">ax.set_ylim(<span class="number">0</span>, <span class="number">25</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">&#x27;borrower_score&#x27;</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&#x27;payment_inc_ratio&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>该图（如图6-7所示）很好地揭示了随机森林的本质。</p>
<p><img src="/img3/面向数据科学家的实用统计学/F6.7.png" alt="F6.7" style="zoom:40%;" /></p>
<p>随机森林方法是一种“<strong>黑箱</strong>”方法。它比单个树产生更准确的预测，但<strong>单个树直观的决策规则却丢失了</strong>。随机森林的预测也有些<strong>噪声</strong>：请注意，一些<strong>借款人得分非常高</strong>（表明信用度高）的贷款，<strong>最终仍被预测为违约</strong>。这是数据中一些不寻常记录的结果，也展示了随机森林<strong>过拟合</strong>的危险（参见第247页的“偏差-方差权衡”）。</p>
<h4 id="变量重要性"><strong>变量重要性</strong></h4>
<p>Variable Importance</p>
<p>当您为具有许多特征和记录的数据构建预测模型时，<strong>随机森林算法的强大之处</strong>就显现出来了。它能够自动确定哪些预测变量是重要的，并发现与<strong>交互项</strong>相对应的预测变量之间的复杂关系（参见第174页的“交互项与主效应”）。例如，使用<strong>所有列</strong>对贷款违约数据进行模型拟合。以下代码在
R 中展示了这一点：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rf_all <span class="operator">&lt;-</span> randomForest<span class="punctuation">(</span>outcome <span class="operator">~</span></span><br><span class="line">.<span class="punctuation">,</span> data<span class="operator">=</span>loan_data<span class="punctuation">,</span> importance<span class="operator">=</span><span class="literal">TRUE</span><span class="punctuation">)</span></span><br><span class="line">rf_all</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Call:</span><br><span class="line"> randomForest(formula = outcome ~</span><br><span class="line">., data = loan_data, importance = TRUE)</span><br><span class="line">               Type of random forest: classification</span><br><span class="line">                     Number of trees: 500</span><br><span class="line">No. of variables tried at each split: 4</span><br><span class="line"></span><br><span class="line">        OOB estimate of  error rate: 33.79%</span><br><span class="line">Confusion matrix:</span><br><span class="line">         paid off default class.error</span><br><span class="line">paid off    14676    7995  0.3526532</span><br><span class="line">default      7325   15346  0.3231000</span><br></pre></td></tr></table></figure>
<p>在 Python 中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">predictors = [<span class="string">&#x27;loan_amnt&#x27;</span>, <span class="string">&#x27;term&#x27;</span>, <span class="string">&#x27;annual_inc&#x27;</span>, <span class="string">&#x27;dti&#x27;</span>, <span class="string">&#x27;payment_inc_ratio&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;revol_bal&#x27;</span>, <span class="string">&#x27;revol_util&#x27;</span>, <span class="string">&#x27;purpose&#x27;</span>, <span class="string">&#x27;delinq_2yrs_zero&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;pub_rec_zero&#x27;</span>, <span class="string">&#x27;open_acc&#x27;</span>, <span class="string">&#x27;grade&#x27;</span>, <span class="string">&#x27;emp_length&#x27;</span>, <span class="string">&#x27;purpose_&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;home_&#x27;</span>, <span class="string">&#x27;emp_len_&#x27;</span>, <span class="string">&#x27;borrower_score&#x27;</span>]</span><br><span class="line">outcome = <span class="string">&#x27;outcome&#x27;</span></span><br><span class="line">X = pd.get_dummies(loan_data[predictors], drop_first=<span class="literal">True</span>)</span><br><span class="line">y = loan_data[outcome]</span><br><span class="line">rf_all = RandomForestClassifier(n_estimators=<span class="number">500</span>, random_state=<span class="number">1</span>)</span><br><span class="line">rf_all.fit(X, y)</span><br></pre></td></tr></table></figure>
<p><code>importance=TRUE</code> 参数要求 <code>randomForest</code>
存储关于不同变量重要性的额外信息。<code>varImpPlot</code>
函数将绘制变量的相对性能（相对于排列该变量）：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">varImpPlot<span class="punctuation">(</span>rf_all<span class="punctuation">,</span> type<span class="operator">=</span><span class="number">1</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<p><strong><code>mean decrease in accuracy</code></strong></p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">varImpPlot<span class="punctuation">(</span>rf_all<span class="punctuation">,</span> type<span class="operator">=</span><span class="number">2</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<p><strong><code>mean decrease in node impurity</code></strong></p>
<p>在 Python 中，<code>RandomForestClassifier</code>
在训练期间会收集特征重要性的信息，并通过
<code>feature_importances_</code> 字段使其可用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">importances = rf_all.feature_importances_</span><br></pre></td></tr></table></figure>
<p>分类器的 <code>feature_importance_</code> 属性提供了<strong>“Gini
减少量”（Gini decrease）</strong>。然而，Python
中<strong>“准确率减少量”（Accuracy
decrease）</strong>并非开箱即用。我们可以使用以下代码来计算它：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">rf = RandomForestClassifier(n_estimators=<span class="number">500</span>)</span><br><span class="line">scores = defaultdict(<span class="built_in">list</span>)</span><br><span class="line"><span class="comment"># cross-validate the scores on a number of different random splits of the data</span></span><br><span class="line"><span class="keyword">for</span></span><br><span class="line">_</span><br><span class="line"><span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">train_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size=<span class="number">0.3</span>)</span><br><span class="line">rf.fit(train_X, train_y)</span><br><span class="line">acc = metrics.accuracy_score(valid_y, rf.predict(valid_X))</span><br><span class="line"><span class="keyword">for</span> column <span class="keyword">in</span> X.columns:</span><br><span class="line">X_t = valid_X.copy()</span><br><span class="line">X_t[column] = np.random.permutation(X_t[column].values)</span><br></pre></td></tr></table></figure>
<p><img src="/img3/面向数据科学家的实用统计学/F6.8.png" alt="F6.8" style="zoom:50%;" /></p>
<p>结果如图6-8所示。 类似的图表可以使用这段Python代码创建：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">df = pd.DataFrame(&#123;</span><br><span class="line"><span class="string">&#x27;feature&#x27;</span>: X.columns,</span><br><span class="line"><span class="string">&#x27;Accuracy decrease&#x27;</span>: [np.mean(scores[column]) <span class="keyword">for</span> column <span class="keyword">in</span> X.columns],</span><br><span class="line"><span class="string">&#x27;Gini decrease&#x27;</span>: rf_all.feature_importances_,</span><br><span class="line">&#125;)</span><br><span class="line">df = df.sort_values(<span class="string">&#x27;Accuracy decrease&#x27;</span>)</span><br><span class="line">fig, axes = plt.subplots(ncols=<span class="number">2</span>, figsize=(<span class="number">8</span>, <span class="number">4.5</span>))</span><br><span class="line">ax = df.plot(kind=<span class="string">&#x27;barh&#x27;</span>, x=<span class="string">&#x27;feature&#x27;</span>, y=<span class="string">&#x27;Accuracy decrease&#x27;</span>,</span><br><span class="line">legend=<span class="literal">False</span>, ax=axes[<span class="number">0</span>])</span><br><span class="line">ax.set_ylabel(<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">ax = df.plot(kind=<span class="string">&#x27;barh&#x27;</span>, x=<span class="string">&#x27;feature&#x27;</span>, y=<span class="string">&#x27;Gini decrease&#x27;</span>,</span><br><span class="line">legend=<span class="literal">False</span>, ax=axes[<span class="number">1</span>])</span><br><span class="line">ax.set_ylabel(<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">ax.get_yaxis().set_visible(<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>有两种方法来衡量<strong>变量重要性</strong>：</p>
<ul>
<li>通过<strong>模型准确率的减少量</strong>来衡量（<code>type=1</code>），当一个变量的值被随机排列时。随机排列这些值的作用是<strong>消除该变量的所有预测能力</strong>。准确率是从<strong>袋外数据（out-of-bag
data）计算得出的（因此这个度量实际上是一个交叉验证估计</strong>）。</li>
<li>通过<strong>所有节点
Gini不纯度评分的平均减少量</strong>来衡量（<code>type=2</code>），这些节点都曾根据该变量进行分割（参见第254页的“测量同质性或不纯度”）。这衡量了包含该变量能<strong>在多大程度上提高节点纯度</strong>。这个度量是<strong>基于训练集</strong>的，因此比在袋外数据上计算的度量<strong>更不可靠</strong>。</li>
</ul>
<p>图6-8的顶部和底部面板分别显示了根据<strong>准确率减少量</strong>和<strong>Gini不纯度减少量</strong>计算出的变量重要性。两个面板中的变量都按准确率减少量进行了排名。这两种方法产生的变量重要性得分<strong>差异很大</strong>。</p>
<p>既然准确率减少量是一个更可靠的指标，为什么我们还要使用
Gini不纯度减少量呢？默认情况下，<code>randomForest</code> 只计算
Gini不纯度：Gini不纯度是<strong>算法的副产品</strong>，而按变量计算的模型准确率需要<strong>额外的计算</strong>（随机排列数据并对这些数据进行预测）。在<strong>计算复杂度</strong>很重要的场景中，例如在生产环境中需要拟合数千个模型时，额外的计算工作可能不值得。此外，Gini减少量能揭示<strong>随机森林使用了哪些变量来制定其分割规则</strong>（回想一下，这些信息在单棵树中很容易看到，但在随机森林中实际上丢失了）。</p>
<h4 id="超参数"><strong>超参数</strong></h4>
<p>Hyperparameters</p>
<p><strong>随机森林</strong>与许多统计机器学习算法一样，可以被视为一个<strong>“黑箱算法”</strong>，其内部有一些<strong>可调整的“旋钮”</strong>。这些“旋钮”被称为<strong>超参数（hyperparameters）</strong>，它们是您在拟合模型之前需要设定的参数；它们<strong>不作为训练过程的一部分进行优化</strong>。虽然传统的统计模型也需要选择（例如，在回归模型中选择要使用的预测变量），但随机森林的超参数更为关键，尤其是在<strong>避免过拟合</strong>方面。特别是，随机森林的两个最重要的超参数是：</p>
<ul>
<li><p><strong><code>nodesize/min_samples_leaf</code></strong>
终端节点（树中的叶子）的<strong>最小大小</strong>。在 R
中，分类的默认值为1，回归的默认值为5。Python 的 scikit-learn
实现中，两者默认值均为1。</p></li>
<li><p><strong><code>maxnodes/max_leaf_nodes</code></strong>
每个决策树中的<strong>最大节点数</strong>。默认情况下没有限制，将在
<code>nodesize</code> 约束下拟合最大尺寸的树。请注意，在 Python
中，您指定的是<strong>最大终端节点数</strong>。这两个参数之间存在关系：</p>
<p><code>maxnodes = 2 * max_leaf_nodes - 1</code></p></li>
</ul>
<p>您可能会很想忽略这些参数，直接使用默认值。然而，当您将随机森林应用于<strong>噪声数据</strong>时，使用默认值可能导致<strong>过拟合</strong>。当您增加
<code>nodesize/min_samples_leaf</code> 或设置
<code>maxnodes/max_leaf_nodes</code>
时，算法会拟合较小的树，从而<strong>更不容易创建虚假的预测规则</strong>。可以使用<strong>交叉验证</strong>（参见第155页的“交叉验证”）来测试设置不同超参数值所带来的影响。</p>
<p><strong>关键思想</strong></p>
<ul>
<li><strong>集成模型</strong>通过结合多个模型的结果来提高模型准确率。</li>
<li><strong>装袋法</strong>是一种特殊的集成模型，它基于对数据的<strong>自举样本</strong>拟合多个模型并进行平均。</li>
<li><strong>随机森林</strong>是应用于决策树的一种特殊类型的装袋法。除了对数据进行重采样外，随机森林算法在<strong>分割树时还会对预测变量进行抽样</strong>。</li>
<li>随机森林的一个有用输出是<strong>变量重要性</strong>的度量，它根据预测变量对模型准确率的贡献进行排名。</li>
<li>随机森林有一组<strong>超参数</strong>，应使用交叉验证进行<strong>调优</strong>以避免过拟合。</li>
</ul>
<h3 id="提升法"><strong>提升法</strong></h3>
<p>Boosting</p>
<p>集成模型已成为预测建模的标准工具。<strong>提升法（Boosting）</strong>是一种创建模型集成的通用技术。它与<strong>装袋法（bagging）</strong>差不多是同时开发的（参见第259页的“装袋法与随机森林”）。与装袋法一样，提升法最常用于<strong>决策树</strong>。尽管它们有相似之处，但提升法采取了截然不同的方法——这种方法带有更多的“花哨功能”。因此，装袋法只需相对较少的调优即可完成，而提升法在其应用中需要更多的关注。<strong>如果将这两种方法比作汽车，装袋法可以被看作是本田雅阁（可靠且稳定），而提升法则可以被看作是保时捷（强大但需要更细心的呵护）</strong>。</p>
<p>在线性回归模型中，通常会检查<strong>残差</strong>以看是否可以改进拟合（参见第185页的“偏残差图和非线性”）。提升法将这个概念推向了更远，它<strong>拟合了一系列模型，其中每个后续模型都试图最小化前一个模型的误差</strong>。通常使用该算法的几个变体：<strong>Adaboost</strong>、<strong>梯度提升（gradient
boosting）</strong>和<strong>随机梯度提升（stochastic gradient
boosting）</strong>。后者，即随机梯度提升，是最通用和应用最广泛的。事实上，通过正确的参数选择，该算法可以模拟随机森林。</p>
<p><strong>提升法的关键术语</strong></p>
<ul>
<li><p><strong>集成（Ensemble）</strong> 通过使用一系列模型来形成预测。
同义词：<strong>模型平均（Model averaging）</strong></p></li>
<li><p><strong>提升法（Boosting）</strong>
一种通用技术，通过在每个连续轮次中对具有较大残差的记录赋予更多权重来拟合一系列模型。</p></li>
<li><p><strong>Adaboost</strong>
提升法的早期版本，根据残差对数据进行重新加权。</p></li>
<li><p><strong>梯度提升（Gradient boosting）</strong>
一种更通用的提升形式，被定义为<strong>最小化成本函数</strong>。</p></li>
<li><p><strong>随机梯度提升（Stochastic gradient boosting）</strong>
最通用的提升算法，在每一轮中都包含了<strong>记录和列的重采样</strong>。</p></li>
<li><p><strong>正则化（Regularization）</strong>
一种通过在成本函数中<strong>添加惩罚项</strong>以避免模型过拟合的技术，该惩罚项与模型中的参数数量相关。</p></li>
<li><p><strong>超参数（Hyperparameters）</strong>
在拟合算法之前需要设定的参数。</p></li>
</ul>
<h4 id="提升算法"><strong>提升算法</strong></h4>
<p>The Boosting Algorithm</p>
<p>提升算法有多种，但它们的基本思想本质上是相同的。最容易理解的是
<strong>Adaboost</strong>，它的过程如下：</p>
<ol type="1">
<li>初始化<strong>最大拟合模型数 <span
class="math inline">\(M\)</span></strong>，并设置迭代计数器 <span
class="math inline">\(m = 1\)</span>。初始化<strong>观测权重</strong>
<span class="math inline">\(w_i = 1/N\)</span>（对于 <span
class="math inline">\(i=1, 2, \dots,
N\)</span>）。初始化<strong>集成模型</strong> <span
class="math inline">\(F_0 = 0\)</span>。</li>
<li>使用观测权重 <span class="math inline">\(w_1, w_2, \dots,
w_N\)</span> 训练模型 <span class="math inline">\(\hat
f_m\)</span>，使其<strong>最小化加权误差</strong> <span
class="math inline">\(e_m\)</span>，该误差由错误分类观测的权重总和定义。</li>
<li>将模型添加到集成中：<span class="math inline">\(\hat F_m = \hat
F_{m-1} + \alpha_m \hat f_m\)</span>，其中 <span
class="math inline">\(\alpha_m = \log\frac{1 - e_m}{e_m}\)</span>。</li>
<li>更新权重 <span class="math inline">\(w_1, w_2, \dots,
w_N\)</span>，以便<strong>增加被错误分类的观测的权重</strong>。增加的幅度取决于
<span class="math inline">\(\alpha_m\)</span>，<span
class="math inline">\(\alpha_m\)</span> 的值越大，权重增加得越多。</li>
<li>递增模型计数器 <span class="math inline">\(m = m + 1\)</span>。如果
<span class="math inline">\(m \le M\)</span>，则返回步骤2。</li>
</ol>
<p>提升后的估计值由下式给出：</p>
<p><span class="math display">\[
\hat F = \alpha_1 \hat f_1 + \alpha_2 \hat f_2 + \cdots + \alpha_M \hat
f_M
\]</span>
通过增加被错误分类的观测的权重，该算法<strong>迫使模型更侧重于对其表现不佳的数据进行训练</strong>。因子
<span class="math inline">\(\alpha_m\)</span>
确保<strong>误差较低的模型拥有更大的权重</strong>。</p>
<p><strong>梯度提升</strong>与 Adaboost
类似，但它将问题视为<strong>成本函数的优化</strong>。梯度提升不是调整权重，而是<strong>拟合模型以适应伪残差</strong>，这起到了更侧重于较大残差进行训练的作用。</p>
<p>本着随机森林的精神，<strong>随机梯度提升</strong>通过在每个阶段对观测和预测变量进行抽样，为算法增加了<strong>随机性</strong>。</p>
<h4 id="xgboost"><strong>XGBoost</strong></h4>
<p>XGBoost</p>
<p>最广泛使用的用于提升法的开源软件是
<strong>XGBoost</strong>，它是由华盛顿大学的<strong>陈天奇</strong>和<strong>Carlos
Guestrin</strong>最初开发的随机梯度提升的一种实现。它是一个计算高效且具有许多选项的实现，作为软件包可用于大多数主要的数据科学编程语言。在
R 中，XGBoost 可作为 <code>xgboost</code> 包使用，在 Python
中也使用相同的名称。</p>
<p><code>xgboost</code>
方法有许多可以且应该调整的参数（参见第279页的“超参数与交叉验证”）。两个非常重要的参数是
<strong><code>subsample</code></strong>，它控制在每次迭代中应该采样的观测记录比例；以及
<strong><code>eta</code></strong>，一个应用于提升算法中 <span
class="math inline">\(\alpha_m\)</span>
的<strong>收缩因子</strong>（参见第271页的“提升算法”）。</p>
<p>使用 <code>subsample</code>
会让提升法的行为类似于随机森林，只是采样是<strong>无放回</strong>的。收缩参数
<code>eta</code>
有助于通过<strong>减少权重的变化</strong>来防止过拟合（权重的变化越小，算法就越不容易过拟合训练集）。</p>
<p>以下代码在 R 中将 <code>xgboost</code>
应用于只有两个预测变量的贷款数据：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">predictors <span class="operator">&lt;-</span> data.matrix<span class="punctuation">(</span>loan3000<span class="punctuation">[</span><span class="punctuation">,</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;borrower_score&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;payment_inc_ratio&#x27;</span><span class="punctuation">)</span><span class="punctuation">]</span><span class="punctuation">)</span></span><br><span class="line">label <span class="operator">&lt;-</span> <span class="built_in">as.numeric</span><span class="punctuation">(</span>loan3000<span class="punctuation">[</span><span class="punctuation">,</span><span class="string">&#x27;outcome&#x27;</span><span class="punctuation">]</span><span class="punctuation">)</span> <span class="operator">-</span> <span class="number">1</span></span><br><span class="line">xgb <span class="operator">&lt;-</span> xgboost<span class="punctuation">(</span>data<span class="operator">=</span>predictors<span class="punctuation">,</span> label<span class="operator">=</span>label<span class="punctuation">,</span> objective<span class="operator">=</span><span class="string">&quot;binary:logistic&quot;</span><span class="punctuation">,</span></span><br><span class="line">params<span class="operator">=</span><span class="built_in">list</span><span class="punctuation">(</span>subsample<span class="operator">=</span><span class="number">0.63</span><span class="punctuation">,</span> eta<span class="operator">=</span><span class="number">0.1</span><span class="punctuation">)</span><span class="punctuation">,</span> nrounds<span class="operator">=</span><span class="number">100</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[1] train-error:0.358333</span><br><span class="line">[2] train-error:0.346333</span><br><span class="line">[3] train-error:0.347333</span><br><span class="line">...</span><br><span class="line">[99] train-error:0.239333</span><br><span class="line">[100] train-error:0.241000</span><br></pre></td></tr></table></figure>
<p>请注意，<code>xgboost</code>
<strong>不支持公式语法</strong>，因此预测变量需要转换为
<code>data.matrix</code>，响应变量需要转换为0/1变量。<code>objective</code>
参数告诉 <code>xgboost</code>
这是哪种类型的问题；基于此，<code>xgboost</code>
会选择一个要优化的指标。</p>
<p>在 Python 中，<code>xgboost</code>
有两种不同的接口：<strong>scikit-learn API</strong> 和一个更像 R
的<strong>函数式接口</strong>。为了与其他 scikit-learn
方法保持一致，一些参数被重新命名了。例如，<code>eta</code> 被重命名为
<code>learning_rate</code>；使用 <code>eta</code>
虽然不会导致失败，但也不会产生预期的效果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">predictors = [<span class="string">&#x27;borrower_score&#x27;</span>, <span class="string">&#x27;payment_inc_ratio&#x27;</span>]</span><br><span class="line">outcome = <span class="string">&#x27;outcome&#x27;</span></span><br><span class="line">X = loan3000[predictors]</span><br><span class="line">y = loan3000[outcome]</span><br><span class="line">xgb = XGBClassifier(objective=<span class="string">&#x27;binary:logistic&#x27;</span>, subsample=<span class="number">0.63</span>)</span><br><span class="line">xgb.fit(X, y)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">--</span><br><span class="line">XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, colsample_bylevel=1,</span><br><span class="line">colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.1,</span><br><span class="line">max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,</span><br><span class="line">n_estimators=100, n_jobs=1, nthread=None, objective=&#x27;binary:logistic&#x27;,</span><br><span class="line">random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,</span><br><span class="line">silent=None, subsample=0.63, verbosity=1)</span><br></pre></td></tr></table></figure>
<p>预测值可以从 R 中的 <code>predict</code>
函数获得，并且因为只有两个变量，可以针对预测变量进行绘制：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">pred <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>xgb<span class="punctuation">,</span> newdata<span class="operator">=</span>predictors<span class="punctuation">)</span></span><br><span class="line">xgb_df <span class="operator">&lt;-</span> cbind<span class="punctuation">(</span>loan3000<span class="punctuation">,</span> pred_default <span class="operator">=</span> pred <span class="operator">&gt;</span> <span class="number">0.5</span><span class="punctuation">,</span> prob_default <span class="operator">=</span> pred<span class="punctuation">)</span></span><br><span class="line">ggplot<span class="punctuation">(</span>data<span class="operator">=</span>xgb_df<span class="punctuation">,</span> aes<span class="punctuation">(</span>x<span class="operator">=</span>borrower_score<span class="punctuation">,</span> y<span class="operator">=</span>payment_inc_ratio<span class="punctuation">,</span></span><br><span class="line">color<span class="operator">=</span>pred_default<span class="punctuation">,</span> shape<span class="operator">=</span>pred_default<span class="punctuation">,</span> size<span class="operator">=</span>pred_default<span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">geom_point<span class="punctuation">(</span>alpha<span class="operator">=</span><span class="number">.8</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">scale_color_manual<span class="punctuation">(</span>values <span class="operator">=</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;FALSE&#x27;</span><span class="operator">=</span><span class="string">&#x27;#b8e186&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;TRUE&#x27;</span><span class="operator">=</span><span class="string">&#x27;#d95f02&#x27;</span><span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">scale_shape_manual<span class="punctuation">(</span>values <span class="operator">=</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;FALSE&#x27;</span><span class="operator">=</span><span class="number">0</span><span class="punctuation">,</span> <span class="string">&#x27;TRUE&#x27;</span><span class="operator">=</span><span class="number">1</span><span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">scale_size_manual<span class="punctuation">(</span>values <span class="operator">=</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;FALSE&#x27;</span><span class="operator">=</span><span class="number">0.5</span><span class="punctuation">,</span> <span class="string">&#x27;TRUE&#x27;</span><span class="operator">=</span><span class="number">2</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<p>使用以下代码可以在 Python 中创建相同的图：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">6</span>, <span class="number">4</span>))</span><br><span class="line">xgb_df.loc[xgb_df.prediction==<span class="string">&#x27;paid off&#x27;</span>].plot(</span><br><span class="line">x=<span class="string">&#x27;borrower_score&#x27;</span>, y=<span class="string">&#x27;payment_inc_ratio&#x27;</span>, style=<span class="string">&#x27;.&#x27;</span>,</span><br><span class="line">markerfacecolor=<span class="string">&#x27;none&#x27;</span>, markeredgecolor=<span class="string">&#x27;C1&#x27;</span>, ax=ax)</span><br><span class="line"></span><br><span class="line">xgb_df.loc[xgb_df.prediction==<span class="string">&#x27;default&#x27;</span>].plot(</span><br><span class="line">x=<span class="string">&#x27;borrower_score&#x27;</span>, y=<span class="string">&#x27;payment_inc_ratio&#x27;</span>, style=<span class="string">&#x27;o&#x27;</span>,</span><br><span class="line">markerfacecolor=<span class="string">&#x27;none&#x27;</span>, markeredgecolor=<span class="string">&#x27;C0&#x27;</span>, ax=ax)</span><br><span class="line">ax.legend([<span class="string">&#x27;paid off&#x27;</span>, <span class="string">&#x27;default&#x27;</span>]);</span><br><span class="line">ax.set_xlim(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">ax.set_ylim(<span class="number">0</span>, <span class="number">25</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">&#x27;borrower_score&#x27;</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&#x27;payment_inc_ratio&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>结果如图6-9所示。
从定性上看，这与随机森林的预测结果相似；参见图6-7。预测结果有些<strong>噪声</strong>，因为一些<strong>借款人得分非常高</strong>（表明信用度高）的贷款，最终仍被预测为违约。</p>
<p><img src="/img3/面向数据科学家的实用统计学/F6.9.png" alt="F6.9" style="zoom:50%;" /></p>
<h4 id="正则化避免过拟合"><strong>正则化：避免过拟合</strong></h4>
<p>Regularization: Avoiding Overfitting</p>
<p>盲目应用 <code>xgboost</code>
可能会因<strong>过拟合训练数据</strong>而导致模型不稳定。过拟合问题是双重的：</p>
<ul>
<li>模型在<strong>不属于训练集的新数据</strong>上的准确率会<strong>降低</strong>。</li>
<li>模型的预测<strong>高度可变</strong>，导致结果不稳定。</li>
</ul>
<p>任何建模技术都可能容易过拟合。例如，如果回归方程中包含过多变量，模型最终可能会产生虚假预测。然而，对于大多数统计技术，通过<strong>明智地选择预测变量</strong>可以避免过拟合。即使是随机森林，通常在不调整参数的情况下也能产生一个合理的模型。</p>
<p>但是，<code>xgboost</code>
的情况并非如此。使用模型中包含的所有变量来拟合训练集上的贷款数据。在 R
中，您可以这样做：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">seed <span class="operator">&lt;-</span> 400820</span><br><span class="line">predictors <span class="operator">&lt;-</span> data.matrix<span class="punctuation">(</span>loan_data<span class="punctuation">[</span><span class="punctuation">,</span> <span class="operator">-</span>which<span class="punctuation">(</span><span class="built_in">names</span><span class="punctuation">(</span>loan_data<span class="punctuation">)</span> <span class="operator">%in%</span></span><br><span class="line"><span class="string">&#x27;outcome&#x27;</span><span class="punctuation">)</span><span class="punctuation">]</span><span class="punctuation">)</span></span><br><span class="line">label <span class="operator">&lt;-</span> <span class="built_in">as.numeric</span><span class="punctuation">(</span>loan_data<span class="operator">$</span>outcome<span class="punctuation">)</span> <span class="operator">-</span> <span class="number">1</span></span><br><span class="line">test_idx <span class="operator">&lt;-</span> sample<span class="punctuation">(</span>nrow<span class="punctuation">(</span>loan_data<span class="punctuation">)</span><span class="punctuation">,</span> <span class="number">10000</span><span class="punctuation">)</span></span><br><span class="line">xgb_default <span class="operator">&lt;-</span> xgboost<span class="punctuation">(</span>data<span class="operator">=</span>predictors<span class="punctuation">[</span><span class="operator">-</span>test_idx<span class="punctuation">,</span><span class="punctuation">]</span><span class="punctuation">,</span> label<span class="operator">=</span>label<span class="punctuation">[</span><span class="operator">-</span>test_idx<span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">objective<span class="operator">=</span><span class="string">&#x27;binary:logistic&#x27;</span><span class="punctuation">,</span> nrounds<span class="operator">=</span><span class="number">250</span><span class="punctuation">,</span> verbose<span class="operator">=</span><span class="number">0</span><span class="punctuation">)</span></span><br><span class="line">pred_default <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>xgb_default<span class="punctuation">,</span> predictors<span class="punctuation">[</span>test_idx<span class="punctuation">,</span><span class="punctuation">]</span><span class="punctuation">)</span></span><br><span class="line">error_default <span class="operator">&lt;-</span> <span class="built_in">abs</span><span class="punctuation">(</span>label<span class="punctuation">[</span>test_idx<span class="punctuation">]</span> <span class="operator">-</span> pred_default<span class="punctuation">)</span> <span class="operator">&gt;</span> <span class="number">0.5</span></span><br><span class="line">xgb_default<span class="operator">$</span>evaluation_log<span class="punctuation">[</span><span class="number">250</span><span class="punctuation">,</span><span class="punctuation">]</span></span><br><span class="line">mean<span class="punctuation">(</span>error_default<span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">-</span><br><span class="line">iter train_error</span><br><span class="line">1: 250 0.133043</span><br><span class="line">[1] 0.3529</span><br></pre></td></tr></table></figure>
<p>我们使用 Python 中的 <code>train_test_split</code>
函数将数据集分为训练集和测试集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">predictors = [<span class="string">&#x27;loan_amnt&#x27;</span>, <span class="string">&#x27;term&#x27;</span>, <span class="string">&#x27;annual_inc&#x27;</span>, <span class="string">&#x27;dti&#x27;</span>, <span class="string">&#x27;payment_inc_ratio&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;revol_bal&#x27;</span>, <span class="string">&#x27;revol_util&#x27;</span>, <span class="string">&#x27;purpose&#x27;</span>, <span class="string">&#x27;delinq_2yrs_zero&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;pub_rec_zero&#x27;</span>, <span class="string">&#x27;open_acc&#x27;</span>, <span class="string">&#x27;grade&#x27;</span>, <span class="string">&#x27;emp_length&#x27;</span>, <span class="string">&#x27;purpose_&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;home_&#x27;</span>, <span class="string">&#x27;emp_len_&#x27;</span>, <span class="string">&#x27;borrower_score&#x27;</span>]</span><br><span class="line">outcome = <span class="string">&#x27;outcome&#x27;</span></span><br><span class="line">X = pd.get_dummies(loan_data[predictors], drop_first=<span class="literal">True</span>)</span><br><span class="line">y = pd.Series([<span class="number">1</span> <span class="keyword">if</span> o == <span class="string">&#x27;default&#x27;</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> o <span class="keyword">in</span> loan_data[outcome]])</span><br><span class="line">train_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size=<span class="number">10000</span>)</span><br><span class="line">xgb_default = XGBClassifier(objective=<span class="string">&#x27;binary:logistic&#x27;</span>, n_estimators=<span class="number">250</span>,</span><br><span class="line">max_depth=<span class="number">6</span>, reg_lambda=<span class="number">0</span>, learning_rate=<span class="number">0.3</span>,</span><br><span class="line">subsample=<span class="number">1</span>)</span><br><span class="line">xgb_default.fit(train_X, train_y)</span><br><span class="line">pred_default = xgb_default.predict_proba(valid_X)[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">error_default = <span class="built_in">abs</span>(valid_y - pred_default) &gt; <span class="number">0.5</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;default: &#x27;</span>, np.mean(error_default))</span><br></pre></td></tr></table></figure>
<p>测试集由从完整数据中随机抽样的10,000条记录组成，训练集则由剩余的记录组成。提升法在训练集上的<strong>错误率</strong>仅为13.3%。然而，<strong>测试集</strong>的错误率高达35.3%。这是<strong>过拟合</strong>的结果：虽然提升法可以很好地解释训练集中的可变性，但其预测规则<strong>不适用于新数据</strong>。</p>
<p>提升法提供了几个参数来<strong>避免过拟合</strong>，包括
<code>eta</code> (或 <code>learning_rate</code>) 和
<code>subsample</code>
(参见第272页的“XGBoost”)。另一种方法是<strong>正则化</strong>，该技术通过添加一个<strong>惩罚模型复杂度的惩罚项</strong>来修改成本函数。决策树通过最小化
Gini不纯度评分等成本标准来进行拟合（参见第254页的“测量同质性或不纯度”）。在
<code>xgboost</code>
中，可以通过<strong>添加一个衡量模型复杂度的项</strong>来修改成本函数。</p>
<p><code>xgboost</code>
中有两个用于<strong>正则化模型</strong>的参数：<strong><code>alpha</code></strong>
和
<strong><code>lambda</code></strong>，它们分别对应于<strong>曼哈顿距离（L1-正则化）和欧几里得距离平方（L2-正则化）</strong>（参见第241页的“距离度量”）。增加这些参数会<strong>惩罚更复杂的模型</strong>并<strong>减小拟合树的大小</strong>。例如，看看我们在
R 中将 <code>lambda</code> 设置为1000时会发生什么：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">xgb_penalty <span class="operator">&lt;-</span> xgboost<span class="punctuation">(</span>data<span class="operator">=</span>predictors<span class="punctuation">[</span><span class="operator">-</span>test_idx<span class="punctuation">,</span><span class="punctuation">]</span><span class="punctuation">,</span> label<span class="operator">=</span>label<span class="punctuation">[</span><span class="operator">-</span>test_idx<span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">params<span class="operator">=</span><span class="built_in">list</span><span class="punctuation">(</span>eta<span class="operator">=</span><span class="number">.1</span><span class="punctuation">,</span> subsample<span class="operator">=</span><span class="number">.63</span><span class="punctuation">,</span> lambda<span class="operator">=</span><span class="number">1000</span><span class="punctuation">)</span><span class="punctuation">,</span></span><br><span class="line">objective<span class="operator">=</span><span class="string">&#x27;binary:logistic&#x27;</span><span class="punctuation">,</span> nrounds<span class="operator">=</span><span class="number">250</span><span class="punctuation">,</span> verbose<span class="operator">=</span><span class="number">0</span><span class="punctuation">)</span></span><br><span class="line">pred_penalty <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>xgb_penalty<span class="punctuation">,</span> predictors<span class="punctuation">[</span>test_idx<span class="punctuation">,</span><span class="punctuation">]</span><span class="punctuation">)</span></span><br><span class="line">error_penalty <span class="operator">&lt;-</span> <span class="built_in">abs</span><span class="punctuation">(</span>label<span class="punctuation">[</span>test_idx<span class="punctuation">]</span> <span class="operator">-</span> pred_penalty<span class="punctuation">)</span> <span class="operator">&gt;</span> <span class="number">0.5</span></span><br><span class="line">xgb_penalty<span class="operator">$</span>evaluation_log<span class="punctuation">[</span><span class="number">250</span><span class="punctuation">,</span><span class="punctuation">]</span></span><br><span class="line">mean<span class="punctuation">(</span>error_penalty<span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">-</span><br><span class="line">iter train_error</span><br><span class="line">1: 250 0.30966</span><br><span class="line">[1] 0.3286</span><br></pre></td></tr></table></figure>
<p>在 scikit-learn API 中，参数被称为 <code>reg_alpha</code> 和
<code>reg_lambda</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">xgb_penalty = XGBClassifier(objective=<span class="string">&#x27;binary:logistic&#x27;</span>, n_estimators=<span class="number">250</span>,</span><br><span class="line">max_depth=<span class="number">6</span>, reg_lambda=<span class="number">1000</span>, learning_rate=<span class="number">0.1</span>,</span><br><span class="line">subsample=<span class="number">0.63</span>)</span><br><span class="line">xgb_penalty.fit(train_X, train_y)</span><br><span class="line">pred_penalty = xgb_penalty.predict_proba(valid_X)[:, <span class="number">1</span>]</span><br><span class="line">error_penalty = <span class="built_in">abs</span>(valid_y - pred_penalty) &gt; <span class="number">0.5</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;penalty: &#x27;</span>, np.mean(error_penalty))</span><br></pre></td></tr></table></figure>
<p>现在，<strong>训练误差仅略低于测试集上的误差</strong>。</p>
<p>在 R 中，<code>predict</code> 方法提供了一个方便的参数
<code>ntreelimit</code>，它强制仅使用前 <span
class="math inline">\(i\)</span>
棵树进行预测。这使我们能够随着模型的增加，直接比较<strong>样本内（in-sample）</strong>和<strong>样本外（out-of-sample）</strong>的错误率：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">error_default <span class="operator">&lt;-</span> <span class="built_in">rep</span><span class="punctuation">(</span><span class="number">0</span><span class="punctuation">,</span> <span class="number">250</span><span class="punctuation">)</span></span><br><span class="line">error_penalty <span class="operator">&lt;-</span> <span class="built_in">rep</span><span class="punctuation">(</span><span class="number">0</span><span class="punctuation">,</span> <span class="number">250</span><span class="punctuation">)</span></span><br><span class="line"><span class="keyword">for</span><span class="punctuation">(</span>i <span class="keyword">in</span> <span class="number">1</span><span class="operator">:</span><span class="number">250</span><span class="punctuation">)</span><span class="punctuation">&#123;</span></span><br><span class="line">pred_def <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>xgb_default<span class="punctuation">,</span> predictors<span class="punctuation">[</span>test_idx<span class="punctuation">,</span><span class="punctuation">]</span><span class="punctuation">,</span> ntreelimit<span class="operator">=</span>i<span class="punctuation">)</span></span><br><span class="line">error_default<span class="punctuation">[</span>i<span class="punctuation">]</span> <span class="operator">&lt;-</span> mean<span class="punctuation">(</span><span class="built_in">abs</span><span class="punctuation">(</span>label<span class="punctuation">[</span>test_idx<span class="punctuation">]</span> <span class="operator">-</span> pred_def<span class="punctuation">)</span> <span class="operator">&gt;=</span> <span class="number">0.5</span><span class="punctuation">)</span></span><br><span class="line">pred_pen <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>xgb_penalty<span class="punctuation">,</span> predictors<span class="punctuation">[</span>test_idx<span class="punctuation">,</span><span class="punctuation">]</span><span class="punctuation">,</span> ntreelimit<span class="operator">=</span>i<span class="punctuation">)</span></span><br><span class="line">error_penalty<span class="punctuation">[</span>i<span class="punctuation">]</span> <span class="operator">&lt;-</span> mean<span class="punctuation">(</span><span class="built_in">abs</span><span class="punctuation">(</span>label<span class="punctuation">[</span>test_idx<span class="punctuation">]</span> <span class="operator">-</span> pred_pen<span class="punctuation">)</span> <span class="operator">&gt;=</span> <span class="number">0.5</span><span class="punctuation">)</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>在 Python 中，我们可以用 <code>ntree_limit</code> 参数调用
<code>predict_proba</code> 方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">results = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">250</span>):</span><br><span class="line">train_default = xgb_default.predict_proba(train_X, ntree_limit=i)[:, <span class="number">1</span>]</span><br><span class="line">train_penalty = xgb_penalty.predict_proba(train_X, ntree_limit=i)[:, <span class="number">1</span>]</span><br><span class="line">pred_default = xgb_default.predict_proba(valid_X, ntree_limit=i)[:, <span class="number">1</span>]</span><br><span class="line">pred_penalty = xgb_penalty.predict_proba(valid_X, ntree_limit=i)[:, <span class="number">1</span>]</span><br><span class="line">results.append(&#123;</span><br><span class="line"><span class="string">&#x27;iterations&#x27;</span>: i,</span><br><span class="line"><span class="string">&#x27;default train&#x27;</span>: np.mean(<span class="built_in">abs</span>(train_y - train_default) &gt; <span class="number">0.5</span>),</span><br><span class="line"><span class="string">&#x27;penalty train&#x27;</span>: np.mean(<span class="built_in">abs</span>(train_y - train_penalty) &gt; <span class="number">0.5</span>),</span><br><span class="line"><span class="string">&#x27;default test&#x27;</span>: np.mean(<span class="built_in">abs</span>(valid_y - pred_default) &gt; <span class="number">0.5</span>),</span><br><span class="line"><span class="string">&#x27;penalty test&#x27;</span>: np.mean(<span class="built_in">abs</span>(valid_y - pred_penalty) &gt; <span class="number">0.5</span>),</span><br><span class="line">&#125;)</span><br><span class="line">results = pd.DataFrame(results)</span><br><span class="line">results.head()</span><br></pre></td></tr></table></figure>
<p>模型的输出将训练集的误差返回到
<code>xgb_default$evaluation_log</code>
组件中。通过将其与样本外误差结合，我们可以绘制<strong>误差与迭代次数</strong>的关系图：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">errors <span class="operator">&lt;-</span> rbind<span class="punctuation">(</span>xgb_default<span class="operator">$</span>evaluation_log<span class="punctuation">,</span></span><br><span class="line">xgb_penalty<span class="operator">$</span>evaluation_log<span class="punctuation">,</span></span><br><span class="line">ata.frame<span class="punctuation">(</span>iter<span class="operator">=</span><span class="number">1</span><span class="operator">:</span><span class="number">250</span><span class="punctuation">,</span> train_error<span class="operator">=</span>error_default<span class="punctuation">)</span><span class="punctuation">,</span></span><br><span class="line">data.frame<span class="punctuation">(</span>iter<span class="operator">=</span><span class="number">1</span><span class="operator">:</span><span class="number">250</span><span class="punctuation">,</span> train_error<span class="operator">=</span>error_penalty<span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">errors<span class="operator">$</span>type <span class="operator">&lt;-</span> <span class="built_in">rep</span><span class="punctuation">(</span><span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;default train&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;penalty train&#x27;</span><span class="punctuation">,</span></span><br><span class="line"><span class="string">&#x27;default test&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;penalty test&#x27;</span><span class="punctuation">)</span><span class="punctuation">,</span> <span class="built_in">rep</span><span class="punctuation">(</span><span class="number">250</span><span class="punctuation">,</span> <span class="number">4</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">ggplot<span class="punctuation">(</span>errors<span class="punctuation">,</span> aes<span class="punctuation">(</span>x<span class="operator">=</span>iter<span class="punctuation">,</span> y<span class="operator">=</span>train_error<span class="punctuation">,</span> group<span class="operator">=</span>type<span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">geom_line<span class="punctuation">(</span>aes<span class="punctuation">(</span>linetype<span class="operator">=</span>type<span class="punctuation">,</span> color<span class="operator">=</span>type<span class="punctuation">)</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<p>我们可以使用 pandas 的 <code>plot</code>
方法创建折线图。从第一个图返回的坐标轴允许我们在同一个图上叠加额外的线条。这是许多
Python 图形包支持的模式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ax = results.plot(x=<span class="string">&#x27;iterations&#x27;</span>, y=<span class="string">&#x27;default test&#x27;</span>)</span><br><span class="line">results.plot(x=<span class="string">&#x27;iterations&#x27;</span>, y=<span class="string">&#x27;penalty test&#x27;</span>, ax=ax)</span><br><span class="line">results.plot(x=<span class="string">&#x27;iterations&#x27;</span>, y=<span class="string">&#x27;default train&#x27;</span>, ax=ax)</span><br><span class="line">results.plot(x=<span class="string">&#x27;iterations&#x27;</span>, y=<span class="string">&#x27;penalty train&#x27;</span>, ax=ax)</span><br></pre></td></tr></table></figure>
<p>结果如图6-10所示。
这表明<strong>默认模型</strong>在训练集上的准确率稳步提高，但<strong>在测试集上的表现实际上却变差了</strong>。而<strong>正则化后的模型</strong>没有表现出这种行为。</p>
<p><img src="/img3/面向数据科学家的实用统计学/F6.10.png" alt="F6.10" style="zoom:50%;" /></p>
<p><strong>岭回归和Lasso</strong></p>
<p>通过对模型的复杂度施加惩罚来帮助避免过拟合的技术可以追溯到20世纪70年代。<strong>最小二乘回归</strong>最小化<strong>残差平方和（RSS）</strong>；参见第148页的“最小二乘”。<strong>岭回归（Ridge
regression）</strong>最小化<strong>残差平方和</strong>加上一个<strong>惩罚项</strong>，该惩罚项是系数数量和大小的函数：</p>
<p><span class="math display">\[
\sum_{i=1}^{n} (Y_i - b_0 - b_1X_1 - \dots - b_pX_p)^2 + \lambda
\sum_{j=1}^{p} b_j^2
\]</span> <span class="math inline">\(\lambda\)</span>
的值决定了对系数的惩罚程度；值越大，产生的模型就<strong>越不容易过拟合数据</strong>。<strong>Lasso</strong>
与此类似，不同之处在于它使用<strong>曼哈顿距离</strong>而不是<strong>欧几里得距离</strong>作为惩罚项：</p>
<p><span class="math display">\[
\sum_{i=1}^{n} (Y_i - b_0 - b_1X_1 - \dots - b_pX_p)^2 + \alpha
\sum_{j=1}^{p} |b_j|
\]</span> 使用欧几里得距离也称为 <strong>L2
正则化</strong>，使用曼哈顿距离则称为 <strong>L1
正则化</strong>。<code>xgboost</code> 的参数 <code>lambda</code>
(<code>reg_lambda</code>) 和 <code>alpha</code> (<code>reg_alpha</code>)
的作用与此类似。</p>
<h4 id="超参数和交叉验证"><strong>超参数和交叉验证</strong></h4>
<p>Hyperparameters and Cross-Validation</p>
<p><code>xgboost</code>
具有一系列<strong>令人望而生畏</strong>的超参数；关于讨论，请参见第281页的“XGBoost超参数”。正如在第274页的“正则化：避免过拟合”中所看到的，具体的选择可以<strong>显著改变模型拟合</strong>。面对如此多的超参数组合可供选择，我们应该如何做出指导性选择呢？解决这个问题的标准方案是使用<strong>交叉验证</strong>；参见第155页的“交叉验证”。</p>
<p><strong>交叉验证</strong>将数据随机分成 <span
class="math inline">\(K\)</span>
个不同的组，也称为<strong>折叠（folds）</strong>。对于每个折叠，模型在<strong>不包含该折叠数据</strong>的其余数据上进行训练，然后在该折叠的数据上进行评估。这能得到一个模型在<strong>样本外数据</strong>上的<strong>准确率度量</strong>。最佳的超参数集由<strong>通过对每个折叠的误差取平均</strong>计算出的<strong>总体误差最低</strong>的模型所决定。</p>
<p>为了说明这项技术，我们将其应用于 <code>xgboost</code>
的参数选择。在这个例子中，我们探讨了两个参数：<strong>收缩参数
<code>eta</code>
(<code>learning_rate</code>)</strong>（参见第272页的“XGBoost”）和树的<strong>最大深度
<code>max_depth</code></strong>。参数 <code>max_depth</code>
是从叶子节点到树根的最大深度，默认值为6。这给了我们另一种控制<strong>过拟合</strong>的方法：<strong>深层树往往更复杂，可能导致数据过拟合</strong>。</p>
<p>首先，我们设置折叠和参数列表。在 R 中，操作如下：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">N <span class="operator">&lt;-</span> nrow<span class="punctuation">(</span>loan_data<span class="punctuation">)</span></span><br><span class="line">fold_number <span class="operator">&lt;-</span> sample<span class="punctuation">(</span><span class="number">1</span><span class="operator">:</span><span class="number">5</span><span class="punctuation">,</span> N<span class="punctuation">,</span> replace<span class="operator">=</span><span class="literal">TRUE</span><span class="punctuation">)</span></span><br><span class="line">params <span class="operator">&lt;-</span> data.frame<span class="punctuation">(</span>eta <span class="operator">=</span> <span class="built_in">rep</span><span class="punctuation">(</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">.1</span><span class="punctuation">,</span> <span class="number">.5</span><span class="punctuation">,</span> <span class="number">.9</span><span class="punctuation">)</span><span class="punctuation">,</span> <span class="number">3</span><span class="punctuation">)</span><span class="punctuation">,</span></span><br><span class="line">max_depth <span class="operator">=</span> <span class="built_in">rep</span><span class="punctuation">(</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">3</span><span class="punctuation">,</span> <span class="number">6</span><span class="punctuation">,</span> <span class="number">12</span><span class="punctuation">)</span><span class="punctuation">,</span> <span class="built_in">rep</span><span class="punctuation">(</span><span class="number">3</span><span class="punctuation">,</span><span class="number">3</span><span class="punctuation">)</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<p>现在，我们使用五个折叠，应用前面描述的算法来计算每个模型和每个折叠的误差：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">error <span class="operator">&lt;-</span> matrix<span class="punctuation">(</span><span class="number">0</span><span class="punctuation">,</span> nrow<span class="operator">=</span><span class="number">9</span><span class="punctuation">,</span> ncol<span class="operator">=</span><span class="number">5</span><span class="punctuation">)</span></span><br><span class="line"><span class="keyword">for</span><span class="punctuation">(</span>i <span class="keyword">in</span> <span class="number">1</span><span class="operator">:</span>nrow<span class="punctuation">(</span>params<span class="punctuation">)</span><span class="punctuation">)</span><span class="punctuation">&#123;</span></span><br><span class="line"><span class="keyword">for</span><span class="punctuation">(</span>k <span class="keyword">in</span> <span class="number">1</span><span class="operator">:</span><span class="number">5</span><span class="punctuation">)</span><span class="punctuation">&#123;</span></span><br><span class="line">fold_idx <span class="operator">&lt;-</span> <span class="punctuation">(</span><span class="number">1</span><span class="operator">:</span>N<span class="punctuation">)</span><span class="punctuation">[</span>fold_number <span class="operator">==</span> k<span class="punctuation">]</span></span><br><span class="line">xgb <span class="operator">&lt;-</span> xgboost<span class="punctuation">(</span>data<span class="operator">=</span>predictors<span class="punctuation">[</span><span class="operator">-</span>fold_idx<span class="punctuation">,</span><span class="punctuation">]</span><span class="punctuation">,</span> label<span class="operator">=</span>label<span class="punctuation">[</span><span class="operator">-</span>fold_idx<span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">params<span class="operator">=</span><span class="built_in">list</span><span class="punctuation">(</span>eta<span class="operator">=</span>params<span class="punctuation">[</span>i<span class="punctuation">,</span> <span class="string">&#x27;eta&#x27;</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">max_depth<span class="operator">=</span>params<span class="punctuation">[</span>i<span class="punctuation">,</span> <span class="string">&#x27;max_depth&#x27;</span><span class="punctuation">]</span><span class="punctuation">)</span><span class="punctuation">,</span></span><br><span class="line">objective<span class="operator">=</span><span class="string">&#x27;binary:logistic&#x27;</span><span class="punctuation">,</span> nrounds<span class="operator">=</span><span class="number">100</span><span class="punctuation">,</span> verbose<span class="operator">=</span><span class="number">0</span><span class="punctuation">)</span></span><br><span class="line">pred <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>xgb<span class="punctuation">,</span> predictors<span class="punctuation">[</span>fold_idx<span class="punctuation">,</span><span class="punctuation">]</span><span class="punctuation">)</span></span><br><span class="line">error<span class="punctuation">[</span>i<span class="punctuation">,</span> k<span class="punctuation">]</span> <span class="operator">&lt;-</span> mean<span class="punctuation">(</span><span class="built_in">abs</span><span class="punctuation">(</span>label<span class="punctuation">[</span>fold_idx<span class="punctuation">]</span> <span class="operator">-</span> pred<span class="punctuation">)</span> <span class="operator">&gt;=</span> <span class="number">0.5</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>在下面的 Python
代码中，我们创建了所有可能的超参数组合，并使用每种组合来拟合和评估模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">idx = np.random.choice(<span class="built_in">range</span>(<span class="number">5</span>), size=<span class="built_in">len</span>(X), replace=<span class="literal">True</span>)</span><br><span class="line">error = []</span><br><span class="line"><span class="keyword">for</span> eta, max_depth <span class="keyword">in</span> product([<span class="number">0.1</span>, <span class="number">0.5</span>, <span class="number">0.9</span>], [<span class="number">3</span>, <span class="number">6</span>, <span class="number">9</span>]):</span><br><span class="line">xgb = XGBClassifier(objective=<span class="string">&#x27;binary:logistic&#x27;</span>, n_estimators=<span class="number">250</span>,</span><br><span class="line">max_depth=max_depth, learning_rate=eta)</span><br><span class="line">cv_error = []</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">fold_idx = idx == k</span><br><span class="line">train_X = X.loc[~fold_idx]; train_y = y[~fold_idx]</span><br><span class="line">valid_X = X.loc[fold_idx]; valid_y = y[fold_idx]</span><br><span class="line">xgb.fit(train_X, train_y)</span><br><span class="line">pred = xgb.predict_proba(valid_X)[:, <span class="number">1</span>]</span><br><span class="line">cv_error.append(np.mean(<span class="built_in">abs</span>(valid_y - pred) &gt; <span class="number">0.5</span>))</span><br><span class="line">error.append(&#123;</span><br><span class="line"><span class="string">&#x27;eta&#x27;</span>: eta,</span><br><span class="line"><span class="string">&#x27;max_depth&#x27;</span>: max_depth,</span><br><span class="line"><span class="string">&#x27;avg_error&#x27;</span>: np.mean(cv_error)</span><br><span class="line">&#125;)</span><br><span class="line"><span class="built_in">print</span>(error[-<span class="number">1</span>])</span><br><span class="line">errors = pd.DataFrame(error)</span><br></pre></td></tr></table></figure>
<p>我们使用 Python 标准库中的 <code>itertools.product</code>
函数来创建这两个超参数的所有可能组合。</p>
<p>由于我们总共要拟合45个模型，这可能需要一些时间。误差以矩阵形式存储，行代表模型，列代表折叠。使用
<code>rowMeans</code> 函数，我们可以比较不同参数集的误差率：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">avg_error <span class="operator">&lt;-</span> 100 <span class="operator">*</span> <span class="built_in">round</span><span class="punctuation">(</span>rowMeans<span class="punctuation">(</span>error<span class="punctuation">)</span><span class="punctuation">,</span> <span class="number">4</span><span class="punctuation">)</span></span><br><span class="line">cbind<span class="punctuation">(</span>params<span class="punctuation">,</span> avg_error<span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">eta max_depth avg_error</span><br><span class="line">1 0.1 3 32.90</span><br><span class="line">2 0.5 3 33.43</span><br><span class="line">3 0.9 3 34.36</span><br><span class="line">4 0.1 6 33.08</span><br><span class="line">5 0.5 6 35.60</span><br><span class="line">6 0.9 6 37.82</span><br><span class="line">7 0.1 12 34.56</span><br><span class="line">8 0.5 12 36.83</span><br><span class="line">9 0.9 12 38.18</span><br></pre></td></tr></table></figure>
<p><strong>交叉验证</strong>表明，使用<strong>较浅的树</strong>和<strong>较小的
<code>eta/learning_rate</code>
值</strong>可以得到更准确的结果。由于这些模型也更稳定，因此最佳参数是
<code>eta=0.1</code> 和 <code>max_depth=3</code>（或者可能是
<code>max_depth=6</code>）。</p>
<p><strong>XGBoost 超参数</strong></p>
<p><code>xgboost</code>
的超参数主要用于在<strong>准确率</strong>、<strong>计算复杂度</strong>与<strong>过拟合</strong>之间取得平衡。有关参数的完整讨论，请参阅
<code>xgboost</code> 文档。</p>
<ul>
<li><p><strong><code>eta</code>/<code>learning_rate</code></strong>
应用于提升算法中 <span class="math inline">\(\alpha\)</span>
的<strong>收缩因子</strong>，取值范围在0到1之间。默认值为0.3，但对于噪声数据，建议使用较小的值（例如0.1）。在
Python 中，默认值为0.1。</p></li>
<li><p><strong><code>nrounds</code>/<code>n_estimators</code></strong>
<strong>提升轮数</strong>。如果 <code>eta</code>
被设置为一个较小的值，增加轮数很重要，因为算法学习得更慢了。只要包含了一些参数来防止过拟合，更多的轮次并不会带来坏处。</p></li>
<li><p><strong><code>max_depth</code></strong>
树的<strong>最大深度</strong>（默认值为6）。与拟合非常深的树的<strong>随机森林</strong>相反，提升法通常拟合<strong>较浅的树</strong>。这有一个好处，即可以避免因噪声数据而在模型中产生<strong>虚假复杂的交互作用</strong>。在
Python 中，默认值为3。</p></li>
<li><p><strong><code>subsample</code> 和
<code>colsample_bytree</code></strong>
<strong>无放回抽样</strong>的记录比例和用于拟合树的<strong>预测变量抽样</strong>比例。这些参数类似于随机森林中的参数，有助于<strong>避免过拟合</strong>。默认值为1.0。</p></li>
<li><p><strong><code>lambda</code>/<code>reg_lambda</code> 和
<code>alpha</code>/<code>reg_alpha</code></strong>
用于帮助控制<strong>过拟合</strong>的<strong>正则化参数</strong>（参见第274页的“正则化：避免过拟合”）。Python
的默认值为 <code>reg_lambda=1</code> 和 <code>reg_alpha=0</code>。在 R
中，这两个值的默认值均为0。</p></li>
</ul>
<p><strong>关键思想</strong></p>
<ul>
<li><strong>提升法</strong>是一类集成模型，它基于拟合一系列模型，并在连续轮次中对具有较大误差的记录赋予更多的权重。</li>
<li><strong>随机梯度提升</strong>是最通用的提升类型，并提供最佳性能。最常见的随机梯度提升形式使用树模型。</li>
<li><strong>XGBoost</strong>
是一种流行且计算高效的<strong>随机梯度提升</strong>软件包；它可用于数据科学中所有常用语言。</li>
<li>提升法<strong>容易过拟合</strong>数据，因此需要<strong>调优超参数</strong>以避免这种情况。</li>
<li><strong>正则化</strong>是一种避免过拟合的方法，它通过在模型的参数数量（例如，树的大小）上包含一个惩罚项来实现。</li>
<li>由于需要设置大量的超参数，<strong>交叉验证</strong>对于提升法尤其重要。</li>
</ul>
<h3 id="小结"><strong>小结</strong></h3>
<p>本章描述了两种分类和预测方法，它们<strong>灵活且局部地</strong>从数据中“学习”，而不是像线性回归那样，从一个对整个数据集进行拟合的结构化模型开始。<strong>K-近邻</strong>是一种简单的过程，它通过查看周围相似的记录，并将它们的<strong>多数类别</strong>（或平均值）分配给被预测的记录。<strong>树模型</strong>则通过尝试各种预测变量的截止（分割）值，迭代地将数据划分为<strong>越来越同质</strong>的区域和子区域。最有效的分割值形成一条路径，也形成一条通往分类或预测的“规则”。树模型是一种非常强大且流行的预测工具，通常优于其他方法。它们催生了各种<strong>集成方法</strong>（随机森林、提升法、装袋法），这些方法增强了树的预测能力。</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/AI/" rel="tag"># AI</a>
              <a href="/tags/Python/" rel="tag"># Python</a>
              <a href="/tags/%E7%BB%9F%E8%AE%A1/" rel="tag"># 统计</a>
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a>
              <a href="/tags/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6/" rel="tag"># 数据科学</a>
              <a href="/tags/R/" rel="tag"># R</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2025/09/25/%E7%AC%AC7%E7%AB%A0%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" rel="prev" title="第7章 无监督学习">
                  <i class="fa fa-chevron-left"></i> 第7章 无监督学习
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2025/09/25/%E7%AC%AC4%E7%AB%A0%20%E5%9B%9E%E5%BD%92%E4%B8%8E%E9%A2%84%E6%B5%8B/" rel="next" title="第4章 回归与预测">
                  第4章 回归与预测 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Rayman.hung</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
<script src="https://cdn.jsdelivr.net/npm/hexo-generator-searchdb@1.4.0/dist/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js","integrity":"sha256-r+3itOMtGGjap0x+10hu6jW/gZCzxHsoKrOd7gyRSGY="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>

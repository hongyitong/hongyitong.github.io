<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.1.1">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" integrity="sha256-DfWjNxDkM94fVBWx1H5BMMp0Zq7luBlV8QRcSES7s+0=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"hongyitong.github.io","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.11.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="第11章 高斯混合模型的密度估计 Density Estimation with Gaussian Mixture Models 在前几章中，我们已经讨论了机器学习中的两个基本问题：回归（第 9 章）和降维（第 10 章）。在本章中，我们将探讨机器学习的第三个支柱：密度估计。在这一过程中，我们会引入一些重要的概念，例如期望最大化（EM）算法，以及通过混合模型来理解密度估计的潜在变量视角。  当我们">
<meta property="og:type" content="article">
<meta property="og:title" content="《机器学习的数学基础》第11章&quot;高斯混合模型的密度估计&quot;">
<meta property="og:url" content="http://hongyitong.github.io/2025/09/09/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E7%AC%AC11%E7%AB%A0%22%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AF%86%E5%BA%A6%E4%BC%B0%E8%AE%A1%22/index.html">
<meta property="og:site_name" content="忆桐之家的博客">
<meta property="og:description" content="第11章 高斯混合模型的密度估计 Density Estimation with Gaussian Mixture Models 在前几章中，我们已经讨论了机器学习中的两个基本问题：回归（第 9 章）和降维（第 10 章）。在本章中，我们将探讨机器学习的第三个支柱：密度估计。在这一过程中，我们会引入一些重要的概念，例如期望最大化（EM）算法，以及通过混合模型来理解密度估计的潜在变量视角。  当我们">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Part2/F11.1.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Part2/F11.2.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Part2/F11.3.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Part2/F11.4.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Part2/F11.5.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Part2/F11.6.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Part2/F11.7.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Part2/F11.8.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Part2/F11.9.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Part2/F11.10.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Part2/F11.11.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Part2/F11.12.png">
<meta property="og:image" content="http://hongyitong.github.io/2025/09/09/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E7%AC%AC11%E7%AB%A0%22%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AF%86%E5%BA%A6%E4%BC%B0%E8%AE%A1%22/F11.13.png">
<meta property="article:published_time" content="2025-09-08T16:00:00.000Z">
<meta property="article:modified_time" content="2025-09-10T02:27:40.545Z">
<meta property="article:author" content="Rayman.hung">
<meta property="article:tag" content="数学">
<meta property="article:tag" content="算法">
<meta property="article:tag" content="统计">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://hongyitong.github.io/img3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Part2/F11.1.png">


<link rel="canonical" href="http://hongyitong.github.io/2025/09/09/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E7%AC%AC11%E7%AB%A0%22%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AF%86%E5%BA%A6%E4%BC%B0%E8%AE%A1%22/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://hongyitong.github.io/2025/09/09/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E7%AC%AC11%E7%AB%A0%22%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AF%86%E5%BA%A6%E4%BC%B0%E8%AE%A1%22/","path":"2025/09/09/《机器学习的数学基础》第11章\"高斯混合模型的密度估计\"/","title":"《机器学习的数学基础》第11章\"高斯混合模型的密度估计\""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>《机器学习的数学基础》第11章"高斯混合模型的密度估计" | 忆桐之家的博客</title>
  





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">忆桐之家的博客</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Rayman&Tony</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section">首页</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section">分类</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section">关于</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section">归档</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section">标签</a></li><li class="menu-item menu-item-commonweal"><a href="/404.html" rel="section">公益 404</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC11%E7%AB%A0-%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AF%86%E5%BA%A6%E4%BC%B0%E8%AE%A1"><span class="nav-number">1.</span> <span class="nav-text">第11章
高斯混合模型的密度估计</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.1.</span> <span class="nav-text">11.1 高斯混合模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0"><span class="nav-number">1.2.</span> <span class="nav-text">11.2 最大似然参数学习</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%B4%A3%E4%BB%BB"><span class="nav-number">1.2.1.</span> <span class="nav-text">11.2.1 责任</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9B%B4%E6%96%B0%E5%9D%87%E5%80%BC"><span class="nav-number">1.2.2.</span> <span class="nav-text">11.2.2 更新均值</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9A%84%E6%9B%B4%E6%96%B0"><span class="nav-number">1.2.3.</span> <span class="nav-text">11.2.3 协方差的更新</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9B%B4%E6%96%B0%E6%B7%B7%E5%90%88%E6%9D%83%E9%87%8D"><span class="nav-number">1.2.4.</span> <span class="nav-text">11.2.4 更新混合权重</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#em-%E7%AE%97%E6%B3%95"><span class="nav-number">1.3.</span> <span class="nav-text">11.3 EM 算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%BD%9C%E5%8F%98%E9%87%8F%E8%A7%86%E8%A7%92"><span class="nav-number">1.4.</span> <span class="nav-text">11.4 潜变量视角</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%94%9F%E6%88%90%E8%BF%87%E7%A8%8B%E4%B8%8E%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.4.1.</span> <span class="nav-text">11.4.1
生成过程与概率模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BC%BC%E7%84%B6"><span class="nav-number">1.4.2.</span> <span class="nav-text">11.4.2 似然</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%90%8E%E9%AA%8C%E5%88%86%E5%B8%83"><span class="nav-number">1.4.3.</span> <span class="nav-text">11.4.3 后验分布</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%89%A9%E5%B1%95%E5%88%B0%E6%95%B4%E4%B8%AA%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">1.4.4.</span> <span class="nav-text">11.4.4 扩展到整个数据集</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#em-%E7%AE%97%E6%B3%95%E5%86%8D%E6%8E%A2"><span class="nav-number">1.4.5.</span> <span class="nav-text">11.4.5 EM 算法再探</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BB%B6%E4%BC%B8%E9%98%85%E8%AF%BB"><span class="nav-number">1.5.</span> <span class="nav-text">11.5 延伸阅读</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Rayman.hung</p>
  <div class="site-description" itemprop="description">技术分享、读书心得、亲子时刻</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives">
          <span class="site-state-item-count">900</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags">
        <span class="site-state-item-count">1149</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/hongyitong" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;hongyitong" rel="noopener" target="_blank">GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="http://blog.sina.com.cn/yitonghong" title="忆桐之家 → http:&#x2F;&#x2F;blog.sina.com.cn&#x2F;yitonghong" rel="noopener" target="_blank">忆桐之家</a>
      </span>
      <span class="links-of-author-item">
        <a href="http://douban.com/people/2780741" title="豆瓣 → http:&#x2F;&#x2F;douban.com&#x2F;people&#x2F;2780741" rel="noopener" target="_blank">豆瓣</a>
      </span>
      <span class="links-of-author-item">
        <a href="http://www.zhihu.com/people/rayman-36" title="知乎 → http:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;rayman-36" rel="noopener" target="_blank">知乎</a>
      </span>
      <span class="links-of-author-item">
        <a href="http://www.liaoxuefeng.com/" title="廖雪峰的官方网站 → http:&#x2F;&#x2F;www.liaoxuefeng.com&#x2F;" rel="noopener" target="_blank">廖雪峰的官方网站</a>
      </span>
      <span class="links-of-author-item">
        <a href="http://www.vaikan.com/" title="外刊IT评论 → http:&#x2F;&#x2F;www.vaikan.com&#x2F;" rel="noopener" target="_blank">外刊IT评论</a>
      </span>
  </div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://hongyitong.github.io/2025/09/09/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E7%AC%AC11%E7%AB%A0%22%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AF%86%E5%BA%A6%E4%BC%B0%E8%AE%A1%22/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Rayman.hung">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="忆桐之家的博客">
      <meta itemprop="description" content="技术分享、读书心得、亲子时刻">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="《机器学习的数学基础》第11章"高斯混合模型的密度估计" | 忆桐之家的博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          《机器学习的数学基础》第11章"高斯混合模型的密度估计"
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-09-09 00:00:00" itemprop="dateCreated datePublished" datetime="2025-09-09T00:00:00+08:00">2025-09-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-09-10 10:27:40" itemprop="dateModified" datetime="2025-09-10T10:27:40+08:00">2025-09-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/" itemprop="url" rel="index"><span itemprop="name">读书心得</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h2 id="第11章-高斯混合模型的密度估计">第11章
高斯混合模型的密度估计</h2>
<p>Density Estimation with Gaussian Mixture Models</p>
<p>在前几章中，我们已经讨论了机器学习中的两个基本问题：回归（第 9
章）和降维（第 10
章）。在本章中，我们将探讨机器学习的第三个支柱：<strong>密度估计</strong>。在这一过程中，我们会引入一些重要的概念，例如期望最大化（EM）算法，以及通过混合模型来理解密度估计的潜在变量视角。</p>
<p><img src="/img3/机器学习的数学基础Part2/F11.1.png" alt="F11.1" style="zoom:50%;" /></p>
<p>当我们将机器学习应用于数据时，往往希望<strong>以某种方式来表示数据</strong>。<strong>一个直接的方法就是将数据点本身作为数据的表示</strong>；例如见图
11.1。
然而，如果数据集非常庞大，或者我们更关心数据的整体特征，那么这种方式可能就不太有用了。<strong>在密度估计中，我们通过使用某个参数化分布族中的分布（例如高斯分布或
Beta
分布），来紧凑地表示数据。</strong>比如，我们可能希望通过数据集的均值和方差，用一个高斯分布来进行紧凑表示。均值和方差可以通过第
8.3
节讨论的工具（最大似然估计或最大后验估计）来求得。接着，<strong>我们就可以用这个高斯分布的均值和方差来表示数据背后的分布，即我们认为该数据集是从这个分布中采样得到的一次典型实现。</strong></p>
<blockquote>
<p>个人注：高斯混合模型的<strong>密度估计</strong>是一种表示数据的方法！其实也是一种建模，因为这个分布就是对应该类数据的模型表达。</p>
</blockquote>
<span id="more"></span>
<p>在实际应用中，高斯分布（或类似我们迄今遇到的其他分布）具有有限的建模能力。例如，用一个高斯分布去近似图
11.1
中生成数据的真实密度，就会是一个很差的近似。接下来，我们将研究一种更具表现力的分布族，可以用来进行密度估计：<strong>混合模型</strong>。</p>
<p>混合模型可以通过 <strong>K 个简单（基）分布的凸组合</strong>
来描述一个分布 <span class="math inline">\(p(x)\)</span>： <span
class="math display">\[
p(x) = \sum_{k=1}^K \pi_k p_k(x) \tag{11.1}
\]</span> 满足： <span class="math display">\[
0 \leq \pi_k \leq 1, \quad \sum_{k=1}^K \pi_k = 1 \tag{11.2}
\]</span> 其中，各个成分分布 <span class="math inline">\(p_k\)</span>
属于某个基本分布族，例如高斯分布、伯努利分布或伽马分布，而 <span
class="math inline">\(\pi_k\)</span>
是混合权重。混合模型比相应的单个基分布更具表现力，因为它们可以表示多峰的数据分布，即能够描述具有多个“簇”的数据集，就像图
11.1 中的例子。</p>
<p>我们将重点讨论<strong>高斯混合模型（GMMs）</strong>，其中的基本分布是高斯分布。给定一个数据集，我们的目标是通过最大化模型参数的似然来训练
GMM。为此，我们将会用到第 5 章、第 6 章和第 7.2
节的结果。然而，与之前讨论过的应用（线性回归或主成分分析）不同，这里我们无法得到最大似然的闭式解。相反，我们将得到一组相互依赖的联立方程，而这些方程只能通过<strong>迭代方法</strong>来求解。</p>
<h3 id="高斯混合模型">11.1 高斯混合模型</h3>
<p>高斯混合模型（Gaussian Mixture Model,
GMM）是一种密度模型，它通过组合有限个 <span
class="math inline">\(K\)</span> 个高斯分布 <span
class="math inline">\(\mathcal{N}(x|\mu_k, \Sigma_k)\)</span> 来表示：
<span class="math display">\[
p(x|\theta) = \sum_{k=1}^K \pi_k \, \mathcal{N}(x|\mu_k, \Sigma_k)
\tag{11.3}
\]</span> 其中： <span class="math display">\[
0 \leq \pi_k \leq 1, \quad \sum_{k=1}^K \pi_k = 1, \tag{11.4}
\]</span> 我们定义 <span class="math display">\[
\theta := \{ \mu_k, \Sigma_k, \pi_k : k = 1, \ldots, K \}
\]</span> 为该模型的所有参数集合。</p>
<p>这种对高斯分布的凸组合，相比单一的高斯分布，能为复杂密度的建模提供更强的灵活性（当
<span class="math inline">\(K=1\)</span> 时，(11.3)
就退化为单一高斯分布）。图 11.2
给出了一个示例，展示了加权后的各个分量以及最终的混合密度： <span
class="math display">\[
p(x|\theta) = 0.5 \, \mathcal{N}(x|-2, \tfrac{1}{2}) + 0.2 \,
\mathcal{N}(x|1, 2) + 0.3 \, \mathcal{N}(x|4, 1). \tag{11.5}
\]</span>
<img src="/img3/机器学习的数学基础Part2/F11.2.png" alt="F11.2" style="zoom:50%;" /></p>
<h3 id="最大似然参数学习">11.2 最大似然参数学习</h3>
<p>假设我们给定一个数据集<span class="math inline">\(X = \{x_1, \ldots,
x_N\},\)</span>其中 <span class="math inline">\(x_n,
n=1,\ldots,N\)</span>，是从未知分布 <span
class="math inline">\(p(x)\)</span>
中独立同分布采样得到的。我们的目标是用一个含有 <span
class="math inline">\(K\)</span> 个混合分量的 GMM
来近似/表示这个未知分布 <span class="math inline">\(p(x)\)</span>。GMM
的参数包括： <span class="math inline">\(K\)</span> 个均值 <span
class="math inline">\(\mu_k\)</span>， 协方差矩阵 <span
class="math inline">\(\Sigma_k\)</span>， 混合权重 <span
class="math inline">\(\pi_k\)</span>。我们把这些自由参数统一记为：<span
class="math inline">\(\theta := \{\pi_k, \mu_k, \Sigma_k : k = 1,
\ldots, K\}.\)</span></p>
<p><img src="/img3/机器学习的数学基础Part2/F11.3.png" alt="F11.3" style="zoom:50%;" /></p>
<p><strong>例 11.1（初始设定）</strong></p>
<p>在本章中，我们将用一个简单的贯穿示例来帮助说明和可视化关键概念。我们考虑一个一维数据集</p>
<p><span class="math display">\[
X = \{-3, -2.5, -1, 0, 2, 4, 5\},
\]</span> 它包含 7 个数据点。我们希望用一个含有 <span
class="math inline">\(K=3\)</span> 个分量的 GMM
来建模该数据的密度。初始化混合分量如下： <span class="math display">\[
p_1(x) = \mathcal{N}(x|-4, 1) \tag{11.6}
\]</span></p>
<p><span class="math display">\[
p_2(x) = \mathcal{N}(x|0, 0.2) \tag{11.7}
\]</span></p>
<p><span class="math display">\[
p_3(x) = \mathcal{N}(x|8, 3) \tag{11.8}
\]</span></p>
<p>并给它们分配相等的权重： <span class="math display">\[
\pi_1 = \pi_2 = \pi_3 = \tfrac{1}{3}.
\]</span> 相应的模型（以及数据点）如图 11.3 所示。</p>
<p>在下面的内容中，我们将详细说明如何得到模型参数 θ 的最大似然估计 <span
class="math inline">\(θ_{ML}\)</span>。我们首先写出似然函数，即在给定参数的情况下训练数据的预测分布。我们利用独立同分布
(i.i.d.) 假设，这会导致似然函数的分解形式： <span
class="math display">\[
p(X|\theta) = \prod_{n=1}^N p(x_n|\theta), \quad
p(x_n|\theta) = \sum_{k=1}^K \pi_k \, \mathcal{N}(x_n|\mu_k,\Sigma_k),
\tag{11.9}
\]</span> 其中每个单独的似然项 <span
class="math inline">\(p(x_n|\theta)\)</span>
都是一个高斯混合密度。然后我们得到对数似然： <span
class="math display">\[
\log p(X|\theta) = \sum_{n=1}^N \log p(x_n|\theta)
= \sum_{n=1}^N \log \Bigg( \sum_{k=1}^K \pi_k \,
\mathcal{N}(x_n|\mu_k,\Sigma_k) \Bigg). \tag{11.10}
\]</span> 记作 <span
class="math inline">\(L\)</span>。我们的目标是找到能最大化对数似然 <span
class="math inline">\(L\)</span> 的参数 <span
class="math inline">\(\theta^*_{ML}\)</span>。通常的做法是计算对数似然关于模型参数
θ 的梯度 <span class="math inline">\(dL/d\theta\)</span>，令其等于
0，然后解出 θ。</p>
<p>然而，与之前最大似然估计的例子（例如第 9.2
节讨论的线性回归）不同，这里无法得到一个封闭解。但我们可以利用一种迭代方法来找到较好的模型参数
<span
class="math inline">\(θ_{ML}\)</span>，这实际上就是用于<strong>高斯混合模型
(GMM) 的 EM
算法。关键思想是：在保持其他参数固定的情况下，逐一更新模型参数。</strong></p>
<p><strong>备注</strong>：如果我们考虑的目标密度只是单个高斯分布，那么式
(11.10) 中对<span
class="math inline">\(k\)</span>的求和就消失了，此时对数可以直接作用于高斯分布，从而得到：
<span class="math display">\[
\log \mathcal{N}(x|\mu,\Sigma)
= -\frac{D}{2}\log(2\pi) - \tfrac{1}{2}\log \det(\Sigma) -
\tfrac{1}{2}(x-\mu)^\top \Sigma^{-1}(x-\mu). \tag{11.11}
\]</span> 这个简单的形式使得我们可以得到 <span
class="math inline">\(\mu\)</span> 和 <span
class="math inline">\(\Sigma\)</span> 的封闭形式最大似然估计（第 8
章已讨论）。但在 (11.10) 中，我们无法将对数移入对<span
class="math inline">\(k\)</span>的求和中，因此无法得到简单的封闭解。</p>
<p>任何函数的局部最优点都必须满足梯度关于参数为零的性质（必要条件，见第
7 章）。在我们的情形下，对数似然 (11.10) 关于 GMM 参数 <span
class="math inline">\(\mu_k, \Sigma_k, \pi_k\)</span>
的优化给出了如下必要条件： <span class="math display">\[
\begin{align}
\frac{\partial \mathcal{L}}{\partial \mu_k} = \mathbf{0}^\top
&amp;\iff \sum_{n=1}^N \frac{\partial \log p(x_n \mid \theta)}{\partial
\mu_k} = \mathbf{0}^\top, \tag{11.12}\\[1em]
\frac{\partial \mathcal{L}}{\partial \Sigma_k} = 0
&amp;\iff \sum_{n=1}^N \frac{\partial \log p(x_n \mid \theta)}{\partial
\Sigma_k} = 0, \tag{11.13}\\[1em]
\frac{\partial \mathcal{L}}{\partial \pi_k} = 0
&amp;\iff \sum_{n=1}^N \frac{\partial \log p(x_n \mid \theta)}{\partial
\pi_k} = 0. \tag{11.14}
\end{align}
\]</span> 对于所有这三类必要条件，通过应用链式法则（见 5.2.2
节），我们需要如下形式的偏导数： <span class="math display">\[
\frac{\partial \log p(x_n|\theta)}{\partial \theta}
= \frac{1}{p(x_n|\theta)} \, \frac{\partial p(x_n|\theta)}{\partial
\theta}, \tag{11.15}
\]</span> 其中 <span class="math inline">\(\theta = \{\mu_k, \Sigma_k,
\pi_k, k=1,\ldots,K\}\)</span> 是模型参数，并且 <span
class="math display">\[
\frac{1}{p(x_n|\theta)}
= \frac{1}{\sum_{j=1}^K \pi_j \, \mathcal{N}(x_n|\mu_j,\Sigma_j)}.
\tag{11.16}
\]</span> 在接下来的内容中，我们将计算 (11.12) 到 (11.14)
的偏导数。但在此之前，我们先引入一个将在本章余下部分起核心作用的重要量：<strong>责任
(responsibilities)</strong>。</p>
<h4 id="责任">11.2.1 责任</h4>
<p>Responsibilities</p>
<p>我们定义如下量： <span class="math display">\[
r_{nk} := \frac{\pi_k \mathcal{N}(x_n|\mu_k, \Sigma_k)}{\sum_{j=1}^K
\pi_j \mathcal{N}(x_n|\mu_j, \Sigma_j)} \tag{11.17}
\]</span> 它表示<strong>第 <span class="math inline">\(k\)</span>
个混合成分对第 <span class="math inline">\(n\)</span>
个数据点的责任</strong>。第 <span class="math inline">\(k\)</span>
个混合成分对数据点 <span class="math inline">\(x_n\)</span> 的责任 <span
class="math inline">\(r_{nk}\)</span>，与下式成正比：</p>
<p><span class="math display">\[
p(x_n|\pi_k, \mu_k, \Sigma_k) = \pi_k \mathcal{N}(x_n|\mu_k, \Sigma_k)
\tag{11.18}
\]</span>
即该混合成分在给定数据点下的似然。因此，当某个数据点更可能是由某个混合成分生成时，该成分对该数据点的责任就会更大。注意：</p>
<p><span class="math display">\[
r_n := [r_{n1}, \ldots, r_{nK}]^\top \in \mathbb{R}^K
\]</span> 是一个<strong>归一化的概率向量</strong>，即满足 <span
class="math inline">\(\sum_k r_{nk} = 1\)</span>，且 <span
class="math inline">\(r_{nk} \geq 0\)</span>。这个概率向量在 <span
class="math inline">\(K\)</span> 个混合成分之间分配概率质量，我们可以把
<span class="math inline">\(r_n\)</span> 看作是对 <span
class="math inline">\(x_n\)</span> 在 <span
class="math inline">\(K\)</span> 个混合成分之间的“软分配”。因此，公式
(11.17) 中的责任 <span class="math inline">\(r_{nk}\)</span>，就代表了
<span class="math inline">\(x_n\)</span> 是由第 <span
class="math inline">\(k\)</span> 个混合成分生成的概率。</p>
<p><strong>例 11.2 （责任）</strong></p>
<p>针对图 11.3 中的例子，我们计算得到责任 <span
class="math inline">\(r_{nk}\)</span>： <span class="math display">\[
\begin{bmatrix}
1.0 &amp; 0.0 &amp; 0.0 \\
1.0 &amp; 0.0 &amp; 0.0 \\
0.057 &amp; 0.943 &amp; 0.0 \\
0.001 &amp; 0.999 &amp; 0.0 \\
0.0 &amp; 0.066 &amp; 0.934 \\
0.0 &amp; 0.0 &amp; 1.0 \\
0.0 &amp; 0.0 &amp; 1.0
\end{bmatrix}
\in \mathbb{R}^{N \times K}. \tag{11.19}
\]</span> 这里，第 <span class="math inline">\(n\)</span>
行表示所有混合成分对数据点 <span class="math inline">\(x_n\)</span>
的责任。每个数据点的 <span class="math inline">\(K\)</span>
个责任（即每一行的和）都等于 1。第 <span
class="math inline">\(k\)</span> 列则概括了第 <span
class="math inline">\(k\)</span>
个混合成分的责任分布。我们可以看到，第三个混合成分（第三列）对前四个数据点没有责任，但在后面的数据点上承担了较多责任。某一列所有元素的和给出了
<span class="math inline">\(N_k\)</span>，即第 <span
class="math inline">\(k\)</span>
个混合成分的总责任。在本例中，我们得到：</p>
<p><span class="math display">\[
N_1 = 2.058, \quad N_2 = 2.008, \quad N_3 = 2.934.
\]</span> 下面我们来确定在给定责任值的情况下，模型参数<span
class="math inline">\(\mu_k, \Sigma_k, \pi_k\)</span>
的更新方式。我们将看到，这些更新方程都依赖于责任值，这使得最大似然估计问题无法得到闭式解。然而，在给定责任值的情况下，我们会一次只更新一个模型参数，同时保持其他参数固定。之后，我们会重新计算责任值。通过反复迭代这两个步骤，最终会收敛到一个局部最优解，这就是
EM 算法的一种具体实现。我们将在第 11.3 节更详细地讨论这一点。</p>
<h4 id="更新均值">11.2.2 更新均值</h4>
<p><strong>定理 11.1（GMM 均值的更新）</strong>GMM 的均值参数 <span
class="math inline">\(\mu_k, k=1,\dots,K\)</span> 的更新公式为 <span
class="math display">\[
\mu_k^{\text{new}} = \frac{\sum_{n=1}^N r_{nk} x_n}{\sum_{n=1}^N
r_{nk}}, \tag{11.20}
\]</span> 其中责任值 <span class="math inline">\(r_{nk}\)</span>
定义在公式 (11.17) 中。</p>
<p><strong>注释</strong>：在公式 (11.20) 中，每个混合成分的均值 <span
class="math inline">\(\mu_k\)</span> 的更新依赖于所有均值、协方差矩阵
<span class="math inline">\(\Sigma_k\)</span> 和混合权重 <span
class="math inline">\(\pi_k\)</span>，因为它们都通过 <span
class="math inline">\(r_{nk}\)</span>（定义见
(11.17)）联系起来。因此，我们无法一次性求出所有 <span
class="math inline">\(\mu_k\)</span> 的闭式解。</p>
<p><strong>证明</strong>：由 (11.15) 可知，对均值参数 <span
class="math inline">\(\mu_k, k=1,\dots,K\)</span>
的对数似然函数梯度需要我们计算偏导数： <span class="math display">\[
\frac{\partial p(x_n|\theta)}{\partial \mu_k}
= \sum_{j=1}^K \pi_j \frac{\partial
\mathcal{N}(x_n|\mu_j,\Sigma_j)}{\partial \mu_k}
= \pi_k \frac{\partial \mathcal{N}(x_n|\mu_k,\Sigma_k)}{\partial \mu_k},
\tag{11.21a}
\]</span> 因为只有第 <span class="math inline">\(k\)</span>
个混合成分依赖于 <span
class="math inline">\(\mu_k\)</span>。进一步得到： <span
class="math display">\[
= \pi_k (x_n - \mu_k)^\top \Sigma_k^{-1}
\mathcal{N}(x_n|\mu_k,\Sigma_k). \tag{11.21b}
\]</span> 将 (11.21b) 的结果代入
(11.15)，并整理后可以得到所需的对数似然函数关于 <span
class="math inline">\(\mu_k\)</span> 的偏导数： <span
class="math display">\[
\begin{align}
\frac{\partial \mathcal{L}}{\partial \boldsymbol{\mu}_k}
&amp;= \sum_{n=1}^{N}
\frac{\partial \log p(\mathbf{x}_n \mid \theta)}{\partial
\boldsymbol{\mu}_k} \notag\\[6pt]
&amp;= \sum_{n=1}^{N}
\frac{1}{p(\mathbf{x}_n \mid \theta)}
\frac{\partial p(\mathbf{x}_n \mid \theta)}{\partial \boldsymbol{\mu}_k}
\tag{11.22a}\\[6pt]
&amp;= \sum_{n=1}^{N}
\left( \mathbf{x}_n - \boldsymbol{\mu}_k \right)^{\top}
\boldsymbol{\Sigma}_k^{-1}
\frac{\pi_k \mathcal{N}\!\bigl(\mathbf{x}_n \mid \boldsymbol{\mu}_k,
\boldsymbol{\Sigma}_k\bigr)}
{\sum_{j=1}^{K} \pi_j \mathcal{N}\!\bigl(\mathbf{x}_n \mid
\boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j\bigr)}
\tag{11.22b}\\[6pt]
&amp;= \sum_{n=1}^{N}
r_{nk} \left( \mathbf{x}_n - \boldsymbol{\mu}_k \right)^{\top}
\boldsymbol{\Sigma}_k^{-1}
\tag{11.22c}
\end{align}
\]</span></p>
<p>其中，<span class="math inline">\(r_{nk}\)</span> 就是我们在 (11.17)
中定义的责任值。</p>
<p>我们现在解 (11.22c) 关于 <span
class="math inline">\(\mu_k^{\text{new}}\)</span>，使得 <span
class="math display">\[
\frac{\partial L(\mu_k^{\text{new}})}{\partial \mu_k} = 0^\top
\]</span> 并得到 <span class="math display">\[
\sum_{n=1}^N r_{nk} x_n = \sum_{n=1}^N r_{nk} \mu_k^{\text{new}}
\;\;\Longleftrightarrow\;\;
\mu_k^{\text{new}} = \frac{\sum_{n=1}^N r_{nk} x_n}{\sum_{n=1}^N r_{nk}}
= \frac{1}{N_k} \sum_{n=1}^N r_{nk} x_n,
\tag{11.23}
\]</span> 其中我们定义 <span class="math display">\[
N_k := \sum_{n=1}^N r_{nk}
\tag{11.24}
\]</span> 为第 <span class="math inline">\(k\)</span>
个混合成分对整个数据集的总责任值。至此，定理 11.1 的证明完成。</p>
<p>直观上，(11.20)
可以解释为均值的一个带重要性权重的蒙特卡罗估计，其中数据点 <span
class="math inline">\(x_n\)</span> 的重要性权重由第 <span
class="math inline">\(k\)</span> 个簇对 <span
class="math inline">\(x_n\)</span> 的责任值 <span
class="math inline">\(r_{nk}\)</span> 给出，<span
class="math inline">\(k = 1, \dots, K\)</span>。</p>
<p>因此，均值 <span class="math inline">\(\mu_k\)</span>
会被“拉向”数据点 <span class="math inline">\(x_n\)</span>，其强度由
<span class="math inline">\(r_{nk}\)</span>
决定。当对应的混合成分对数据点具有较高责任（即较高似然）时，均值会更强烈地被该数据点所吸引。图
11.4 对此进行了说明。</p>
<p><img src="/img3/机器学习的数学基础Part2/F11.4.png" alt="F11.4" style="zoom:67%;" /></p>
<p>我们还可以将 (11.20) 中的均值更新解释为在如下分布下所有数据点的期望：
<span class="math display">\[
r_k := \frac{[r_{1k}, \dots, r_{Nk}]^\top}{N_k},
\tag{11.25}
\]</span> 这是一个归一化的概率向量，即 <span class="math display">\[
\mu_k \;\;\leftarrow\;\; \mathbb{E}_{r_k}[X].
\tag{11.26}
\]</span> 例 11.3（均值更新）</p>
<p>在图 11.3 的例子中，均值的更新如下： <span class="math display">\[
\mu_1: -4 \;\to\; -2.7 \tag{11.27}
\]</span></p>
<p><span class="math display">\[
\mu_2: 0 \;\to\; -0.4 \tag{11.28}
\]</span></p>
<p><span class="math display">\[
\mu_3: 8 \;\to\; 3.7 \tag{11.29}
\]</span></p>
<p>这里可以看到，第一个和第三个混合成分的均值向数据所在的区域移动，而第二个成分的均值变化并不显著。图
11.5 展示了这一变化，其中图 11.5(a) 显示了在均值更新之前的 GMM 密度，图
11.5(b) 显示了均值 <span class="math inline">\(\mu_k\)</span> 更新之后的
GMM 密度。</p>
<p><img src="/img3/机器学习的数学基础Part2/F11.5.png" alt="F11.5" style="zoom:67%;" /></p>
<p>式 (11.20) 中均值参数的更新看起来相当直接。然而，需要注意的是，责任值
<span class="math inline">\(r_{nk}\)</span> 是 <span
class="math inline">\(\pi_j, \mu_j, \Sigma_j\)</span> （其中 <span
class="math inline">\(j = 1, \dots, K\)</span>）的函数，因此 (11.20)
中的更新依赖于 GMM 的所有参数。而像我们在第 9.2 节（线性回归）或第 10
章（PCA）中得到的那种闭式解，在这里是无法得到的。</p>
<h4 id="协方差的更新">11.2.3 协方差的更新</h4>
<p><strong>定理 11.2（GMM 协方差的更新）</strong>GMM 中第 <span
class="math inline">\(k\)</span> 个分量的协方差参数 <span
class="math inline">\(\Sigma_k, \; k = 1, \dots, K\)</span>
的更新公式为： <span class="math display">\[
\Sigma_k^{\text{new}} = \frac{1}{N_k} \sum_{n=1}^N r_{nk}(x_n -
\mu_k)(x_n - \mu_k)^\top,
\tag{11.30}
\]</span> 其中 <span class="math inline">\(r_{nk}\)</span> 和 <span
class="math inline">\(N_k\)</span> 分别定义在 (11.17) 和 (11.24)
中。</p>
<p><strong>证明</strong> 为了证明定理 11.2，我们的方法是对对数似然函数
<span class="math inline">\(L\)</span> 关于协方差 <span
class="math inline">\(\Sigma_k\)</span> 求偏导，将其设为 0，然后解出
<span class="math inline">\(\Sigma_k\)</span>。</p>
<p>首先写出一般形式： <span class="math display">\[
\frac{\partial L}{\partial \Sigma_k} = \sum_{n=1}^N \frac{\partial \log
p(x_n|\theta)}{\partial \Sigma_k}
= \sum_{n=1}^N \frac{1}{p(x_n|\theta)} \frac{\partial
p(x_n|\theta)}{\partial \Sigma_k}. \tag{11.31}
\]</span> 我们已经从 (11.16) 知道了 <span
class="math inline">\(1/p(x_n|\theta)\)</span>。为了得到剩余的偏导 <span
class="math inline">\(\partial p(x_n|\theta)/\partial
\Sigma_k\)</span>，我们写出高斯分布的定义 <span
class="math inline">\(p(x_n|\theta)\)</span>（见 (11.9)），并只保留第
<span class="math inline">\(k\)</span> 项： <span
class="math display">\[
\frac{\partial p(x_n|\theta)}{\partial \Sigma_k}
= \frac{\partial}{\partial \Sigma_k} \pi_k (2\pi)^{-\frac{D}{2}}
\det(\Sigma_k)^{-\frac{1}{2}}
\exp\left(-\tfrac{1}{2}(x_n - \mu_k)^\top \Sigma_k^{-1}(x_n -
\mu_k)\right). \tag{11.32}
\]</span> 利用以下恒等式： <span class="math display">\[
\frac{\partial}{\partial \Sigma_k} \det(\Sigma_k)^{-\tfrac{1}{2}}
= -\tfrac{1}{2} \det(\Sigma_k)^{-\tfrac{1}{2}} \Sigma_k^{-1},
\tag{11.33}
\]</span></p>
<p><span class="math display">\[
\frac{\partial}{\partial \Sigma_k} (x_n - \mu_k)^\top \Sigma_k^{-1} (x_n
- \mu_k)
= -\Sigma_k^{-1}(x_n - \mu_k)(x_n - \mu_k)^\top \Sigma_k^{-1},
\tag{11.34}
\]</span></p>
<p>得到（整理后）： <span class="math display">\[
\frac{\partial p(x_n|\theta)}{\partial \Sigma_k}
= \pi_k \mathcal{N}(x_n|\mu_k, \Sigma_k) \cdot
\left[-\tfrac{1}{2}\big(\Sigma_k^{-1} - \Sigma_k^{-1}(x_n - \mu_k)(x_n -
\mu_k)^\top \Sigma_k^{-1}\big)\right]. \tag{11.35}
\]</span> 代入 (11.31)，对数似然函数关于 <span
class="math inline">\(\Sigma_k\)</span> 的偏导为： <span
class="math display">\[
\frac{\partial L}{\partial \Sigma_k}
= \sum_{n=1}^N r_{nk} \cdot \left[-\tfrac{1}{2}\big(\Sigma_k^{-1} -
\Sigma_k^{-1}(x_n - \mu_k)(x_n - \mu_k)^\top \Sigma_k^{-1}\big)\right],
\tag{11.36}
\]</span> 其中 <span class="math inline">\(r_{nk}\)</span>
就是责任度（见 (11.17)）。</p>
<p>将其设为 0，得到最优条件： <span class="math display">\[
N_k \Sigma_k^{-1} = \Sigma_k^{-1} \left(\sum_{n=1}^N r_{nk}(x_n -
\mu_k)(x_n - \mu_k)^\top \right)\Sigma_k^{-1}. \tag{11.37}
\]</span> 解得： <span class="math display">\[
\Sigma_k^{\text{new}} = \frac{1}{N_k} \sum_{n=1}^N r_{nk}(x_n -
\mu_k)(x_n - \mu_k)^\top. \tag{11.38}
\]</span> 其中 <span class="math inline">\(r_k\)</span> 是定义在 (11.25)
中的概率向量。这样就得到了一个简单的更新规则，从而证明了定理 11.2。</p>
<p>类似于均值更新公式 (11.20)，协方差更新公式 (11.30)
可以解释为中心化数据平方的加权期望，其中权重为责任度： <span
class="math display">\[
\tilde{X}_k := \{x_1 - \mu_k, \dots, x_N - \mu_k\}.
\]</span>
也就是说，<strong>协方差是以责任值为权重的期望估计。</strong></p>
<p><strong>示例 11.4（方差更新）</strong></p>
<p>在图 11.3 的例子中，方差的更新如下： <span class="math display">\[
\sigma^2_1 : 1 \;\to\; 0.14 \tag{11.39}
\]</span></p>
<p><span class="math display">\[
\sigma^2_2 : 0.2 \;\to\; 0.44 \tag{11.40}
\]</span></p>
<p><span class="math display">\[
\sigma^2_3 : 3 \;\to\; 1.53 \tag{11.41}
\]</span></p>
<p>在这里我们看到，第一个和第三个分量的方差显著收缩，而第二个分量的方差则略微增加。</p>
<p><img src="/img3/机器学习的数学基础Part2/F11.6.png" alt="F11.6" style="zoom:67%;" /></p>
<p>图 11.6 展示了这一情况。图 11.6(a) 与图 11.5(b)
相同（但进行了放大），显示了在更新方差之前的 GMM 密度及其各个分量。图
11.6(b) 显示了更新方差之后的 GMM 密度。</p>
<p>与均值参数的更新类似，我们可以将 (11.30) 理解为对与第 <span
class="math inline">\(k\)</span> 个混合分量相关的数据点 <span
class="math inline">\(x_n\)</span>
的加权协方差的蒙特卡罗估计，其中权重是责任度 <span
class="math inline">\(r_{nk}\)</span>。同样地，正如均值参数的更新，这一更新依赖于所有
<span class="math inline">\(\pi_j, \mu_j, \Sigma_j\)</span>（<span
class="math inline">\(j = 1, \ldots, K\)</span>），通过责任度 <span
class="math inline">\(r_{nk}\)</span>
体现出来，从而使得无法得到闭式解。</p>
<blockquote>
<p>个人注：Monte Carlo estimate 是什么意思？</p>
<p>在概率论里，一个期望可以用样本的平均值（或带权平均）来近似，这种近似就叫
<strong>Monte Carlo估计</strong>。 比如，协方差的真实定义是： <span
class="math display">\[
\text{Cov}_k = \mathbb{E}_{\mathbf{x}\sim p_k}\big[
(\mathbf{x}-\boldsymbol{\mu}_k)(\mathbf{x}-\boldsymbol{\mu}_k)^\top
\big]
\]</span> 我们没有真实的 <span
class="math inline">\(p_k\)</span>，只有数据 <span
class="math inline">\(\mathbf{x}_n\)</span>，于是我们用样本的加权和来近似它：
<span class="math display">\[
\hat{\text{Cov}}_k =
\frac{\sum_n r_{nk}
(\mathbf{x}_n - \boldsymbol{\mu}_k)(\mathbf{x}_n -
\boldsymbol{\mu}_k)^\top}
{\sum_n r_{nk}}
\]</span> 这正是 Monte Carlo
估计的典型形式：用有限样本加权平均来近似真实分布下的期望。</p>
</blockquote>
<h4 id="更新混合权重">11.2.4 更新混合权重</h4>
<p><strong>定理 11.3（GMM 混合权重的更新）。</strong>GMM
的混合权重按如下方式更新： <span class="math display">\[
\pi^{\text{new}}_k = \frac{N_k}{N}, \quad k=1,\dots,K, \tag{11.42}
\]</span></p>
<p>其中 <span class="math inline">\(N\)</span> 是数据点的数量，<span
class="math inline">\(N_k\)</span> 定义见 (11.24)。</p>
<p><strong>证明</strong> 为了求对权重参数 <span
class="math inline">\(\pi_k, k=1,\dots,K\)</span>
的对数似然函数的偏导数，我们利用拉格朗日乘子法（参见第 7.2
节）来处理约束条件 <span class="math inline">\(\sum_k \pi_k =
1\)</span>。 拉格朗日函数为：</p>
<p><span class="math display">\[
\mathcal{L} = L + \lambda \left(\sum_{k=1}^K \pi_k - 1\right)
\tag{11.43a}
\]</span></p>
<p><span class="math display">\[
= \sum_{n=1}^N \log\left(\sum_{k=1}^K \pi_k
\mathcal{N}(x_n|\mu_k,\Sigma_k)\right)
+ \lambda \left(\sum_{k=1}^K \pi_k - 1\right), \tag{11.43b}
\]</span></p>
<p>其中 <span class="math inline">\(L\)</span> 是 (11.10)
的对数似然，第二项表示所有混合权重之和为 1 的约束条件。</p>
<p>对 <span class="math inline">\(\pi_k\)</span> 求偏导得到： <span
class="math display">\[
\begin{align}
\frac{\partial \mathcal{L}}{\partial \pi_k}
&amp;= \sum_{n=1}^{N}
\frac{\mathcal{N}(\mathbf{x}_n \mid \boldsymbol{\mu}_k,
\boldsymbol{\Sigma}_k)}
{\sum_{j=1}^{K} \pi_j \mathcal{N}(\mathbf{x}_n \mid \boldsymbol{\mu}_j,
\boldsymbol{\Sigma}_j)}
+ \lambda
\tag{11.44a}\\[1em]
&amp;= \frac{1}{\pi_k}
\sum_{n=1}^{N}
\frac{\pi_k \mathcal{N}(\mathbf{x}_n \mid \boldsymbol{\mu}_k,
\boldsymbol{\Sigma}_k)}
{\sum_{j=1}^{K} \pi_j \mathcal{N}(\mathbf{x}_n \mid \boldsymbol{\mu}_j,
\boldsymbol{\Sigma}_j)}
+ \lambda
= \frac{N_k}{\pi_k} + \lambda,
\tag{11.44b}
\end{align}
\]</span> 对拉格朗日乘子 <span class="math inline">\(\lambda\)</span>
求偏导：</p>
<p><span class="math display">\[
\frac{\partial \mathcal{L}}{\partial \lambda} = \sum_{k=1}^K \pi_k - 1.
\tag{11.45}
\]</span></p>
<p>将两者都置为 0（最优条件）得到方程组：</p>
<p><span class="math display">\[
\pi_k = -\frac{N_k}{\lambda}, \tag{11.46}
\]</span></p>
<p><span class="math display">\[
\sum_{k=1}^K \pi_k = 1. \tag{11.47}
\]</span></p>
<p>把 (11.46) 代入 (11.47) 并解 <span
class="math inline">\(\pi_k\)</span>：</p>
<p><span class="math display">\[
\sum_{k=1}^K \pi_k = 1 \quad \Leftrightarrow \quad -\frac{\sum_{k=1}^K
N_k}{\lambda}=1
\quad \Leftrightarrow \quad -\frac{N}{\lambda}=1
\quad \Leftrightarrow \quad \lambda=-N. \tag{11.48}
\]</span></p>
<p>于是可以将 <span class="math inline">\(-N\)</span> 代入
(11.46)，得到：</p>
<p><span class="math display">\[
\pi^{\text{new}}_k = \frac{N_k}{N}, \tag{11.49}
\]</span></p>
<p>这就是权重参数 <span class="math inline">\(\pi_k\)</span>
的更新公式，证明了定理 11.3。</p>
<p>我们可以将 (11.42) 中的混合权重理解为：第 <span
class="math inline">\(k\)</span>
个聚类的总责任（responsibility）占数据点总数的比例。由于 <span
class="math inline">\(N=\sum_k
N_k\)</span>，数据点总数也可以看作所有混合成分的总责任，因此 <span
class="math inline">\(\pi_k\)</span> 表示第 <span
class="math inline">\(k\)</span> 个混合成分对于数据集的相对重要性。</p>
<p><strong>备注</strong>:由于 <span
class="math inline">\(N_k=\sum_{i=1}^N r_{nk}\)</span>，(11.42)
中混合权重 <span class="math inline">\(\pi_k\)</span>
的更新公式也通过责任值 <span class="math inline">\(r_{nk}\)</span>
依赖于所有的 <span
class="math inline">\(\pi_j,\mu_j,\Sigma_j,j=1,\dots,K\)</span>。</p>
<p><strong>例 11.5（权重参数更新）</strong> 在图 11.3
的示例中，混合权重的更新如下： <span class="math display">\[
\pi_1 : \frac{1}{3} \rightarrow 0.29 \tag{11.50}
\]</span></p>
<p><span class="math display">\[
\pi_2 : \frac{1}{3} \rightarrow 0.29 \tag{11.51}
\]</span></p>
<p><span class="math display">\[
\pi_3 : \frac{1}{3} \rightarrow 0.42 \tag{11.52}
\]</span></p>
<p>在这里我们可以看到，第三个分量获得了更多的权重/重要性，而其他分量则稍微变得不那么重要。图
11.7 说明了更新混合权重的效果。图 11.7(a) 与图 11.6(b)
完全相同，展示了在更新混合权重之前的 GMM 密度及其各个分量。图 11.7(b)
则展示了更新混合权重之后的 GMM 密度。</p>
<p><img src="/img3/机器学习的数学基础Part2/F11.7.png" alt="F11.7" style="zoom:67%;" /></p>
<p>总体而言，当均值、方差以及权重都更新过一次之后，我们得到图 11.7(b)
所示的 GMM。与图 11.3 的初始化相比，<strong>我们可以看到，参数更新使 GMM
的密度把部分概率质量向数据点方向移动。</strong></p>
<p>在均值、方差和权重都更新过一次之后，图 11.7(b) 中的 GMM
拟合效果已经明显优于图 11.3
中的初始化。这一点也从对数似然值可以看出：从初始化时的 −28.3
提升到了完整更新循环后的 −14.4。</p>
<h3 id="em-算法"><strong>11.3 EM 算法</strong></h3>
<p>不幸的是，(11.20)、(11.30) 和 (11.42) 中的更新并不能构成混合模型参数
<span class="math inline">\(\mu_k, \Sigma_k, \pi_k\)</span>
的一个闭式解，因为职责（responsibilities）<span
class="math inline">\(r_{nk}\)</span>
以复杂的方式依赖于这些参数。不过，这些结果提示了一个简单的迭代方案，可以通过极大似然来求解参数估计问题。期望最大化算法（EM
算法）由 Dempster
等人（1977）提出，是一种通用的迭代方法，用于在混合模型以及更一般的潜变量模型中学习参数（极大似然或
MAP）。在高斯混合模型的例子中，我们选择 <span
class="math inline">\(\mu_k, \Sigma_k, \pi_k\)</span>
的初始值，并在下述两步之间交替迭代直至收敛：</p>
<ul>
<li><strong>E 步（Expectation step）</strong>：计算职责 <span
class="math inline">\(r_{nk}\)</span>（即数据点 <span
class="math inline">\(n\)</span> 属于混合分量 <span
class="math inline">\(k\)</span> 的后验概率）。</li>
<li><strong>M 步（Maximization
step）</strong>：利用更新后的职责重新估计参数 <span
class="math inline">\(\mu_k, \Sigma_k, \pi_k\)</span>。</li>
</ul>
<p>EM 算法的每一步都会使对数似然函数增加（Neal 和
Hinton，1999）。为了判断收敛，我们可以检查对数似然值或直接检查参数的变化。</p>
<p>一个具体的 EM 算法用于估计 GMM 的参数如下：</p>
<ol type="1">
<li><p><strong>初始化</strong> <span class="math inline">\(\mu_k,
\Sigma_k, \pi_k\)</span>。</p></li>
<li><p><strong>E 步</strong>：使用当前参数 <span
class="math inline">\(\pi_k, \mu_k, \Sigma_k\)</span> 计算每个数据点
<span class="math inline">\(x_n\)</span> 的职责 <span
class="math inline">\(r_{nk}\)</span>：</p></li>
</ol>
<p><span class="math display">\[
r_{nk} =
\frac{\pi_k \,\mathcal{N}(x_n|\mu_k,\Sigma_k)}
{\sum_j \pi_j \,\mathcal{N}(x_n|\mu_j,\Sigma_j)}
\tag{11.53}
\]</span></p>
<ol start="3" type="1">
<li><strong>M 步</strong>：利用 E 步得到的职责 <span
class="math inline">\(r_{nk}\)</span> 重新估计参数 <span
class="math inline">\(\pi_k, \mu_k, \Sigma_k\)</span>：</li>
</ol>
<p><span class="math display">\[
\mu_k = \frac{1}{N_k}\sum_{n=1}^{N} r_{nk}x_n \tag{11.54}
\]</span></p>
<p><span class="math display">\[
\Sigma_k = \frac{1}{N_k}\sum_{n=1}^{N} r_{nk}(x_n-\mu_k)(x_n-\mu_k)^\top
\tag{11.55}
\]</span></p>
<p><span class="math display">\[
\pi_k = \frac{N_k}{N} \tag{11.56}
\]</span></p>
<p><strong>例 11.6（GMM 拟合）</strong></p>
<p>当我们在图 11.3 的例子上运行 EM 算法时，经过 5
次迭代，我们得到的最终结果如图 11.8(a) 所示；图 11.8(b)
则显示了负对数似然随 EM 迭代次数的变化情况。最终的 GMM 表达式为</p>
<p><span class="math display">\[
p(x) = 0.29\,\mathcal{N}(x|-2.75, 0.06)
+ 0.28\,\mathcal{N}(x|-0.50, 0.25)
+ 0.43\,\mathcal{N}(x|3.64, 1.63)
\tag{11.57}
\]</span></p>
<p>我们将 EM 算法应用到图 11.1 所示的二维数据集上，设混合分量数 <span
class="math inline">\(K=3\)</span>。图 11.9 展示了 EM
算法的一些步骤，并显示了负对数似然随 EM 迭代次数的变化（图 11.9(b)）。图
11.10(a) 展示了最终的 GMM 拟合结果。图 11.10(b) 可视化了在 EM
收敛时混合分量对各个数据点的最终职责（responsibilities）。</p>
<p><img src="/img3/机器学习的数学基础Part2/F11.8.png" alt="F11.8" style="zoom:67%;" /></p>
<p><img src="/img3/机器学习的数学基础Part2/F11.9.png" alt="F11.9" style="zoom:67%;" /></p>
<p><img src="/img3/机器学习的数学基础Part2/F11.10.png" alt="F11.10" style="zoom:67%;" /></p>
<p><strong>数据集按照混合分量的职责进行着色。</strong>左侧的数据显然主要由单一的混合分量负责，而右侧两个数据簇的重叠部分可能由两个混合分量生成。可以清楚地看到，有些数据点无法唯一地分配给单个分量（既非完全蓝色也非完全黄色），因此这两个簇对这些点的职责大约是
0.5。</p>
<h3 id="潜变量视角"><strong>11.4 潜变量视角</strong></h3>
<p>我们可以从<strong>离散潜变量模型</strong>的角度来看待
GMM（高斯混合模型），即潜变量 <span class="math inline">\(z\)</span>
只能取有限个值。这与 PCA 形成对比：在 PCA 中，潜变量是 <span
class="math inline">\(\mathbb{R}^M\)</span>
中的连续值。从概率视角来看有以下优点： (i)
它能对我们在前面章节中做出的一些临时性（ad hoc）决定进行合理化； (ii)
它允许我们将<strong>责任度（responsibilities）</strong>解释为后验概率；
(iii)
更新模型参数的迭代算法可以以一种有原则的方式导出，即作为潜变量模型中最大似然参数估计的
EM 算法。</p>
<h4 id="生成过程与概率模型"><strong>11.4.1
生成过程与概率模型</strong></h4>
<p>为了推导 GMM
的概率模型，把它看作<strong>生成过程</strong>是有用的，也就是使用一个概率模型生成数据的过程。</p>
<p>我们假设一个具有 <span class="math inline">\(K\)</span>
个成分的混合模型，并且一个数据点 <span class="math inline">\(x\)</span>
由其中恰好一个混合成分生成。我们引入一个<strong>二元指示变量</strong>
<span class="math inline">\(z_k \in \{0,1\}\)</span>（见第 6.2
节），用以指示第 <span class="math inline">\(k\)</span>
个混合成分是否生成了该数据点，使得：</p>
<p><span class="math display">\[
p(x|z_k=1)=\mathcal{N}\bigl(x\mid \mu_k,\Sigma_k\bigr). \tag{11.58}
\]</span></p>
<blockquote>
<p>个人注：式 (11.58)
并非从别的式子推导，而是<strong>模型的条件分布定义</strong>——“若隐藏变量指示第
<span class="math inline">\(k\)</span> 个分量被选中，则 <span
class="math inline">\(x\)</span> 就从该分量的高斯分布中产生”。</p>
</blockquote>
<p>我们定义 <span class="math display">\[
z := [z_1,\ldots,z_K]^\top \in \mathbb{R}^K
\]</span></p>
<p>为一个概率向量，它包含 <span class="math inline">\(K-1\)</span> 个 0
和恰好一个 1。例如，当 <span class="math inline">\(K=3\)</span>
时，一个合法的 <span class="math inline">\(z\)</span> 可以是</p>
<p><span class="math display">\[
z=[z_1,z_2,z_3]^\top=[0,1,0]^\top,
\]</span></p>
<p>这就选择了第二个混合成分，因为 <span
class="math inline">\(z_2=1\)</span>。</p>
<p><strong>注：</strong>这种概率分布有时称为“多项伯努利分布（multinoulli）”，是伯努利分布向两个以上值的推广（Murphy，2012）。</p>
<p>由 <span class="math inline">\(z\)</span> 的性质可知</p>
<p><span class="math display">\[
\sum_{k=1}^K z_k =1。
\]</span></p>
<p>因此，<span class="math inline">\(z\)</span>
是一种<strong>独热编码（one-hot encoding，也叫 1-of-K
表示）</strong>。</p>
<p>到目前为止，我们假设指示变量 <span class="math inline">\(z_k\)</span>
是已知的。然而在实际中并非如此，因此我们对潜变量 <span
class="math inline">\(z\)</span> 施加先验分布：</p>
<p><span class="math display">\[
p(z)=\pi=[\pi_1,\ldots,\pi_K]^\top,\quad
\sum_{k=1}^K \pi_k=1。 \tag{11.59}
\]</span></p>
<p>其中第 <span class="math inline">\(k\)</span> 个分量</p>
<p><span class="math display">\[
\pi_k=p(z_k=1) \tag{11.60}
\]</span></p>
<p>描述了第 <span class="math inline">\(k\)</span> 个混合成分生成数据点
<span class="math inline">\(x\)</span> 的概率。</p>
<p><img src="/img3/机器学习的数学基础Part2/F11.11.png" alt="F11.11" style="zoom:67%;" /></p>
<p><strong>注（从 GMM 采样）</strong>这种潜变量模型的构造（见图 11.11
中对应的图形模型）自然地导出一个非常简单的<strong>采样过程（生成过程）来生成数据</strong>：</p>
<ol type="1">
<li>从 <span class="math inline">\(p(z)\)</span> 中采样 <span
class="math inline">\(z^{(i)}\)</span>。</li>
<li>从 <span class="math inline">\(p\bigl(x\mid z^{(i)}=1\bigr)\)</span>
中采样 <span class="math inline">\(x^{(i)}\)</span>。</li>
</ol>
<p>在第一步，我们根据 <span class="math inline">\(p(z)=\pi\)</span>
随机选择一个混合成分 <span class="math inline">\(i\)</span>（通过
one-hot 编码 <span
class="math inline">\(z\)</span>）；在第二步，我们从对应的混合成分中抽取一个样本。当我们丢弃潜变量的样本，仅保留
<span class="math inline">\(x^{(i)}\)</span> 时，我们就得到了来自 GMM
的有效样本。这种采样方式，即随机变量的样本依赖于其在图模型中父节点的样本，被称为<strong>祖先采样（ancestral
sampling）</strong>。</p>
<p>一般地，一个概率模型是由数据和潜变量的联合分布定义的（见第 8.4
节）。结合 (11.59)、(11.60) 中的先验 <span
class="math inline">\(p(z)\)</span> 以及 (11.58) 中的条件分布 <span
class="math inline">\(p(x|z)\)</span>，我们得到该联合分布的所有 <span
class="math inline">\(K\)</span> 个成分：</p>
<p><span class="math display">\[
p(x,z_k=1)=p(x|z_k=1)p(z_k=1)=\pi_k\mathcal{N}\bigl(x\mid
\mu_k,\Sigma_k\bigr)\tag{11.61}
\]</span></p>
<p>其中 <span class="math inline">\(k=1,\ldots,K\)</span>，因此</p>
<p><span class="math display">\[
p(x,z)=
\begin{bmatrix}
p(x,z_1=1)\\
\vdots\\
p(x,z_K=1)
\end{bmatrix}
=
\begin{bmatrix}
\pi_1\mathcal{N}\bigl(x\mid \mu_1,\Sigma_1\bigr)\\
\vdots\\
\pi_K\mathcal{N}\bigl(x\mid \mu_K,\Sigma_K\bigr)
\end{bmatrix} \tag{11.62}
\]</span></p>
<p>这就完全刻画了该概率模型。</p>
<h4 id="似然"><strong>11.4.2 似然</strong></h4>
<p>为了在潜变量模型中得到似然 <span
class="math inline">\(p(x|\theta)\)</span>，我们需要把潜变量积分（求和）掉（见第
8.4.3 节）。在我们的例子中，可以通过对 (11.62) 中的联合分布 <span
class="math inline">\(p(x,z)\)</span> 对所有潜变量求和得到：</p>
<p><span class="math display">\[
p(x|\theta)=\sum_{z}p(x|\theta,z)p(z|\theta),
\quad \theta:=\{\mu_k,\Sigma_k,\pi_k: k=1,\ldots,K\}. \tag{11.63}
\]</span></p>
<p>这里我们显式地对概率模型的参数 <span
class="math inline">\(\theta\)</span>
进行了条件化，而在之前省略了这一点。 在 (11.63) 中，我们对所有 <span
class="math inline">\(K\)</span> 种可能的 one-hot 编码 <span
class="math inline">\(z\)</span> 求和（记作 <span
class="math inline">\(\sum_z\)</span>）。因为每个 <span
class="math inline">\(z\)</span> 中只有一个非零条目，所以 <span
class="math inline">\(z\)</span> 只有 <span
class="math inline">\(K\)</span> 种可能的配置。例如，当 <span
class="math inline">\(K=3\)</span> 时，<span
class="math inline">\(z\)</span> 的配置可以是： <span
class="math display">\[
\begin{bmatrix}1\\0\\0\end{bmatrix},\quad
\begin{bmatrix}0\\1\\0\end{bmatrix},\quad
\begin{bmatrix}0\\0\\1\end{bmatrix}. \tag{11.64}
\]</span></p>
<p>对 (11.63) 中所有可能的 <span class="math inline">\(z\)</span>
求和等价于取 <span
class="math inline">\(z\)</span>-向量中非零的那个位置并写成：</p>
<p><span class="math display">\[
p(x|\theta)=\sum_{z}p(x|\theta,z)p(z|\theta)\tag{11.65a}
\]</span></p>
<p><span class="math display">\[
=\sum_{k=1}^K p(x|\theta,z_k=1)p(z_k=1|\theta)\tag{11.65b}
\]</span></p>
<p>从而所需的边缘分布为：</p>
<p><span class="math display">\[
p(x|\theta)=\sum_{k=1}^K p(x|\theta,z_k=1)p(z_k=1|\theta)\tag{11.66a}
\]</span></p>
<p><span class="math display">\[
=\sum_{k=1}^K \pi_k\mathcal{N}\bigl(x\mid \mu_k,\Sigma_k\bigr),
\tag{11.66b}
\]</span></p>
<p>我们可以把它识别为 (11.3) 中的 GMM 模型。给定一个数据集 <span
class="math inline">\(X\)</span>，我们立刻得到其似然：</p>
<p><span class="math display">\[
p(X|\theta)=\prod_{n=1}^N p(x_n|\theta)
=\prod_{n=1}^N \sum_{k=1}^K \pi_k\mathcal{N}\bigl(x_n\mid
\mu_k,\Sigma_k\bigr). \tag{11.67}
\]</span></p>
<p>这正是 (11.9) 中的 GMM 似然。因此，带有潜在指示变量 <span
class="math inline">\(z_k\)</span>
的潜变量模型，是理解高斯混合模型的一个等价方式。</p>
<h4 id="后验分布"><strong>11.4.3 后验分布</strong></h4>
<p>我们简单看一下潜在变量 <span class="math inline">\(z\)</span>
的后验分布。根据贝叶斯定理，第 <span class="math inline">\(k\)</span>
个分量生成数据点 <span class="math inline">\(x\)</span> 的后验为</p>
<p><span class="math display">\[
p(z_k = 1 \mid x)=\frac{p(z_k=1)p(x\mid z_k=1)}{p(x)} \tag{11.68}
\]</span></p>
<p>其中边际分布 <span class="math inline">\(p(x)\)</span> 在 (11.66b)
中给出。这就得到了第 <span class="math inline">\(k\)</span> 个指示变量
<span class="math inline">\(z_k\)</span> 的后验分布：</p>
<p><span class="math display">\[
p(z_k = 1 \mid x)=\frac{p(z_k=1)p(x\mid z_k=1)}
{\sum_{j=1}^K p(z_j=1)p(x\mid z_j=1)}
=\frac{\pi_k\mathcal{N}(x\mid\mu_k,\Sigma_k)}
{\sum_{j=1}^K\pi_j\mathcal{N}(x\mid\mu_j,\Sigma_j)}
\tag{11.69}
\]</span></p>
<p>我们把它识别为第 <span class="math inline">\(k\)</span>
个混合成分对数据点 <span class="math inline">\(x\)</span>
的“责任”。注意我们省略了对 GMM 参数 <span
class="math inline">\(\pi_k,\mu_k,\Sigma_k\)</span> （<span
class="math inline">\(k=1,\dots,K\)</span>）的显式条件。</p>
<h4 id="扩展到整个数据集"><strong>11.4.4 扩展到整个数据集</strong></h4>
<p>到目前为止，我们只讨论了数据集只包含单个数据点 <span
class="math inline">\(x\)</span>
的情形。然而，先验和后验的概念可以直接扩展到 <span
class="math inline">\(N\)</span> 个数据点的情况 <span
class="math inline">\(X=\{x_1,\dots,x_N\}\)</span>。在 GMM
的概率解释中，每个数据点 <span class="math inline">\(x_n\)</span>
都有自己的潜在变量</p>
<p><span class="math display">\[
z_n=[z_{n1},\dots,z_{nK}]^\top\in\mathbb{R}^K \tag{11.70}
\]</span></p>
<p>之前（当我们只考虑一个数据点 <span class="math inline">\(x\)</span>
时）我们省略了下标 <span
class="math inline">\(n\)</span>，但现在它变得重要了。</p>
<p>我们在所有潜在变量 <span class="math inline">\(z_n\)</span>
上共享相同的先验分布 <span
class="math inline">\(\pi\)</span>。对应的图模型如图 11.12
所示，我们使用了 plate 符号。</p>
<p><img src="/img3/机器学习的数学基础Part2/F11.12.png" alt="F11.12" style="zoom:67%;" /></p>
<p>条件分布 <span class="math inline">\(p(x_1,\dots,x_N\mid
z_1,\dots,z_N)\)</span> 在数据点上是可分解的：</p>
<p><span class="math display">\[
p(x_1,\dots,x_N\mid z_1,\dots,z_N)=\prod_{n=1}^N p(x_n\mid z_n)
\tag{11.71}
\]</span></p>
<p>为了得到后验分布 <span class="math inline">\(p(z_{nk}=1\mid
x_n)\)</span>，我们像 11.4.3 节一样应用贝叶斯定理得到：</p>
<p>$$ <span class="math display">\[\begin{align}
p(z_{nk} = 1 \mid \boldsymbol{x}_n) &amp; = \frac{p(\boldsymbol{x}_n
\mid z_{nk} = 1) p(z_{nk} = 1)}{\sum_{j=1}^{K} p(\boldsymbol{x}_n \mid
z_{nj} = 1) p(z_{nj} = 1)} \tag{11.72a} \\

&amp; = \frac{\pi_k \mathcal{N}(\boldsymbol{x}_n \mid
\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)}{\sum_{j=1}^{K} \pi_j
\mathcal{N}(\boldsymbol{x}_n \mid \boldsymbol{\mu}_j,
\boldsymbol{\Sigma}_j)} = r_{nk}. \tag{11.72b}
\end{align}\]</span> $$</p>
<p>这意味着 <span class="math inline">\(p(z_k=1\mid x_n)\)</span> 是第
<span class="math inline">\(k\)</span> 个混合成分生成数据点 <span
class="math inline">\(x_n\)</span> 的（后验）概率，并对应于我们在
(11.17) 中引入的责任 <span
class="math inline">\(r_{nk}\)</span>。现在这些责任不仅有了直观含义，而且也有了作为后验概率的数学解释。</p>
<h4 id="em-算法再探"><strong>11.4.5 EM 算法再探</strong></h4>
<p>我们前面介绍过 EM
算法，作为最大似然估计的一种迭代方案；其实，它可以从潜在变量的视角以一种更有原则的方法推导出来。给定模型参数的当前设置
<span class="math inline">\(\theta^{(t)}\)</span>，E
步计算期望对数似然： <span class="math display">\[
Q(\theta|\theta^{(t)}) = \mathbb{E}_{z|x,\theta^{(t)}}[\log
p(x,z|\theta)] \tag{11.73a}
\]</span></p>
<p><span class="math display">\[
= \int \log p(x,z|\theta)\,p(z|x,\theta^{(t)})\,dz \tag{11.73b}
\]</span></p>
<p>其中 <span class="math inline">\(\log p(x,z|\theta)\)</span>
的期望是关于潜在变量的后验 <span
class="math inline">\(p(z|x,\theta^{(t)})\)</span> 取的。M
步则通过最大化式 (11.73b) 来选择更新后的模型参数 <span
class="math inline">\(\theta^{(t+1)}\)</span>。</p>
<p>虽然每次 EM 迭代都会增加对数似然，但并没有保证 EM
会收敛到最大似然解。EM
算法有可能收敛到对数似然的局部最大值。<strong>为了减少陷入糟糕局部最优的风险，可以在多次
EM 运行中使用不同的 <span class="math inline">\(\theta\)</span>
初始值。</strong>我们在此不作进一步展开，读者可以参考 Rogers 和 Girolami
(2016) 以及 Bishop (2006) 的精彩阐述。</p>
<h3 id="延伸阅读"><strong>11.5 延伸阅读</strong></h3>
<p>高斯混合模型（GMM）可以被视为生成式模型，因为可以很容易地通过祖先采样来生成新数据（Bishop,
2006）。给定 GMM 的参数 <span
class="math inline">\(\pi_k,\mu_k,\Sigma_k,
k=1,\ldots,K\)</span>，我们先从概率向量 <span
class="math inline">\([\pi_1,\ldots,\pi_K]^\top\)</span> 中采样一个索引
<span class="math inline">\(k\)</span>，再从正态分布 <span
class="math inline">\(x \sim \mathcal{N}(\mu_k,\Sigma_k)\)</span>
中采样一个数据点。如果重复该过程 <span class="math inline">\(N\)</span>
次，就能得到一个由 GMM 生成的数据集。图 11.1 就是用这种方法生成的。</p>
<p>在本章中，我们始终假设混合成分数 <span
class="math inline">\(K\)</span>
是已知的。但在实践中往往不是这样。我们可以用 8.6.1
节讨论过的嵌套交叉验证来寻找合适的模型。高斯混合模型与 K-means
聚类算法密切相关。K-means 同样使用 EM 算法将数据点分配到簇。如果把 GMM
中的均值视为聚类中心并忽略协方差（或设为 <span
class="math inline">\(I\)</span>），就得到 K-means。正如 MacKay (2003)
所描述的，K-means 对数据点作“硬”分配到簇中心 <span
class="math inline">\(\mu_k\)</span>，而 GMM
则通过责任值作“软”分配。</p>
<p>我们只略微涉及了 GMM（高斯混合模型）和 EM
算法的潜在变量视角。需要注意的是，EM
算法可以用于一般潜在变量模型的参数学习，例如非线性状态空间模型（Ghahramani
和 Roweis, 1999；Roweis 和 Ghahramani, 1999）以及 Barber (2012)
讨论的强化学习。因此，从潜在变量的角度理解
GMM，有助于以更有原则的方法推导出对应的 EM 算法（Bishop, 2006；Barber,
2012；Murphy, 2012）。</p>
<p>我们只讨论了通过 EM 算法进行最大似然估计来求解 GMM
参数。对最大似然的标准批评同样适用于此处：</p>
<ul>
<li>就像在线性回归中一样，最大似然可能会出现严重的过拟合。在 GMM
情形下，当某个混合成分的均值恰好与某个数据点重合且协方差趋于 0
时，就会发生这种情况。此时似然值趋于无穷大。Bishop (2006) 和 Barber
(2012) 对这一问题有详细讨论。</li>
<li>我们只得到参数 <span class="math inline">\(\pi_k,\mu_k,\Sigma_k,
k=1,\ldots,K\)</span>
的点估计，这并不能反映参数值的不确定性。贝叶斯方法会对参数施加先验，从而得到参数的后验分布。这个后验可以用来计算模型证据（即边际似然），并可用于模型比较，从而以更有原则的方式确定混合成分的个数。不幸的是，在此模型中没有共轭先验，因此无法得到封闭形式的推断。但可以使用近似方法（如变分推断）来获得参数后验的近似（Bishop,
2006）。</li>
</ul>
<p>本章我们讨论了<strong>用于密度估计的混合模型。可用的密度估计技术非常多。在实际中，我们常使用直方图和核密度估计。</strong>直方图提供了一种非参数方式来表示连续密度，最早由
Pearson (1895)
提出。直方图的构建方法是对数据空间进行“分箱”，并统计每个箱中有多少数据点。然后在每个箱的中心画出一个矩形柱，其高度与该箱内数据点的数量成正比。箱宽是一个关键的超参数，不合适的选择会导致过拟合或欠拟合。可以通过第
8.2.4 节讨论过的交叉验证来确定一个合适的箱宽。核密度估计是由 Rosenblatt
(1956) 和 Parzen (1962) 各自独立提出的一种非参数密度估计方法。给定 <span
class="math inline">\(N\)</span>
个独立同分布样本，核密度估计器将底层分布表示为</p>
<p><span class="math display">\[
p(x) = \frac{1}{Nh}\sum_{n=1}^{N} k\!\left(\frac{x-x_n}{h}\right)
\tag{11.74}
\]</span></p>
<p>其中 <span class="math inline">\(k\)</span>
是核函数，即一个非负且积分为 1 的函数，<span
class="math inline">\(h&gt;0\)</span>
是平滑/带宽参数，它的作用类似于直方图的箱宽。注意，我们对数据集中每一个样本点
<span class="math inline">\(x_n\)</span>
都放置了一个核。常用的核函数有均匀分布核和高斯核。核密度估计与直方图密切相关，但通过选择合适的核函数，我们可以保证密度估计的平滑性。图
11.13 展示了在一个包含 250
个数据点的数据集上，直方图与高斯核密度估计的差异。</p>
<p><img src="./F11.13.png" alt="F11.13" style="zoom:67%;" /></p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%95%B0%E5%AD%A6/" rel="tag"># 数学</a>
              <a href="/tags/%E7%AE%97%E6%B3%95/" rel="tag"># 算法</a>
              <a href="/tags/%E7%BB%9F%E8%AE%A1/" rel="tag"># 统计</a>
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2025/09/09/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E7%AC%AC10%E7%AB%A0%22%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%E7%9A%84%E9%99%8D%E7%BB%B4%22/" rel="prev" title="《机器学习的数学基础》第10章"主成分分析的降维"">
                  <i class="fa fa-chevron-left"></i> 《机器学习的数学基础》第10章"主成分分析的降维"
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2025/09/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%A6%E7%B1%8D%E4%BB%8B%E7%BB%8D/" rel="next" title="机器学习书籍介绍">
                  机器学习书籍介绍 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Rayman.hung</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
<script src="https://cdn.jsdelivr.net/npm/hexo-generator-searchdb@1.4.0/dist/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js","integrity":"sha256-r+3itOMtGGjap0x+10hu6jW/gZCzxHsoKrOd7gyRSGY="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>

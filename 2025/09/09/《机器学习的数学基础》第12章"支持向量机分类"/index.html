<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.1.1">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" integrity="sha256-DfWjNxDkM94fVBWx1H5BMMp0Zq7luBlV8QRcSES7s+0=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"hongyitong.github.io","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.11.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="第12章 支持向量机分类 在许多情况下，我们希望机器学习算法能够预测若干（离散）结果中的一个。例如，电子邮件客户端会将邮件分为个人邮件和垃圾邮件，这就有两个结果。另一个例子是望远镜识别夜空中的天体是星系、恒星还是行星。通常结果的数量较少，更重要的是，这些结果之间往往没有额外的结构。 在本章中，我们考虑输出二元值的预测器，也就是说，只有两种可能的结果。这种机器学习任务称为二分类（binary cla">
<meta property="og:type" content="article">
<meta property="og:title" content="《机器学习的数学基础》第12章&quot;支持向量机分类&quot;">
<meta property="og:url" content="http://hongyitong.github.io/2025/09/09/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E7%AC%AC12%E7%AB%A0%22%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E5%88%86%E7%B1%BB%22/index.html">
<meta property="og:site_name" content="忆桐之家的博客">
<meta property="og:description" content="第12章 支持向量机分类 在许多情况下，我们希望机器学习算法能够预测若干（离散）结果中的一个。例如，电子邮件客户端会将邮件分为个人邮件和垃圾邮件，这就有两个结果。另一个例子是望远镜识别夜空中的天体是星系、恒星还是行星。通常结果的数量较少，更重要的是，这些结果之间往往没有额外的结构。 在本章中，我们考虑输出二元值的预测器，也就是说，只有两种可能的结果。这种机器学习任务称为二分类（binary cla">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Part2/F12.1.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Part2/F2.13.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Part2/F12.2.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Part2/F12.3.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Part2/F12.4.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Part2/F12.5.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Part2/F12.6.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Part2/F12.7.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Part2/F12.8.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Part2/F12.9.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Part2/F12.10.png">
<meta property="article:published_time" content="2025-09-08T16:00:00.000Z">
<meta property="article:modified_time" content="2025-09-10T02:26:39.663Z">
<meta property="article:author" content="Rayman.hung">
<meta property="article:tag" content="数学">
<meta property="article:tag" content="算法">
<meta property="article:tag" content="统计">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://hongyitong.github.io/img3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Part2/F12.1.png">


<link rel="canonical" href="http://hongyitong.github.io/2025/09/09/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E7%AC%AC12%E7%AB%A0%22%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E5%88%86%E7%B1%BB%22/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://hongyitong.github.io/2025/09/09/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E7%AC%AC12%E7%AB%A0%22%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E5%88%86%E7%B1%BB%22/","path":"2025/09/09/《机器学习的数学基础》第12章\"支持向量机分类\"/","title":"《机器学习的数学基础》第12章\"支持向量机分类\""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>《机器学习的数学基础》第12章"支持向量机分类" | 忆桐之家的博客</title>
  





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">忆桐之家的博客</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Rayman&Tony</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section">首页</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section">分类</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section">关于</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section">归档</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section">标签</a></li><li class="menu-item menu-item-commonweal"><a href="/404.html" rel="section">公益 404</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC12%E7%AB%A0-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E5%88%86%E7%B1%BB"><span class="nav-number">1.</span> <span class="nav-text">第12章 支持向量机分类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%86%E7%A6%BB%E8%B6%85%E5%B9%B3%E9%9D%A2"><span class="nav-number">1.1.</span> <span class="nav-text">12.1 分离超平面</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8E%9F%E5%A7%8B%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA"><span class="nav-number">1.2.</span> <span class="nav-text">12.2 原始支持向量机</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%97%B4%E9%9A%94%E7%9A%84%E6%A6%82%E5%BF%B5"><span class="nav-number">1.2.1.</span> <span class="nav-text">12.2.1 间隔的概念</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BE%B9%E7%95%8C%E7%9A%84%E4%BC%A0%E7%BB%9F%E6%8E%A8%E5%AF%BC"><span class="nav-number">1.2.2.</span> <span class="nav-text">12.2.2 边界的传统推导</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E4%BB%AC%E5%8F%AF%E4%BB%A5%E6%8A%8A%E9%97%B4%E9%9A%94%E8%AE%BE%E4%B8%BA-1"><span class="nav-number">1.2.3.</span> <span class="nav-text">12.2.3 为什么我们可以把间隔设为
1</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BD%AF%E9%97%B4%E9%9A%94-svm%E5%87%A0%E4%BD%95%E8%A7%86%E8%A7%92"><span class="nav-number">1.2.4.</span> <span class="nav-text">12.2.4 软间隔 SVM：几何视角</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BD%AF%E9%97%B4%E9%9A%94-svm%E4%BB%8E%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E8%A7%92%E5%BA%A6"><span class="nav-number">1.2.5.</span> <span class="nav-text">12.2.5 软间隔
SVM：从损失函数的角度</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AF%B9%E5%81%B6%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA"><span class="nav-number">1.3.</span> <span class="nav-text">12.3 对偶支持向量机</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%80%9A%E8%BF%87%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E5%AD%90%E7%9C%8B%E5%87%B8%E5%AF%B9%E5%81%B6"><span class="nav-number">1.3.1.</span> <span class="nav-text">12.3.1 通过拉格朗日乘子看凸对偶</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AF%B9%E5%81%B6-svm%E5%87%B8%E5%8C%85%E8%A7%86%E8%A7%92"><span class="nav-number">1.3.2.</span> <span class="nav-text">12.3.2 对偶 SVM：凸包视角</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B8%E5%87%BD%E6%95%B0"><span class="nav-number">1.4.</span> <span class="nav-text">12.4 核函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E5%80%BC%E6%B1%82%E8%A7%A3"><span class="nav-number">1.5.</span> <span class="nav-text">12.5 数值求解</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BB%B6%E4%BC%B8%E9%98%85%E8%AF%BB"><span class="nav-number">1.6.</span> <span class="nav-text">12.6 延伸阅读</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Rayman.hung</p>
  <div class="site-description" itemprop="description">技术分享、读书心得、亲子时刻</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives">
          <span class="site-state-item-count">893</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags">
        <span class="site-state-item-count">1147</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/hongyitong" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;hongyitong" rel="noopener" target="_blank">GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="http://blog.sina.com.cn/yitonghong" title="忆桐之家 → http:&#x2F;&#x2F;blog.sina.com.cn&#x2F;yitonghong" rel="noopener" target="_blank">忆桐之家</a>
      </span>
      <span class="links-of-author-item">
        <a href="http://douban.com/people/2780741" title="豆瓣 → http:&#x2F;&#x2F;douban.com&#x2F;people&#x2F;2780741" rel="noopener" target="_blank">豆瓣</a>
      </span>
      <span class="links-of-author-item">
        <a href="http://www.zhihu.com/people/rayman-36" title="知乎 → http:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;rayman-36" rel="noopener" target="_blank">知乎</a>
      </span>
      <span class="links-of-author-item">
        <a href="http://www.liaoxuefeng.com/" title="廖雪峰的官方网站 → http:&#x2F;&#x2F;www.liaoxuefeng.com&#x2F;" rel="noopener" target="_blank">廖雪峰的官方网站</a>
      </span>
      <span class="links-of-author-item">
        <a href="http://www.vaikan.com/" title="外刊IT评论 → http:&#x2F;&#x2F;www.vaikan.com&#x2F;" rel="noopener" target="_blank">外刊IT评论</a>
      </span>
  </div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://hongyitong.github.io/2025/09/09/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E7%AC%AC12%E7%AB%A0%22%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E5%88%86%E7%B1%BB%22/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Rayman.hung">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="忆桐之家的博客">
      <meta itemprop="description" content="技术分享、读书心得、亲子时刻">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="《机器学习的数学基础》第12章"支持向量机分类" | 忆桐之家的博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          《机器学习的数学基础》第12章"支持向量机分类"
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-09-09 00:00:00" itemprop="dateCreated datePublished" datetime="2025-09-09T00:00:00+08:00">2025-09-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-09-10 10:26:39" itemprop="dateModified" datetime="2025-09-10T10:26:39+08:00">2025-09-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/" itemprop="url" rel="index"><span itemprop="name">读书心得</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h2 id="第12章-支持向量机分类">第12章 支持向量机分类</h2>
<p>在许多情况下，我们希望机器学习算法能够预测若干（离散）结果中的一个。例如，电子邮件客户端会将邮件分为个人邮件和垃圾邮件，这就有两个结果。另一个例子是望远镜识别夜空中的天体是星系、恒星还是行星。通常结果的数量较少，更重要的是，这些结果之间往往没有额外的结构。
在本章中，我们考虑输出二元值的预测器，也就是说，只有两种可能的结果。这种机器学习任务称为<strong>二分类（binary
classification）</strong>。这与第 9
章形成对比，当时我们讨论的是连续值输出的预测问题。对于二分类，标签/输出可以取的值是二元的，本章中我们用
{+1,−1} 表示。换句话说，我们考虑的预测器形式为 <span
class="math display">\[
f:\mathbb{R}^D \to \{+1,-1\}. \tag{12.1}
\]</span></p>
<p>回忆第 8 章，我们用 <span class="math inline">\(D\)</span>
个实数的特征向量表示每个样本（数据点）<span
class="math inline">\(x_n\)</span>。标签通常分别称为正类（positive
class）和负类（negative
class）。但应当注意，不要根据“正”或“负”字面意义推断+1类的直观属性。例如，在癌症检测任务中，有癌症的患者往往被标记为+1。原则上可以使用任何两个不同的值，例如{True,False}、{0,1}或{red,blue}。二分类问题研究得比较充分，其他方法的综述我们放到
12.6 节再介绍。我们将介绍一种称为<strong>支持向量机（Support Vector
Machine,
SVM）</strong>的方法，它用于解决二分类任务。与回归类似，这是一个<strong>监督学习</strong>任务：我们有一组样本
<span class="math inline">\(x_n \in
\mathbb{R}^D\)</span>，以及对应的（二元）标签 <span
class="math inline">\(y_n \in
\{+1,-1\}\)</span>。给定一个包含样本–标签对 <span
class="math inline">\(\{(x_1,y_1),\dots,(x_N,y_N)\}\)</span>
的训练数据集，我们希望估计模型参数，使分类错误率最小。类似第 9
章，<strong>我们考虑线性模型，并把非线性隐藏在对样本的一个变换 <span
class="math inline">\(\phi\)</span> 中（见式(9.13)）</strong>。我们将在
12.4 节重新讨论 <span class="math inline">\(\phi\)</span>。</p>
<p>支持向量机（SVM）在许多应用中都能提供最先进的结果，并且具有坚实的理论保证（Steinwart
和 Christmann, 2008）。我们之所以选择用 SVM
来说明二分类问题，主要有两个原因。</p>
<p>首先，SVM 提供了一种<strong>几何方式</strong>来思考监督学习问题。
在第 9
章中，我们是从<strong>概率模型</strong>的角度来看待机器学习问题，并用极大似然估计和贝叶斯推断来解决。而在这里，我们考虑一种替代的方法，即从几何角度来推理机器学习任务。这种方法高度依赖于我们在第
3 章讨论过的<strong>内积</strong>和<strong>投影</strong>等概念。</p>
<p>第二个原因是，与第 9 章不同，SVM
的优化问题<strong>没有解析解</strong>，因此需要借助第 7
章介绍的各种优化工具。 SVM 对机器学习的理解与第 9
章的极大似然方法存在细微的差异：极大似然方法是基于数据分布的概率观点提出一个模型，再由此推导出一个优化问题；而
SVM
的方法则是从几何直觉出发，先设计一个需要在训练过程中被优化的函数。我们已经在第
10 章看到过类似的情况，当时我们从几何原理出发推导了 PCA。在 SVM
的例子中，我们通过设计一个损失函数来度量训练数据上的误差，并遵循<strong>经验风险最小化原则</strong>（第
8.2 节），在训练中加以最小化。</p>
<span id="more"></span>
<p>接下来，我们推导 SVM 的训练优化问题。
直观上，我们可以想象一个二分类数据，它们能够被一个超平面分开，如图 12.1
所示。</p>
<p><img src="/img3/机器学习的数学基础Part2/F12.1.png" alt="F12.1" style="zoom:50%;" /></p>
<p>在这里，每个样本 <span
class="math inline">\(x_n\)</span>（一个二维向量）是一个二维位置（<span
class="math inline">\(x^{(1)}_n, x^{(2)}_n\)</span>），对应的二元标签
<span class="math inline">\(y_n\)</span>
则是两种不同的符号（橙色叉号或蓝色圆点）。“超平面”（hyperplane）是机器学习中常用的术语，我们在第
2.8 节已经遇到过。超平面是一个仿射子空间，维度为 <span
class="math inline">\(D-1\)</span>（如果对应的向量空间维度是 <span
class="math inline">\(D\)</span>）。在二分类中，两个类别的样本（对应两种可能的标签）在特征空间中分布得刚好可以用一条直线（在二维情况下）来分开。</p>
<blockquote>
<p>个人注：注意：<strong>在 <span class="math inline">\(n\)</span>
维空间 (<span class="math inline">\(\mathbb{R}^n\)</span>)
中，超平面（hyperplane）按定义就是一个 <span
class="math inline">\((n-1)\)</span> 维的仿射子空间</strong>。</p>
</blockquote>
<blockquote>
<p>个人注：引用前文2.8.1节的例子。</p>
<p>例 2.26（仿射子空间）</p>
<ul>
<li>一维仿射子空间称为<strong>直线</strong></li>
</ul>
<p>可写为 <span class="math display">\[
y = x_0 + \lambda b_1,
\]</span></p>
<p>其中 <span class="math inline">\(\lambda \in \mathbb{R}\)</span>，且
<span class="math inline">\(U=\text{span}[b_1]\subseteq
\mathbb{R}^n\)</span> 是 <span
class="math inline">\(\mathbb{R}^n\)</span> 的一维子空间。
这意味着一条直线由一个<strong>支撑点</strong>（support point）<span
class="math inline">\(x_0\)</span> 和一个定义方向的向量 <span
class="math inline">\(b_1\)</span> 决定。图 2.13 给出了示意图。</p>
<ul>
<li><span class="math inline">\(\mathbb{R}^n\)</span>
中的二维仿射子空间称为<strong>平面</strong></li>
</ul>
<p>其参数方程为</p>
<p><span class="math display">\[
y = x_0 + \lambda_1 b_1 + \lambda_2 b_2,
\]</span></p>
<p>其中 <span class="math inline">\(\lambda_1,\lambda_2 \in
\mathbb{R}\)</span>，且 <span
class="math inline">\(U=\text{span}[b_1,b_2]\subseteq
\mathbb{R}^n\)</span>。 这意味着一个平面由一个支撑点 <span
class="math inline">\(x_0\)</span> 和<strong>两个线性无关的向量 <span
class="math inline">\(b_1,b_2\)</span>
确定</strong>，这两个向量张成方向空间。</p>
<ul>
<li>在 <span class="math inline">\(\mathbb{R}^n\)</span> 中，<span
class="math inline">\((n-1)\)</span>
维的仿射子空间称为<strong>超平面</strong>（hyperplane）</li>
</ul>
<p>其对应的参数方程为</p>
<p><span class="math display">\[
y = x_0 + \sum_{i=1}^{n-1} \lambda_i b_i,
\]</span></p>
<p>其中 <span class="math inline">\(b_1,\dots,b_{n-1}\)</span> 组成
<span class="math inline">\(\mathbb{R}^n\)</span> 中一个 <span
class="math inline">\((n-1)\)</span> 维子空间 <span
class="math inline">\(U\)</span> 的一组基。
这意味着一个超平面由一个支撑点 <span class="math inline">\(x_0\)</span>
和 <span class="math inline">\((n-1)\)</span> 个线性无关的向量 <span
class="math inline">\(b_1,\dots,b_{n-1}\)</span>
确定，这些向量张成方向空间。</p>
<p><strong>在 <span class="math inline">\(\mathbb{R}^2\)</span>
中，一条直线也是一个超平面；在 <span
class="math inline">\(\mathbb{R}^3\)</span>
中，一个平面也是一个超平面。</strong></p>
<p><img src="/img3/机器学习的数学基础Part2/F2.13.png" alt="F2.13" style="zoom:50%;" /></p>
</blockquote>
<p>在下面的推导中，我们将形式化“寻找一个线性分隔器”的思想。我们会引入<strong>间隔（margin）</strong>的概念，并进一步推广线性分隔器，使其允许某些样本落在“错误”的一侧，从而引入分类误差。我们将介绍两种等价的
SVM 形式化方式：</p>
<ul>
<li><strong>几何视角</strong>（第 12.2.4 节），</li>
<li><strong>损失函数视角</strong>（第 12.2.5 节）。</li>
</ul>
<p>随后，我们利用 <strong>拉格朗日乘子法</strong>（第 7.2 节）推导 SVM
的<strong>对偶形式</strong>。对偶 SVM 使我们能够看到第三种形式化 SVM
的方式：即从每个类别样本的<strong>凸包</strong>的角度来理解（第 12.3.2
节）。最后，我们会简要介绍<strong>核方法</strong>，以及如何数值化地求解<strong>非线性核
SVM</strong>的优化问题。</p>
<h3 id="分离超平面">12.1 分离超平面</h3>
<p>给定两个用向量 <span class="math inline">\(x_i\)</span> 和 <span
class="math inline">\(x_j\)</span>
表示的样本，衡量它们相似度的一种方法是使用<strong>内积</strong> <span
class="math inline">\(\langle x_i, x_j\rangle\)</span>。回忆第 3.2
节，内积与两个向量之间的<strong>夹角</strong>密切相关。两个向量内积的值不仅取决于它们之间的夹角，还取决于每个向量的<strong>长度（范数）</strong>。此外，内积还允许我们严格地定义<strong>几何概念</strong>，比如正交性和投影。</p>
<p>许多分类算法背后的主要思想是：把数据表示在 <span
class="math inline">\(\mathbb{R}^D\)</span>
空间中，然后对这个空间进行划分，理想情况下，同一标签的样本（且不包含其他样本）会位于同一个分区中。对于二分类，空间会被划分成两部分，分别对应正类和负类。我们考虑一种特别方便的划分方式：用一个<strong>超平面</strong>将空间（线性地）分成两半。设样本
<span class="math inline">\(x\in \mathbb{R}^D\)</span>
是数据空间中的一个元素，考虑一个函数</p>
<p><span class="math display">\[
f:\mathbb{R}^D \to \mathbb{R} \tag{12.2a}
\]</span></p>
<p><span class="math display">\[
x \mapsto f(x):=\langle w,x\rangle+b \tag{12.2b}
\]</span></p>
<p>其中参数 <span class="math inline">\(w\in\mathbb{R}^D\)</span>，<span
class="math inline">\(b\in\mathbb{R}\)</span>。回忆第 2.8
节，超平面是一个<strong>仿射子空间</strong>。因此，我们可以将二分类问题中分隔两类的超平面定义为：</p>
<p><span class="math display">\[
\{ x\in\mathbb{R}^D: f(x)=0. \} \tag{12.3}
\]</span></p>
<p>图 12.2 给出了这个超平面的示意图，其中向量 <span
class="math inline">\(w\)</span>
是超平面的<strong>法向量</strong>，<span
class="math inline">\(b\)</span> 是<strong>截距</strong>。</p>
<p><img src="/img3/机器学习的数学基础Part2/F12.2.png" alt="F12.2" style="zoom:50%;" /></p>
<p>我们可以证明，<span class="math inline">\(w\)</span> 确实是 (12.3)
中超平面的法向量。方法是：任选两个在超平面上的样本 <span
class="math inline">\(x_a\)</span> 和 <span
class="math inline">\(x_b\)</span>，证明它们的差向量与 <span
class="math inline">\(w\)</span> <strong>正交</strong>。</p>
<p>写成方程形式：</p>
<p><span class="math display">\[
\begin{align}
f(x_a)-f(x_b) &amp;= \langle w,x_a\rangle+b - \big(\langle
w,x_b\rangle+b\big) \tag{12.4a}\\
&amp;=\langle w,x_a-x_b\rangle, \tag{12.4b}
\end{align}
\]</span></p>
<p>第二行利用了内积的<strong>线性性</strong>（第 3.2
节）。由于我们选择了 <span class="math inline">\(x_a\)</span> 和 <span
class="math inline">\(x_b\)</span> 在超平面上，这意味着 <span
class="math inline">\(f(x_a)=0\)</span> 且 <span
class="math inline">\(f(x_b)=0\)</span>，因此 <span
class="math inline">\(\langle
w,x_a-x_b\rangle=0\)</span>。回忆：当两个向量的内积为 0
时，它们正交。因此，我们得到 <span class="math inline">\(w\)</span>
与超平面上任意向量都正交，也就是说 <span
class="math inline">\(w\)</span> 是超平面的法向量。</p>
<p><strong>注释</strong>:回忆第 2
章，我们可以从不同角度理解向量。在本章中，我们把参数向量 <span
class="math inline">\(w\)</span>
看作一个表示方向的箭头，也就是说，我们把 <span
class="math inline">\(w\)</span>
当作几何向量来理解。相反，我们把样本向量 <span
class="math inline">\(x\)</span>看作一个数据点（由其坐标表示），也就是说，我们把
<span class="math inline">\(x\)</span>
当作相对于标准基的一个向量坐标来理解。</p>
<p>在给定一个测试样本时，我们根据它位于超平面的哪一侧将其分类为正例或负例。注意，式
(12.3)
不仅定义了一个超平面，还定义了一个方向。换句话说，它规定了超平面的正侧和负侧。因此，为了分类测试样本
<span class="math inline">\(x_{test}\)</span>，我们计算函数 <span
class="math inline">\(f(x_{test})\)</span> 的值，如果 <span
class="math inline">\(f(x_{test}) ≥ 0\)</span> 则判为 +1，否则判为
−1。从几何角度看，正例位于超平面“上方”，负例位于“下方”。</p>
<p>在训练分类器时，我们希望带有正标签的样本位于超平面的正侧，即</p>
<p><span class="math display">\[
\langle w, x_n \rangle + b \ge 0 \quad \text{当} \; y_n = +1 \tag{12.5}
\]</span></p>
<p>并且希望带有负标签的样本位于超平面的负侧，即</p>
<p><span class="math display">\[
\langle w, x_n \rangle + b &lt; 0 \quad \text{当} \; y_n = -1 \tag{12.6}
\]</span></p>
<p>参见图 12.2
来获得关于正负样本的几何直观。这两个条件常常可以写成一个式子：</p>
<p><span class="math display">\[
y_n \,(\langle w, x_n \rangle + b) \ge 0 \tag{12.7}
\]</span></p>
<p>式 (12.7) 与 (12.5) 和 (12.6) 是等价的，因为我们分别把 (12.5) 和
(12.6) 两边乘上 <span class="math inline">\(y_n = +1\)</span> 和 <span
class="math inline">\(y_n = -1\)</span>。</p>
<h3 id="原始支持向量机">12.2 原始支持向量机</h3>
<p>基于点到超平面的距离概念，我们现在可以讨论支持向量机了。对于一个线性可分的数据集<span
class="math inline">\(\{(x_1, y_1), \ldots, (x_N,
y_N)\}\)</span>，我们有无穷多候选超平面（见图
12.3），也就有无穷多候选分类器，可以在没有（训练）错误的情况下解决分类问题。为了找到唯一解，一个思路是选择能够<strong>最大化正例和负例之间间隔（margin）</strong>的分离超平面。换句话说，我们希望正负样本被一个“间隔很大”的超平面分开（第
12.2.1 节）。下面我们将计算样本与超平面的距离，从而推导出 margin
的定义。回忆给定点（样本 <span
class="math inline">\(x_n\)</span>）到超平面最近的点是通过<strong>正交投影</strong>获得的（第
3.8 节）。</p>
<p><img src="/img3/机器学习的数学基础Part2/F12.3.png" alt="F12.3" style="zoom:50%;" /></p>
<h4 id="间隔的概念">12.2.1 间隔的概念</h4>
<p>Margin</p>
<p>间隔的概念直观上非常简单：它是分离超平面到数据集中<strong>最近样本</strong>的距离（假设数据集线性可分）。然而，在形式化这个距离时有一个技术小问题，可能会让人困惑。这个小问题是我们需要定义一个<strong>测量距离的尺度</strong>。一个潜在的尺度是数据本身的尺度，即
<span class="math inline">\(x_n\)</span>
的原始值。但这样做有问题：我们可以改变 <span
class="math inline">\(x_n\)</span> 的度量单位而改变 <span
class="math inline">\(x_n\)</span>
的值，从而改变到超平面的距离。正如我们很快将看到的，我们用超平面方程
(12.3) 本身来定义尺度。考虑一个超平面 <span
class="math inline">\(\langle w,x\rangle + b\)</span>，以及一个样本
<span class="math inline">\(x_a\)</span>，如图 12.4
所示。不失一般性，我们可以假设样本 <span
class="math inline">\(x_a\)</span> 在超平面的正侧，即 <span
class="math inline">\(\langle w, x_a\rangle + b &gt;
0\)</span>。我们希望计算 <span class="math inline">\(x_a\)</span>
到超平面的距离 <span class="math inline">\(r&gt;0\)</span>。我们通过考虑
<span class="math inline">\(x_a\)</span>
到超平面的<strong>正交投影</strong>（第 3.8 节）来实现，投影点记为 <span
class="math inline">\(x_a&#39;\)</span>。</p>
<p><img src="/img3/机器学习的数学基础Part2/F12.4.png" alt="F12.4" style="zoom:50%;" /></p>
<p>由于 <span class="math inline">\(w\)</span>
垂直于超平面，我们知道距离 <span class="math inline">\(r\)</span>
只是向量 <span class="math inline">\(w\)</span> 的一个缩放。如果 <span
class="math inline">\(w\)</span> 的长度已知，我们就可以用这个缩放因子
<span class="math inline">\(r\)</span> 求出 <span
class="math inline">\(x_a\)</span> 和 <span
class="math inline">\(x_a&#39;\)</span>
之间的绝对距离。为了方便，我们选择使用一个单位长度的向量（范数为
1），通过将 <span class="math inline">\(w\)</span>
除以它的范数得到：<span
class="math inline">\(\frac{w}{\|w\|}\)</span>。使用向量加法（第 2.4
节），我们得到：</p>
<p><span class="math display">\[
x_a = x_a&#39; + r \,\frac{w}{\|w\|} \tag{12.8}
\]</span></p>
<p>另一种理解 <span class="math inline">\(r\)</span> 的方式是：它是
<span class="math inline">\(x_a\)</span> 在由 <span
class="math inline">\(\frac{w}{\|w\|}\)</span>
张成的子空间中的坐标。现在我们把 <span
class="math inline">\(x_a\)</span> 到超平面的距离表达为 <span
class="math inline">\(r\)</span>。如果我们选择 <span
class="math inline">\(x_a\)</span> 是距离超平面最近的点，那么这个距离
<span class="math inline">\(r\)</span> 就是
<strong>margin（间隔）</strong>。回忆我们希望正例在超平面正方向上距离至少为
<span class="math inline">\(r\)</span>，负例在超平面负方向上距离至少为
<span class="math inline">\(r\)</span>。与把 (12.5) 和 (12.6) 合并为
(12.7) 类似，我们将这个目标写为：</p>
<p><span class="math display">\[
y_n\bigl(\langle w,x_n\rangle + b\bigr) \;\geq\; r \tag{12.9}
\]</span></p>
<p>换句话说，我们把样本在正负方向上都至少距离超平面 <span
class="math inline">\(r\)</span> 的要求合并到了一条不等式里。</p>
<p>由于我们只关心方向，我们在模型中加一个假设：参数向量 <span
class="math inline">\(w\)</span> 是单位长度的，即 <span
class="math inline">\(\|w\|=1\)</span>，其中 <span
class="math inline">\(\|w\|=\sqrt{w^\top w}\)</span> 是欧几里得范数（第
3.1 节）。这个假设也使得 (12.8) 中距离 <span
class="math inline">\(r\)</span> 有了更直观的解释，因为它是一个长度为 1
的向量的缩放因子。</p>
<p><strong>注</strong>：熟悉其他 margin 定义的读者会注意到，我们这里取
<span class="math inline">\(\|w\|=1\)</span> 的方式与 Schölkopf 和 Smola
(2002) 等书中的标准表述不同。在第 12.2.3
节我们会展示两种方法的等价性。</p>
<p>将三个要求收集到一个带约束的优化问题中，我们得到目标函数：</p>
<p><span class="math display">\[
\max_{w,b,r} \; r  \tag{12.10}
\]</span></p>
<p>满足约束：</p>
<ul>
<li><span class="math inline">\(y_n(\langle w,x_n\rangle +
b)\;\geq\;r\)</span> （数据满足约束），</li>
<li><span class="math inline">\(\|w\|=1\)</span> （归一化），</li>
<li><span class="math inline">\(r&gt;0\)</span>，</li>
</ul>
<p>这表示我们要在保证所有数据在超平面正确一侧的前提下，最大化间隔 <span
class="math inline">\(r\)</span>。</p>
<p><strong>注</strong>：margin 的概念在机器学习中无处不在。Vladimir
Vapnik 和 Alexey Chervonenkis 利用这个概念证明了当 margin
很大时，函数类的“复杂度”较低，因此学习是可行的（Vapnik,
2000）。事实证明，这个概念在许多不同的理论分析泛化误差的方法中都非常有用（Steinwart
和 Christmann, 2008；Shalev-Shwartz 和 Ben-David, 2014）。</p>
<h4 id="边界的传统推导">12.2.2 边界的传统推导</h4>
<p>Margin</p>
<p>在前一节中，我们通过观察只关心 <span class="math inline">\(w\)</span>
的方向而不关心其长度，得到了假设 <span
class="math inline">\(\|w\|=1\)</span>，并由此推导出式
(12.10)。在本节中，我们将通过另一种假设来推导边界最大化问题。我们不再选择参数向量归一化，而是选择对数据进行缩放。我们通过这样的缩放，使得预测器
<span class="math inline">\(\langle w,x\rangle + b\)</span>
在距离超平面最近的样本处等于 1。我们将数据集中距离超平面最近的样本记作
<span class="math inline">\(x_a\)</span>。</p>
<p><img src="/img3/机器学习的数学基础Part2/F12.5.png" alt="F12.5" style="zoom:50%;" /></p>
<p>图 12.5 与图 12.4 相同，只是现在我们重新缩放了坐标轴，使得样本 <span
class="math inline">\(x_a\)</span> 正好位于边界上，即</p>
<p><span class="math display">\[
\langle w,x_a\rangle + b = 1.
\]</span></p>
<p>由于 <span class="math inline">\(x&#39;_a\)</span> 是 <span
class="math inline">\(x_a\)</span>
在超平面上的正交投影，按定义它必定位于超平面上，即</p>
<p><span class="math display">\[
\langle w,x&#39;_a\rangle + b = 0. \tag{12.11}
\]</span></p>
<p>将 (12.8) 代入 (12.11)，我们得到</p>
<p><span class="math display">\[
\langle w,\;x_a - \frac{r}{\|w\|} w \rangle + b = 0. \tag{12.12}
\]</span></p>
<p>利用内积的双线性性质（见第 3.2 节），我们得到</p>
<p><span class="math display">\[
\langle w,x_a\rangle + b - r \frac{\langle w,w\rangle}{\|w\|} = 0.
\tag{12.13}
\]</span></p>
<p>注意，根据我们的缩放假设，第一项为 1，即 <span
class="math inline">\(\langle w,x_a\rangle + b=1\)</span>。根据第 3.1
节的式 (3.16)，<span class="math inline">\(\langle w,w\rangle =
\|w\|^2\)</span>。因此，第二项可简化为 <span
class="math inline">\(r\|w\|\)</span>。利用这些简化，我们得到</p>
<p><span class="math display">\[
\frac{1}{r} = \|w\|. \tag{12.14}
\]</span></p>
<p>这意味着我们将距离 <span class="math inline">\(r\)</span>
表示为了超平面的法向量 <span class="math inline">\(w\)</span>
的形式。乍一看，这个等式似乎不直观，因为我们好像是用向量 <span
class="math inline">\(w\)</span>
的长度来表示超平面的距离，但我们还不知道这个向量。一种理解方式是，将距离
<span class="math inline">\(r\)</span>
看作一个临时变量，仅用于推导。因此，在本节余下部分，我们用 <span
class="math inline">\(\frac{1}{\|w\|}\)</span> 表示到超平面的距离。在第
12.2.3 节中，我们将看到“边界等于 1”这一选择与第 12.2.1 节中假设 <span
class="math inline">\(\|w\|=1\)</span> 是等价的。类似于推导式 (12.9)
的思路，我们希望正例和负例都至少距离超平面 1 个单位，因此得到条件</p>
<blockquote>
<p>我们还可以把这个距离理解为当 <span class="math inline">\(x_a\)</span>
投影到超平面上时产生的投影误差。</p>
</blockquote>
<p><span class="math display">\[
y_n(\langle w,x_n\rangle + b)\ge 1. \tag{12.15}
\]</span></p>
<p>将边界（margin）最大化与“样本需要位于超平面正确一侧（依据其标签）”这一事实结合起来，就得到</p>
<p><span class="math display">\[
\max_{w,b}\;\frac{1}{\|w\|} \tag{12.16}
\]</span></p>
<p>满足约束条件</p>
<p><span class="math display">\[
y_n(\langle w,x_n\rangle + b)\ge 1,\quad \text{对于所有 }n=1,\dots,N.
\tag{12.17}
\]</span></p>
<p>与其像式 (12.16)
那样最大化范数的倒数，我们通常最小化范数的平方。<strong>同时我们还经常加入一个常数
<span class="math inline">\(\tfrac{1}{2}\)</span>，它不会影响最优的
<span
class="math inline">\(w,b\)</span>，但在计算梯度时能使形式更简洁。</strong>于是我们的目标变为</p>
<p><span class="math display">\[
\min_{w,b}\;\frac{1}{2}\|w\|^2 \tag{12.18}
\]</span></p>
<p>满足约束条件</p>
<p><span class="math display">\[
y_n(\langle w,x_n\rangle + b)\ge 1,\quad \text{对于所有 }n=1,\dots,N.
\tag{12.19}
\]</span></p>
<p>式 (12.18) 被称为<strong>硬间隔支持向量机（hard margin
SVM）</strong>。“硬（hard）”这一说法是因为这种形式不允许任何违反边界条件的情况。
我们将在第 12.2.4
节看到，如果数据不是线性可分的，这个“硬”条件可以放宽，以容纳一些违反边界条件的样本。</p>
<h4 id="为什么我们可以把间隔设为-1">12.2.3 为什么我们可以把间隔设为
1</h4>
<p>在第 12.2.1 节，我们提出我们希望最大化某个值 <span
class="math inline">\(r\)</span>，它表示距离超平面最近的样本点到超平面的距离。在第
12.2.2 节，我们通过缩放数据，使得最近的样本点到超平面的距离正好为
1。在本节，我们把这两种推导联系起来，并展示它们是等价的。</p>
<p><strong>定理 12.1</strong>:在 (12.10)
中考虑归一化权重的情况下，最大化间隔 <span
class="math inline">\(r\)</span>： <span class="math display">\[
\max_{w,b,r} \; r
\quad\text{（margin）}
\]</span></p>
<p>满足</p>
<p><span class="math display">\[
y_n(\langle w,x_n\rangle + b) \ge r
\quad\text{（data fitting）},\qquad
\|w\|=1
\quad\text{（normalization）},\qquad
r&gt;0,
\tag{12.20}
\]</span></p>
<p>等价于对数据进行缩放，使得间隔为 1：</p>
<p><span class="math display">\[
\min_{w,b} \;\frac{1}{2}\|w\|^2
\quad\text{（margin）}
\]</span></p>
<p>满足</p>
<p><span class="math display">\[
y_n(\langle w,x_n\rangle + b) \ge 1
\quad\text{（data fitting）}.
\tag{12.21}
\]</span></p>
<p><strong>证明</strong> 考虑
(12.20)。因为平方对于非负自变量是严格单调的，所以在目标函数中把 <span
class="math inline">\(r\)</span> 换成 <span
class="math inline">\(r^2\)</span> 最大值保持不变。由于 <span
class="math inline">\(\|w\|=1\)</span>，我们可以通过引入一个未归一化的新权重向量
<span class="math inline">\(w&#39;\)</span> 重新参数化：显式写成 <span
class="math inline">\(w=w&#39;/\|w&#39;\|\)</span>。于是得到： <span
class="math display">\[
\max_{w&#39;,b,r} r^2
\quad \text{s.t.}\quad
y_n\Bigl( \frac{w&#39;}{\|w&#39;\|},x_n\Bigr)+b \ge r,\quad r&gt;0.
\tag{12.22}
\]</span></p>
<p>式 (12.22) 明确表明距离 <span class="math inline">\(r\)</span>
是正的。因此，我们可以将第一个约束除以 <span
class="math inline">\(r\)</span>，得到：</p>
<p><span class="math display">\[
\max_{w&#39;,b,r} r^2
\quad \text{s.t.}\quad
y_n\!\left(
\frac{w&#39;}{\|w&#39;\|r},x_n
+\frac{b}{r}\right)\ge 1,\quad r&gt;0,
\tag{12.23}
\]</span></p>
<p>然后把参数重命名为 <span class="math inline">\(w&#39;&#39;\)</span>
和 <span class="math inline">\(b&#39;&#39;\)</span>。因为 <span
class="math inline">\(w&#39;&#39;=\frac{w&#39;}{\|w&#39;\|r}\)</span>
得到<span class="math inline">\(\|w&#39;&#39;\|=\frac{1}{r}\cdot
r\)</span>。将这一结果代入 (12.23)，我们得到：</p>
<p><span class="math display">\[
\max_{w&#39;&#39;,b&#39;&#39;} \frac{1}{\|w&#39;&#39;\|^2}
\quad \text{s.t.}\quad
y_n(\langle w&#39;&#39;,x_n\rangle + b&#39;&#39;)\ge 1.
\tag{12.24}
\]</span></p>
<p>最后一步是注意到，最大化 <span
class="math inline">\(\frac{1}{\|w&#39;&#39;\|^2}\)</span> 与最小化
<span class="math inline">\(\frac{1}{2}\|w&#39;&#39;\|^2\)</span>
得到的解是相同的，这就完成了定理 12.1 的证明。</p>
<p><span class="math display">\[
\begin{equation}
\begin{aligned}
&amp; \max_{\boldsymbol{w}&#39;&#39;, b&#39;&#39;}
\frac{1}{\|\boldsymbol{w}&#39;&#39;\|^2} \\
&amp; \text{subject to} \quad y_n \left( \langle
\boldsymbol{w}&#39;&#39;, \boldsymbol{x}_n \rangle + b&#39;&#39; \right)
\geq 1.
\end{aligned}
\tag{12.25}
\end{equation}
\]</span></p>
<h4 id="软间隔-svm几何视角">12.2.4 软间隔 SVM：几何视角</h4>
<p>当数据不是线性可分时，我们可能希望允许某些样本落在间隔区域内，甚至位于超平面的错误一侧，如图
12.6 所示。允许一定分类错误的模型称为<strong>软间隔 SVM（Soft Margin
SVM）</strong>。在本节中，我们用几何方法推导其优化问题。在第 12.2.5
节，我们将用损失函数的思想推导一个等价的优化问题。在第 12.3
节中，我们将利用拉格朗日乘子（第 7.2 节）推导 SVM 的对偶优化问题。</p>
<p><img src="/img3/机器学习的数学基础Part2/F12.6.png" alt="F12.6" style="zoom:50%;" /></p>
<p><img src="/img3/机器学习的数学基础Part2/F12.7.png" alt="F12.7" style="zoom:50%;" /></p>
<p>这个对偶优化问题使我们能够从第三种角度理解
SVM：即作为一个超平面，它平分了正样本和负样本对应的凸包之间的线段（第
12.3.2 节）。关键的几何思想是：为每个样本—标签对 <span
class="math inline">\((x_n,y_n)\)</span> 引入一个松弛变量 <span
class="math inline">\(\xi_n\)</span>，允许某个样本位于间隔之内，甚至在超平面的错误一侧（参见图
12.7）。我们从间隔中减去 <span class="math inline">\(\xi_n\)</span>
的值，并约束 <span class="math inline">\(\xi_n\)</span>
为非负。为了鼓励样本的正确分类，我们在目标函数中加入 <span
class="math inline">\(\xi_n\)</span>： <span class="math display">\[
\min_{w,b,\xi} \frac{1}{2}\|w\|^2 + C\sum_{n=1}^N \xi_n \tag{12.26a}
\]</span></p>
<p>约束条件为</p>
<p><span class="math display">\[
y_n(\langle w,x_n\rangle+b)\ge 1-\xi_n \tag{12.26b}
\]</span></p>
<p><span class="math display">\[
\xi_n \ge 0 \tag{12.26c}
\]</span></p>
<p>其中 <span class="math inline">\(n=1,\dots,N\)</span>。与硬间隔 SVM
的优化问题 (12.18) 相比，这里称为软间隔 SVM。参数 <span
class="math inline">\(C&gt;0\)</span>
在间隔大小与总松弛量之间进行权衡。这个参数称为<strong>正则化参数</strong>，因为（如我们在下一节将看到的那样）目标函数中的间隔项
(12.26a) 实际上是一个正则化项。其中的 <span
class="math inline">\(\|w\|^2\)</span>
被称为<strong>正则器</strong>，在许多数值优化的书籍中，正则化参数会乘在这一项上（第
8.2.3 节）。这与我们在本节的表述不同。<strong>在这里，较大的 <span
class="math inline">\(C\)</span>
值意味着较低的正则化，因为我们给松弛变量更大的权重，从而更优先考虑那些没有落在正确间隔一侧的样本。</strong></p>
<p><strong>备注：</strong>在软间隔 SVM (12.26a) 的表述中，<span
class="math inline">\(w\)</span> 被正则化，而 <span
class="math inline">\(b\)</span>
没有被正则化。我们可以通过观察正则化项不包含 <span
class="math inline">\(b\)</span> 来看出这一点。未正则化的 <span
class="math inline">\(b\)</span> 会使理论分析更加复杂（Steinwart 和
Christmann, 2008，第 1 章），并降低计算效率（Fan 等人, 2008）。</p>
<blockquote>
<p>个人注：</p>
<p><strong>软间隔 SVM 中参数 <span class="math inline">\(C\)</span>
的几何意义</strong>，并和前面章节的正则化写法对比。</p>
<ol type="1">
<li><strong>软间隔 SVM 的目标函数</strong></li>
</ol>
<p>在书的 12.2.4 里，软间隔 SVM 写成：</p>
<p><span class="math display">\[
\min_{w,b,\xi}
\frac{1}{2}\|w\|^2 + C\sum_{n=1}^N \xi_n
\]</span></p>
<p>其中</p>
<ul>
<li><span class="math inline">\(\frac{1}{2}\|w\|^2\)</span>
是正则化项（控制间隔的宽度）；</li>
<li><span class="math inline">\(\xi_n\)</span> 是松弛变量（表示第 <span
class="math inline">\(n\)</span> 个样本违反间隔或分错的程度）；</li>
<li><span class="math inline">\(C\)</span> 是惩罚系数。</li>
</ul>
<ol start="2" type="1">
<li><strong>两种等价的写法</strong></li>
</ol>
<p>有的书（或有的章节）把目标函数写成：</p>
<p><span class="math display">\[
\min_{w,b,\xi}
\lambda\frac{1}{2}\|w\|^2 + \sum_{n=1}^N \xi_n
\]</span></p>
<p>这时 <span class="math inline">\(\lambda\)</span>
越大，正则化越强。</p>
<p>而在《Mathematics for Machine Learning》这一节，写法是：</p>
<p><span class="math display">\[
\frac{1}{2}\|w\|^2 + C\sum \xi_n
\]</span></p>
<p>它把 <span class="math inline">\(C\)</span>
放在松弛项前面，而不是正则项前面。于是</p>
<ul>
<li>这里 <span class="math inline">\(C\)</span> 大
<strong>不表示正则化强</strong>，反而表示对松弛变量的惩罚更大；</li>
<li>也就是模型会“尽量减少”违反间隔的点，宁可让 <span
class="math inline">\(\|w\|\)</span> 增大（间隔变窄）。</li>
</ul>
<p>因此 <strong>正则化的程度与 <span class="math inline">\(C\)</span>
反比</strong>：</p>
<ul>
<li><p><span class="math inline">\(C\)</span>
<strong>大</strong>：更重视分类正确（少违例），放松正则化（间隔窄一些）；</p></li>
<li><p><span class="math inline">\(C\)</span>
<strong>小</strong>：更重视间隔宽（强正则化），允许更多点在间隔内或分错。</p>
<ol start="3" type="1">
<li><strong>书中那句话的意思</strong></li>
</ol></li>
</ul>
<blockquote>
<p>Here a large value of C implies low regularization, as we give the
slack variables larger weight, hence giving more priority to examples
that do not lie on the correct side of the margin.</p>
</blockquote>
<p>翻译＋解释：</p>
<ul>
<li>“在这一节的写法里，<span class="math inline">\(C\)</span>
越大意味着正则化越弱，因为我们给松弛变量更大的权重”；</li>
<li>“这样模型就更关注那些不在正确间隔一侧的样本（尽量减少违例），而不是保持大间隔”。</li>
</ul>
<p>这和前面章节把 <span class="math inline">\(\lambda\)</span> 乘在
<span class="math inline">\(\|w\|^2\)</span>
上的写法相反，所以要特别注意。</p>
<ol start="4" type="1">
<li><strong>几何直观</strong></li>
</ol>
<ul>
<li><span class="math inline">\(C\)</span> 大： 惩罚违例强 →
尽量让所有点都分对 → 容忍小间隔（<span
class="math inline">\(\|w\|\)</span> 大） → 正则化弱。</li>
<li><span class="math inline">\(C\)</span> 小： 惩罚违例弱 →
可以容忍点进入间隔或分错 → 追求大间隔（<span
class="math inline">\(\|w\|\)</span> 小） → 正则化强。</li>
</ul>
<p>所以几何上看：<span class="math inline">\(C\)</span>
控制了“宽间隔”与“少违例”之间的折中。</p>
</blockquote>
<h4 id="软间隔-svm从损失函数的角度">12.2.5 软间隔
SVM：从损失函数的角度</h4>
<p>让我们考虑一种推导 SVM 的不同方法，遵循经验风险最小化的原则（第 8.2
节）。对于 SVM，我们选择超平面作为假设类，也就是说：</p>
<p><span class="math display">\[
f(x)=\langle w,x\rangle+b. \tag{12.27}
\]</span></p>
<p>我们将在本节看到，间隔对应于正则化项。剩下的问题是：损失函数是什么？与第
9 章不同，在第 9
章我们考虑的是回归问题（预测器的输出是一个实数），而在本章我们考虑的是二分类问题（预测器的输出是两个标签
<span class="math inline">\(\{+1,-1\}\)</span>
之一）。因此，每一个样本–标签对的误差/损失函数需要适合二分类。例如，回归所用的平方损失（公式
9.10b）并不适合二分类。</p>
<p><strong>注：</strong><u>二元标签之间的理想损失函数是统计预测与真实标签不一致的次数。</u>这意味着，对于某个预测器
<span class="math inline">\(f\)</span> 作用在样本 <span
class="math inline">\(x_n\)</span> 上，我们将输出 <span
class="math inline">\(f(x_n)\)</span> 与标签 <span
class="math inline">\(y_n\)</span> 进行比较。如果二者一致，则定义损失为
0；不一致，则定义为 1。记为 <span class="math inline">\(1(f(x_n)\neq
y_n)\)</span>，称为“0-1 损失”。不幸的是，0-1 损失会导致求最优参数 <span
class="math inline">\(w,b\)</span>
时出现组合优化问题。组合优化问题（与第 7
章讨论的连续优化问题不同）一般更难求解。</p>
<p>那么，SVM 对应的损失函数是什么？考虑预测器 <span
class="math inline">\(f(x_n)\)</span> 的输出与标签 <span
class="math inline">\(y_n\)</span>
之间的误差。损失描述了在训练数据上产生的错误。推导 (12.26a)
的等价方法是使用<strong>铰链损失（hinge loss）</strong></p>
<p><span class="math display">\[
\ell(t)=\max\{0,1-t\},\quad \text{其中}\ t=yf(x)=y(\langle
w,x\rangle+b). \tag{12.28}
\]</span></p>
<p>如果 <span class="math inline">\(f(x)\)</span> 落在超平面对应标签
<span class="math inline">\(y\)</span> 的正确一侧，且距离大于
1，也就是说 <span class="math inline">\(t\ge 1\)</span>，则铰链损失返回
0。如果 <span class="math inline">\(f(x)\)</span>
在正确一侧但离超平面太近（<span
class="math inline">\(0&lt;t&lt;1\)</span>），样本 <span
class="math inline">\(x\)</span>
处于间隔区内，铰链损失返回一个正值。当样本在超平面的错误一侧（<span
class="math inline">\(t&lt;0\)</span>）时，铰链损失返回更大的值，且值线性增加。换句话说，一旦我们距离超平面小于间隔，即使预测正确也要支付代价，而且代价随距离线性增加。</p>
<p>另一种表达铰链损失的方法是把它看成两个线性分段：</p>
<p><span class="math display">\[
\ell(t)=\begin{cases}
0 &amp; \text{如果 } t\ge 1\\[4pt]
1-t &amp; \text{如果 } t&lt;1
\end{cases}\tag{12.29}
\]</span></p>
<p>如图 12.8 所示。</p>
<p><img src="/img3/机器学习的数学基础Part2/F12.8.png" alt="F12.8" style="zoom:50%;" /></p>
<p>对应硬间隔 SVM (公式 12.18) 的损失函数定义为：</p>
<p><span class="math display">\[
\ell(t)=\begin{cases}
0 &amp; \text{如果 } t\ge 1\\[4pt]
\infty &amp; \text{如果 } t&lt;1
\end{cases}\tag{12.30}
\]</span></p>
<p>这个损失可以理解为：<strong>绝不允许任何样本进入间隔之内</strong>。</p>
<p>对于一个给定的训练集 <span
class="math inline">\(\{(x_1,y_1),\ldots,(x_N,y_N)\}\)</span>，我们希望在正则化目标函数的同时最小化总损失，其中正则化采用
<span class="math inline">\(\ell_2\)</span>-正则化（参见第 8.2.3
节）。使用铰链损失 (12.28)
可以得到如下<strong>无约束优化问题</strong>：</p>
<p><span class="math display">\[
\min_{w,b}
\;\;\frac{1}{2}\|w\|^2
\quad \text{（正则化项）}
+ C\sum_{n=1}^N
\max\{0,1-y_n(\langle w,x_n\rangle+b)\}
\quad \text{（误差项）}.
\tag{12.31}
\]</span></p>
<p>(12.31) 中的第一项称为<strong>正则化项</strong>（或正则器，见第 8.2.3
节），第二项称为<strong>损失项</strong>或<strong>误差项</strong>。回忆第
12.2.4 节，项 <span class="math inline">\(\frac{1}{2}\|w\|^2\)</span>
直接来自于间隔。换句话说，<strong>最大化间隔可以解释为正则化</strong>。</p>
<p>原则上，(12.31)
中的无约束优化问题可以直接用（子）梯度下降方法求解（参见第 7.1
节）。要看出 (12.31) 和 (12.26a) 等价，只需注意铰链损失 (12.28)
本质上由两段线性部分组成，如 (12.29) 所示。考虑单个样本–标签对的铰链损失
(12.28)，我们可以把对 <span class="math inline">\(t\)</span>
的铰链损失最小化，等价地替换成对一个松弛变量 <span
class="math inline">\(\xi\)</span>
的最小化，并加上两个约束。公式形式为：</p>
<p><span class="math display">\[
\min_t \max\{0,1-t\} \tag{12.32}
\]</span></p>
<p>等价于</p>
<p><span class="math display">\[
\min_{\xi,t}\;\xi
\quad \text{subject to } \xi\ge 0,\;\xi\ge 1-t.
\tag{12.33}
\]</span></p>
<p>把这个表达式代入
(12.31)，并重排其中一个约束，我们就能得到<strong>软间隔 SVM</strong>
(12.26a) 的形式。</p>
<p><strong>注：</strong>让我们对比一下本节选择的损失函数和第 9
章线性回归中的损失函数。回忆第 9.2.1
节，对于求最大似然估计，我们通常最小化负对数似然。此外，因为带高斯噪声的线性回归的似然项是高斯分布，所以每个样本的负对数似然就是一个平方误差函数。<strong>平方误差函数正是我们在寻找最大似然解时最小化的损失函数。</strong></p>
<h3 id="对偶支持向量机">12.3 对偶支持向量机</h3>
<p>前面几节中，我们用变量 <span class="math inline">\(w\)</span> 和
<span class="math inline">\(b\)</span> 来描述
SVM，这种形式被称为<strong>原始（primal）SVM</strong>。回忆我们考虑的输入
<span class="math inline">\(x\in \mathbb{R}^D\)</span> 有 <span
class="math inline">\(D\)</span> 个特征。由于 <span
class="math inline">\(w\)</span> 与 <span
class="math inline">\(x\)</span>
维度相同，这意味着在优化问题中参数的数量（即 <span
class="math inline">\(w\)</span>
的维度）会随着特征数量线性增长。接下来，我们考虑一个<strong>等价的优化问题</strong>（所谓的<strong>对偶形式</strong>），它与特征数无关。相反，参数的数量随着训练集中的样本数增长。我们在第
10
章中已经看到过类似的思想，当时我们将学习问题表达为一种不随特征数扩展的形式。这对于特征数远多于训练样本数的问题尤其有用。对偶
SVM
还有一个额外的优点，就是它很容易引入核函数（kernel），我们将在本章末尾看到这一点。“对偶”这个词在数学文献中经常出现，在本例中它指的是<strong>凸对偶性</strong>。接下来的各小节本质上是第
7.2 节中讨论过的凸对偶理论的一个应用。</p>
<h4 id="通过拉格朗日乘子看凸对偶">12.3.1 通过拉格朗日乘子看凸对偶</h4>
<p>回忆软间隔 SVM 的原始形式 (12.26a)。我们称与原始 SVM 对应的变量 <span
class="math inline">\(w\)</span>、<span class="math inline">\(b\)</span>
和 <span class="math inline">\(\xi\)</span> 为<strong>原始变量（primal
variables）</strong>。我们使用 <span class="math inline">\(\alpha_n \ge
0\)</span> 作为拉格朗日乘子，对应于约束 (12.26b)
中“样本被正确分类”的要求；使用 <span class="math inline">\(\gamma_n \ge
0\)</span> 作为拉格朗日乘子，对应于松弛变量非负性的约束
(12.26c)。拉格朗日函数为：</p>
<p><span class="math display">\[
\begin{aligned}
L(w,b,\xi,\alpha,\gamma)
&amp;= \frac{1}{2}\|w\|^2 + C \sum_{n=1}^N \xi_n  \\
&amp;\quad - \sum_{n=1}^N \alpha_n \bigl( y_n (\langle w,x_n\rangle + b)
- 1 + \xi_n \bigr) \\
&amp;\quad - \sum_{n=1}^N \gamma_n \xi_n
\end{aligned} \tag{12.34}
\]</span></p>
<p>其中</p>
<ul>
<li>第一行是原目标，</li>
<li>第二行对应约束 (12.26b)，</li>
<li>第三行对应约束 (12.26c)。</li>
</ul>
<p>分别对三个原始变量 <span class="math inline">\(w\)</span>、<span
class="math inline">\(b\)</span> 和 <span
class="math inline">\(\xi\)</span> 求偏导，我们得到：</p>
<p><span class="math display">\[
\frac{\partial L}{\partial w} =
w^\top - \sum_{n=1}^N \alpha_n y_n x_n^\top \tag{12.35}
\]</span></p>
<p><span class="math display">\[
\frac{\partial L}{\partial b} = - \sum_{n=1}^N \alpha_n y_n \tag{12.36}
\]</span></p>
<p><span class="math display">\[
\frac{\partial L}{\partial \xi_n} = C - \alpha_n - \gamma_n \tag{12.37}
\]</span></p>
<p>通过令这些偏导数分别为零来求解拉格朗日函数的极值。将 (12.35)
置零，可得：</p>
<p><span class="math display">\[
w = \sum_{n=1}^N \alpha_n y_n x_n \tag{12.38}
\]</span></p>
<p>这是“表示定理（representer theorem）”的一个具体实例（Kimeldorf 和
Wahba, 1970）。式 (12.38) 表明，在原始形式下最优的权重向量是样本 <span
class="math inline">\(x_n\)</span> 的线性组合。回忆 2.6.1
节的内容，这意味着优化问题的解位于训练数据的线性张成空间内。此外，由
(12.36) 置零得到的约束意味着最优权重向量是样本的一个仿射组合。</p>
<p>表示定理在非常一般的“正则化经验风险最小化”设定下都成立（Hofmann 等,
2008; Argyriou 和 Dinuzzo, 2014）。该定理还有更一般的版本（Schölkopf 等,
2001），其存在的充要条件可见 Yu 等 (2013)。</p>
<p><strong>注释：</strong> 表示定理（式
12.38）还解释了“支持向量机”这一名称的来源。对于那些对应参数 <span
class="math inline">\(\alpha_n = 0\)</span> 的样本 <span
class="math inline">\(x_n\)</span>，它们对解 <span
class="math inline">\(w\)</span> 完全没有贡献。而其他满足 <span
class="math inline">\(\alpha_n &gt; 0\)</span>
的样本则被称为“支持向量”，因为它们“支撑”了超平面。</p>
<p>将 <span class="math inline">\(w\)</span>
的表达式代入拉格朗日函数（12.34）后，我们得到对偶式</p>
<p><span class="math display">\[
\begin{align}
D(\xi,\alpha,\gamma)
&amp; =
\frac{1}{2}\, y_i y_j \alpha_i \alpha_j \langle x_i, x_j\rangle
-
\sum_{i=1}^N y_i\alpha_i
+
C\sum_{i=1}^N \xi_i
+
\sum_{i=1}^N \alpha_i \\
&amp; -\sum_{i=1}^N y_i \alpha_i
-\sum_{i=1}^N\sum_{j=1}^N \alpha_i \xi_i
-\sum_{i=1}^N y_j\alpha_j \langle x_j,x_i\rangle
-
\sum_{i=1}^N \gamma_i \xi_i.
\tag{12.39}
\end{align}
\]</span></p>
<p>注意，现在不再有任何涉及原始变量 <span
class="math inline">\(w\)</span> 的项。通过将式（12.36）设为零，我们得到
<span class="math inline">\(\sum_{n=1}^N y_n \alpha_n =
0\)</span>。因此，涉及 <span class="math inline">\(b\)</span>
的项也消失了。回忆内积是对称且双线性的（见 3.2
节），所以（12.39）中前两项（蓝色部分）是针对同样对象的，可以进行简化，于是我们得到拉格朗日函数</p>
<p><span class="math display">\[
D(\xi,\alpha,\gamma)
=
-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N
y_i y_j \alpha_i \alpha_j \langle x_i, x_j\rangle
+\sum_{i=1}^N \alpha_i
+\sum_{i=1}^N (C-\alpha_i-\gamma_i)\xi_i.
\tag{12.40}
\]</span></p>
<p>这个式子的最后一项包含了所有带有松弛变量 <span
class="math inline">\(\xi_i\)</span>
的项。通过将（12.37）设为零，我们看到（12.40）中最后一项也为零。此外，利用同一个方程并回忆拉格朗日乘子
<span class="math inline">\(\gamma_i\)</span> 非负，我们可以推出 <span
class="math inline">\(\alpha_i \leq
C\)</span>。于是我们得到支持向量机的<strong>对偶优化问题</strong>，它仅用拉格朗日乘子
<span class="math inline">\(\alpha_i\)</span>
表示。根据拉格朗日对偶性（定义
7.1），我们要最大化对偶问题。这等价于最小化负的对偶问题，从而得到<strong>对偶
SVM</strong>：</p>
<p><span class="math display">\[
\begin{align}
&amp; \min_{\alpha}\quad
\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N
y_i y_j \alpha_i \alpha_j \langle x_i, x_j\rangle
-\sum_{i=1}^N \alpha_i
\\[6pt]
&amp; \text{subject to}\quad
\sum_{i=1}^N y_i \alpha_i = 0,\\[4pt]
&amp; 0\leq \alpha_i \leq C,\quad i=1,\dots,N.
\end{align}
\tag{12.41}
\]</span></p>
<p>其中，等式约束 <span class="math inline">\(\sum_{i=1}^N y_i \alpha_i
= 0\)</span> 来自将（12.36）设为零；不等式约束 <span
class="math inline">\(\alpha_i \ge 0\)</span>
是对不等式约束的拉格朗日乘子的条件（见 7.2 节）；不等式约束 <span
class="math inline">\(\alpha_i \le C\)</span>
则是上一段中讨论的结果。</p>
<p>在 SVM 中的不等式约束集合被称为“盒约束（box
constraints）”，因为它们限制拉格朗日乘子向量</p>
<p><span class="math display">\[
\boldsymbol{\alpha} = [\alpha_1,\dots,\alpha_N]^\top \in \mathbb{R}^N
\]</span></p>
<p>在每个坐标轴上都位于由 0 和 <span class="math inline">\(C\)</span>
定义的盒子内。这样的轴对齐盒子在数值求解器中实现起来特别高效（Dostál，2009，第
5 章）。一旦我们得到了对偶参数 <span
class="math inline">\(\alpha\)</span>，就可以通过表示定理（12.38）恢复原始参数
<span class="math inline">\(w\)</span>。我们把最优的原始参数称为 <span
class="math inline">\(w^*\)</span>。不过，仍然存在如何得到参数 <span
class="math inline">\(b^*\)</span>
的问题。考虑一个恰好位于间隔边界上的样本 <span
class="math inline">\(x_n\)</span>，即</p>
<p><span class="math display">\[
\langle w^*,x_n\rangle + b = y_n.
\]</span></p>
<p>回忆 <span class="math inline">\(y_n\)</span> 只能取 +1 或
−1。因此唯一未知的是 <span
class="math inline">\(b\)</span>，它可以通过下式计算：</p>
<p><span class="math display">\[
b^* = y_n - \langle w^*,x_n\rangle. \tag{12.42}
\]</span></p>
<p><strong>注释。</strong>
原则上，可能没有样本恰好落在间隔上。在这种情况下，我们应该对所有支持向量计算<span
class="math inline">\(|y_n - \langle
w^*,x_n\rangle|\)</span>，然后取这个绝对值差的<strong>中位数</strong>作为
<span class="math inline">\(b^*\)</span> 的值。推导可以在<a
target="_blank" rel="noopener" href="http://fouryears.eu/2012/06/07/the-svm-bias-term-conspiracy/">http://fouryears.eu/2012/06/07/the-svm-bias-term-conspiracy/</a>
找到。</p>
<h4 id="对偶-svm凸包视角">12.3.2 对偶 SVM：凸包视角</h4>
<p>获得对偶 SVM
的另一种方法是考虑一个替代性的几何论证。考虑所有标签相同的样本 <span
class="math inline">\(x_n\)</span>
的集合。我们希望构建一个包含所有样本的<strong>凸集</strong>，并且这个集合尽可能小。这就是所谓的“凸包（convex
hull）”，如图 12.9 所示。</p>
<p><img src="/img3/机器学习的数学基础Part2/F12.9.png" alt="F12.9" style="zoom:50%;" /></p>
<p>我们先直观理解一下点的凸组合。考虑两个点 <span
class="math inline">\(x_1\)</span> 和 <span
class="math inline">\(x_2\)</span>，以及对应的非负权重 <span
class="math inline">\(\alpha_1,\alpha_2 \ge 0\)</span>，并且 <span
class="math inline">\(\alpha_1+\alpha_2=1\)</span>。方程 <span
class="math inline">\(\alpha_1x_1+\alpha_2x_2\)</span> 描述了 <span
class="math inline">\(x_1\)</span> 和 <span
class="math inline">\(x_2\)</span>
之间的每一个点。接下来考虑加入第三个点 <span
class="math inline">\(x_3\)</span> 以及权重 <span
class="math inline">\(\alpha_3\ge 0\)</span>，满足<span
class="math inline">\(\sum_{n=1}^{3}\alpha_n=1\)</span>。这三个点 <span
class="math inline">\(x_1,x_2,x_3\)</span>
的凸组合张成了一个二维区域。这个区域的凸包就是由每一对点连成的三角形边界。随着我们加入更多的点，并且点的数量大于维度时，其中一些点将会位于凸包内部，如图
12.9(a) 所示。一般来说，构造一个凸包可以通过为每个样本 <span
class="math inline">\(x_n\)</span> 引入非负权重 <span
class="math inline">\(\alpha_n \ge 0\)</span>
来完成。这样，凸包可以表示为集合</p>
<p><span class="math display">\[
\text{conv}(X)= \sum_{n=1}^N \alpha_n x_n \quad \text{其中} \quad
\sum_{n=1}^N \alpha_n=1,\;\alpha_n\ge 0, \tag{12.43}
\]</span></p>
<p>对于所有 <span class="math inline">\(n=1,\dots,N\)</span>
都成立。如果正类和负类对应的两组点云是分开的，那么它们的凸包就不会重叠。给定训练数据
<span
class="math inline">\((x_1,y_1),\dots,(x_N,y_N)\)</span>，我们分别构造正类和负类的两个凸包。</p>
<blockquote>
<p>个人注：直观理解</p>
<ul>
<li><strong>两个点 <span
class="math inline">\(x_1,x_2\)</span></strong>： 取 <span
class="math inline">\(\alpha_1=t,\alpha_2=1-t,t\in[0,1]\)</span>，得到
<span class="math inline">\(\alpha_1x_1+\alpha_2x_2\)</span>
是这两个点连线上的所有点；这就是它们的凸包。</li>
<li><strong>三个点 <span
class="math inline">\(x_1,x_2,x_3\)</span></strong>： 同理，权重非负且和
1，就得到三角形内部所有点；这就是三个点的凸包。</li>
<li><strong>更多点</strong>： 就是把所有点“包”起来的多面体。</li>
</ul>
</blockquote>
<p>我们在正类样本凸包中选取一个点 <span
class="math inline">\(c\)</span>，它距离负类分布最近；同样，我们在负类样本凸包中选取一个点
<span class="math inline">\(d\)</span>，它距离正类分布最近；见图
12.9(b)。我们将 <span class="math inline">\(d\)</span> 与 <span
class="math inline">\(c\)</span> 的差定义为 <span
class="math display">\[
w := c - d. \tag{12.44}
\]</span></p>
<p>像上面那样选取 <span class="math inline">\(c\)</span> 和 <span
class="math inline">\(d\)</span>，并要求它们彼此尽可能接近，这等价于最小化
<span class="math inline">\(w\)</span>
的长度/范数，从而得到相应的优化问题</p>
<p><span class="math display">\[
\arg\min_w \|w\| = \arg\min_w \frac{1}{2}\|w\|^2. \tag{12.45}
\]</span></p>
<p>由于 <span class="math inline">\(c\)</span>
必须在正类凸包中，它可以表示为正类样本的凸组合，即对于非负系数 <span
class="math inline">\(\alpha_n^+\)</span>：</p>
<p><span class="math display">\[
c = \sum_{n:\,y_n=+1}\alpha_n^+ x_n. \tag{12.46}
\]</span></p>
<p>在 (12.46) 中，我们使用记号 <span class="math inline">\(n:
y_n=+1\)</span> 来表示标签 <span class="math inline">\(y_n=+1\)</span>
的索引集合。类似地，对于负类样本我们有</p>
<p><span class="math display">\[
d = \sum_{n:\,y_n=-1}\alpha_n^- x_n. \tag{12.47}
\]</span></p>
<p>将 (12.44)、(12.46) 和 (12.47) 代入 (12.45)，我们得到目标函数</p>
<p><span class="math display">\[
\min_{\alpha} \frac{1}{2}\left\|
\sum_{n:\,y_n=+1}\alpha_n^+x_n
-
\sum_{n:\,y_n=-1}\alpha_n^-x_n
\right\|^2. \tag{12.48}
\]</span></p>
<p>设 <span class="math inline">\(\alpha\)</span> 是所有系数的集合，即
<span class="math inline">\(\alpha^+\)</span> 和 <span
class="math inline">\(\alpha^-\)</span>
的拼接。回忆我们要求每个凸包内的系数之和为 1：</p>
<p><span class="math display">\[
\sum_{n:\,y_n=+1}\alpha_n^+=1,\quad
\sum_{n:\,y_n=-1}\alpha_n^-=1. \tag{12.49}
\]</span></p>
<p>这意味着约束条件</p>
<p><span class="math display">\[
\sum_{n=1}^N y_n\alpha_n=0. \tag{12.50}
\]</span></p>
<p>通过将每一类分别展开可以看到这一点：</p>
<p><span class="math display">\[
\sum_{n=1}^N y_n\alpha_n
=
\sum_{n:\,y_n=+1}(+1)\alpha_n^+
+\sum_{n:\,y_n=-1}(-1)\alpha_n^- \tag{12.51a}
\]</span></p>
<p><span class="math display">\[
=\sum_{n:\,y_n=+1}\alpha_n^+-\sum_{n:\,y_n=-1}\alpha_n^-
=1-1=0. \tag{12.51b}
\]</span></p>
<p>目标函数 (12.48) 与约束 (12.50)，再加上 <span
class="math inline">\(\alpha \ge 0\)</span>
的假设，给我们一个受约束的（凸）优化问题。可以证明这个优化问题与对偶硬间隔
SVM 的优化问题是相同的（Bennett 和 Bredensteiner，2000a）。</p>
<p><strong>注释。</strong>
要得到软间隔对偶问题，我们考虑<strong>缩减凸包（reduced
hull）</strong>。缩减凸包与凸包类似，但对系数 <span
class="math inline">\(\alpha\)</span> 的大小有一个上界。 <span
class="math inline">\(\alpha\)</span>
的元素最大可能值限制了凸包所能取的大小。换句话说，对 <span
class="math inline">\(\alpha\)</span>
的上界将凸包缩小到一个更小的体积（Bennett 和
Bredensteiner，2000b）。</p>
<h3 id="核函数">12.4 核函数</h3>
<p>Kernels</p>
<p>考虑对偶 SVM（式 12.41）的形式。注意到，目标函数中的内积只出现在样本
<span class="math inline">\(x_i\)</span> 和 <span
class="math inline">\(x_j\)</span>
之间；并没有样本与参数之间的内积。因此，如果我们用一组特征 <span
class="math inline">\(\phi(x_i)\)</span> 来表示 <span
class="math inline">\(x_i\)</span>，在对偶 SVM
中唯一的变化就是把内积替换掉。这种<strong>模块化的形式，使得分类方法（SVM
的选择）与特征表示 <span class="math inline">\(\phi(x)\)</span>
的选择可以分开考虑，从而为我们独立探索这两个问题提供了灵活性。</strong>本节我们讨论
<span class="math inline">\(\phi(x)\)</span>
的表示，并简要介绍核函数的概念，但不深入技术细节。</p>
<p>由于 <span class="math inline">\(\phi(x)\)</span>
可以是一个非线性函数，我们就可以使用假设线性分类器的 SVM 来构造在样本
<span class="math inline">\(x_n\)</span>
上<strong>非线性</strong>的分类器。这为处理非线性可分的数据集提供了第二条途径（第一条是软间隔）。事实证明，有许多算法和统计方法也具有我们在对偶
SVM
中看到的这种性质：内积只出现在样本之间。我们无需显式定义非线性特征映射
<span class="math inline">\(\phi(\cdot)\)</span> 并计算样本 <span
class="math inline">\(x_i\)</span> 与 <span
class="math inline">\(x_j\)</span> 之间的内积，而是定义样本 <span
class="math inline">\(x_i\)</span> 与 <span
class="math inline">\(x_j\)</span> 之间的相似度函数 <span
class="math inline">\(k(x_i,x_j)\)</span>。对于某一类<strong>相似度函数（称为核函数）</strong>，相似度函数<strong>隐式地</strong>定义了一个非线性特征映射
<span class="math inline">\(\phi(\cdot)\)</span>。</p>
<blockquote>
<p>个人注：</p>
<ul>
<li><strong>软间隔 SVM + 无核函数</strong> →
仍然是线性分割（超平面）。</li>
<li><strong>软间隔 SVM + 核函数</strong> →
在原空间里是非线性分割（在特征空间里仍然是线性超平面）。</li>
</ul>
</blockquote>
<p>核函数按定义是函数 <span class="math inline">\(k:
\mathcal{X}\times\mathcal{X}\to\mathbb{R}\)</span>，满足存在一个希尔伯特空间
<span class="math inline">\(\mathcal{H}\)</span> 以及一个特征映射 <span
class="math inline">\(\phi: \mathcal{X}\to\mathcal{H}\)</span>，使得</p>
<p><span class="math display">\[
k(x_i,x_j)=\langle\phi(x_i),\phi(x_j)\rangle_{\mathcal{H}}. \tag{12.52}
\]</span></p>
<p>每个核函数 <span class="math inline">\(k\)</span>
都有一个唯一的再生核希尔伯特空间（RKHS）与之对应（Aronszajn,
1950；Berlinet 和 Thomas-Agnan, 2004）。在这种唯一对应下，<span
class="math inline">\(\phi(x)=k(\cdot,x)\)</span>
被称为<strong>规范特征映射</strong>（canonical feature
map）。从<strong>内积推广到核函数（式 12.52）被称为核技巧（kernel
trick），它把显式的非线性特征映射隐藏了起来，</strong>（Schölkopf 和
Smola, 2002；Shawe-Taylor 和 Cristianini, 2004）。</p>
<blockquote>
<p>个人注：注意<strong>核函数（kernel
function）</strong>和<strong>核密度函数（kernel density
function）</strong>的区别。详见《<strong>核函数</strong>和<strong>核密度函数</strong>的区别.md》,特别是高斯核（RBF核）<span
class="math inline">\(k(x,x&#39;)=\exp(-\|x-x&#39;\|^2/2\sigma^2)\)</span>
和 高斯核密度函数 <span
class="math inline">\(K(u)=\frac{1}{\sqrt{2\pi}}e^{-u^2/2}\)</span></p>
<p><strong>例子</strong>：</p>
<ul>
<li>线性核 <span class="math inline">\(k(x,x&#39;)=x^\top
x&#39;\)</span></li>
<li>多项式核 <span class="math inline">\(k(x,x&#39;)=(x^\top
x&#39;+c)^d\)</span></li>
<li>高斯核（RBF核）<span
class="math inline">\(k(x,x&#39;)=\exp(-\|x-x&#39;\|^2/2\sigma^2)\)</span></li>
</ul>
<p>注意：这里的“核”只是指“内积核（Mercer核）”，与概率密度没有关系。</p>
</blockquote>
<p>矩阵 <span class="math inline">\(K \in \mathbb{R}^{N \times
N}\)</span>，由内积或将 <span
class="math inline">\(k(\cdot,\cdot)\)</span>
应用于一个数据集而得到，被称为<strong>Gram
矩阵</strong>，通常也直接称为<strong>核矩阵</strong>。核函数必须是对称且半正定的函数，这样每一个核矩阵
<span class="math inline">\(K\)</span> 都是对称且半正定的（见 3.2.3
节）：</p>
<p><span class="math display">\[
\forall z\in\mathbb{R}^N: \quad z^\top K z \ge 0. \tag{12.53}
\]</span></p>
<p>对于多元实值数据 <span
class="math inline">\(x_i\in\mathbb{R}^D\)</span>，一些常见的核函数包括多项式核、Gaussian
径向基函数核（RBF 核），以及有理二次核（Schölkopf 和 Smola,
2002；Rasmussen 和 Williams, 2006）。图 12.10
展示了不同核函数对分离超平面的影响。<strong>请注意，我们仍然在求解超平面，也就是说，假设空间里的函数依然是线性的；那些非线性分割面是由核函数造成的。</strong>（个人注：正如本节开头说的模块化！）</p>
<p><img src="/img3/机器学习的数学基础Part2/F12.10.png" alt="F12.10" style="zoom:50%;" /></p>
<p><strong>注：</strong>
<strong>对于刚接触机器学习的人来说，不幸的是，“核”这个词有多种含义。在本章中，“核”一词来自再生核希尔伯特空间（RKHS）的概念（Aronszajn,
1950；Saitoh, 1988）。我们已经在 2.7.3
节线性代数中讨论过“kernel”的另一种含义，即核空间或零空间（null
space）。在机器学习中，“kernel”还有第三种常见的用法，即核密度估计（11.5
节）中的平滑核（smoothing kernel）。</strong></p>
<p>由于<strong>显式表示 <span class="math inline">\(\phi(x)\)</span>
在数学上与核表示 <span class="math inline">\(k(x_i,x_j)\)</span>
等价</strong>，实践中往往会设计核函数使其比显式特征映射之间的内积计算更高效。例如，多项式核（Schölkopf
和 Smola,
2002），当输入维度很大时，即便是低阶多项式，其显式展开的项数也会迅速增长；而<strong>核函数</strong>每个输入维度只需要一次乘法，从而显著<strong>节省计算量</strong>。另一个例子是
Gaussian 径向基核函数（Schölkopf 和 Smola, 2002；Rasmussen 和 Williams,
2006），其对应的特征空间是无限维的。在这种情况下，我们无法显式表示特征空间，但仍然可以通过核函数计算任意一对样本之间的相似度。</p>
<p>核技巧的另一个有用之处在于：原始数据不必已经表示为多元实值数据。注意，内积是定义在函数
<span class="math inline">\(\phi(\cdot)\)</span>
的输出上的，但并不限制输入必须是实数。因此，函数 <span
class="math inline">\(\phi(\cdot)\)</span> 和核函数 <span
class="math inline">\(k(\cdot,\cdot)\)</span>
可以定义在任意对象上，例如集合、序列、字符串、图、分布等（Ben-Hur 等,
2008；Gärtner, 2008；Shi 等, 2009；Sriperumbudur 等, 2010；Vishwanathan
等, 2010）。</p>
<h3 id="数值求解">12.5 数值求解</h3>
<p>我们通过考察如何将本章推导出的 SVM 问题用第 7
章介绍的概念来表达，来结束对 SVM 的讨论。我们考虑两种不同的方法来求解
SVM 的最优解。首先，我们考虑 SVM 的损失函数视角（8.2.2
节），并把它表达为一个无约束优化问题。然后，我们把 SVM
的原始形式和对偶形式的约束版本都表示为标准形式（7.3.2
节）的二次规划问题。</p>
<p>考虑 SVM 的损失函数视角（公式
12.31）。这是一个凸的无约束优化问题，但铰链损失（公式
12.28）不可微。因此，我们采用次梯度方法来求解它。不过，铰链损失几乎在所有地方都是可微的，除了铰链处
<span class="math inline">\(t=1\)</span>
的单一点。在该点处，梯度是一组可能值，介于 0 和 -1
之间。因此，铰链损失的次梯度 <span class="math inline">\(g\)</span>
表示为：</p>
<p><span class="math display">\[
g(t)=
\begin{cases}
-1, &amp; t&lt;1\\[4pt]
[-1,0], &amp; t=1\\[4pt]
0, &amp; t&gt;1
\end{cases}
\tag{12.54}
\]</span></p>
<p>利用这个次梯度，我们可以应用第 7.1 节介绍的优化方法。</p>
<p>SVM
的原始形式和对偶形式都可以归结为一个凸二次规划问题（带约束的优化问题）。注意，公式
(12.26a) 中的原始 SVM 的优化变量大小为输入样本的维度 <span
class="math inline">\(D\)</span>；而公式 (12.41) 中的对偶 SVM
的优化变量大小为样本数 <span class="math inline">\(N\)</span>。</p>
<p>为了将原始 SVM（primal
SVM）写成标准二次规划（7.45）的形式，我们假设内积使用的是点积
(3.5)。我们重新排列原始 SVM (12.26a)
的公式，使得所有的优化变量都位于右边，并且约束的不等式形式与标准形式匹配。这样得到如下的优化问题：</p>
<p><span class="math display">\[
\begin{aligned}
\min_{w,b,\xi} \quad &amp;
\frac{1}{2}\|w\|^2 + C\sum_{n=1}^{N}\xi_n \\
\text{s.t.}\quad
&amp; -y_n x_n^\top w - y_n b - \xi_n \le -1, \\
&amp; -\xi_n \le 0, \quad n=1,\dots,N.
\end{aligned}
\tag{12.55}
\]</span></p>
<p>通过把变量 <span class="math inline">\(w,b,\xi\)</span>
合并成一个单一的向量，并小心地收集各个项，我们得到软间隔 SVM
的如下矩阵形式：</p>
<p><span class="math display">\[
\begin{align}
\min_{w,b,\xi} \quad &amp;
\begin{bmatrix}
b\\[3pt]
w\\[3pt]
\xi
\end{bmatrix}^{\!\top}
\begin{bmatrix}
I_D &amp; 0_{D,N+1}\\[3pt]
0_{N+1,D} &amp; 0_{N+1,N+1}
\end{bmatrix}
\begin{bmatrix}
b\\[3pt]
w\\[3pt]
\xi
\end{bmatrix} \\[5pt]
\text{s.t.}\quad &amp;
\begin{bmatrix}
-Y X - y - I_N &amp; 0_{N,D+1} - I_N
\end{bmatrix}
\begin{bmatrix}
b\\[3pt]
w\\[3pt]
\xi
\end{bmatrix}
+
\begin{bmatrix}
0_{D+1,1}\\[3pt]
C\,1_{N,1}
\end{bmatrix}
\le
\begin{bmatrix}
-1_{N,1}\\[3pt]
0_{N,1}
\end{bmatrix}.
\tag{12.56}
\end{align}
\]</span></p>
<p>在上述优化问题中，最小化是针对参数</p>
<p><span class="math display">\[
[w^\top,b,\xi^\top]^\top \in \mathbb{R}^{D+1+N},
\]</span></p>
<p>并且我们用以下记号：</p>
<ul>
<li><span class="math inline">\(I_m\)</span> 表示大小为 <span
class="math inline">\(m\times m\)</span> 的单位矩阵，</li>
<li><span class="math inline">\(0_{m,n}\)</span> 表示大小为 <span
class="math inline">\(m\times n\)</span> 的全零矩阵，</li>
<li><span class="math inline">\(1_{m,n}\)</span> 表示大小为 <span
class="math inline">\(m\times n\)</span> 的全 1 矩阵。</li>
</ul>
<p>此外，<span class="math inline">\(y\)</span> 是标签向量 <span
class="math inline">\([y_1,\dots,y_N]^\top\)</span>，<span
class="math inline">\(Y=\mathrm{diag}(y)\)</span> 是一个 <span
class="math inline">\(N\times N\)</span> 的对角矩阵，其对角元素来自
<span class="math inline">\(y\)</span>，而 <span
class="math inline">\(X\in \mathbb{R}^{N\times D}\)</span>
是将所有样本拼接得到的矩阵。</p>
<p>我们同样可以对 SVM (12.41) 的对偶形式进行项的收集。为了将对偶 SVM
写成标准形式，我们首先必须表示核矩阵 <span
class="math inline">\(K\)</span>，其中每个元素为 <span
class="math inline">\(K_{ij}=k(x_i,x_j)\)</span>。如果我们有显式的特征表示
<span class="math inline">\(x_i\)</span>，那么我们定义<span
class="math inline">\(K_{ij}=\langle
x_i,x_j\rangle\)</span>。为了方便记号，我们引入一个矩阵 <span
class="math inline">\(Y=\mathrm{diag}(y)\)</span>，它在对角线上存储标签，其余位置全为
0。对偶 SVM 可以写为： <span class="math display">\[
\begin{align}
\min_{\alpha}\quad &amp;
\frac{1}{2}\,\alpha^\top YKY\alpha - \mathbf{1}_{N,1}^\top \alpha \\
\text{s.t.}\quad &amp;
\begin{bmatrix}
y^\top\\[3pt]
- y^\top\\[3pt]
- I_N\\[3pt]
I_N
\end{bmatrix}
\alpha \;\le\;
\begin{bmatrix}
0_{N+2,1}\\[3pt]
C\mathbf{1}_{N,1}
\end{bmatrix}.
\tag{12.57}
\end{align}
\]</span></p>
<p><strong>备注：</strong> 在 7.3.1 和 7.3.2
节，我们介绍了标准形式的约束均写成不等式约束。我们将对偶 SVM
的等式约束表示为两个不等式约束，即 <span class="math display">\[
Ax=b\quad \text{被替换为}\quad
Ax\le b \quad \text{和}\quad Ax\ge b。 \tag{12.58}
\]</span></p>
<p>某些凸优化算法的软件实现可能直接支持等式约束。</p>
<p>由于对 SVM
存在多种不同的视角，因此求解相应优化问题的方法也有很多。这里介绍的这种将
SVM 问题写成标准凸优化形式的方法在实践中并不常用。SVM
求解器的两个主要实现分别是 Chang 和 Lin (2011)（开源）以及 Joachims
(1999)。由于 SVM
具有清晰且定义良好的优化问题，因此可以应用许多基于数值优化技术（Nocedal
和 Wright, 2006）的方法（Shawe-Taylor 和 Sun, 2011）。</p>
<h3 id="延伸阅读">12.6 延伸阅读</h3>
<p>支持向量机（SVM）只是研究二分类问题的众多方法之一。其他方法包括感知机、逻辑回归、Fisher
判别、最近邻、朴素贝叶斯以及随机森林（Bishop，2006；Murphy，2012）。Ben-Hur
等（2008）给出了一个关于离散序列上 SVM 与核方法的简短教程。</p>
<p>SVM 的发展与第 8.2
节讨论的<strong>经验风险最小化</strong>密切相关，因此 SVM
拥有坚实的理论性质（Vapnik，2000；Steinwart 和
Christmann，2008）。关于核方法的书（Schölkopf 和
Smola，2002）包含了大量有关支持向量机及其优化的细节。Shawe-Taylor 和
Cristianini（2004）撰写的一本更广泛的核方法书籍，也包含了许多针对不同机器学习问题的线性代数方法。</p>
<p>SVM 的对偶形式可以用 <strong>Legendre–Fenchel 变换</strong>（第 7.3.3
节）的思想推导出来。该推导将 SVM 的无约束形式 (12.31)
中的每一项分别考虑，并计算其凸共轭（Rifkin 和 Lippert，2007）。对 SVM
感兴趣的读者（尤其是函数分析视角或正则化方法视角）可参考
Wahba（1990）的工作。核方法的理论阐述（Aronszajn，1950；Schwartz，1964；Saitoh，1988；Manton
和 Amblard，2015）需要线性算子（Akhiezer 和
Glazman，1993）的基本知识作为基础。核方法的思想已推广到巴拿赫空间（Zhang
等，2009）和克雷因空间（Ong 等，2004；Loosli 等，2016）。</p>
<p>需要注意的是，hinge 损失（铰链损失）有三种等价表示，如 (12.28) 和
(12.29)，以及 (12.33) 中的约束优化问题。公式 (12.28) 常用于比较 SVM
损失函数与其他损失函数（Steinwart，2007）。两段式表示 (12.29)
便于计算次梯度，因为每一段都是线性的。第三种表示 (12.33)，如 12.5
节所示，使得可以使用凸二次规划（第 7.3.2 节）工具。</p>
<p>由于二分类在机器学习中是一个研究得非常充分的任务，人们有时也会使用其他词语来描述它，例如“判别”“分离”和“决策”。此外，二分类器的输出可以有三种不同的形式：</p>
<ul>
<li><p><strong>第一种</strong>是线性函数本身的输出（通常称为“分数”score），它可以取任意实数值。这个输出可以用来对样本进行排序，而二分类可以被看作是在排好序的样本上选择一个阈值（Shawe-Taylor
和 Cristianini，2004）。</p></li>
<li><p><strong>第二种</strong>常被视为二分类器输出的，是将线性输出经过一个非线性函数后得到的结果，用来将取值限制在一个有界范围内，例如区间
<span class="math inline">\([0,1]\)</span>。常见的非线性函数是 sigmoid
函数（Bishop，2006）。当这种非线性转换得到的是经过良好校准的概率（Gneiting
和 Raftery，2007；Reid 和
Williamson，2011）时，这称为<strong>类别概率估计</strong>（class
probability estimation）。</p></li>
<li><p><strong>第三种</strong>输出是最终的二元决策 <span
class="math inline">\(\{+1,
-1\}\)</span>，这也是最常被假设为分类器输出的形式。</p></li>
</ul>
<p>支持向量机（SVM）是一种二分类器，它本身并不自然地对应于概率解释。<strong>有多种方法可以将线性函数（分数）的原始输出转化为一个经过良好校准的类别概率估计
<span
class="math inline">\(P(Y=1|X=x)\)</span>，这需要一个额外的校准步骤（Platt，2000；Zadrozny
和 Elkan，2001；Lin 等，2007）。</strong></p>
<p>从训练的角度看，有许多与概率相关的方法。我们在 12.2.5
节的末尾提到过损失函数与似然之间的关系（也可比较 8.2 节和 8.3
节）。在训练过程中对应于良好<strong>校准的转换的最大似然方法称为逻辑回归</strong>，它来自一类称为<strong>广义线性模型</strong>（generalized
linear models）的方法。从这一视角看逻辑回归的细节可参见
Agresti（2002，第 5 章）以及 McCullagh 和 Nelder（1989，第 4 章）。</p>
<p>当然，人们也可以用更<strong>贝叶斯的视角</strong>来看待分类器输出，即通过贝叶斯逻辑回归来估计后验分布。贝叶斯视角还包括<strong>先验的设定</strong>，这其中包含了与似然的共轭性（第
6.6.1
节）等设计选择。此外，人们还可以将潜在函数视为先验，这样就得到<strong>高斯过程分类</strong>（Rasmussen
和 Williams，2006，第 3 章）。</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%95%B0%E5%AD%A6/" rel="tag"># 数学</a>
              <a href="/tags/%E7%AE%97%E6%B3%95/" rel="tag"># 算法</a>
              <a href="/tags/%E7%BB%9F%E8%AE%A1/" rel="tag"># 统计</a>
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2025/09/09/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E7%AC%AC9%E7%AB%A0%22%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%22/" rel="prev" title="《机器学习的数学基础》第9章"线性回归"">
                  <i class="fa fa-chevron-left"></i> 《机器学习的数学基础》第9章"线性回归"
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2025/09/09/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E7%AC%AC8%E7%AB%A0%22%E5%BD%93%E6%A8%A1%E5%9E%8B%E9%81%87%E5%88%B0%E6%95%B0%E6%8D%AE%22/" rel="next" title="《机器学习的数学基础》第8章"当模型遇到数据"">
                  《机器学习的数学基础》第8章"当模型遇到数据" <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Rayman.hung</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
<script src="https://cdn.jsdelivr.net/npm/hexo-generator-searchdb@1.4.0/dist/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js","integrity":"sha256-r+3itOMtGGjap0x+10hu6jW/gZCzxHsoKrOd7gyRSGY="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>

<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.1.1">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" integrity="sha256-DfWjNxDkM94fVBWx1H5BMMp0Zq7luBlV8QRcSES7s+0=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"hongyitong.github.io","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.11.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="第八章 模型推断与平均化 本书的大部分章节中，对于回归而言，模型的拟合（学习）通过最小化平方和实现；或对于分类而言，通过最小化交叉熵实现．事实上，这两种最小化都是用极大似然来拟合的实例． 这章中，我们给出极大似然法的一个一般性的描述，以及用于推断的贝叶斯方法．在第7章中讨论的自助法在本章中也继续讨论，而且描述了它与极大似然和贝叶斯之间的联系．最后，我们提出模型平均和改善的相关技巧，包括 commi">
<meta property="og:type" content="article">
<meta property="og:title" content="《统计学习基础》 (4&#x2F;n)">
<meta property="og:url" content="http://hongyitong.github.io/2025/01/27/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/index.html">
<meta property="og:site_name" content="墨语浮生">
<meta property="og:description" content="第八章 模型推断与平均化 本书的大部分章节中，对于回归而言，模型的拟合（学习）通过最小化平方和实现；或对于分类而言，通过最小化交叉熵实现．事实上，这两种最小化都是用极大似然来拟合的实例． 这章中，我们给出极大似然法的一个一般性的描述，以及用于推断的贝叶斯方法．在第7章中讨论的自助法在本章中也继续讨论，而且描述了它与极大似然和贝叶斯之间的联系．最后，我们提出模型平均和改善的相关技巧，包括 commi">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-01-26T16:00:00.000Z">
<meta property="article:modified_time" content="2025-12-15T02:46:42.630Z">
<meta property="article:author" content="Rayman.hung">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://hongyitong.github.io/2025/01/27/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://hongyitong.github.io/2025/01/27/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/","path":"2025/01/27/《统计学习基础》读书心得/","title":"《统计学习基础》 (4/n)"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>《统计学习基础》 (4/n) | 墨语浮生</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-ZXGEJDXQ33"></script>
  <script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"G-ZXGEJDXQ33","only_pageview":false}</script>
  <script src="/js/third-party/analytics/google-analytics.js"></script>





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">墨语浮生</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Rayman</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section">首页</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section">分类</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section">关于</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section">归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E5%85%AB%E7%AB%A0-%E6%A8%A1%E5%9E%8B%E6%8E%A8%E6%96%AD%E4%B8%8E%E5%B9%B3%E5%9D%87%E5%8C%96"><span class="nav-text">第八章 模型推断与平均化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%87%AA%E5%8A%A9%E6%B3%95%E5%92%8C%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E6%B3%95"><span class="nav-text">自助法和最大似然法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%96%B9%E6%B3%95"><span class="nav-text">贝叶斯方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#em%E7%AE%97%E6%B3%95"><span class="nav-text">EM算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#bagging"><span class="nav-text">Bagging</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E4%B9%9D%E7%AB%A0-%E5%8A%A0%E6%80%A7%E6%A8%A1%E5%9E%8B%E6%A0%91%E6%A8%A1%E5%9E%8B%E5%8F%8A%E7%9B%B8%E5%85%B3%E6%96%B9%E6%B3%95"><span class="nav-text">第九章
加性模型、树模型及相关方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B9%BF%E4%B9%89%E5%8F%AF%E5%8A%A0%E6%A8%A1%E5%9E%8B"><span class="nav-text">广义可加模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%91%E6%A8%A1%E5%9E%8B"><span class="nav-text">树模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#roc"><span class="nav-text">ROC</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%93%E5%AE%B6%E7%9A%84%E5%88%86%E5%B1%82%E6%B7%B7%E5%90%88-hme"><span class="nav-text">专家的分层混合 (HME)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E5%8D%81%E7%AB%A0-%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95%E4%B8%8E%E5%8A%A0%E6%80%A7%E6%A0%91%E6%A8%A1%E5%9E%8B"><span class="nav-text">第十章
提升方法与加性树模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E7%9A%84%E7%8E%B0%E8%B4%A7%E6%96%B9%E6%B3%95"><span class="nav-text">数据挖掘的现货方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%A7%E5%B0%8F%E5%90%88%E9%80%82%E7%9A%84boosting%E6%A0%91"><span class="nav-text">大小合适的boosting树</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E5%8D%81%E4%B8%80%E7%AB%A0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-text">第十一章 神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8A%95%E5%BD%B1%E5%AF%BB%E8%B8%AA%E5%9B%9E%E5%BD%92"><span class="nav-text">投影寻踪回归</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8B%9F%E5%90%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-text">拟合神经网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98"><span class="nav-text">训练神经网络的一些问题</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E5%8D%81%E4%BA%8C%E7%AB%A0-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E4%B8%8E%E7%81%B5%E6%B4%BB%E5%88%A4%E5%88%AB%E6%96%B9%E6%B3%95"><span class="nav-text">第十二章
支持向量机与灵活判别方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E5%8D%81%E4%B8%89%E7%AB%A0-%E5%8E%9F%E5%9E%8B%E6%96%B9%E6%B3%95%E4%B8%8E%E6%9C%80%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95"><span class="nav-text">第十三章
原型方法与最近邻算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E5%8D%81%E5%9B%9B%E7%AB%A0-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="nav-text">第十四章 无监督学习</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E5%8D%81%E4%BA%94%E7%AB%A0-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97"><span class="nav-text">第十五章 随机森林</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E5%8D%81%E5%85%AD%E7%AB%A0-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0"><span class="nav-text">第十六章 集成学习</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E5%8D%81%E4%B8%83%E7%AB%A0-%E6%97%A0%E5%90%91%E5%9B%BE%E6%A8%A1%E5%9E%8B"><span class="nav-text">第十七章 无向图模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BF%9E%E7%BB%AD%E5%8F%98%E9%87%8F%E7%9A%84%E6%97%A0%E5%90%91%E5%9B%BE%E6%A8%A1%E5%9E%8B"><span class="nav-text">连续变量的无向图模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%BE%E7%BB%93%E6%9E%84%E7%9A%84%E4%BC%B0%E8%AE%A1"><span class="nav-text">图结构的估计</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%99%90%E5%88%B6%E7%8E%BB%E5%B0%94%E5%85%B9%E6%9B%BC%E6%9C%BA"><span class="nav-text">限制玻尔兹曼机</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E5%8D%81%E5%85%AB%E7%AB%A0-%E9%AB%98%E7%BB%B4%E9%97%AE%E9%A2%98"><span class="nav-text">第十八章 高维问题</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Rayman.hung</p>
  <div class="site-description" itemprop="description">技术分享、读书心得、心情记录</div>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://hongyitong.github.io/2025/01/27/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Rayman.hung">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="墨语浮生">
      <meta itemprop="description" content="技术分享、读书心得、心情记录">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="《统计学习基础》 (4/n) | 墨语浮生">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          《统计学习基础》 (4/n)
        </h1>

        </h1>
          
             <p class="post-subtitle">读书笔记之四：基函数扩展与正则化</p>
          
        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-01-27 00:00:00" itemprop="dateCreated datePublished" datetime="2025-01-27T00:00:00+08:00">2025-01-27</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97%EF%BC%88%E8%87%AA%E7%84%B6%E7%A7%91%E5%AD%A6%EF%BC%89/" itemprop="url" rel="index"><span itemprop="name">读书心得（自然科学）</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h2 id="第八章-模型推断与平均化">第八章 模型推断与平均化</h2>
<p>本书的大部分章节中，对于回归而言，模型的拟合（学习）通过最小化平方和实现；或对于分类而言，通过最小化交叉熵实现．事实上，这两种最小化都是用极大似然来拟合的实例．</p>
<p>这章中，我们给出极大似然法的一个一般性的描述，以及用于推断的贝叶斯方法．在第7章中讨论的自助法在本章中也继续讨论，而且描述了它与极大似然和贝叶斯之间的联系．最后，我们提出模型平均和改善的相关技巧，包括
committee 方法、bagging、stacking 和 bumping．</p>
<h3 id="自助法和最大似然法">自助法和最大似然法</h3>
<ul>
<li>自助法，通过从训练集中有放回地采样，称作<strong>非参</strong>自助法(nonparametric
bootstrap),这实际上意味着这个方法是与模型无关的，因为它使用原始数据来得到新的数据集，而不是一个特定的含参数的模型．</li>
<li><strong>本质上自助法是非参最大似然或者参数最大似然法的计算机实现</strong>．与最大似然法相比自助法的好处是允许我们在没有公式的情况下计算标准误差和其他一些量的最大似然估计．
（详见原书的图8.2）</li>
</ul>
<h3 id="贝叶斯方法">贝叶斯方法</h3>
<ul>
<li>贝叶斯方法与一般推断方法的不同之处在于，用先验分布来表达知道数据之前的这种不确定性，而且在知道数据之后允许不确定性继续存在，将它表示成后验分布．</li>
<li>最大似然方法会使用在最大概率估计那个点的密度来预测未来的数据．不同于贝叶斯方法的预测分布，它不能说明估计
<span class="math inline">\(\theta\)</span> 的不确定性</li>
</ul>
<blockquote>
<p>个人注：推断中最大似然是预测某个最大的<span
class="math inline">\(\theta\)</span>值，贝叶斯方法是预测<span
class="math inline">\(\theta\)</span>的分布？</p>
</blockquote>
<ul>
<li>自助法分布表示我们参数的（近似的）非参、无信息后验分布．但是自助法分布可以很方便地得到——不需要正式地确定一个先验而且不需要从后验分布中取样．因此我们或许可以把自助法分布看成一个“穷人的”贝叶斯后验．通过扰动数据，自助法近似于扰动参数后的贝叶斯效应，而且一般实施起来更简单．</li>
</ul>
<h3 id="em算法">EM算法</h3>
<p>EM 算法是简化复杂极大似然问题的一种很受欢迎的工具；</p>
<blockquote>
<p>个人注：入门的视频见B站 博主“风中摇曳的小萝卜”的视频“EM算法
你到底是哪个班级的”</p>
</blockquote>
<h3 id="bagging">Bagging</h3>
<p>简单来说，bagging 和随机森林都是针对 bootstrap
样本，且前者可以看成后者的特殊形式；而 boosting 是针对残差样本．</p>
<h2 id="第九章-加性模型树模型及相关方法">第九章
加性模型、树模型及相关方法</h2>
<p>这章中我们开始对监督学习中一些特定的方法进行讨论．这里每个技巧都<strong>假设了未知回归函数（不同的）结构形式</strong>，而且通过这样处理巧妙地解决了维数灾难．<strong>当然，它们要为错误地确定模型类型付出可能的代价</strong>，所以在每种情形下都需要做出一个权衡．第
3-6 章留下的问题都将继续讨论．我们描述 5
个相关的技巧：<strong>广义可加模型 (generalized additive
models)</strong>，<strong>树
(trees)</strong>，<strong>多元自适应回归样条
(MARS)</strong>，<strong>耐心规则归纳法 (PRIM)</strong>，以及
<strong>混合层次专家 (HME)</strong>．</p>
<h3 id="广义可加模型">广义可加模型</h3>
<p>在回归的设定中，广义可加模型有如下形式</p>
<p><span class="math display">\[
\text E(Y\mid X_1,X_2,\ldots,X_p) =
\alpha+f_1(X_1)+f_2(X_2)+\cdots+f_p(X_p).\tag{9.1}
\]</span></p>
<h3 id="树模型">树模型</h3>
<ul>
<li>为什么二值分割？
与其在每一步对每个结点只分割成两个群体（如上面讨论的），我们或许可以考虑多重分割成多于两个群体．尽管这个在某些情况下是有用的，但不是一个好的一般策略．问题在于多重分割将数据分得太快，以至于在下一层次没有充分多的数据．因此我们仅仅当需要的时候采用这种分割．因为多重分割可以通过一系列的二值分割实现，所以后者更好一点．</li>
<li>树的不稳定性
树的一个主要问题是它们的高方差性．经常在数据中的一个小改动导致完全不同的分割点序列，使得解释不稳定．这种不稳定的主要原因是这个过程的层次性：上一个分割点的误差会传递到下面所有的分割点上．可以试图采取更加稳定的分离准则在某种程度上减轻这一影响，但是固有的不稳定性没有移除．这是从数据中估计一个简单的、基于树结构的代价．Bagging（<a
href="/08-Model-Inference-and-Averaging/8.7-Bagging/index.html">8.7
节</a>）对很多树进行平均来降低方差．</li>
<li>缺乏光滑性 树的另一个限制是预测表面缺乏光滑性，如在图 9.2
中的右下图中那样．在 0/1
损失的分类问题中，这不会有太大的损伤，因为类别概率估计的偏差的影响有限．然而，在回归问题中这会降低效果，正常情况下我们期望潜在的函数是光滑的．<a
href="9.4-MARS/index.html">9.4 节</a>介绍的 MARS 过程可以看出是为了减轻
CART 缺乏光滑性而做的改动．</li>
</ul>
<h3 id="roc">ROC</h3>
<p>在医学分类问题中，<strong>敏感度 (sensitivity)</strong> 和
<strong>特异度 (specificity)</strong>
经常用来衡量一个准则．它们按如下定义：</p>
<ul>
<li>敏感度：给定真实状态为患病预测为患病的概率</li>
<li>特异度：给定真实状态为未患病预测为未患病的概率</li>
</ul>
<p><strong>受试者工作特征曲线 (receiver operating characteristic curve,
ROC)</strong>
是用于评估敏感度和特异度之间折中的常用概述．当我们改变分类规则的参数便会得到敏感度关于特异度的图像．
越靠近东北角落的曲线表示越好的分类器． ROC 曲线下的面积有时被称作
<strong><span class="math inline">\(c\)</span> 统计量
(c-statistics)</strong>．</p>
<h3 id="专家的分层混合-hme"><strong>专家的分层混合 (HME)</strong></h3>
<p>过程可以看成是基于树方法的变种．主要的差异是树的分割不是<strong>硬决定
(hard decision)</strong>，而是<strong>软概率的决定 (soft
probabilistic)</strong>．在每个结点观测往左或者往右的概率取决于输入值．因为最后的参数优化问题是光滑的，所以有一些计算的优势，不像在基于树的方式中的离散分割点的搜索．软分割或许也可以帮助预测准确性，并且提供另外一种有用的数据描述方式．</p>
<h2 id="第十章-提升方法与加性树模型">第十章
<strong>提升方法与加性树模型</strong></h2>
<p>Boosting
是最近20年内提出的最有力的学习方法．最初是为了分类问题而设计的，但是我们将在这章中看到，它也可以很好地扩展到回归问题上．Boosting的动机是集合许多弱学习的结果来得到有用的“committee”．</p>
<p>弱分类器是误差率仅仅比随机猜测要好一点的分类器．Boosting
的目的是依次对反复修改的数据应用弱分类器算法，因此得到弱分类器序列 <span
class="math inline">\(G_m(x),m=1,2,\ldots,M\)</span>
根据它们得到的预测再通过一个加权来得到最终的预测</p>
<blockquote>
<p><strong>事实上，Breiman(NIPS Workshop，1996) 将树的 AdaBoost
称为“世界上最好的现成分类器”(best off-the-shelf classifier in the
world)．</strong> <strong>有人认为决策树是 boosting
是数据挖掘应用中理想的基学习器．</strong></p>
</blockquote>
<h3 id="数据挖掘的现货方法">数据挖掘的现货方法</h3>
<blockquote>
<p>个人注：<strong>“现货”(off-the-shelf)</strong>
方法．现货方法指的是可以直接应用到数据中而不需要大量时间进行数据预处理或者学习过程的精心调参．</p>
</blockquote>
<p><strong>预测学习 (predictive learning)</strong>
是数据挖掘中很重要的一部分．正如在这本书中看到的一样，已经提出了大量的方法对数据进行学习，然后预测．对于每个特定的方法，有些情形特别适用，但在其他情形表现得很差．我们已经试图在每个方法的讨论中明确合适的应用情形．然而，对于给定的问题，我们不会事先知道哪种方法表现得最好．表
10.1 总结了一些学习方法的特点．</p>
<p>工业和商业数据挖掘应用在学习过程的要求往往特别具有挑战性．数据集中观测值的个数以及每个观测值上衡量的变量个数往往都非常大．因此，需要注意计算的复杂度．并且，数据经常是混乱的
(messy)：输入往往是定量，二值以及类别型变量的混合，而且类别型变量往往有很多层次．一般还会有许多缺失值，完整的观测值是很稀少的．预测变量和响应变量的分布经常是
<strong>长尾 (long-tailed)</strong> 并且 <strong>高偏的 (highly
skewed)</strong>．垃圾邮件的数据就是这种情形(<a
href="../09-Additive-Models-Trees-and-Related-Methods/9.1-Generalized-Additive-Models/index.html">9.1.2
节</a>)；当拟合一个广义可加模型，我们首先对每个预测变量进行对数变换以期得到合理的拟合．另外，它们通常会包含很大一部分的严重的误测量值（离群值）．预测变量通常在差异很大的尺度下进行测量．</p>
<p>在数据挖掘应用中，通常只有大量预测变量中的一小部分真正与预测值相关的变量才被包含在分析中．另外，不同于很多应用的是，比如模式识别，很少有可信的专业知识来创建相关的特征，或者过滤掉不相关的，
这些不相关的特征显著降低了很多方法的效果．</p>
<p>另外，数据挖掘一般需要可解释性的模型．简单地得到预测值是不够的．提供定性
(qualitative)
理解输入变量和预测的响应变量之间的关系的信息是迫切的．因此，<strong>黑箱方法(black
box)</strong>，比如神经网络，在单纯的预测情形，比如模式识别中是很有用的，但在数据挖掘中不是很有用．</p>
<p>这些计算速度、可解释性的要求以及数据的混乱本性严重限制了许多学习过程作为数据挖掘的
<strong>“现货”(off-the-shelf)</strong>
方法．现货方法指的是可以直接应用到数据中而不需要大量时间进行数据预处理或者学习过程的精心调参．</p>
<p><strong>在所有的有名的学习方法中，决策树最能达到数据挖掘的现货方法的要求．它们相对很快地构造出模型并且得到可解释的模型（如果树很小）</strong>．如
<a
href="../09-Additive-Models-Trees-and-Related-Methods/9.2-Tree-Based-Methods/index.html">9.2
节</a>
中讨论的，它们自然地包含数值型和类别型预测变量以及缺失值的混合．它们在对单个预测变量的（严格单调）的变换中保持不变．结果是，尺寸变换和（或）更一般的变换不是问题，并且它们不受预测变量中的离群值的影响．它们将中间的特征选择作为算法过程的一部分．从而它们抑制（如果不是完全不受影响）包含许多不相关预测变量．
决策树的这些性质在很大程度上是它们成为数据挖掘中最受欢迎的学习方法的原因．</p>
<p><strong>树的不准确性导致其无法作为预测学习的最理想的工具．它们很少达到那个将数据训练得最好的方法的准确性．正如在
<a
href="../10-Boosting-and-Additive-Trees/10.1-Boosting-Methods/index.html">10.1
节</a>看到的，boosting
决策树提高了它们的准确性，经常是显著提高．同时它保留着数据挖掘中所需要的性质</strong>．一些树的优势被
boosting 牺牲的是计算速度、可解释性，以及对于 AdaBoost
而言，对重叠类的鲁棒性，特别是训练数据的误分类．<strong>gradient boosted
model(GBM)</strong>是 tree boosting
的一般化，它试图减轻这些问题，以便为数据挖掘提供准确且有效的现货方法．</p>
<h3 id="大小合适的boosting树">大小合适的boosting树</h3>
<p>曾经，boosting 被认为是一种将模型结合起来(combing
models)的技巧，在这里模型是树．同样地，生成树的算法可看成是产生用于
boosting 进行结合的模型的
<strong>原型(primitive)</strong>．这种情形下，在生成树的时候以通常的方式分别估计每棵树的最优大小（<a
href="9.2-Tree-Based-Methods/index.html">9.2
节</a>）．首先诱导出非常大（过大的）的一棵树，接着应用自下而上的过程剪枝得到估计的最优终止结点个数的树．这种方式隐含地假设了每棵树是式
公式（10.28） 中的最后一棵．</p>
<ul>
<li>解释性</li>
</ul>
<p>​
单个决策树有着很高的解释性．整个模型可以用简单的二维图象（二叉树）完整地表示，其中二叉树也很容易可视化．树的线性组合
公式（10.28） 丢失了这条重要的特性，所以必须考虑用不同的方式来解释．</p>
<ul>
<li>预测变量的相对重要性</li>
</ul>
<p>​
在数据挖掘应用中，输入的预测变量与响应变量的相关程度很少是相等的．通常只有一小部分会对响应变量有显著的影响，而绝大部分的变量是不相关的，并且可以简单地不用包含进模型．研究每个输入变量在预测响应变量时的相关重要度或者贡献是很有用的．</p>
<h2 id="第十一章-神经网络">第十一章 <strong>神经网络</strong></h2>
<p>这章中我们描述一类学习方法，它是基于在不同的领域（统计和人工智能）中独立发展起来但本质上相同的模型．中心思想是提取输入的线性组合作为<strong>导出特征
(derived
features)</strong>，然后将目标看成特征的非线性函数进行建模．这是一个很有效的学习方法，在许多领域都有广泛应用．我们首先讨论<strong>投影寻踪模型
(projection pursuit
model)</strong>，这是在半参统计和光滑化领域中发展出来的．本章的剩余部分集中讨论神经网络模型．</p>
<h3 id="投影寻踪回归">投影寻踪回归</h3>
<p>Projection Pursuit Regression</p>
<p>在我们一般监督学习问题中，假设我们有 <span
class="math inline">\(p\)</span> 个组分的输入向量 <span
class="math inline">\(X\)</span>，以及目标变量 <span
class="math inline">\(Y\)</span>．令 <span
class="math inline">\(\omega_m,m=1,2,\ldots, M\)</span> 为未知参数的
<span class="math inline">\(p\)</span> 维单位向量．<strong>投影寻踪回归
(PPR)</strong> 模型有如下形式: <span class="math display">\[
f(X)=\sum\limits_{m=1}^Mg_m(\omega_m^TX)\tag{11.1}\label{11.1}
\]</span> 这是一个可加模型，但是是关于导出特征 <span
class="math inline">\(V_m=\omega_m^TX\)</span>，而不是关于输入变量本身．函数
<span class="math inline">\(g_m\)</span>
未定，而是用一些灵活的光滑化方法来估计及 <span
class="math inline">\(\omega_m\)</span> 的方向（见下）．</p>
<p>函数 <span class="math inline">\(g_m(\omega_m^TX)\)</span> 称为 <span
class="math inline">\(\mathbb R^p\)</span> 中的<strong>岭函数 (ridge
function)</strong>．仅仅在由向量 <span
class="math inline">\(\omega_m\)</span> 定义的方向上变化．标量变量 <span
class="math inline">\(V_m=\omega_m^TX\)</span> 是 <span
class="math inline">\(X\)</span> 在单位向量 <span
class="math inline">\(\omega_m\)</span> 上的投影，寻找使得模型拟合好的
<span
class="math inline">\(\omega_m\)</span>，因此称为“投影寻踪”．<strong>图
11.1 显示了岭函数的一些例子</strong>。</p>
<blockquote>
<p>个人注：详见原书图11.1</p>
</blockquote>
<p>实际上，如果 <span class="math inline">\(M\)</span>
任意大，选择合适的 <span class="math inline">\(g_m\)</span>，PPR
模型可以很好地近似 <span class="math inline">\(\mathbb R^p\)</span>
中任意的连续函数．这样的模型类别称为 <strong>通用近似 (universal
approximator)</strong>．然而这种一般性需要付出代价．拟合模型的解释性通常很困难，因为每个输入变量都以复杂且多位面的方式进入模型中．结果使得
PPR 模型对于预测非常有用，但是对于产生一个可理解的模型不是很有用．<span
class="math inline">\(M=1\)</span> 模型是个例外，也是计量经济学中的
<strong>单指标模型 (single index
model)</strong>．这比线性回归模型更加一般，也提供了一个类似（线性回归模型）的解释．</p>
<p>然而，投影寻踪回归模型在统计领域并没有被广泛地使用，或许是因为在它的提出时间（1981），计算上的需求超出大多数已有计算机的能力．但是它确实代表着重要的智力进步，它是一个在神经网络领域的转世中发展起来的</p>
<h3 id="拟合神经网络">拟合神经网络</h3>
<p>神经网络模型中未知的参数，通常称为 <strong>权重
(weights)</strong>，我们需要寻找它们的值使得模型很好地拟合训练数据．我们将参数的全集记为
<span class="math inline">\(\theta\)</span></p>
<p>对于回归，我们采用误差平方和用于衡量拟合的效果（误差函数） <span
class="math display">\[
R(\theta)=\sum\limits_{k=1}^K\sum\limits_{i=1}^N(y_{ik}-f_k(x_i))^2\tag{11.9}
\]</span></p>
<p>对于分类，我们可以采用平方误差或者交叉熵（偏差）：</p>
<p><span class="math display">\[
R(\theta)=-\sum\limits_{i=1}^N\sum\limits_{k=1}^Ky_{ik}\mathrm{log}\;f_k(x_i)\tag{11.10}
\]</span></p>
<p>以及对应的分类器 <span class="math inline">\(G(x)=\mathrm{arg\;
max}_kf_k(x)\)</span>．有了 softmax
激活函数和交叉熵误差函数，神经网络模型实际上是关于隐藏层的线性逻辑斯蒂回归模型，而且所有的参数通过极大似然来估计．</p>
<p>一般地，我们不想要 <span class="math inline">\(R(\theta)\)</span>
的全局最小值，因为这可能会是一个过拟合解．而是需要一些正则化：这个可以通过惩罚项来直接实现，或者提前终止来间接实现．下一节中将给出详细的细节．</p>
<p>最小化 <span class="math inline">\(R(\theta)\)</span>
的一般方法是通过梯度下降，在这种情形下称作 <strong>向后传播
(back-propagation)</strong>．因为模型的组成成分，运用微分的链式法则可以很简单地得到梯度．这个可以通过对网络向前或向后遍历计算得到，仅跟踪每个单元的局部量．</p>
<p>向后传播的优点在于简单，局部自然．在向后传播算法中，每个隐藏层单元仅仅向（从）有其联系的单元传递（接收）信息．因此可以在并行架构的计算机上高效地实现．</p>
<p><strong>批量学习 (batch
learning)</strong>，参数更新为所有训练情形的和．学习也可以 online
地进行——每次处理一个观测，每个训练情形过后更新梯度，然后对训练情形重复多次．在这种情形下，公式（11.13）
式的和可以替换成单个被加数．一个 <strong>训练时期 (training
epoch)</strong> 指的是一次扫描整个训练集．<strong>在线训练 (online
training)</strong>
允许网络处理非常大的训练集，而且当新观测加入时更新权重．</p>
<h3 id="训练神经网络的一些问题">训练神经网络的一些问题</h3>
<p>训练神经网络真的是一门艺术．模型一般会过参量化，而且优化问题非凸而且不稳定，除非遵循某确定的方式．在这节我们总结一些重要的问题．</p>
<ul>
<li>初始值 注意如果权重接近 0，则 sigmoid（图
11.3）起作用的部分近似线性，因此神经网络退化成近似线性模型（<a
target="_blank" rel="noopener" href="https://github.com/szcf-weiya/ESL-CN/issues/177">练习
11.2</a>）．通常权重系数的初始值取为接近 0
的随机值．因此模型开始时近似线性，当系数增大时变成非线性．需要的时候局部化单个单元的方向并且引入非线性．恰巧为
0 的权重的使用导致 0
微分和完美的对称，而且算法将不会移动．而以较大的值开始经常带来不好的解．
<span class="math inline">\(\sigma(sv)\)</span> ，s是权重。</li>
<li>过拟合 通常神经网络有太多的权重而且在 <span
class="math inline">\(R\)</span>
的全局最小处过拟合数据．在神经网络的发展早期，无论是设计还是意外，采用提前终止的规则来避免过拟合．也就是训练一会儿模型，在达到全局最小前终止．因为权重以高正则化（线性）解开始，这有将最终模型收缩成线性模型的效果．验证集对于决定什么时候停止是很有用的，因为我们期望此时验证误差开始增长．
一个更明显的正则化方法是 <strong>权重衰减 (weight
decay)</strong>，类似用于线性模型的岭回归</li>
<li>输入的缩放
因为对输入的缩放决定了在底层中系数缩放的效率，所有它可以对最终解有很大的影响．最开始最好是对所有输入进行标准化使均值为
0，标准差为
1．这保证了在正则化过程中对所有输入公平对待，而且允许为随机的初始权重系数选择一个有意义的区间．有了标准化的输入，一般在
<span class="math inline">\([-0.7,+0.7]\)</span>
范围内均匀随机选择权重系数．</li>
<li>隐藏单元和层的个数
一般来说，太多的隐藏单元比太少的隐藏单元要好．太少的隐藏单元，模型或许没有足够的灵活性来捕捉数据的非线性；太多的隐藏单元，如果使用了合适的正则化，额外的权重系数可以收缩到
0．一般地，隐藏单元的数量处于 5 到 100
的范围之内，而且随着输入个数、训练情形的种数的增加而增加．最常见的是放入相当大数量的单元并且进行正则化训练．一些研究者采用交叉验证来估计最优的数量，但是当交叉验证用来估计正则化系数这似乎是不必要的．<strong>隐藏层的选择由背景知识和经验来指导</strong>．每一层提取输入的特征用于回归或者分类．多重隐藏层的使用允许在不同的分解层次上构造层次特征．</li>
<li>多重最小点 误差函数 <span class="math inline">\(R(\theta)\)</span>
为非凸，具有许多局部最小点．后果是最终得到的解取决于权重系数的初始值．至少需要尝试一系列随机的初始配置，然后选择给出最低（惩罚）误差的解．或许更好的方式是在对一系列网络的预测值进行平均作为最终的预测（Ripley，1996[^1]）．这比平均权重系数更好，因为模型的非线性表明平均的解会很差．另一种方式是通过
bagging，它是对不同网络的预测值进行平均，这些网络是对随机扰动版本的训练数据进行训练得到的．</li>
</ul>
<h2 id="第十二章-支持向量机与灵活判别方法">第十二章
<strong>支持向量机与灵活判别方法</strong></h2>
<p><strong>核函数</strong> 所以 公式（12.19） 和 公式（12.20）
仅仅通过内积涉及 <span
class="math inline">\(h(x)\)</span>．实际上，我们根本不需要明确变换关系
<span
class="math inline">\(h(x)\)</span>，而仅仅要求知道在转换后的空间中计算内积的核函数
<span class="math display">\[
K(x,x&#39;)=\langle h(x), h(x&#39;) \rangle\tag{12.21}
\]</span></p>
<p>在 SVM 中有三种流行的 <span class="math inline">\(K\)</span>
可以选择</p>
<p><span class="math display">\[
\begin{array}{rl}
d\text{ 阶多项式：} &amp; K(x,x&#39;)=(1+\langle x,x&#39; \rangle)^d\\
\text{径向基：} &amp; K(x, x&#39;)=\exp(-\gamma \Vert
x-x&#39;\Vert^2)\tag{12.22}\label{12.22}\\
\text{神经网络：} &amp; K(x,x&#39;)=\tanh(\kappa_1\langle x,x&#39;
\rangle+\kappa_2)\\
\end{array}
\]</span></p>
<p>考虑含有两个输入变量 <span class="math inline">\(X_1\)</span> 和
<span class="math inline">\(X_2\)</span> 的特征空间，以及 2
阶的多项式核．则</p>
<p><span class="math display">\[
\begin{array}{ll}
K(x,x&#39;)&amp;=(1+\langle X,X&#39; \rangle)^2\\
&amp;=(1+X_1X_1&#39;+X_2X_2&#39;)^2\\
&amp;=1+2X_1X_1&#39;+2X_2X_2&#39;+(X_1X_1&#39;)^2+(X_2X_2&#39;)^2+2X_1X_1&#39;X_2X_2&#39;\tag{12.23}\label{12.23}
\end{array}
\]</span></p>
<p>则 <span class="math inline">\(M=6\)</span>，而且如果我们选择 <span
class="math inline">\(h_1(X)=1,h_2(X)=\sqrt{2}X_1,h_3(X)=\sqrt{2}X_2,h_4(X)=X_1^2,h_5(X)=X_2^2\)</span>，以及
<span class="math inline">\(h_6(X)=\sqrt{2}X_1X_2\)</span>，则<span
class="math inline">\(K(X,X&#39;)=\langle
h(X),h(X&#39;)\rangle\)</span>．</p>
<h2 id="第十三章-原型方法与最近邻算法">第十三章
<strong>原型方法与最近邻算法</strong></h2>
<p><strong>不变量和切线距离</strong>
对于每张图像，我们画出了该图像旋转版本的曲线，称为 <strong>不变流形
(invariance
manifolds)</strong>．现在，不是用传统的欧氏距离，而是采用两条曲线间的最短距离．换句话说，两张图像间的距离取为第一张图像的任意旋转版本与第二张图像的任意旋转版本间的最短欧氏距离．这个距离称为
<strong>不变度量 (invariant metric)</strong>．</p>
<p>原则上，可以采用这种不变度量来进行 1
最近邻分类．然而这里有两个问题．第一，对于真实图像很难进行计算．第二，允许大的变换，可能效果很差．举个例子，经过
180° 旋转后，“6” 可能看成是
“9”．<strong>我们需要限制为微小旋转</strong>．</p>
<p><strong>切线距离 (tangent distance)</strong> 解决了这两个问题．如图
13.10
所示，我们可以用图像“3”在其原图像的切线来近似不变流形．这个切线可以通过从图像的微小旋转中来估计方向向量，或者通过更复杂的空间光滑方法（<a
target="_blank" rel="noopener" href="https://github.com/szcf-weiya/ESL-CN/issues/187">练习
13.4</a>）．对于较大的旋转，切线图像不再像“3”，所以大程度的旋转问题可以减轻．</p>
<p>想法是对每个训练图像计算不变切线．对于待分类的 <strong>查询图像
(query
image)</strong>，计算其不变切线，并且在训练集的直线中寻找最近的直线．对应最近的直线的类别（数字）是对查询图像类别的预测值．在图
13.11 中，两条切向直线相交，但也只是因为我们是在二维空间中表示 256
维的情形．在 <span class="math inline">\(\mathbb R^{256}\)</span>
中，两条这样的直线相交的概率是 0．</p>
<p><strong>自适应最近邻方法</strong>
最近邻分类的隐含假设是类别概率在邻域内近似为常值，因此简单的平均会得到不错的估计．然而，在这个例子中，只有水平方向上的类别概率会变化．如果我们提前知道这一点，可以将邻居拉伸为长方形区域．这会降低估计的偏差，同时保持方差不变．</p>
<p>一般地，这要求最近邻分类中采用自适应的度量，使得得到的邻域沿着类别不会改变太多的方向上拉伸．在高维特征空间中，类别概率可能仅仅只在一个低维的子空间中有所改变，因此自适应度量是很重要的优点．</p>
<p><strong>判别自适应最近邻 (DANN)</strong>
方法进行了局部维度降低——也就是，在每个查询点单独降低维度．提出通过逐步剔除包含训练数据的盒子的边来自动寻找长方形邻域．这里我们介绍
Hastie and Tibshirani (1996a)[^2] 提出的 <strong>判别自适应最近邻
(discriminant adaptive nearest-neighbor)
(DANN)</strong>．在每个查询点，构造其大小为 50
个点的邻域，并且用这些点的类别分布来决定怎么对邻域进行变形——也就是，对度量进行更新．接着更新后的度量用在该查询点的最近邻规则中．因此每一个查询点都可能采用不同的度量．很明显邻域应当沿着垂直类别重心连线的方向拉伸．这个方向也与线性判别边界重合，而且是类别概率改变最少的方向．一般地，类别概率变化最大的方向不会与类别重心连线垂直</p>
<p><strong>计算上的考虑</strong></p>
<p>最近邻分类规则的一个缺点通常是它的计算负荷量，无论是寻找最近邻还是存储整个训练集合．</p>
<h2 id="第十四章-无监督学习">第十四章 <strong>无监督学习</strong></h2>
<p>流形（manifold）：数学上，流形是一个拓扑空间，在每一点附近局部地近似欧式空间．更精确地，<span
class="math inline">\(n\)</span> 维流形的每个点与维度为 <span
class="math inline">\(n\)</span> 的欧式空间同态的邻域．
监督学习中，有一个明确的成功或不成功的量度，因此可用于判断特定情况下的<strong>充分性
(adequacy)</strong>，并比较不同方法在各种情况下的<strong>有效性
(effectiveness)</strong>．成功的损失直接用在联合分布 <span
class="math inline">\(\Pr(X,Y)\)</span>
上的期望损失来衡量．这个可以用各种方式来衡量，包括交叉验证．在非监督学习中，没有这些直接衡量成功的量度．从大部分非监督学习的算法的输出中评估推断的有效性是很难确定的．必须诉诸于<strong>启发式变量
(heuristic arguments)</strong>，在监督学习也经常使用，这不仅可以激励
(motivating)
算法，而且为了评价结果的质量．因为有效性是主观问题，不能直接加以证实，这种不舒服
(unconfortable) 的情形导致提出的方法激增，</p>
<p><strong>关联规则分析 (Association rule analysis)</strong>
已经成为挖掘贸易数据的流行工具．目标是寻找变量 <span
class="math inline">\(X=(X_1,X_2,\ldots,X_p)\)</span>
在数据中出现最频繁的联合值．在二值数据 <span
class="math inline">\(X_j\in\{0,1\}\)</span>
中应用最多，也称作“市场篮子”分析．这种情形下观测值为销售交易，比如出现在商店收银台的商品．变量表示所有在商店中出售的商品．对于观测
<span class="math inline">\(i\)</span>，每个变量 <span
class="math inline">\(X_j\)</span> 取值为 0 或 1；如果第 <span
class="math inline">\(j\)</span> 个商品作为该次交易购买的一部分则 <span
class="math inline">\(x_{ij}=1\)</span>，而如果没有购买则 <span
class="math inline">\(x_{ij}=0\)</span>．这些经常有联合值的变量表示物品经常被一起购买．这个信息对于货架、跨营销的促销活动、商品目录的设计，以及基于购买模式的消费者划分都是很有用的．
关联规则成为了在相关的市场篮子的设定下用于分析非常大的交易数据库的流行工具．这是当数据可以转换成多维邻接表的形式时．输出是以容易理解并且可解释的关联规则
公式（14.4）的形式展现的．Apriori
算法允许分析可以用到大的数据库中，更大的数据库适用于其他类型的分析．关联规则是数据挖掘最大的成功之一．</p>
<p><strong>聚类分析</strong></p>
<p>聚类分析的所有目标的核心是度量要聚类的单个点间相似（或不相似）的程度．聚类方法试图基于点间相似性的定义来将其分类．相似性的定义只能从关注的主题得到．某种程度上，这个情形与确定预测问题（监督学习）中的损失或花费函数相似．在预测问题中，损失函数与错误的预测有关，而错误的预测取决于数据之外的考虑．
简而言之，聚类方法中相似性的定义就如同监督学习问题中损失函数一样重要.
<strong>确定一个合适的不相似性的度量远比选择聚类算法来得重要</strong>．(涉及领域知识。)</p>
<p><strong>主成分</strong>，<strong>主曲线和主曲面</strong></p>
<p><strong>流形学习</strong>（manifold learning）</p>
<p>是机器学习、模式识别中的一种方法，在维数约简方面具有广泛的应用。它的主要思想是将高维的数据映射到低维，使该低维的数据能够反映原高维数据的某些本质结构特征。流形学习的前提是有一种假设，即某些高维数据，实际是一种低维的流形结构嵌入在高维空间中。流形学习的目的是将其映射回低维空间中，揭示其本质。</p>
<blockquote>
<p>个人注：关于流行学习知乎上一篇通熟易懂的文章
https://www.zhihu.com/question/24015486/answer/26524937
，最常用的例子就是瑞士卷</p>
</blockquote>
<p><strong>投影是从一个向量空间到其自身的线性变换</strong>，并且投影矩阵满足<span
class="math inline">\(\mathbf P^2=\mathbf P\)</span>．</p>
<blockquote>
<p>个人注：应该是到子空间吧。<strong>不是所有从一个向量空间到自身的线性变换都是投影</strong>，但<strong>所有投影都是线性变换</strong>，而且满足
<span class="math inline">\(P^2 = P\)</span>（即投影两次不变）。</p>
</blockquote>
<p><strong>主成分可以看成是主曲线的特殊情形</strong>。</p>
<h2 id="第十五章-随机森林">第十五章 <strong>随机森林</strong></h2>
<blockquote>
<p>在每次分割时，随机选择 <span class="math inline">\(m\le p\)</span>
个输入变量作为候选变量用来分割</p>
</blockquote>
<p>一般地，<span class="math inline">\(m\)</span> 取为 <span
class="math inline">\(\sqrt{p}\)</span>，或者甚至小到取 1．</p>
<p>Bagging 可以看成是特殊的随机森林，即 <span
class="math inline">\(m=p\)</span> 的随机森林．</p>
<p>另外，发明者给出下面两条推荐：</p>
<ul>
<li>对于分类，<span class="math inline">\(m\)</span> 的默认值为 <span
class="math inline">\(\lfloor \sqrt p \rfloor\)</span>，且最小的结点数为
1．</li>
<li>对于回归，<span class="math inline">\(m\)</span> 的默认值为 <span
class="math inline">\(\lfloor p/3\rfloor\)</span>，且最小的结点数为
5．</li>
</ul>
<p>实际中这些参数的最优值取决于具体问题，并且它们应当被视为
<strong>调整参数 (tunning parameters)</strong>．在图 15.3 中，<span
class="math inline">\(m=6\)</span> 比默认值 <span
class="math inline">\(\lfloor 8/3\rfloor =2\)</span> 更好．</p>
<h2 id="第十六章-集成学习">第十六章 <strong>集成学习</strong></h2>
<p>"Bet on Sparsity" 原则 <span class="math inline">\(L_1\)</span>
的收缩能更好地适应稀疏的情形（在所有可能选择中，非零系数的基函数的个数很少）．
当拟合系数时，我们应该使用 <span class="math inline">\(L_2\)</span>
惩罚，而不是 <span class="math inline">\(L_1\)</span>
惩罚．另一方面，如果这里只有少量的（比如，<span
class="math inline">\(1000\)</span>）系数非零，则 lasso （<span
class="math inline">\(L_1\)</span> 惩罚）会表现得很好．我们将这个看成是
<strong>稀疏 (sparse)</strong> 的情形，而第一种情形（高斯系数）是
<strong>稠密 (dense)</strong> 的．注意到尽管在稠密情形下，<span
class="math inline">\(L_2\)</span>
惩罚是最好的，但没有方法能做得很好，因为数据太少，但却要从中估计大量的非零系数．这是维数的灾难造成的损失．稀疏设定中，我们可以用
<span class="math inline">\(L_1\)</span>
惩罚做得很好，因为非零稀疏的个数很少．但 <span
class="math inline">\(L_2\)</span> 惩罚便不行．</p>
<p>换句话说，<span class="math inline">\(L_1\)</span> 惩罚的使用遵循称作
“bet on sparsity” 的这一高维问题的准则：</p>
<blockquote>
<p>采用在稀疏问题中表现得好的方法，因为没有方法能在稠密问题中表现得好．</p>
</blockquote>
<h2 id="第十七章-无向图模型">第十七章 <strong>无向图模型</strong></h2>
<p><strong>图 (Graph)</strong>
由顶点（结点）集，以及连接顶点对的边集构成．在图模型中，每个顶点表示一个随机变量，并且图给出了一种理解全体随机变量联合分布的可视化方式．对于监督学习和非监督学习它们都是很有用的．在
<strong>无向图 (undirected graph)</strong>
中，边是没有方向的．我们仅限于讨论无向图模型，也称作
<strong>马尔科夫随机域 (Markov random fields)</strong> 或者
<strong>马尔科夫网络 (Markov networks)</strong>． -
在这些图中，两个顶点间缺失一条边有着特殊的含义：对应的随机变量在给定其它变量下是条件独立的．</p>
<p>图中的边用值 (value) 或者 <strong>势 (potential)</strong>
参量化，来表示在对应顶点上的随机变量间条件依赖性的强度大小．采用图模型的主要挑战是模型选择（选择图的结构）、根据数据来估计边的参数，并且从联合分布中计算边缘顶点的概率和期望．后两个任务在计算机科学中有时被称作
<strong>学习 (learning)</strong> 和
<strong>推断(inference)</strong>．</p>
<p>关于 <strong>有向图 (directed graphical models)</strong> 或者
<strong>贝叶斯网络 (Bayesian networks)</strong>
有大量并且活跃的文献；这是边有方向箭头（但是没有有向环）的图模型．有向图模型表示可以分解成条件分布乘积的概率分布，并且有解释因果关系的潜力．</p>
<p>三种等价的 Markov 性质" pairwise Markov properties:
寻找缺失边，在给定其他结点的情况下，缺失边的两个顶点相互独立； global
Markov properties:
寻找分离集，在给定分离集的情况下，被分离的子图相互独立；</p>
<h3 id="连续变量的无向图模型">连续变量的无向图模型</h3>
<p>这里我们考虑所有变量都是连续变量的马尔科夫网络．这样的图模型几乎总是用到高斯分布，因为它有方便的分析性质．我们假设观测值服从均值为
<span class="math inline">\(\mu\)</span>，协方差为 <span
class="math inline">\(\mathbf \Sigma\)</span>
的多元高斯分布．因为高斯分布至多表示二阶的关系，所以它自动地编码了一个成对马尔科夫图．</p>
<p>!!! note "weiya 注："
因为在高斯分布的密度函数中，指数项中关于随机变量的阶数最多是二次，所以说它至多能表示二阶的关系．</p>
<ul>
<li>高斯分布有个性质是所有的条件分布也是高斯分布． 协方差矩阵的逆 <span
class="math inline">\(\mathbf\Sigma^{-1}\)</span> 包含变量之间的
<strong>偏协方差 (partial covariances)</strong>
信息；也就是，在给定其它变量的条件下，<span
class="math inline">\(i\)</span> 与 <span
class="math inline">\(j\)</span> 的协方差．特别地，如果 <span
class="math inline">\(\mathbf {\Theta=\Sigma^{-1}}\)</span> 的第 <span
class="math inline">\(ij\)</span> 个元素为 0，则变量 <span
class="math inline">\(i\)</span> 和 <span
class="math inline">\(j\)</span> 在给定其它变量情况下是条件独立的．</li>
</ul>
<h3 id="图结构的估计">图结构的估计</h3>
<p>大多数情况下，我们不知道哪些边要从图中去掉，因此想试图从数据本身找出．最近几年很多作者提出用于这个目的的
<span class="math inline">\(L_1\)</span> (lasso) 正则化．</p>
<p>!!! note "weiya 注：" 省略图中的边，有点类似于做变量选择，而 lasso
正是应对变量选择的“绝世武功”!:joy:</p>
<p>为了实现这点，它们将每个变量看成响应变量而其它的变量作为预测变量进行拟合
lasso 回归</p>
<h3 id="限制玻尔兹曼机">限制玻尔兹曼机</h3>
<p>离散变量的无向马尔科夫网络是很流行的，而且特别地，二值变量的成对马尔科夫网络更普遍．在统计力学领域有时称为
Ising 模型，在机器学习领域称为 <strong>玻尔兹曼机 (Boltzmann
machines)</strong>，其中顶点称为“<strong>结点
(nodes)</strong>”或“<strong>单元 (units)</strong>”，取值为 0 或 1.</p>
<p>这节我们考虑受神经网络影响的一种特殊的图模型结构，该结构中，单元是按层进行组织的．<strong>限制玻尔兹曼机
(RBM)</strong>
包含一层可见单元和一层隐藏单元，单层之间没有联系．如果隐藏单元的连接被移除掉，计算条件期望变得很简单</p>
<blockquote>
<p>个人注：<strong>虽然标准 RBM
使用二值神经元，但也存在许多变体，可以处理不同类型的输入：</strong></p>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 66%" />
</colgroup>
<thead>
<tr class="header">
<th>变体类型</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Gaussian-Bernoulli RBM</strong></td>
<td>可见层为连续实数（高斯分布），隐藏层仍是二值</td>
</tr>
<tr class="even">
<td><strong>Gaussian-Gaussian RBM</strong></td>
<td>可见层和隐藏层都为连续变量</td>
</tr>
<tr class="odd">
<td><strong>Softmax RBM</strong></td>
<td>可见层或隐藏层是 one-hot 多类别状态</td>
</tr>
<tr class="even">
<td><strong>ReLU RBM</strong></td>
<td>使用 ReLU 激活而非二值状态，用于更复杂的连续特征建模</td>
</tr>
</tbody>
</table>
<p>这些变体适用于图像、音频、自然语言处理等不同类型的数据。</p>
</blockquote>
<h2 id="第十八章-高维问题">第十八章 高维问题</h2>

    </div>

    
    
    

    <footer class="post-footer">

        


          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/03/03/%E3%80%8A%E9%9A%8F%E6%9C%BA%E6%BC%AB%E6%AD%A5%E7%9A%84%E5%82%BB%E7%93%9C%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/" rel="prev" title="《随机漫步的傻瓜》读书心得">
                  <i class="fa fa-chevron-left"></i> 《随机漫步的傻瓜》读书心得
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2025/01/28/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97%E4%B9%8B%E4%B8%89/" rel="next" title="《统计学习基础》 (3/n)">
                  《统计学习基础》 (3/n) <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Rayman.hung</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
<script src="https://cdn.jsdelivr.net/npm/hexo-generator-searchdb@1.4.0/dist/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js","integrity":"sha256-r+3itOMtGGjap0x+10hu6jW/gZCzxHsoKrOd7gyRSGY="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>

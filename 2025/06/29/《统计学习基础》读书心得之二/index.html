<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.1.1">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" integrity="sha256-DfWjNxDkM94fVBWx1H5BMMp0Zq7luBlV8QRcSES7s+0=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"hongyitong.github.io","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.11.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

<meta name="robots" content="noindex,follow">

    <meta name="description" content="第三章：线性回归方法； 第四章：线性分类方法。 第三章 线性回归方法 总结：  对于任意一个有限维的矩阵（实数或复数矩阵），它的行秩 &#x3D; 列秩。这个值也被称为矩阵的秩（rank）； 标准化因数或者 Z-分数，\(z_j\) 分布为 \(t_{N-p-1}\)（自由度为 \(N-p-1\) 的 \(t\) 分布）； \(t\) 分布和标准正态分布在尾概率之间的差异随着样本规模增大可以忽略； \(F\">
<meta property="og:type" content="article">
<meta property="og:title" content="《统计学习基础》 (2&#x2F;6)">
<meta property="og:url" content="http://hongyitong.github.io/2025/06/29/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97%E4%B9%8B%E4%BA%8C/index.html">
<meta property="og:site_name" content="墨语浮生">
<meta property="og:description" content="第三章：线性回归方法； 第四章：线性分类方法。 第三章 线性回归方法 总结：  对于任意一个有限维的矩阵（实数或复数矩阵），它的行秩 &#x3D; 列秩。这个值也被称为矩阵的秩（rank）； 标准化因数或者 Z-分数，\(z_j\) 分布为 \(t_{N-p-1}\)（自由度为 \(N-p-1\) 的 \(t\) 分布）； \(t\) 分布和标准正态分布在尾概率之间的差异随着样本规模增大可以忽略； \(F\">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://hongyitong.github.io/img3/ESL/fig3.9.png">
<meta property="og:image" content="http://hongyitong.github.io/img3/ESL/tab3.4.png">
<meta property="article:published_time" content="2025-06-28T16:00:00.000Z">
<meta property="article:modified_time" content="2025-12-18T02:42:07.156Z">
<meta property="article:author" content="Rayman.hung">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://hongyitong.github.io/img3/ESL/fig3.9.png">


<link rel="canonical" href="http://hongyitong.github.io/2025/06/29/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97%E4%B9%8B%E4%BA%8C/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://hongyitong.github.io/2025/06/29/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97%E4%B9%8B%E4%BA%8C/","path":"2025/06/29/《统计学习基础》读书心得之二/","title":"《统计学习基础》 (2/6)"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>《统计学习基础》 (2/6) | 墨语浮生</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-ZXGEJDXQ33"></script>
  <script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"G-ZXGEJDXQ33","only_pageview":false}</script>
  <script src="/js/third-party/analytics/google-analytics.js"></script>

  <script src="/js/third-party/analytics/baidu-analytics.js"></script>
  <script async src="https://hm.baidu.com/hm.js?ff358b9c7e949787b074bfc9c8019a9d"></script>




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">墨语浮生</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Rayman</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section">首页</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section">分类</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section">关于</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section">归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E4%B8%89%E7%AB%A0-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%96%B9%E6%B3%95"><span class="nav-text">第三章 线性回归方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#guass-markov-%E5%AE%9A%E7%90%86"><span class="nav-text">3.1 Guass-Markov 定理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%8E%E7%AE%80%E5%8D%95%E5%8D%95%E5%8F%98%E9%87%8F%E5%9B%9E%E5%BD%92%E5%88%B0%E5%A4%9A%E9%87%8D%E5%9B%9E%E5%BD%92"><span class="nav-text">3.2 从简单单变量回归到多重回归</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AD%90%E9%9B%86%E7%9A%84%E9%80%89%E6%8B%A9"><span class="nav-text">3.3 子集的选择</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9C%80%E4%BC%98%E9%9B%86%E7%9A%84%E9%80%89%E6%8B%A9"><span class="nav-text">3.3.1 最优集的选择</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%90%91%E5%89%8D%E5%92%8C%E5%90%91%E5%90%8E%E9%80%90%E6%AD%A5%E9%80%89%E6%8B%A9"><span class="nav-text">3.3.2 向前和向后逐步选择</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%90%91%E5%89%8D%E9%80%90%E6%B8%90-forward-stagewise-%E5%9B%9E%E5%BD%92"><span class="nav-text">3.3.3 向前逐渐
(Forward-Stagewise) 回归</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%94%B6%E7%BC%A9%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-text">3.4 收缩的方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B2%AD%E5%9B%9E%E5%BD%92"><span class="nav-text">3.4.1 岭回归</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#lasso"><span class="nav-text">3.4.2 Lasso</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%A8%E8%AE%BA%E5%AD%90%E9%9B%86%E7%9A%84%E9%80%89%E6%8B%A9%E5%B2%AD%E5%9B%9E%E5%BD%92lasso"><span class="nav-text">3.5
讨论：子集的选择，岭回归，Lasso</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E5%9B%9B%E7%AB%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E6%96%B9%E6%B3%95"><span class="nav-text">第四章 线性分类方法</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Rayman.hung</p>
  <div class="site-description" itemprop="description">技术分享、读书心得、心情记录</div>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://hongyitong.github.io/2025/06/29/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97%E4%B9%8B%E4%BA%8C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Rayman.hung">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="墨语浮生">
      <meta itemprop="description" content="技术分享、读书心得、心情记录">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="《统计学习基础》 (2/6) | 墨语浮生">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          《统计学习基础》 (2/6)
        </h1>

        </h1>
          
             <p class="post-subtitle">读书笔记之二：第3章-第4章</p>
          
        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-06-29 00:00:00" itemprop="dateCreated datePublished" datetime="2025-06-29T00:00:00+08:00">2025-06-29</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97%EF%BC%88%E8%87%AA%E7%84%B6%E7%A7%91%E5%AD%A6%EF%BC%89/" itemprop="url" rel="index"><span itemprop="name">读书心得（自然科学）</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>第三章：线性回归方法； 第四章：线性分类方法。</p>
<h2 id="第三章-线性回归方法">第三章 线性回归方法</h2>
<p><strong>总结：</strong></p>
<ul>
<li>对于任意一个有限维的矩阵（实数或复数矩阵），它的行秩 =
列秩。这个值也被称为矩阵的秩（rank）；</li>
<li>标准化因数或者 Z-分数，<span class="math inline">\(z_j\)</span>
分布为 <span class="math inline">\(t_{N-p-1}\)</span>（自由度为 <span
class="math inline">\(N-p-1\)</span> 的 <span
class="math inline">\(t\)</span> 分布）；</li>
<li><span class="math inline">\(t\)</span>
分布和标准正态分布在尾概率之间的差异随着样本规模增大可以忽略；</li>
<li><span class="math inline">\(F\)</span>
统计量衡量了在大模型中每个增加的系数对残差平方和的改变；</li>
<li>当 <span class="math inline">\(N\)</span> 足够大时，<span
class="math inline">\(F_{p_1-p_0,N-p_1-1}\)</span> 近似 <span
class="math inline">\(\chi^2_{p_1-p_0}\)</span>．</li>
</ul>
<span id="more"></span>
<h3 id="guass-markov-定理">3.1 Guass-Markov 定理</h3>
<blockquote>
<p>统计学中一个很有名的结论称参数 <span
class="math inline">\(\beta\)</span>
的最小二乘估计在所有的线性无偏估计中有最小的方差。</p>
</blockquote>
<p>考虑 <span class="math inline">\(\theta\)</span> 的估计值 <span
class="math inline">\(\tilde{\theta}\)</span> 的均方误差 <span
class="math display">\[
\begin{align}
\text MSE(\tilde{\theta})&amp;=\text E(\tilde{\theta}-\theta)^2\notag\\
&amp;=\text Var(\tilde{\theta})+[\text
E(\tilde{\theta})-\theta]^2\tag{3.20}
\end{align}
\]</span> 第一项为方差，第二项为平方偏差．Gauss-Markov
定理表明最小二乘估计在所有无偏线性估计中有最小的均方误差。</p>
<blockquote>
<p>然而，或许存在有较小均方误差的有偏估计．这样的估计用小的偏差来换取方差大幅度的降低．实际中也会经常使用有偏估计.</p>
</blockquote>
<p>任何收缩或者将最小二乘的一些参数设为 0
的方法都可能导致有偏估计．我们将在这章的后半部分讨论许多例子，包括
<strong>变量子集选择</strong> 和
<strong>岭回归</strong>．<strong>从一个更加实际的观点来看，许多模型是对事实的曲解，因此是有偏的；</strong></p>
<p>挑选一个合适的模型意味着要在偏差和方差之间创造某种良好的平衡。</p>
<h3 id="从简单单变量回归到多重回归">3.2 从简单单变量回归到多重回归</h3>
<p>内积表示是线性回归模型一般化到不同度量空间（包括概率空间）建议的方式。</p>
<p>若模型为： <span class="math display">\[
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p +
\varepsilon
\]</span></p>
<p>则： <span class="math display">\[
\hat{\beta}_0 = \bar{y} - \sum_{j=1}^{p} \hat{\beta}_j \bar{x}_j
\]</span></p>
<p>依然体现了：<strong>截距项用于保证模型在均值点的预测值等于 <span
class="math inline">\(\bar{y}\)</span></strong>。
截距和样本均值有严格的线性关系。</p>
<p>在带截距的线性回归中，<strong>回归线总通过样本均值点</strong> <span
class="math inline">\((\bar{x},
\bar{y})\)</span>，这是一个非常重要的几何性质。</p>
<h3 id="子集的选择">3.3 子集的选择</h3>
<p>两个原因使得我们经常不满足最小二乘估计 (3.6)</p>
<ul>
<li>第一个是预测的 <strong>精确性 (prediction
accuracy)</strong>：最小二乘估计经常有小偏差大方差．预测精确性有时可以通过收缩或者令某些系数为
0
来提高．通过这些方法我们牺牲一点偏差来降低预测值的方差，因此可能提高整个预测的精确性．</li>
<li>第二个原因是 <strong>可解释性
(interpretation)</strong>：当有大量的预测变量时，我们经常去确定一个小的子集来保持最强的影响．为了得到“big
picture”，我们愿意牺牲一些小的细节．</li>
</ul>
<p>这节我们描述一些线性回归选择变量子集的方法．在后面的部分中我们讨论用于控制方差的收缩和混合的方法，以及其它降维的策略．这些都属于
<strong>模型选择 (model
selection)</strong>．模型选择不局限于线性模型；第 7
章将详细介绍这个主题．</p>
<p>子集选择意味着我们只保留变量的一个子集，并除去模型中的剩余部分。最小二乘回归用来预测保留下的输入变量的系数．这里有一系列不同的选择子集的策略：</p>
<h4 id="最优集的选择">3.3.1 最优集的选择</h4>
<p>AIC 准则是一个受欢迎的选择</p>
<h4 id="向前和向后逐步选择">3.3.2 向前和向后逐步选择</h4>
<p>其它传统的包中的选择基于 <span class="math inline">\(F\)</span>
统计量，加入“显著性”的项，然后删掉“非显著性”的项．这些不再流行，因为它们没有合理考虑到多重检验的问题．</p>
<h4 id="向前逐渐-forward-stagewise-回归">3.3.3 向前逐渐
(Forward-Stagewise) 回归</h4>
<p>使用“一个标准误差”规则——在最小值的一个标准误差范围内我们选取最简洁的模型。</p>
<h3 id="收缩的方法">3.4 收缩的方法</h3>
<p>通过保留一部分预测变量而丢弃剩余的变量，<strong>子集选择 (subset
selection)</strong>
可得到一个可解释的、预测误差可能比全模型低的模型．然而，因为这是一个离散的过程（变量不是保留就是丢弃），所以经常表现为高方差，因此不会降低全模型的预测误差．而<strong>收缩方法
(shrinkage methods)</strong> 更加连续，因此不会受 <strong>高易变性 (high
variability)</strong> 太大的影响．</p>
<h4 id="岭回归">3.4.1 岭回归</h4>
<ul>
<li>系数向零收缩（并且彼此收缩到一起）；</li>
<li>通过参数的平方和来惩罚的想法也用在了神经网络，也被称作
<strong>权重衰减 (weight decay)</strong></li>
<li>对输入按比例进行缩放时，岭回归的解不相等，因此求解公式 <span
class="math inline">\(\text{3.41}\)</span>前我们需要对输入进行标准化．另外，注意到惩罚项不包含截距
<span
class="math inline">\(\beta_0\)</span>．对截距的惩罚会使得过程依赖于
<span class="math inline">\(\mathbf{Y}\)</span> 的初始选择；</li>
<li>对输入进行中心化（每个 <span class="math inline">\(x_{ij}\)</span>
替换为 <span class="math inline">\(x_{ij}-\bar x_j\)</span>）</li>
<li>主成分回归与岭回归非常相似：都是通过输入矩阵的主成分来操作的．岭回归对主成分系数进行了收缩，收缩更多地依赖对应特征值的大小；主成分回归丢掉
<span class="math inline">\(p-M\)</span> 个最小的特征值分量．</li>
<li><strong>越小的奇异值 <span class="math inline">\(d_j\)</span> 对应
<span class="math inline">\(\mathbf{X}\)</span>
列空间中方差越小的方向，并且岭回归在这些方向上收缩得最厉害．</strong></li>
</ul>
<p><img src="/img3/ESL/fig3.9.png" /></p>
<p>图 3.9
展示了两个维度下部分数据点的主成分．如果我们考虑在这个区域（<span
class="math inline">\(Y\)</span>
轴垂直纸面）内拟合线性曲面，数据的结构形态使得确定梯度时长方向会比短方向更精确．岭回归防止在短方向上估计梯度可能存在的高方差．隐含的假设是响应变量往往在高方差的输入方向上变化．这往往是个合理的假设，因为我们所研究的预测变量随响应变量变化而变化，而不需要保持不变．</p>
<p>图 3.9
部分输入数据点的主成分．<strong>最大主成分是使得投影数据方差最大的方向，最小主成分是使得方差最小的方向．岭回归将
<span class="math inline">\(\mathbf{y}\)</span>
投射到这些成分上，然后对低方差成分的系数比高方差收缩得更厉害．</strong></p>
<h4 id="lasso">3.4.2 Lasso</h4>
<ul>
<li>由于该约束的本质，令 <span class="math inline">\(t\)</span>
充分小会造成一些参数恰恰等于 0．因此 lasso
完成一个温和的连续子集选择．</li>
<li>类似在变量子集选择中子集的大小，或者岭回归的惩罚参数，应该自适应地选择
<span class="math inline">\(t\)</span>
使预测误差期望值的估计最小化．</li>
<li>lasso 曲线会达到 0，然而岭回归不会．曲线是分段线性的</li>
</ul>
<h3 id="讨论子集的选择岭回归lasso">3.5
讨论：子集的选择，岭回归，Lasso</h3>
<p>有约束的线性回归模型的三种方法：子集选择、岭回归和 lasso．</p>
<ul>
<li>在正交输入矩阵的情况下，三种过程都有显式解．每种方法对最小二乘估计
<span class="math inline">\(\hat{\beta}_j\)</span>
应用简单的变换，详见表 3.4．</li>
<li>岭回归做等比例的收缩．lasso 通过常数因子 <span
class="math inline">\(\lambda\)</span> 变换每个系数，在 0
处截去．这也称作“软阈限”，而且用在 5.9
节中基于小波光滑的内容中．最优子集选择删掉所有系数小于第 <span
class="math inline">\(M\)</span>
个大系数的变量；这是“硬阈限”的一种形式．</li>
<li>lasso、岭回归和最优子集选择是有着不同先验分布的贝叶斯估计,然而，注意到它们取自后验分布的众数，即最大化后验分布．在贝叶斯估计中使用后验分布的均值更加常见．岭回归同样是后验分布的均值，但是
lasso 和最优子集选择不是．</li>
</ul>
<p><img src="/img3/ESL/tab3.4.png" /></p>
<h2 id="第四章-线性分类方法">第四章 线性分类方法</h2>
<p><strong>贝叶斯定理</strong>（Bayes’
Theorem）是概率论中一个非常重要的定理，用于在已知结果的情况下推断原因（也就是“后验概率”）。</p>
<blockquote>
<p><strong>贝叶斯定理告诉我们如何根据已有信息更新对某事件的信念。</strong></p>
</blockquote>
<p>对于两个事件 <span class="math inline">\(A\)</span> 和 <span
class="math inline">\(B\)</span>，只要 <span class="math inline">\(P(B)
&gt; 0\)</span>，贝叶斯定理公式如下：</p>
<p><span class="math display">\[
P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}
\]</span></p>
<p>其中：</p>
<ul>
<li><span
class="math inline">\(P(A)\)</span>：<strong>先验概率</strong>，事件 A
发生的原始概率；</li>
<li><span
class="math inline">\(P(B|A)\)</span>：<strong>似然度</strong>，在 A
发生的条件下，观察到 B 的概率；</li>
<li><span
class="math inline">\(P(B)\)</span>：<strong>边缘概率</strong>，B
发生的总概率；</li>
<li><span
class="math inline">\(P(A|B)\)</span>：<strong>后验概率</strong>，在 B
发生的前提下，A 发生的概率。</li>
</ul>
<p>全书的读书笔记（共6篇）如下：<br />
<a href="/2025/12/14/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97%E4%B9%8B%E4%B8%80/" title="《统计学习基础》 (1&#x2F;6)">《统计学习基础》读书笔记之一</a><br />
<a href="/2025/06/29/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97%E4%B9%8B%E4%BA%8C/" title="《统计学习基础》 (2&#x2F;6)">《统计学习基础》读书笔记之二</a><br />
<a href="/2025/06/28/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97%E4%B9%8B%E4%B8%89/" title="《统计学习基础》 (3&#x2F;6)">《统计学习基础》读书笔记之三</a><br />
<a href="/2025/06/27/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97%E4%B9%8B%E5%9B%9B/" title="《统计学习基础》 (4&#x2F;6)">《统计学习基础》读书笔记之四</a><br />
<a href="/2025/06/26/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97%E4%B9%8B%E4%BA%94/" title="《统计学习基础》 (5&#x2F;6)">《统计学习基础》读书笔记之五</a><br />
<a href="/2025/06/25/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97%E4%B9%8B%E5%85%AD/" title="《统计学习基础》 (6&#x2F;6)">《统计学习基础》读书笔记之六</a></p>

    </div>

    
    
    

    <footer class="post-footer">

        


          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2025/06/28/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97%E4%B9%8B%E4%B8%89/" rel="prev" title="《统计学习基础》 (3/6)">
                  <i class="fa fa-chevron-left"></i> 《统计学习基础》 (3/6)
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2025/10/24/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%B8%83/" rel="next" title="《机器学习的数学基础》（7/7）">
                  《机器学习的数学基础》（7/7） <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Rayman.hung</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
<script src="https://cdn.jsdelivr.net/npm/hexo-generator-searchdb@1.4.0/dist/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js","integrity":"sha256-r+3itOMtGGjap0x+10hu6jW/gZCzxHsoKrOd7gyRSGY="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>

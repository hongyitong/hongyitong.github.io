<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.1.1">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" integrity="sha256-DfWjNxDkM94fVBWx1H5BMMp0Zq7luBlV8QRcSES7s+0=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"hongyitong.github.io","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.11.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="技术分享、读书心得、心情记录">
<meta property="og:type" content="website">
<meta property="og:title" content="墨语浮生">
<meta property="og:url" content="http://hongyitong.github.io/page/3/index.html">
<meta property="og:site_name" content="墨语浮生">
<meta property="og:description" content="技术分享、读书心得、心情记录">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Rayman.hung">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://hongyitong.github.io/page/3/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"page/3/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>墨语浮生</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-ZXGEJDXQ33"></script>
  <script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"G-ZXGEJDXQ33","only_pageview":false}</script>
  <script src="/js/third-party/analytics/google-analytics.js"></script>





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">墨语浮生</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Rayman</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section">首页</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section">分类</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section">关于</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section">归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Rayman.hung</p>
  <div class="site-description" itemprop="description">技术分享、读书心得、心情记录</div>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://hongyitong.github.io/2025/10/25/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%85%AD/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Rayman.hung">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="墨语浮生">
      <meta itemprop="description" content="技术分享、读书心得、心情记录">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 墨语浮生">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/10/25/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%85%AD/" class="post-title-link" itemprop="url">《机器学习的数学基础》（6/7）</a>
        </h2>

        </h2>
          
             <p class="post-subtitle">读书笔记之六：概率与分布</p>
          
        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-10-25 00:00:00" itemprop="dateCreated datePublished" datetime="2025-10-25T00:00:00+08:00">2025-10-25</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97%EF%BC%88%E8%87%AA%E7%84%B6%E7%A7%91%E5%AD%A6%EF%BC%89/" itemprop="url" rel="index"><span itemprop="name">读书心得（自然科学）</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="概率与分布">6、概率与分布</h2>
<p>Probability and Distributions</p>
<p>概率论可以看作是布尔逻辑的推广。在机器学习的背景下，它经常以这种方式应用于自动推理系统的形式化设计。</p>
<p>在机器学习和统计学中，有两种主要的概率解释：贝叶斯主义和频率主义(Bishop,
2006;Efron and Hastie,
2016)。贝叶斯主义使用概率来指定用户对事件的不确定性程度。它有时被称为“主观概率”或“置信程度”。频率主义则考虑感兴趣的事件与所发生事件的总数的相对频率。一个事件的概率定义为当发生事件的总数趋于无限时，该事件的相对频率。详细例子见《概率中贝叶斯派与经典频率主义区别例子.md》</p>
<blockquote>
<p>贝叶斯主义和频率主义两者的核心区别在于：频率主义把参数视为固定值，而贝叶斯主义把参数视为随机变量，因此需要引入先验知识。两者的核心区别确实与“是否考虑先验知识”相关，但更根本的区别在于它们对“参数”的哲学认知不同：固定值
vs. 随机变量。</p>
</blockquote>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2025/10/25/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%85%AD/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://hongyitong.github.io/2025/10/24/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%B8%83/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Rayman.hung">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="墨语浮生">
      <meta itemprop="description" content="技术分享、读书心得、心情记录">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 墨语浮生">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/10/24/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%B8%83/" class="post-title-link" itemprop="url">《机器学习的数学基础》（7/7）</a>
        </h2>

        </h2>
          
             <p class="post-subtitle">读书笔记之七：连续优化</p>
          
        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-10-24 00:00:00" itemprop="dateCreated datePublished" datetime="2025-10-24T00:00:00+08:00">2025-10-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97%EF%BC%88%E8%87%AA%E7%84%B6%E7%A7%91%E5%AD%A6%EF%BC%89/" itemprop="url" rel="index"><span itemprop="name">读书心得（自然科学）</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="连续优化">7、连续优化</h2>
<p>Continuous Optimization</p>
<p>由于机器学习算法是在计算机上实现的，其中许多<strong>数学方程式都表示为数值优化方法</strong>。本章描述了训练机器学习模型的基本数值方法。训练机器学习模型通常归结为找到一组好的参数。“好”的概念是由目标函数或概率模型来决定的，我们将在本书的第二部分看到这些例子。给定一个目标函数，使用优化算法来寻找最佳值。<span
class="math inline">\(\mathbb{R}^{D}\)</span>
中考虑数据和模型，所以我们面临的优化问题是连续优化问题，而不是离散变量的组合优化问题。</p>
<p>一般情况下，机器学习中的大多数目标函数都是要被最小化的，即最优值就是最小值。直观上，梯度为目标函数每个点的上坡方向，而我们的目的是下坡（与梯度方向相反），希望找到最深的点。</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2025/10/24/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%B8%83/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://hongyitong.github.io/2025/02/02/%E3%80%8A%E9%87%91%E8%9E%8D%E7%BB%8F%E6%B5%8E%E5%AD%A6%E4%BA%8C%E5%8D%81%E4%BA%94%E8%AE%B2%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Rayman.hung">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="墨语浮生">
      <meta itemprop="description" content="技术分享、读书心得、心情记录">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 墨语浮生">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/02/02/%E3%80%8A%E9%87%91%E8%9E%8D%E7%BB%8F%E6%B5%8E%E5%AD%A6%E4%BA%8C%E5%8D%81%E4%BA%94%E8%AE%B2%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" class="post-title-link" itemprop="url">《金融经济学二十五讲》学习笔记</a>
        </h2>

        </h2>
          
        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-02-02 00:00:00" itemprop="dateCreated datePublished" datetime="2025-02-02T00:00:00+08:00">2025-02-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97%EF%BC%88%E7%A4%BE%E4%BC%9A%E7%A7%91%E5%AD%A6%EF%BC%89/" itemprop="url" rel="index"><span itemprop="name">读书心得（社会科学）</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><u><strong>金融的主干是定价（均衡定价、无套利定价）</strong></u></p>
<p><strong><u>金融是研究赚钱的理论，所以重点是研究市场上各类资产的定价，定价又涉及风险和效用的概念。</u></strong></p>
<ul>
<li>现实的资产价格（例如股票）由人的情绪来确定；一种投票的机制！！！！<br />
</li>
<li>行为经济学：非理性假设；方法论：使用心理学的结论作为起点<br />
</li>
<li>科学的尽头是神学，行为经济学是玄学 ，也是学术的宿命<br />
</li>
<li>VaR 历史模拟法<br />
</li>
<li>表见代理：例如老朱不认高管签订的合同就违反了这一点；案例见光大（？）证券的萝卜章事件。<br />
</li>
<li>场内市场 场外市场OTC市场
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2025/02/02/%E3%80%8A%E9%87%91%E8%9E%8D%E7%BB%8F%E6%B5%8E%E5%AD%A6%E4%BA%8C%E5%8D%81%E4%BA%94%E8%AE%B2%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://hongyitong.github.io/2025/01/29/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97%E4%B9%8B%E4%BA%8C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Rayman.hung">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="墨语浮生">
      <meta itemprop="description" content="技术分享、读书心得、心情记录">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 墨语浮生">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/01/29/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97%E4%B9%8B%E4%BA%8C/" class="post-title-link" itemprop="url">《统计学习基础》 (2/n)</a>
        </h2>

        </h2>
          
             <p class="post-subtitle">读书笔记之二：线性回归方法+线性分类方法</p>
          
        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-01-29 00:00:00" itemprop="dateCreated datePublished" datetime="2025-01-29T00:00:00+08:00">2025-01-29</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97%EF%BC%88%E8%87%AA%E7%84%B6%E7%A7%91%E5%AD%A6%EF%BC%89/" itemprop="url" rel="index"><span itemprop="name">读书心得（自然科学）</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="第三章-线性回归方法">第三章 线性回归方法</h2>
<p><strong>总结：</strong></p>
<ul>
<li>对于任意一个有限维的矩阵（实数或复数矩阵），它的行秩 =
列秩。这个值也被称为矩阵的秩（rank）；</li>
<li>标准化因数或者 Z-分数，<span class="math inline">\(z_j\)</span>
分布为 <span class="math inline">\(t_{N-p-1}\)</span>（自由度为 <span
class="math inline">\(N-p-1\)</span> 的 <span
class="math inline">\(t\)</span> 分布）；</li>
<li><span class="math inline">\(t\)</span>
分布和标准正态分布在尾概率之间的差异随着样本规模增大可以忽略；</li>
<li><span class="math inline">\(F\)</span>
统计量衡量了在大模型中每个增加的系数对残差平方和的改变；</li>
<li>当 <span class="math inline">\(N\)</span> 足够大时，<span
class="math inline">\(F_{p_1-p_0,N-p_1-1}\)</span> 近似 <span
class="math inline">\(\chi^2_{p_1-p_0}\)</span>．</li>
</ul>
<h3 id="guass-markov-定理">3.1 Guass-Markov 定理</h3>
<blockquote>
<p>统计学中一个很有名的结论称参数 <span
class="math inline">\(\beta\)</span>
的最小二乘估计在所有的线性无偏估计中有最小的方差。</p>
</blockquote>
<p>考虑 <span class="math inline">\(\theta\)</span> 的估计值 <span
class="math inline">\(\tilde{\theta}\)</span> 的均方误差 <span
class="math display">\[
\begin{align}
\text MSE(\tilde{\theta})&amp;=\text E(\tilde{\theta}-\theta)^2\notag\\
&amp;=\text Var(\tilde{\theta})+[\text
E(\tilde{\theta})-\theta]^2\tag{3.20}
\end{align}
\]</span> 第一项为方差，第二项为平方偏差．Gauss-Markov
定理表明最小二乘估计在所有无偏线性估计中有最小的均方误差。</p>
<blockquote>
<p>然而，或许存在有较小均方误差的有偏估计．这样的估计用小的偏差来换取方差大幅度的降低．实际中也会经常使用有偏估计.</p>
</blockquote>
<p>任何收缩或者将最小二乘的一些参数设为 0
的方法都可能导致有偏估计．我们将在这章的后半部分讨论许多例子，包括
<strong>变量子集选择</strong> 和
<strong>岭回归</strong>．<strong>从一个更加实际的观点来看，许多模型是对事实的曲解，因此是有偏的；</strong></p>
<p>挑选一个合适的模型意味着要在偏差和方差之间创造某种良好的平衡。</p>
<h3 id="从简单单变量回归到多重回归">3.2 从简单单变量回归到多重回归</h3>
<p>内积表示是线性回归模型一般化到不同度量空间（包括概率空间）建议的方式。</p>
<p>若模型为：<span class="math inline">\(y = \beta_0 + \beta_1 x_1 +
\beta_2 x_2 + \cdots + \beta_p x_p + \varepsilon\)</span></p>
<p>则：<span class="math inline">\(\hat{\beta}_0 = \bar{y} -
\sum_{j=1}^{p} \hat{\beta}_j \bar{x}_j\)</span></p>
<p>依然体现了：<strong>截距项用于保证模型在均值点的预测值等于 <span
class="math inline">\(\bar{y}\)</span></strong>。
截距和样本均值有严格的线性关系。</p>
<p>在带截距的线性回归中，<strong>回归线总通过样本均值点</strong> <span
class="math inline">\((\bar{x},
\bar{y})\)</span>，这是一个非常重要的几何性质。</p>
<h3 id="子集的选择">3.3 子集的选择</h3>
<p>两个原因使得我们经常不满足最小二乘估计 (3.6)</p>
<ul>
<li>第一个是预测的 <strong>精确性 (prediction
accuracy)</strong>：最小二乘估计经常有小偏差大方差．预测精确性有时可以通过收缩或者令某些系数为
0
来提高．通过这些方法我们牺牲一点偏差来降低预测值的方差，因此可能提高整个预测的精确性．</li>
<li>第二个原因是 <strong>可解释性
(interpretation)</strong>：当有大量的预测变量时，我们经常去确定一个小的子集来保持最强的影响．为了得到“big
picture”，我们愿意牺牲一些小的细节．</li>
</ul>
<p>这节我们描述一些线性回归选择变量子集的方法．在后面的部分中我们讨论用于控制方差的收缩和混合的方法，以及其它降维的策略．这些都属于
<strong>模型选择 (model
selection)</strong>．模型选择不局限于线性模型；第 7
章将详细介绍这个主题．</p>
<p>子集选择意味着我们只保留变量的一个子集，并除去模型中的剩余部分。最小二乘回归用来预测保留下的输入变量的系数．这里有一系列不同的选择子集的策略：</p>
<h4 id="最优集的选择">3.3.1 最优集的选择</h4>
<p>AIC 准则是一个受欢迎的选择</p>
<h4 id="向前和向后逐步选择">3.3.2 向前和向后逐步选择</h4>
<p>其它传统的包中的选择基于 <span class="math inline">\(F\)</span>
统计量，加入“显著性”的项，然后删掉“非显著性”的项．这些不再流行，因为它们没有合理考虑到多重检验的问题．</p>
<h4 id="向前逐渐-forward-stagewise-回归">3.3.3 向前逐渐
(Forward-Stagewise) 回归</h4>
<p>使用“一个标准误差”规则——在最小值的一个标准误差范围内我们选取最简洁的模型。</p>
<h3 id="收缩的方法">3.4 收缩的方法</h3>
<p>通过保留一部分预测变量而丢弃剩余的变量，<strong>子集选择 (subset
selection)</strong>
可得到一个可解释的、预测误差可能比全模型低的模型．然而，因为这是一个离散的过程（变量不是保留就是丢弃），所以经常表现为高方差，因此不会降低全模型的预测误差．而<strong>收缩方法
(shrinkage methods)</strong> 更加连续，因此不会受 <strong>高易变性 (high
variability)</strong> 太大的影响．</p>
<h4 id="岭回归">3.4.1 岭回归</h4>
<ul>
<li>系数向零收缩（并且彼此收缩到一起）；</li>
<li>通过参数的平方和来惩罚的想法也用在了神经网络，也被称作
<strong>权重衰减 (weight decay)</strong></li>
<li>对输入按比例进行缩放时，岭回归的解不相等，因此求解公式 <span
class="math inline">\(\text{3.41}\)</span>前我们需要对输入进行标准化．另外，注意到惩罚项不包含截距
<span
class="math inline">\(\beta_0\)</span>．对截距的惩罚会使得过程依赖于
<span class="math inline">\(\mathbf{Y}\)</span> 的初始选择；</li>
<li>对输入进行中心化（每个 <span class="math inline">\(x_{ij}\)</span>
替换为 <span class="math inline">\(x_{ij}-\bar x_j\)</span>）</li>
<li>主成分回归与岭回归非常相似：都是通过输入矩阵的主成分来操作的．岭回归对主成分系数进行了收缩，收缩更多地依赖对应特征值的大小；主成分回归丢掉
<span class="math inline">\(p-M\)</span> 个最小的特征值分量．</li>
<li><strong>越小的奇异值 <span class="math inline">\(d_j\)</span> 对应
<span class="math inline">\(\mathbf{X}\)</span>
列空间中方差越小的方向，并且岭回归在这些方向上收缩得最厉害．</strong></li>
</ul>
<p><img src="/img3/ESL/fig3.9.png" /></p>
<p>图 3.9
展示了两个维度下部分数据点的主成分．如果我们考虑在这个区域（<span
class="math inline">\(Y\)</span>
轴垂直纸面）内拟合线性曲面，数据的结构形态使得确定梯度时长方向会比短方向更精确．岭回归防止在短方向上估计梯度可能存在的高方差．隐含的假设是响应变量往往在高方差的输入方向上变化．这往往是个合理的假设，因为我们所研究的预测变量随响应变量变化而变化，而不需要保持不变．</p>
<p>图 3.9
部分输入数据点的主成分．<strong>最大主成分是使得投影数据方差最大的方向，最小主成分是使得方差最小的方向．岭回归将
<span class="math inline">\(\mathbf{y}\)</span>
投射到这些成分上，然后对低方差成分的系数比高方差收缩得更厉害．</strong></p>
<h4 id="lasso">3.4.2 Lasso</h4>
<ul>
<li>由于该约束的本质，令 <span class="math inline">\(t\)</span>
充分小会造成一些参数恰恰等于 0．因此 lasso
完成一个温和的连续子集选择．</li>
<li>类似在变量子集选择中子集的大小，或者岭回归的惩罚参数，应该自适应地选择
<span class="math inline">\(t\)</span>
使预测误差期望值的估计最小化．</li>
<li>lasso 曲线会达到 0，然而岭回归不会．曲线是分段线性的</li>
</ul>
<h3 id="讨论子集的选择岭回归lasso">3.5
讨论：子集的选择，岭回归，Lasso</h3>
<p>有约束的线性回归模型的三种方法：子集选择、岭回归和 lasso．</p>
<ul>
<li>在正交输入矩阵的情况下，三种过程都有显式解．每种方法对最小二乘估计
<span class="math inline">\(\hat{\beta}_j\)</span>
应用简单的变换，详见表 3.4．</li>
<li>岭回归做等比例的收缩．lasso 通过常数因子 <span
class="math inline">\(\lambda\)</span> 变换每个系数，在 0
处截去．这也称作“软阈限”，而且用在 5.9
节中基于小波光滑的内容中．最优子集选择删掉所有系数小于第 <span
class="math inline">\(M\)</span>
个大系数的变量；这是“硬阈限”的一种形式．</li>
<li>lasso、岭回归和最优子集选择是有着不同先验分布的贝叶斯估计,然而，注意到它们取自后验分布的众数，即最大化后验分布．在贝叶斯估计中使用后验分布的均值更加常见．岭回归同样是后验分布的均值，但是
lasso 和最优子集选择不是．</li>
</ul>
<h2 id="第四章-线性分类方法">第四章 线性分类方法</h2>
<p><strong>贝叶斯定理</strong>（Bayes’
Theorem）是概率论中一个非常重要的定理，用于在已知结果的情况下推断原因（也就是“后验概率”）。</p>
<blockquote>
<p><strong>贝叶斯定理告诉我们如何根据已有信息更新对某事件的信念。</strong></p>
</blockquote>
<p>对于两个事件 <span class="math inline">\(A\)</span> 和 <span
class="math inline">\(B\)</span>，只要 <span class="math inline">\(P(B)
&gt; 0\)</span>，贝叶斯定理公式如下：</p>
<p><span class="math display">\[
P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}
\]</span></p>
<p>其中：</p>
<ul>
<li><span
class="math inline">\(P(A)\)</span>：<strong>先验概率</strong>，事件 A
发生的原始概率；</li>
<li><span
class="math inline">\(P(B|A)\)</span>：<strong>似然度</strong>，在 A
发生的条件下，观察到 B 的概率；</li>
<li><span
class="math inline">\(P(B)\)</span>：<strong>边缘概率</strong>，B
发生的总概率；</li>
<li><span
class="math inline">\(P(A|B)\)</span>：<strong>后验概率</strong>，在 B
发生的前提下，A 发生的概率。</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://hongyitong.github.io/2025/01/28/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97%E4%B9%8B%E4%B8%89/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Rayman.hung">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="墨语浮生">
      <meta itemprop="description" content="技术分享、读书心得、心情记录">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 墨语浮生">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/01/28/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97%E4%B9%8B%E4%B8%89/" class="post-title-link" itemprop="url">《统计学习基础》 (3/n)</a>
        </h2>

        </h2>
          
             <p class="post-subtitle">读书笔记之三：基函数扩展与正则化+核平滑方法+模型评估与选择</p>
          
        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-01-28 00:00:00" itemprop="dateCreated datePublished" datetime="2025-01-28T00:00:00+08:00">2025-01-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97%EF%BC%88%E8%87%AA%E7%84%B6%E7%A7%91%E5%AD%A6%EF%BC%89/" itemprop="url" rel="index"><span itemprop="name">读书心得（自然科学）</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="第五章-基函数扩展与正则化">第五章 基函数扩展与正则化</h2>
<ul>
<li>据说三次样条是人眼看不出结点不连续的最低阶样条．很少有更好的理由去选择更高次的样条，除非对光滑的微分感兴趣．<br />
</li>
<li>固定结点的样条也称作 <strong>回归样条 (regression
splines)</strong>．我们需要选择样条的阶数，结点的个数以及它们的位置．一种简单方式是用基函数或自由度来参量化样条族，并用观测
<span class="math inline">\(x_i\)</span> 来确定结点的位置．<br />
</li>
<li><strong>自然三次样条 (natural cubic spline)</strong>
添加额外的限制，具体地，令边界结点之外的函数是线性的．<br />
</li>
<li>高维特征的预处理是非常普遍的而且对于改善学习算法的效果是很有效的。</li>
</ul>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2025/01/28/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97%E4%B9%8B%E4%B8%89/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://hongyitong.github.io/2025/01/27/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Rayman.hung">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="墨语浮生">
      <meta itemprop="description" content="技术分享、读书心得、心情记录">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 墨语浮生">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/01/27/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/" class="post-title-link" itemprop="url">《统计学习基础》 (4/n)</a>
        </h2>

        </h2>
          
             <p class="post-subtitle">读书笔记之四：基函数扩展与正则化</p>
          
        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-01-27 00:00:00" itemprop="dateCreated datePublished" datetime="2025-01-27T00:00:00+08:00">2025-01-27</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97%EF%BC%88%E8%87%AA%E7%84%B6%E7%A7%91%E5%AD%A6%EF%BC%89/" itemprop="url" rel="index"><span itemprop="name">读书心得（自然科学）</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="第八章-模型推断与平均化">第八章 模型推断与平均化</h2>
<p>本书的大部分章节中，对于回归而言，模型的拟合（学习）通过最小化平方和实现；或对于分类而言，通过最小化交叉熵实现．事实上，这两种最小化都是用极大似然来拟合的实例．</p>
<p>这章中，我们给出极大似然法的一个一般性的描述，以及用于推断的贝叶斯方法．在第7章中讨论的自助法在本章中也继续讨论，而且描述了它与极大似然和贝叶斯之间的联系．最后，我们提出模型平均和改善的相关技巧，包括
committee 方法、bagging、stacking 和 bumping．</p>
<h3 id="自助法和最大似然法">自助法和最大似然法</h3>
<ul>
<li>自助法，通过从训练集中有放回地采样，称作<strong>非参</strong>自助法(nonparametric
bootstrap),这实际上意味着这个方法是与模型无关的，因为它使用原始数据来得到新的数据集，而不是一个特定的含参数的模型．</li>
<li><strong>本质上自助法是非参最大似然或者参数最大似然法的计算机实现</strong>．与最大似然法相比自助法的好处是允许我们在没有公式的情况下计算标准误差和其他一些量的最大似然估计．
（详见原书的图8.2）</li>
</ul>
<h3 id="贝叶斯方法">贝叶斯方法</h3>
<ul>
<li>贝叶斯方法与一般推断方法的不同之处在于，用先验分布来表达知道数据之前的这种不确定性，而且在知道数据之后允许不确定性继续存在，将它表示成后验分布．</li>
<li>最大似然方法会使用在最大概率估计那个点的密度来预测未来的数据．不同于贝叶斯方法的预测分布，它不能说明估计
<span class="math inline">\(\theta\)</span> 的不确定性</li>
</ul>
<blockquote>
<p>个人注：推断中最大似然是预测某个最大的<span
class="math inline">\(\theta\)</span>值，贝叶斯方法是预测<span
class="math inline">\(\theta\)</span>的分布？</p>
</blockquote>
<ul>
<li>自助法分布表示我们参数的（近似的）非参、无信息后验分布．但是自助法分布可以很方便地得到——不需要正式地确定一个先验而且不需要从后验分布中取样．因此我们或许可以把自助法分布看成一个“穷人的”贝叶斯后验．通过扰动数据，自助法近似于扰动参数后的贝叶斯效应，而且一般实施起来更简单．</li>
</ul>
<h3 id="em算法">EM算法</h3>
<p>EM 算法是简化复杂极大似然问题的一种很受欢迎的工具；</p>
<blockquote>
<p>个人注：入门的视频见B站 博主“风中摇曳的小萝卜”的视频“EM算法
你到底是哪个班级的”</p>
</blockquote>
<h3 id="bagging">Bagging</h3>
<p>简单来说，bagging 和随机森林都是针对 bootstrap
样本，且前者可以看成后者的特殊形式；而 boosting 是针对残差样本．</p>
<h2 id="第九章-加性模型树模型及相关方法">第九章
加性模型、树模型及相关方法</h2>
<p>这章中我们开始对监督学习中一些特定的方法进行讨论．这里每个技巧都<strong>假设了未知回归函数（不同的）结构形式</strong>，而且通过这样处理巧妙地解决了维数灾难．<strong>当然，它们要为错误地确定模型类型付出可能的代价</strong>，所以在每种情形下都需要做出一个权衡．第
3-6 章留下的问题都将继续讨论．我们描述 5
个相关的技巧：<strong>广义可加模型 (generalized additive
models)</strong>，<strong>树
(trees)</strong>，<strong>多元自适应回归样条
(MARS)</strong>，<strong>耐心规则归纳法 (PRIM)</strong>，以及
<strong>混合层次专家 (HME)</strong>．</p>
<h3 id="广义可加模型">广义可加模型</h3>
<p>在回归的设定中，广义可加模型有如下形式</p>
<p><span class="math display">\[
\text E(Y\mid X_1,X_2,\ldots,X_p) =
\alpha+f_1(X_1)+f_2(X_2)+\cdots+f_p(X_p).\tag{9.1}
\]</span></p>
<h3 id="树模型">树模型</h3>
<ul>
<li>为什么二值分割？
与其在每一步对每个结点只分割成两个群体（如上面讨论的），我们或许可以考虑多重分割成多于两个群体．尽管这个在某些情况下是有用的，但不是一个好的一般策略．问题在于多重分割将数据分得太快，以至于在下一层次没有充分多的数据．因此我们仅仅当需要的时候采用这种分割．因为多重分割可以通过一系列的二值分割实现，所以后者更好一点．</li>
<li>树的不稳定性
树的一个主要问题是它们的高方差性．经常在数据中的一个小改动导致完全不同的分割点序列，使得解释不稳定．这种不稳定的主要原因是这个过程的层次性：上一个分割点的误差会传递到下面所有的分割点上．可以试图采取更加稳定的分离准则在某种程度上减轻这一影响，但是固有的不稳定性没有移除．这是从数据中估计一个简单的、基于树结构的代价．Bagging（<a
href="/08-Model-Inference-and-Averaging/8.7-Bagging/index.html">8.7
节</a>）对很多树进行平均来降低方差．</li>
<li>缺乏光滑性 树的另一个限制是预测表面缺乏光滑性，如在图 9.2
中的右下图中那样．在 0/1
损失的分类问题中，这不会有太大的损伤，因为类别概率估计的偏差的影响有限．然而，在回归问题中这会降低效果，正常情况下我们期望潜在的函数是光滑的．<a
href="9.4-MARS/index.html">9.4 节</a>介绍的 MARS 过程可以看出是为了减轻
CART 缺乏光滑性而做的改动．</li>
</ul>
<h3 id="roc">ROC</h3>
<p>在医学分类问题中，<strong>敏感度 (sensitivity)</strong> 和
<strong>特异度 (specificity)</strong>
经常用来衡量一个准则．它们按如下定义：</p>
<ul>
<li>敏感度：给定真实状态为患病预测为患病的概率</li>
<li>特异度：给定真实状态为未患病预测为未患病的概率</li>
</ul>
<p><strong>受试者工作特征曲线 (receiver operating characteristic curve,
ROC)</strong>
是用于评估敏感度和特异度之间折中的常用概述．当我们改变分类规则的参数便会得到敏感度关于特异度的图像．
越靠近东北角落的曲线表示越好的分类器． ROC 曲线下的面积有时被称作
<strong><span class="math inline">\(c\)</span> 统计量
(c-statistics)</strong>．</p>
<h3 id="专家的分层混合-hme"><strong>专家的分层混合 (HME)</strong></h3>
<p>过程可以看成是基于树方法的变种．主要的差异是树的分割不是<strong>硬决定
(hard decision)</strong>，而是<strong>软概率的决定 (soft
probabilistic)</strong>．在每个结点观测往左或者往右的概率取决于输入值．因为最后的参数优化问题是光滑的，所以有一些计算的优势，不像在基于树的方式中的离散分割点的搜索．软分割或许也可以帮助预测准确性，并且提供另外一种有用的数据描述方式．</p>
<h2 id="第十章-提升方法与加性树模型">第十章
<strong>提升方法与加性树模型</strong></h2>
<p>Boosting
是最近20年内提出的最有力的学习方法．最初是为了分类问题而设计的，但是我们将在这章中看到，它也可以很好地扩展到回归问题上．Boosting的动机是集合许多弱学习的结果来得到有用的“committee”．</p>
<p>弱分类器是误差率仅仅比随机猜测要好一点的分类器．Boosting
的目的是依次对反复修改的数据应用弱分类器算法，因此得到弱分类器序列 <span
class="math inline">\(G_m(x),m=1,2,\ldots,M\)</span>
根据它们得到的预测再通过一个加权来得到最终的预测</p>
<blockquote>
<p><strong>事实上，Breiman(NIPS Workshop，1996) 将树的 AdaBoost
称为“世界上最好的现成分类器”(best off-the-shelf classifier in the
world)．</strong> <strong>有人认为决策树是 boosting
是数据挖掘应用中理想的基学习器．</strong></p>
</blockquote>
<h3 id="数据挖掘的现货方法">数据挖掘的现货方法</h3>
<blockquote>
<p>个人注：<strong>“现货”(off-the-shelf)</strong>
方法．现货方法指的是可以直接应用到数据中而不需要大量时间进行数据预处理或者学习过程的精心调参．</p>
</blockquote>
<p><strong>预测学习 (predictive learning)</strong>
是数据挖掘中很重要的一部分．正如在这本书中看到的一样，已经提出了大量的方法对数据进行学习，然后预测．对于每个特定的方法，有些情形特别适用，但在其他情形表现得很差．我们已经试图在每个方法的讨论中明确合适的应用情形．然而，对于给定的问题，我们不会事先知道哪种方法表现得最好．表
10.1 总结了一些学习方法的特点．</p>
<p>工业和商业数据挖掘应用在学习过程的要求往往特别具有挑战性．数据集中观测值的个数以及每个观测值上衡量的变量个数往往都非常大．因此，需要注意计算的复杂度．并且，数据经常是混乱的
(messy)：输入往往是定量，二值以及类别型变量的混合，而且类别型变量往往有很多层次．一般还会有许多缺失值，完整的观测值是很稀少的．预测变量和响应变量的分布经常是
<strong>长尾 (long-tailed)</strong> 并且 <strong>高偏的 (highly
skewed)</strong>．垃圾邮件的数据就是这种情形(<a
href="../09-Additive-Models-Trees-and-Related-Methods/9.1-Generalized-Additive-Models/index.html">9.1.2
节</a>)；当拟合一个广义可加模型，我们首先对每个预测变量进行对数变换以期得到合理的拟合．另外，它们通常会包含很大一部分的严重的误测量值（离群值）．预测变量通常在差异很大的尺度下进行测量．</p>
<p>在数据挖掘应用中，通常只有大量预测变量中的一小部分真正与预测值相关的变量才被包含在分析中．另外，不同于很多应用的是，比如模式识别，很少有可信的专业知识来创建相关的特征，或者过滤掉不相关的，
这些不相关的特征显著降低了很多方法的效果．</p>
<p>另外，数据挖掘一般需要可解释性的模型．简单地得到预测值是不够的．提供定性
(qualitative)
理解输入变量和预测的响应变量之间的关系的信息是迫切的．因此，<strong>黑箱方法(black
box)</strong>，比如神经网络，在单纯的预测情形，比如模式识别中是很有用的，但在数据挖掘中不是很有用．</p>
<p>这些计算速度、可解释性的要求以及数据的混乱本性严重限制了许多学习过程作为数据挖掘的
<strong>“现货”(off-the-shelf)</strong>
方法．现货方法指的是可以直接应用到数据中而不需要大量时间进行数据预处理或者学习过程的精心调参．</p>
<p><strong>在所有的有名的学习方法中，决策树最能达到数据挖掘的现货方法的要求．它们相对很快地构造出模型并且得到可解释的模型（如果树很小）</strong>．如
<a
href="../09-Additive-Models-Trees-and-Related-Methods/9.2-Tree-Based-Methods/index.html">9.2
节</a>
中讨论的，它们自然地包含数值型和类别型预测变量以及缺失值的混合．它们在对单个预测变量的（严格单调）的变换中保持不变．结果是，尺寸变换和（或）更一般的变换不是问题，并且它们不受预测变量中的离群值的影响．它们将中间的特征选择作为算法过程的一部分．从而它们抑制（如果不是完全不受影响）包含许多不相关预测变量．
决策树的这些性质在很大程度上是它们成为数据挖掘中最受欢迎的学习方法的原因．</p>
<p><strong>树的不准确性导致其无法作为预测学习的最理想的工具．它们很少达到那个将数据训练得最好的方法的准确性．正如在
<a
href="../10-Boosting-and-Additive-Trees/10.1-Boosting-Methods/index.html">10.1
节</a>看到的，boosting
决策树提高了它们的准确性，经常是显著提高．同时它保留着数据挖掘中所需要的性质</strong>．一些树的优势被
boosting 牺牲的是计算速度、可解释性，以及对于 AdaBoost
而言，对重叠类的鲁棒性，特别是训练数据的误分类．<strong>gradient boosted
model(GBM)</strong>是 tree boosting
的一般化，它试图减轻这些问题，以便为数据挖掘提供准确且有效的现货方法．</p>
<h3 id="大小合适的boosting树">大小合适的boosting树</h3>
<p>曾经，boosting 被认为是一种将模型结合起来(combing
models)的技巧，在这里模型是树．同样地，生成树的算法可看成是产生用于
boosting 进行结合的模型的
<strong>原型(primitive)</strong>．这种情形下，在生成树的时候以通常的方式分别估计每棵树的最优大小（<a
href="9.2-Tree-Based-Methods/index.html">9.2
节</a>）．首先诱导出非常大（过大的）的一棵树，接着应用自下而上的过程剪枝得到估计的最优终止结点个数的树．这种方式隐含地假设了每棵树是式
公式（10.28） 中的最后一棵．</p>
<ul>
<li>解释性</li>
</ul>
<p>​
单个决策树有着很高的解释性．整个模型可以用简单的二维图象（二叉树）完整地表示，其中二叉树也很容易可视化．树的线性组合
公式（10.28） 丢失了这条重要的特性，所以必须考虑用不同的方式来解释．</p>
<ul>
<li>预测变量的相对重要性</li>
</ul>
<p>​
在数据挖掘应用中，输入的预测变量与响应变量的相关程度很少是相等的．通常只有一小部分会对响应变量有显著的影响，而绝大部分的变量是不相关的，并且可以简单地不用包含进模型．研究每个输入变量在预测响应变量时的相关重要度或者贡献是很有用的．</p>
<h2 id="第十一章-神经网络">第十一章 <strong>神经网络</strong></h2>
<p>这章中我们描述一类学习方法，它是基于在不同的领域（统计和人工智能）中独立发展起来但本质上相同的模型．中心思想是提取输入的线性组合作为<strong>导出特征
(derived
features)</strong>，然后将目标看成特征的非线性函数进行建模．这是一个很有效的学习方法，在许多领域都有广泛应用．我们首先讨论<strong>投影寻踪模型
(projection pursuit
model)</strong>，这是在半参统计和光滑化领域中发展出来的．本章的剩余部分集中讨论神经网络模型．</p>
<h3 id="投影寻踪回归">投影寻踪回归</h3>
<p>Projection Pursuit Regression</p>
<p>在我们一般监督学习问题中，假设我们有 <span
class="math inline">\(p\)</span> 个组分的输入向量 <span
class="math inline">\(X\)</span>，以及目标变量 <span
class="math inline">\(Y\)</span>．令 <span
class="math inline">\(\omega_m,m=1,2,\ldots, M\)</span> 为未知参数的
<span class="math inline">\(p\)</span> 维单位向量．<strong>投影寻踪回归
(PPR)</strong> 模型有如下形式: <span class="math display">\[
f(X)=\sum\limits_{m=1}^Mg_m(\omega_m^TX)\tag{11.1}\label{11.1}
\]</span> 这是一个可加模型，但是是关于导出特征 <span
class="math inline">\(V_m=\omega_m^TX\)</span>，而不是关于输入变量本身．函数
<span class="math inline">\(g_m\)</span>
未定，而是用一些灵活的光滑化方法来估计及 <span
class="math inline">\(\omega_m\)</span> 的方向（见下）．</p>
<p>函数 <span class="math inline">\(g_m(\omega_m^TX)\)</span> 称为 <span
class="math inline">\(\mathbb R^p\)</span> 中的<strong>岭函数 (ridge
function)</strong>．仅仅在由向量 <span
class="math inline">\(\omega_m\)</span> 定义的方向上变化．标量变量 <span
class="math inline">\(V_m=\omega_m^TX\)</span> 是 <span
class="math inline">\(X\)</span> 在单位向量 <span
class="math inline">\(\omega_m\)</span> 上的投影，寻找使得模型拟合好的
<span
class="math inline">\(\omega_m\)</span>，因此称为“投影寻踪”．<strong>图
11.1 显示了岭函数的一些例子</strong>。</p>
<blockquote>
<p>个人注：详见原书图11.1</p>
</blockquote>
<p>实际上，如果 <span class="math inline">\(M\)</span>
任意大，选择合适的 <span class="math inline">\(g_m\)</span>，PPR
模型可以很好地近似 <span class="math inline">\(\mathbb R^p\)</span>
中任意的连续函数．这样的模型类别称为 <strong>通用近似 (universal
approximator)</strong>．然而这种一般性需要付出代价．拟合模型的解释性通常很困难，因为每个输入变量都以复杂且多位面的方式进入模型中．结果使得
PPR 模型对于预测非常有用，但是对于产生一个可理解的模型不是很有用．<span
class="math inline">\(M=1\)</span> 模型是个例外，也是计量经济学中的
<strong>单指标模型 (single index
model)</strong>．这比线性回归模型更加一般，也提供了一个类似（线性回归模型）的解释．</p>
<p>然而，投影寻踪回归模型在统计领域并没有被广泛地使用，或许是因为在它的提出时间（1981），计算上的需求超出大多数已有计算机的能力．但是它确实代表着重要的智力进步，它是一个在神经网络领域的转世中发展起来的</p>
<h3 id="拟合神经网络">拟合神经网络</h3>
<p>神经网络模型中未知的参数，通常称为 <strong>权重
(weights)</strong>，我们需要寻找它们的值使得模型很好地拟合训练数据．我们将参数的全集记为
<span class="math inline">\(\theta\)</span></p>
<p>对于回归，我们采用误差平方和用于衡量拟合的效果（误差函数） <span
class="math display">\[
R(\theta)=\sum\limits_{k=1}^K\sum\limits_{i=1}^N(y_{ik}-f_k(x_i))^2\tag{11.9}
\]</span></p>
<p>对于分类，我们可以采用平方误差或者交叉熵（偏差）：</p>
<p><span class="math display">\[
R(\theta)=-\sum\limits_{i=1}^N\sum\limits_{k=1}^Ky_{ik}\mathrm{log}\;f_k(x_i)\tag{11.10}
\]</span></p>
<p>以及对应的分类器 <span class="math inline">\(G(x)=\mathrm{arg\;
max}_kf_k(x)\)</span>．有了 softmax
激活函数和交叉熵误差函数，神经网络模型实际上是关于隐藏层的线性逻辑斯蒂回归模型，而且所有的参数通过极大似然来估计．</p>
<p>一般地，我们不想要 <span class="math inline">\(R(\theta)\)</span>
的全局最小值，因为这可能会是一个过拟合解．而是需要一些正则化：这个可以通过惩罚项来直接实现，或者提前终止来间接实现．下一节中将给出详细的细节．</p>
<p>最小化 <span class="math inline">\(R(\theta)\)</span>
的一般方法是通过梯度下降，在这种情形下称作 <strong>向后传播
(back-propagation)</strong>．因为模型的组成成分，运用微分的链式法则可以很简单地得到梯度．这个可以通过对网络向前或向后遍历计算得到，仅跟踪每个单元的局部量．</p>
<p>向后传播的优点在于简单，局部自然．在向后传播算法中，每个隐藏层单元仅仅向（从）有其联系的单元传递（接收）信息．因此可以在并行架构的计算机上高效地实现．</p>
<p><strong>批量学习 (batch
learning)</strong>，参数更新为所有训练情形的和．学习也可以 online
地进行——每次处理一个观测，每个训练情形过后更新梯度，然后对训练情形重复多次．在这种情形下，公式（11.13）
式的和可以替换成单个被加数．一个 <strong>训练时期 (training
epoch)</strong> 指的是一次扫描整个训练集．<strong>在线训练 (online
training)</strong>
允许网络处理非常大的训练集，而且当新观测加入时更新权重．</p>
<h3 id="训练神经网络的一些问题">训练神经网络的一些问题</h3>
<p>训练神经网络真的是一门艺术．模型一般会过参量化，而且优化问题非凸而且不稳定，除非遵循某确定的方式．在这节我们总结一些重要的问题．</p>
<ul>
<li>初始值 注意如果权重接近 0，则 sigmoid（图
11.3）起作用的部分近似线性，因此神经网络退化成近似线性模型（<a
target="_blank" rel="noopener" href="https://github.com/szcf-weiya/ESL-CN/issues/177">练习
11.2</a>）．通常权重系数的初始值取为接近 0
的随机值．因此模型开始时近似线性，当系数增大时变成非线性．需要的时候局部化单个单元的方向并且引入非线性．恰巧为
0 的权重的使用导致 0
微分和完美的对称，而且算法将不会移动．而以较大的值开始经常带来不好的解．
<span class="math inline">\(\sigma(sv)\)</span> ，s是权重。</li>
<li>过拟合 通常神经网络有太多的权重而且在 <span
class="math inline">\(R\)</span>
的全局最小处过拟合数据．在神经网络的发展早期，无论是设计还是意外，采用提前终止的规则来避免过拟合．也就是训练一会儿模型，在达到全局最小前终止．因为权重以高正则化（线性）解开始，这有将最终模型收缩成线性模型的效果．验证集对于决定什么时候停止是很有用的，因为我们期望此时验证误差开始增长．
一个更明显的正则化方法是 <strong>权重衰减 (weight
decay)</strong>，类似用于线性模型的岭回归</li>
<li>输入的缩放
因为对输入的缩放决定了在底层中系数缩放的效率，所有它可以对最终解有很大的影响．最开始最好是对所有输入进行标准化使均值为
0，标准差为
1．这保证了在正则化过程中对所有输入公平对待，而且允许为随机的初始权重系数选择一个有意义的区间．有了标准化的输入，一般在
<span class="math inline">\([-0.7,+0.7]\)</span>
范围内均匀随机选择权重系数．</li>
<li>隐藏单元和层的个数
一般来说，太多的隐藏单元比太少的隐藏单元要好．太少的隐藏单元，模型或许没有足够的灵活性来捕捉数据的非线性；太多的隐藏单元，如果使用了合适的正则化，额外的权重系数可以收缩到
0．一般地，隐藏单元的数量处于 5 到 100
的范围之内，而且随着输入个数、训练情形的种数的增加而增加．最常见的是放入相当大数量的单元并且进行正则化训练．一些研究者采用交叉验证来估计最优的数量，但是当交叉验证用来估计正则化系数这似乎是不必要的．<strong>隐藏层的选择由背景知识和经验来指导</strong>．每一层提取输入的特征用于回归或者分类．多重隐藏层的使用允许在不同的分解层次上构造层次特征．</li>
<li>多重最小点 误差函数 <span class="math inline">\(R(\theta)\)</span>
为非凸，具有许多局部最小点．后果是最终得到的解取决于权重系数的初始值．至少需要尝试一系列随机的初始配置，然后选择给出最低（惩罚）误差的解．或许更好的方式是在对一系列网络的预测值进行平均作为最终的预测（Ripley，1996[^1]）．这比平均权重系数更好，因为模型的非线性表明平均的解会很差．另一种方式是通过
bagging，它是对不同网络的预测值进行平均，这些网络是对随机扰动版本的训练数据进行训练得到的．</li>
</ul>
<h2 id="第十二章-支持向量机与灵活判别方法">第十二章
<strong>支持向量机与灵活判别方法</strong></h2>
<p><strong>核函数</strong> 所以 公式（12.19） 和 公式（12.20）
仅仅通过内积涉及 <span
class="math inline">\(h(x)\)</span>．实际上，我们根本不需要明确变换关系
<span
class="math inline">\(h(x)\)</span>，而仅仅要求知道在转换后的空间中计算内积的核函数
<span class="math display">\[
K(x,x&#39;)=\langle h(x), h(x&#39;) \rangle\tag{12.21}
\]</span></p>
<p>在 SVM 中有三种流行的 <span class="math inline">\(K\)</span>
可以选择</p>
<p><span class="math display">\[
\begin{array}{rl}
d\text{ 阶多项式：} &amp; K(x,x&#39;)=(1+\langle x,x&#39; \rangle)^d\\
\text{径向基：} &amp; K(x, x&#39;)=\exp(-\gamma \Vert
x-x&#39;\Vert^2)\tag{12.22}\label{12.22}\\
\text{神经网络：} &amp; K(x,x&#39;)=\tanh(\kappa_1\langle x,x&#39;
\rangle+\kappa_2)\\
\end{array}
\]</span></p>
<p>考虑含有两个输入变量 <span class="math inline">\(X_1\)</span> 和
<span class="math inline">\(X_2\)</span> 的特征空间，以及 2
阶的多项式核．则</p>
<p><span class="math display">\[
\begin{array}{ll}
K(x,x&#39;)&amp;=(1+\langle X,X&#39; \rangle)^2\\
&amp;=(1+X_1X_1&#39;+X_2X_2&#39;)^2\\
&amp;=1+2X_1X_1&#39;+2X_2X_2&#39;+(X_1X_1&#39;)^2+(X_2X_2&#39;)^2+2X_1X_1&#39;X_2X_2&#39;\tag{12.23}\label{12.23}
\end{array}
\]</span></p>
<p>则 <span class="math inline">\(M=6\)</span>，而且如果我们选择 <span
class="math inline">\(h_1(X)=1,h_2(X)=\sqrt{2}X_1,h_3(X)=\sqrt{2}X_2,h_4(X)=X_1^2,h_5(X)=X_2^2\)</span>，以及
<span class="math inline">\(h_6(X)=\sqrt{2}X_1X_2\)</span>，则<span
class="math inline">\(K(X,X&#39;)=\langle
h(X),h(X&#39;)\rangle\)</span>．</p>
<h2 id="第十三章-原型方法与最近邻算法">第十三章
<strong>原型方法与最近邻算法</strong></h2>
<p><strong>不变量和切线距离</strong>
对于每张图像，我们画出了该图像旋转版本的曲线，称为 <strong>不变流形
(invariance
manifolds)</strong>．现在，不是用传统的欧氏距离，而是采用两条曲线间的最短距离．换句话说，两张图像间的距离取为第一张图像的任意旋转版本与第二张图像的任意旋转版本间的最短欧氏距离．这个距离称为
<strong>不变度量 (invariant metric)</strong>．</p>
<p>原则上，可以采用这种不变度量来进行 1
最近邻分类．然而这里有两个问题．第一，对于真实图像很难进行计算．第二，允许大的变换，可能效果很差．举个例子，经过
180° 旋转后，“6” 可能看成是
“9”．<strong>我们需要限制为微小旋转</strong>．</p>
<p><strong>切线距离 (tangent distance)</strong> 解决了这两个问题．如图
13.10
所示，我们可以用图像“3”在其原图像的切线来近似不变流形．这个切线可以通过从图像的微小旋转中来估计方向向量，或者通过更复杂的空间光滑方法（<a
target="_blank" rel="noopener" href="https://github.com/szcf-weiya/ESL-CN/issues/187">练习
13.4</a>）．对于较大的旋转，切线图像不再像“3”，所以大程度的旋转问题可以减轻．</p>
<p>想法是对每个训练图像计算不变切线．对于待分类的 <strong>查询图像
(query
image)</strong>，计算其不变切线，并且在训练集的直线中寻找最近的直线．对应最近的直线的类别（数字）是对查询图像类别的预测值．在图
13.11 中，两条切向直线相交，但也只是因为我们是在二维空间中表示 256
维的情形．在 <span class="math inline">\(\mathbb R^{256}\)</span>
中，两条这样的直线相交的概率是 0．</p>
<p><strong>自适应最近邻方法</strong>
最近邻分类的隐含假设是类别概率在邻域内近似为常值，因此简单的平均会得到不错的估计．然而，在这个例子中，只有水平方向上的类别概率会变化．如果我们提前知道这一点，可以将邻居拉伸为长方形区域．这会降低估计的偏差，同时保持方差不变．</p>
<p>一般地，这要求最近邻分类中采用自适应的度量，使得得到的邻域沿着类别不会改变太多的方向上拉伸．在高维特征空间中，类别概率可能仅仅只在一个低维的子空间中有所改变，因此自适应度量是很重要的优点．</p>
<p><strong>判别自适应最近邻 (DANN)</strong>
方法进行了局部维度降低——也就是，在每个查询点单独降低维度．提出通过逐步剔除包含训练数据的盒子的边来自动寻找长方形邻域．这里我们介绍
Hastie and Tibshirani (1996a)[^2] 提出的 <strong>判别自适应最近邻
(discriminant adaptive nearest-neighbor)
(DANN)</strong>．在每个查询点，构造其大小为 50
个点的邻域，并且用这些点的类别分布来决定怎么对邻域进行变形——也就是，对度量进行更新．接着更新后的度量用在该查询点的最近邻规则中．因此每一个查询点都可能采用不同的度量．很明显邻域应当沿着垂直类别重心连线的方向拉伸．这个方向也与线性判别边界重合，而且是类别概率改变最少的方向．一般地，类别概率变化最大的方向不会与类别重心连线垂直</p>
<p><strong>计算上的考虑</strong></p>
<p>最近邻分类规则的一个缺点通常是它的计算负荷量，无论是寻找最近邻还是存储整个训练集合．</p>
<h2 id="第十四章-无监督学习">第十四章 <strong>无监督学习</strong></h2>
<p>流形（manifold）：数学上，流形是一个拓扑空间，在每一点附近局部地近似欧式空间．更精确地，<span
class="math inline">\(n\)</span> 维流形的每个点与维度为 <span
class="math inline">\(n\)</span> 的欧式空间同态的邻域．
监督学习中，有一个明确的成功或不成功的量度，因此可用于判断特定情况下的<strong>充分性
(adequacy)</strong>，并比较不同方法在各种情况下的<strong>有效性
(effectiveness)</strong>．成功的损失直接用在联合分布 <span
class="math inline">\(\Pr(X,Y)\)</span>
上的期望损失来衡量．这个可以用各种方式来衡量，包括交叉验证．在非监督学习中，没有这些直接衡量成功的量度．从大部分非监督学习的算法的输出中评估推断的有效性是很难确定的．必须诉诸于<strong>启发式变量
(heuristic arguments)</strong>，在监督学习也经常使用，这不仅可以激励
(motivating)
算法，而且为了评价结果的质量．因为有效性是主观问题，不能直接加以证实，这种不舒服
(unconfortable) 的情形导致提出的方法激增，</p>
<p><strong>关联规则分析 (Association rule analysis)</strong>
已经成为挖掘贸易数据的流行工具．目标是寻找变量 <span
class="math inline">\(X=(X_1,X_2,\ldots,X_p)\)</span>
在数据中出现最频繁的联合值．在二值数据 <span
class="math inline">\(X_j\in\{0,1\}\)</span>
中应用最多，也称作“市场篮子”分析．这种情形下观测值为销售交易，比如出现在商店收银台的商品．变量表示所有在商店中出售的商品．对于观测
<span class="math inline">\(i\)</span>，每个变量 <span
class="math inline">\(X_j\)</span> 取值为 0 或 1；如果第 <span
class="math inline">\(j\)</span> 个商品作为该次交易购买的一部分则 <span
class="math inline">\(x_{ij}=1\)</span>，而如果没有购买则 <span
class="math inline">\(x_{ij}=0\)</span>．这些经常有联合值的变量表示物品经常被一起购买．这个信息对于货架、跨营销的促销活动、商品目录的设计，以及基于购买模式的消费者划分都是很有用的．
关联规则成为了在相关的市场篮子的设定下用于分析非常大的交易数据库的流行工具．这是当数据可以转换成多维邻接表的形式时．输出是以容易理解并且可解释的关联规则
公式（14.4）的形式展现的．Apriori
算法允许分析可以用到大的数据库中，更大的数据库适用于其他类型的分析．关联规则是数据挖掘最大的成功之一．</p>
<p><strong>聚类分析</strong></p>
<p>聚类分析的所有目标的核心是度量要聚类的单个点间相似（或不相似）的程度．聚类方法试图基于点间相似性的定义来将其分类．相似性的定义只能从关注的主题得到．某种程度上，这个情形与确定预测问题（监督学习）中的损失或花费函数相似．在预测问题中，损失函数与错误的预测有关，而错误的预测取决于数据之外的考虑．
简而言之，聚类方法中相似性的定义就如同监督学习问题中损失函数一样重要.
<strong>确定一个合适的不相似性的度量远比选择聚类算法来得重要</strong>．(涉及领域知识。)</p>
<p><strong>主成分</strong>，<strong>主曲线和主曲面</strong></p>
<p><strong>流形学习</strong>（manifold learning）</p>
<p>是机器学习、模式识别中的一种方法，在维数约简方面具有广泛的应用。它的主要思想是将高维的数据映射到低维，使该低维的数据能够反映原高维数据的某些本质结构特征。流形学习的前提是有一种假设，即某些高维数据，实际是一种低维的流形结构嵌入在高维空间中。流形学习的目的是将其映射回低维空间中，揭示其本质。</p>
<blockquote>
<p>个人注：关于流行学习知乎上一篇通熟易懂的文章
https://www.zhihu.com/question/24015486/answer/26524937
，最常用的例子就是瑞士卷</p>
</blockquote>
<p><strong>投影是从一个向量空间到其自身的线性变换</strong>，并且投影矩阵满足<span
class="math inline">\(\mathbf P^2=\mathbf P\)</span>．</p>
<blockquote>
<p>个人注：应该是到子空间吧。<strong>不是所有从一个向量空间到自身的线性变换都是投影</strong>，但<strong>所有投影都是线性变换</strong>，而且满足
<span class="math inline">\(P^2 = P\)</span>（即投影两次不变）。</p>
</blockquote>
<p><strong>主成分可以看成是主曲线的特殊情形</strong>。</p>
<h2 id="第十五章-随机森林">第十五章 <strong>随机森林</strong></h2>
<blockquote>
<p>在每次分割时，随机选择 <span class="math inline">\(m\le p\)</span>
个输入变量作为候选变量用来分割</p>
</blockquote>
<p>一般地，<span class="math inline">\(m\)</span> 取为 <span
class="math inline">\(\sqrt{p}\)</span>，或者甚至小到取 1．</p>
<p>Bagging 可以看成是特殊的随机森林，即 <span
class="math inline">\(m=p\)</span> 的随机森林．</p>
<p>另外，发明者给出下面两条推荐：</p>
<ul>
<li>对于分类，<span class="math inline">\(m\)</span> 的默认值为 <span
class="math inline">\(\lfloor \sqrt p \rfloor\)</span>，且最小的结点数为
1．</li>
<li>对于回归，<span class="math inline">\(m\)</span> 的默认值为 <span
class="math inline">\(\lfloor p/3\rfloor\)</span>，且最小的结点数为
5．</li>
</ul>
<p>实际中这些参数的最优值取决于具体问题，并且它们应当被视为
<strong>调整参数 (tunning parameters)</strong>．在图 15.3 中，<span
class="math inline">\(m=6\)</span> 比默认值 <span
class="math inline">\(\lfloor 8/3\rfloor =2\)</span> 更好．</p>
<h2 id="第十六章-集成学习">第十六章 <strong>集成学习</strong></h2>
<p>"Bet on Sparsity" 原则 <span class="math inline">\(L_1\)</span>
的收缩能更好地适应稀疏的情形（在所有可能选择中，非零系数的基函数的个数很少）．
当拟合系数时，我们应该使用 <span class="math inline">\(L_2\)</span>
惩罚，而不是 <span class="math inline">\(L_1\)</span>
惩罚．另一方面，如果这里只有少量的（比如，<span
class="math inline">\(1000\)</span>）系数非零，则 lasso （<span
class="math inline">\(L_1\)</span> 惩罚）会表现得很好．我们将这个看成是
<strong>稀疏 (sparse)</strong> 的情形，而第一种情形（高斯系数）是
<strong>稠密 (dense)</strong> 的．注意到尽管在稠密情形下，<span
class="math inline">\(L_2\)</span>
惩罚是最好的，但没有方法能做得很好，因为数据太少，但却要从中估计大量的非零系数．这是维数的灾难造成的损失．稀疏设定中，我们可以用
<span class="math inline">\(L_1\)</span>
惩罚做得很好，因为非零稀疏的个数很少．但 <span
class="math inline">\(L_2\)</span> 惩罚便不行．</p>
<p>换句话说，<span class="math inline">\(L_1\)</span> 惩罚的使用遵循称作
“bet on sparsity” 的这一高维问题的准则：</p>
<blockquote>
<p>采用在稀疏问题中表现得好的方法，因为没有方法能在稠密问题中表现得好．</p>
</blockquote>
<h2 id="第十七章-无向图模型">第十七章 <strong>无向图模型</strong></h2>
<p><strong>图 (Graph)</strong>
由顶点（结点）集，以及连接顶点对的边集构成．在图模型中，每个顶点表示一个随机变量，并且图给出了一种理解全体随机变量联合分布的可视化方式．对于监督学习和非监督学习它们都是很有用的．在
<strong>无向图 (undirected graph)</strong>
中，边是没有方向的．我们仅限于讨论无向图模型，也称作
<strong>马尔科夫随机域 (Markov random fields)</strong> 或者
<strong>马尔科夫网络 (Markov networks)</strong>． -
在这些图中，两个顶点间缺失一条边有着特殊的含义：对应的随机变量在给定其它变量下是条件独立的．</p>
<p>图中的边用值 (value) 或者 <strong>势 (potential)</strong>
参量化，来表示在对应顶点上的随机变量间条件依赖性的强度大小．采用图模型的主要挑战是模型选择（选择图的结构）、根据数据来估计边的参数，并且从联合分布中计算边缘顶点的概率和期望．后两个任务在计算机科学中有时被称作
<strong>学习 (learning)</strong> 和
<strong>推断(inference)</strong>．</p>
<p>关于 <strong>有向图 (directed graphical models)</strong> 或者
<strong>贝叶斯网络 (Bayesian networks)</strong>
有大量并且活跃的文献；这是边有方向箭头（但是没有有向环）的图模型．有向图模型表示可以分解成条件分布乘积的概率分布，并且有解释因果关系的潜力．</p>
<p>三种等价的 Markov 性质" pairwise Markov properties:
寻找缺失边，在给定其他结点的情况下，缺失边的两个顶点相互独立； global
Markov properties:
寻找分离集，在给定分离集的情况下，被分离的子图相互独立；</p>
<h3 id="连续变量的无向图模型">连续变量的无向图模型</h3>
<p>这里我们考虑所有变量都是连续变量的马尔科夫网络．这样的图模型几乎总是用到高斯分布，因为它有方便的分析性质．我们假设观测值服从均值为
<span class="math inline">\(\mu\)</span>，协方差为 <span
class="math inline">\(\mathbf \Sigma\)</span>
的多元高斯分布．因为高斯分布至多表示二阶的关系，所以它自动地编码了一个成对马尔科夫图．</p>
<p>!!! note "weiya 注："
因为在高斯分布的密度函数中，指数项中关于随机变量的阶数最多是二次，所以说它至多能表示二阶的关系．</p>
<ul>
<li>高斯分布有个性质是所有的条件分布也是高斯分布． 协方差矩阵的逆 <span
class="math inline">\(\mathbf\Sigma^{-1}\)</span> 包含变量之间的
<strong>偏协方差 (partial covariances)</strong>
信息；也就是，在给定其它变量的条件下，<span
class="math inline">\(i\)</span> 与 <span
class="math inline">\(j\)</span> 的协方差．特别地，如果 <span
class="math inline">\(\mathbf {\Theta=\Sigma^{-1}}\)</span> 的第 <span
class="math inline">\(ij\)</span> 个元素为 0，则变量 <span
class="math inline">\(i\)</span> 和 <span
class="math inline">\(j\)</span> 在给定其它变量情况下是条件独立的．</li>
</ul>
<h3 id="图结构的估计">图结构的估计</h3>
<p>大多数情况下，我们不知道哪些边要从图中去掉，因此想试图从数据本身找出．最近几年很多作者提出用于这个目的的
<span class="math inline">\(L_1\)</span> (lasso) 正则化．</p>
<p>!!! note "weiya 注：" 省略图中的边，有点类似于做变量选择，而 lasso
正是应对变量选择的“绝世武功”!:joy:</p>
<p>为了实现这点，它们将每个变量看成响应变量而其它的变量作为预测变量进行拟合
lasso 回归</p>
<h3 id="限制玻尔兹曼机">限制玻尔兹曼机</h3>
<p>离散变量的无向马尔科夫网络是很流行的，而且特别地，二值变量的成对马尔科夫网络更普遍．在统计力学领域有时称为
Ising 模型，在机器学习领域称为 <strong>玻尔兹曼机 (Boltzmann
machines)</strong>，其中顶点称为“<strong>结点
(nodes)</strong>”或“<strong>单元 (units)</strong>”，取值为 0 或 1.</p>
<p>这节我们考虑受神经网络影响的一种特殊的图模型结构，该结构中，单元是按层进行组织的．<strong>限制玻尔兹曼机
(RBM)</strong>
包含一层可见单元和一层隐藏单元，单层之间没有联系．如果隐藏单元的连接被移除掉，计算条件期望变得很简单</p>
<blockquote>
<p>个人注：<strong>虽然标准 RBM
使用二值神经元，但也存在许多变体，可以处理不同类型的输入：</strong></p>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 66%" />
</colgroup>
<thead>
<tr class="header">
<th>变体类型</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Gaussian-Bernoulli RBM</strong></td>
<td>可见层为连续实数（高斯分布），隐藏层仍是二值</td>
</tr>
<tr class="even">
<td><strong>Gaussian-Gaussian RBM</strong></td>
<td>可见层和隐藏层都为连续变量</td>
</tr>
<tr class="odd">
<td><strong>Softmax RBM</strong></td>
<td>可见层或隐藏层是 one-hot 多类别状态</td>
</tr>
<tr class="even">
<td><strong>ReLU RBM</strong></td>
<td>使用 ReLU 激活而非二值状态，用于更复杂的连续特征建模</td>
</tr>
</tbody>
</table>
<p>这些变体适用于图像、音频、自然语言处理等不同类型的数据。</p>
</blockquote>
<h2 id="第十八章-高维问题">第十八章 高维问题</h2>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://hongyitong.github.io/2024/03/03/%E3%80%8A%E9%9A%8F%E6%9C%BA%E6%BC%AB%E6%AD%A5%E7%9A%84%E5%82%BB%E7%93%9C%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Rayman.hung">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="墨语浮生">
      <meta itemprop="description" content="技术分享、读书心得、心情记录">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 墨语浮生">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/03/03/%E3%80%8A%E9%9A%8F%E6%9C%BA%E6%BC%AB%E6%AD%A5%E7%9A%84%E5%82%BB%E7%93%9C%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/" class="post-title-link" itemprop="url">《随机漫步的傻瓜》读书心得</a>
        </h2>

        </h2>
          
        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-03-03 00:00:00" itemprop="dateCreated datePublished" datetime="2024-03-03T00:00:00+08:00">2024-03-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97%EF%BC%88%E7%A4%BE%E4%BC%9A%E7%A7%91%E5%AD%A6%EF%BC%89/" itemprop="url" rel="index"><span itemprop="name">读书心得（社会科学）</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>之前买错了《漫步华尔街》，现在补买回来这本书。本书有点哲学书的味道，讲了很多的人生，可以称之为轻哲学。</p>
<p><strong>本书的核心思想是概率论，而概率论的核心是随机性、非对称、非线形，在生活和社会经济领域皆如此。</strong></p>
<p><strong>点题：</strong> <img
src="/img2/随机漫步的傻瓜/随机漫步的傻瓜40.pic.jpg" /> <img
src="/img2/随机漫步的傻瓜/随机漫步的傻瓜41.pic.jpg" /> <img
src="/img2/随机漫步的傻瓜/随机漫步的傻瓜42.pic.jpg" /></p>
<p><strong>学习方法</strong></p>
<ul>
<li>阅读前人的事迹，向过去学习；</li>
<li>利用蒙特卡罗随机序列，向未来学习。</li>
</ul>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2024/03/03/%E3%80%8A%E9%9A%8F%E6%9C%BA%E6%BC%AB%E6%AD%A5%E7%9A%84%E5%82%BB%E7%93%9C%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://hongyitong.github.io/2023/12/29/%E3%80%8A%E6%BC%AB%E6%AD%A5%E5%8D%8E%E5%B0%94%E8%A1%97%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Rayman.hung">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="墨语浮生">
      <meta itemprop="description" content="技术分享、读书心得、心情记录">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 墨语浮生">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/12/29/%E3%80%8A%E6%BC%AB%E6%AD%A5%E5%8D%8E%E5%B0%94%E8%A1%97%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/" class="post-title-link" itemprop="url">《漫步华尔街》读书心得</a>
        </h2>

        </h2>
          
        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-12-29 00:00:00" itemprop="dateCreated datePublished" datetime="2023-12-29T00:00:00+08:00">2023-12-29</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97%EF%BC%88%E7%A4%BE%E4%BC%9A%E7%A7%91%E5%AD%A6%EF%BC%89/" itemprop="url" rel="index"><span itemprop="name">读书心得（社会科学）</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>本来是要买塔勒布的《随机漫步的傻瓜》，错买成了这本书。不过作为一部不错的股市、债券、期货、期权入门的书倒还不错。</p>
<h1 id="主要思想">主要思想</h1>
<p><strong>一定要明白，所谓原则、道理其实都是统计学意义上的合理性，真正的个体并不一定。</strong></p>
<ul>
<li>随机漫步（Random walk）<br />
</li>
<li>均值回归<br />
金融万有引力定律<br />
</li>
<li>风险管理<br />
不同年龄的风险承受能力不同，适配不同的投资组合。<br />
杠杆的风险<br />
</li>
<li>指数投资<br />
作者重点推荐的投资思想，其实就是个人不可能持续战胜市场的明智选择。</li>
</ul>
<h1 id="价格-or-价值">价格 or 价值</h1>
<p>股票无非是关注价值还是价格，而价值又可以细分为关注当前价值还是未来价值（增长、成长性），价格关注的是人的心理（普通大众的心理预期），关键是否有人愿意出更高的价格。价值的难点在于需要很高的专业知识和洞察力。</p>
<h1 id="泡沫-萧条">泡沫 萧条</h1>
<p>经济发展-&gt;民众手中有钱-&gt;投资渠道少-&gt;从众-&gt;泡沫形成<br />
恐慌情绪就像病毒，或者说开闸的洪水，很难软着陆。<br />
就像气球，吹大的时候有个过程，当泄气是一瞬间的事情。<strong>上升像爬楼梯，下降像坐电梯。</strong><br />
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2023/12/29/%E3%80%8A%E6%BC%AB%E6%AD%A5%E5%8D%8E%E5%B0%94%E8%A1%97%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://hongyitong.github.io/2023/12/20/%E3%80%8A%E6%8A%95%E8%B5%84%E6%9C%80%E9%87%8D%E8%A6%81%E7%9A%84%E4%BA%8B%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Rayman.hung">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="墨语浮生">
      <meta itemprop="description" content="技术分享、读书心得、心情记录">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 墨语浮生">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/12/20/%E3%80%8A%E6%8A%95%E8%B5%84%E6%9C%80%E9%87%8D%E8%A6%81%E7%9A%84%E4%BA%8B%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/" class="post-title-link" itemprop="url">《投资最重要的事》读书心得</a>
        </h2>

        </h2>
          
        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-12-20 00:00:00" itemprop="dateCreated datePublished" datetime="2023-12-20T00:00:00+08:00">2023-12-20</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97%EF%BC%88%E7%A4%BE%E4%BC%9A%E7%A7%91%E5%AD%A6%EF%BC%89/" itemprop="url" rel="index"><span itemprop="name">读书心得（社会科学）</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="如何才能战胜市场">如何才能战胜市场？</h1>
<ol type="1">
<li>稳妥的思考框架。<br />
原则，方法论。<br />
</li>
<li>情绪控制<br />
必须自己做到。压力应对。</li>
</ol>
<p>面对未来的时机选择，风险管理。</p>
<p>巴菲特：价值投资，在别人恐惧时贪婪，在别人贪婪时恐惧。</p>
<h1 id="降龙十八掌">降龙十八掌</h1>
<p>投资需要平衡很多基本问题，面面俱到，又有机统一。</p>
<h2 id="学习第二层次思维">学习第二层次思维</h2>
<ul>
<li>环境的不可控+人心里的波动</li>
<li>第二层次思维<br />
<strong>在大众思维的基础上进行思考，反过来想。</strong><br />
<img src="/img2/投资最重要的事/投资者最重要的事6.jpg" /><br />
</li>
<li><strong>第二层思维需要考虑的东西</strong><br />
<img src="/img2/投资最重要的事/投资者最重要的事7.jpg" /></li>
</ul>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2023/12/20/%E3%80%8A%E6%8A%95%E8%B5%84%E6%9C%80%E9%87%8D%E8%A6%81%E7%9A%84%E4%BA%8B%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://hongyitong.github.io/2023/12/17/%E3%80%8A%E5%87%A0%E4%BD%95%E5%AD%A6%E7%9A%84%E5%8A%9B%E9%87%8F%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Rayman.hung">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="墨语浮生">
      <meta itemprop="description" content="技术分享、读书心得、心情记录">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 墨语浮生">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/12/17/%E3%80%8A%E5%87%A0%E4%BD%95%E5%AD%A6%E7%9A%84%E5%8A%9B%E9%87%8F%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/" class="post-title-link" itemprop="url">《几何学的力量》读书心得</a>
        </h2>

        </h2>
          
        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-12-17 00:00:00" itemprop="dateCreated datePublished" datetime="2023-12-17T00:00:00+08:00">2023-12-17</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97%EF%BC%88%E8%87%AA%E7%84%B6%E7%A7%91%E5%AD%A6%EF%BC%89/" itemprop="url" rel="index"><span itemprop="name">读书心得（自然科学）</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>几何学最早是丈量世界而诞生，从而抽象了现实的世界。<br />
一个人想要谈论某个事物，前提是他必须弄清楚该事物的定义。定义和概念是很重要的，于数学及现实皆然。<br />
学几何学的是其中的逻辑思想，特别是欧几里得几何的研究问题的方法。整个体系的建设，从几个不证自明的公理出发，通过推理、演绎，建立相关的定理、命题，搭建了整个壮丽的几何学大厦。基于不同的公理可以构建不同的几何体系，例如欧氏几何和非欧几何。<br />
数学有很大一部分作用在于，厘清哪些事物是我们暂时或者永远都不用关心的，这种选择性注意是人类理性的基本组成部分。这其实也是抽象的过程，抽象就是省略细节，关注你研究重点的过程，例如算术关注的是数量，而事物的本身是什么并不重要；而拓扑学（庞加莱创立，研究洞的问题）关注的是形状，大小、距离反而不关心；而物理关心的是位置和速度；欧氏几何研究的是长度、面积等不变量。</p>
<h1 id="给不同的事物赋予相同的名称">给不同的事物赋予相同的名称</h1>
<p>其实就是考虑研究的特征，把形同特征的事物归类为同一事物，也是一种抽象的过程。<br />
庞加莱：数学是一门给不同事物赋予相同的名称的艺术。<br />
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2023/12/17/%E3%80%8A%E5%87%A0%E4%BD%95%E5%AD%A6%E7%9A%84%E5%8A%9B%E9%87%8F%E3%80%8B%E8%AF%BB%E4%B9%A6%E5%BF%83%E5%BE%97/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/17/">17</a><a class="extend next" rel="next" href="/page/4/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Rayman.hung</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
<script src="https://cdn.jsdelivr.net/npm/hexo-generator-searchdb@1.4.0/dist/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js","integrity":"sha256-r+3itOMtGGjap0x+10hu6jW/gZCzxHsoKrOd7gyRSGY="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
